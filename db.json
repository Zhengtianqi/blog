{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-3-hexo/source/img/alipay.jpg","path":"img/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/article-list-background.jpeg","path":"img/article-list-background.jpeg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/avatar.jpg","path":"img/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/brown-papersq.png","path":"img/brown-papersq.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/gov.png","path":"img/gov.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/school-book.png","path":"img/school-book.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/img/weixin.jpg","path":"img/weixin.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/gitalk.css","path":"css/gitalk.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/mobile.styl","path":"css/mobile.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/gitalk.js","path":"js/gitalk.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/gitment.js","path":"js/gitment.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/iconfont.js","path":"js/iconfont.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/jquery.pjax.js","path":"js/jquery.pjax.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/js/titleTip.js","path":"js/titleTip.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.eot","path":"css/fonts/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.svg","path":"css/fonts/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.ttf","path":"css/fonts/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.eot","path":"css/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.woff","path":"css/fonts/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.svg","path":"css/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.ttf","path":"css/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.woff","path":"css/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.woff2","path":"css/fonts/iconfont.woff2","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/selection.json","path":"css/fonts/selection.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/atom-dark.styl","path":"css/hl_theme/atom-dark.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/atom-light.styl","path":"css/hl_theme/atom-light.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/brown-paper.styl","path":"css/hl_theme/brown-paper.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/darcula.styl","path":"css/hl_theme/darcula.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/github-gist.styl","path":"css/hl_theme/github-gist.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/github.styl","path":"css/hl_theme/github.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/gruvbox-dark.styl","path":"css/hl_theme/gruvbox-dark.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/gruvbox-light.styl","path":"css/hl_theme/gruvbox-light.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/kimbie-dark.styl","path":"css/hl_theme/kimbie-dark.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/kimbie-light.styl","path":"css/hl_theme/kimbie-light.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/railscasts.styl","path":"css/hl_theme/railscasts.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/rainbow.styl","path":"css/hl_theme/rainbow.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/sublime.styl","path":"css/hl_theme/sublime.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/school-book.styl","path":"css/hl_theme/school-book.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/sunburst.styl","path":"css/hl_theme/sunburst.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/zenbum.styl","path":"css/hl_theme/zenbum.styl","modified":1,"renderable":1},{"_id":"source/img/108cd6b414ab2dbb30126c0fb0700e23.png","path":"img/108cd6b414ab2dbb30126c0fb0700e23.png","modified":1,"renderable":0},{"_id":"source/img/1112dfc7b11a214351a67103cd58a5c8.png","path":"img/1112dfc7b11a214351a67103cd58a5c8.png","modified":1,"renderable":0},{"_id":"source/img/1337b059.png","path":"img/1337b059.png","modified":1,"renderable":0},{"_id":"source/img/145d5664.png","path":"img/145d5664.png","modified":1,"renderable":0},{"_id":"source/img/1618244073306.png","path":"img/1618244073306.png","modified":1,"renderable":0},{"_id":"source/img/157e6450653a0fc457342b5794658fc9.png","path":"img/157e6450653a0fc457342b5794658fc9.png","modified":1,"renderable":0},{"_id":"source/img/1618244110010.png","path":"img/1618244110010.png","modified":1,"renderable":0},{"_id":"source/img/1618244002479.png","path":"img/1618244002479.png","modified":1,"renderable":0},{"_id":"source/img/1618244137853.png","path":"img/1618244137853.png","modified":1,"renderable":0},{"_id":"source/img/1618244166891.png","path":"img/1618244166891.png","modified":1,"renderable":0},{"_id":"source/img/1618244251013.png","path":"img/1618244251013.png","modified":1,"renderable":0},{"_id":"source/img/1618244289269.png","path":"img/1618244289269.png","modified":1,"renderable":0},{"_id":"source/img/1618244193984.png","path":"img/1618244193984.png","modified":1,"renderable":0},{"_id":"source/img/1618244361109.png","path":"img/1618244361109.png","modified":1,"renderable":0},{"_id":"source/img/1618244387386.png","path":"img/1618244387386.png","modified":1,"renderable":0},{"_id":"source/img/1618244497271.png","path":"img/1618244497271.png","modified":1,"renderable":0},{"_id":"source/img/1618244544994.png","path":"img/1618244544994.png","modified":1,"renderable":0},{"_id":"source/img/1618244522945.png","path":"img/1618244522945.png","modified":1,"renderable":0},{"_id":"source/img/1618244592782.png","path":"img/1618244592782.png","modified":1,"renderable":0},{"_id":"source/img/1618244566899.png","path":"img/1618244566899.png","modified":1,"renderable":0},{"_id":"source/img/1618244623527.png","path":"img/1618244623527.png","modified":1,"renderable":0},{"_id":"source/img/1618295759372.png","path":"img/1618295759372.png","modified":1,"renderable":0},{"_id":"source/img/1618675966507.png","path":"img/1618675966507.png","modified":1,"renderable":0},{"_id":"source/img/1618676025236.png","path":"img/1618676025236.png","modified":1,"renderable":0},{"_id":"source/img/1618675987731.png","path":"img/1618675987731.png","modified":1,"renderable":0},{"_id":"source/img/1618676190337.png","path":"img/1618676190337.png","modified":1,"renderable":0},{"_id":"source/img/1618676232016.png","path":"img/1618676232016.png","modified":1,"renderable":0},{"_id":"source/img/1618676280257.png","path":"img/1618676280257.png","modified":1,"renderable":0},{"_id":"source/img/1618676358941.png","path":"img/1618676358941.png","modified":1,"renderable":0},{"_id":"source/img/1618676411155.png","path":"img/1618676411155.png","modified":1,"renderable":0},{"_id":"source/img/1618676461026.png","path":"img/1618676461026.png","modified":1,"renderable":0},{"_id":"source/img/1618676506490.png","path":"img/1618676506490.png","modified":1,"renderable":0},{"_id":"source/img/1618676526483.png","path":"img/1618676526483.png","modified":1,"renderable":0},{"_id":"source/img/1618677203153.png","path":"img/1618677203153.png","modified":1,"renderable":0},{"_id":"source/img/227d0458.png","path":"img/227d0458.png","modified":1,"renderable":0},{"_id":"source/img/2b97031c92882db452d126852b74cfb2.png","path":"img/2b97031c92882db452d126852b74cfb2.png","modified":1,"renderable":0},{"_id":"source/img/2cfc91e2dc6104321827e210ccfc595c.png","path":"img/2cfc91e2dc6104321827e210ccfc595c.png","modified":1,"renderable":0},{"_id":"source/img/41d2985c3545a456341af1e4f5b8231a.png","path":"img/41d2985c3545a456341af1e4f5b8231a.png","modified":1,"renderable":0},{"_id":"source/img/480c47d9.png","path":"img/480c47d9.png","modified":1,"renderable":0},{"_id":"source/img/4e12cc06.png","path":"img/4e12cc06.png","modified":1,"renderable":0},{"_id":"source/img/57c9d55222f65769c0ab0844bc5b6432.png","path":"img/57c9d55222f65769c0ab0844bc5b6432.png","modified":1,"renderable":0},{"_id":"source/img/591a3d39db7558c3fd1db79821abec5e.png","path":"img/591a3d39db7558c3fd1db79821abec5e.png","modified":1,"renderable":0},{"_id":"source/img/5a377b3b.png","path":"img/5a377b3b.png","modified":1,"renderable":0},{"_id":"source/img/5a824e2f.png","path":"img/5a824e2f.png","modified":1,"renderable":0},{"_id":"source/img/70db7f87.jpg","path":"img/70db7f87.jpg","modified":1,"renderable":0},{"_id":"source/img/743e6cec.jpg","path":"img/743e6cec.jpg","modified":1,"renderable":0},{"_id":"source/img/79c1c0a4746f50074f0e44491d3cb8b3.png","path":"img/79c1c0a4746f50074f0e44491d3cb8b3.png","modified":1,"renderable":0},{"_id":"source/img/7f3f75ca.png","path":"img/7f3f75ca.png","modified":1,"renderable":0},{"_id":"source/img/7fdd593ac3e140d5cd2e9328b51110c9.png","path":"img/7fdd593ac3e140d5cd2e9328b51110c9.png","modified":1,"renderable":0},{"_id":"source/img/879bfdfc7cab85a54bed5a33275c6dc0.png","path":"img/879bfdfc7cab85a54bed5a33275c6dc0.png","modified":1,"renderable":0},{"_id":"source/img/8ab72b98.png","path":"img/8ab72b98.png","modified":1,"renderable":0},{"_id":"source/img/8fb0cd0c.png","path":"img/8fb0cd0c.png","modified":1,"renderable":0},{"_id":"source/img/9b9139fd46f7419279ba6346b090ddc7.png","path":"img/9b9139fd46f7419279ba6346b090ddc7.png","modified":1,"renderable":0},{"_id":"source/img/9c9e497f4dc73587aa9d620a4149101c.png","path":"img/9c9e497f4dc73587aa9d620a4149101c.png","modified":1,"renderable":0},{"_id":"source/img/9cb319ab.png","path":"img/9cb319ab.png","modified":1,"renderable":0},{"_id":"source/img/DNS1.png","path":"img/DNS1.png","modified":1,"renderable":0},{"_id":"source/img/DNS2.png","path":"img/DNS2.png","modified":1,"renderable":0},{"_id":"source/img/GETPOST.png","path":"img/GETPOST.png","modified":1,"renderable":0},{"_id":"source/img/Git工作流程.png","path":"img/Git工作流程.png","modified":1,"renderable":0},{"_id":"source/img/HDFS-liucheng.png","path":"img/HDFS-liucheng.png","modified":1,"renderable":0},{"_id":"source/img/HDFS上传.png","path":"img/HDFS上传.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS1.png","path":"img/HTTPS1.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS2.png","path":"img/HTTPS2.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS3.png","path":"img/HTTPS3.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS4.png","path":"img/HTTPS4.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS5.png","path":"img/HTTPS5.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS6.png","path":"img/HTTPS6.png","modified":1,"renderable":0},{"_id":"source/img/Hive-运算2.png","path":"img/Hive-运算2.png","modified":1,"renderable":0},{"_id":"source/img/IO复用select模型.png","path":"img/IO复用select模型.png","modified":1,"renderable":0},{"_id":"source/img/JVMMemory.png","path":"img/JVMMemory.png","modified":1,"renderable":0},{"_id":"source/img/JVM类加载过程.png","path":"img/JVM类加载过程.png","modified":1,"renderable":0},{"_id":"source/img/QQ.jpg","path":"img/QQ.jpg","modified":1,"renderable":0},{"_id":"source/img/Socket通讯模型.png","path":"img/Socket通讯模型.png","modified":1,"renderable":0},{"_id":"source/img/Spark.png","path":"img/Spark.png","modified":1,"renderable":0},{"_id":"source/img/SpringBean3.png","path":"img/SpringBean3.png","modified":1,"renderable":0},{"_id":"source/img/StringBuilder.png","path":"img/StringBuilder.png","modified":1,"renderable":0},{"_id":"source/img/StringStringBuilderStringBuffer.png","path":"img/StringStringBuilderStringBuffer.png","modified":1,"renderable":0},{"_id":"source/img/StringUpdate.png","path":"img/StringUpdate.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP服务器接收请求.png","path":"img/TCPIP服务器接收请求.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP模型.png","path":"img/TCPIP模型.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP用户发送请求.png","path":"img/TCPIP用户发送请求.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程.png","path":"img/TCP协议通讯过程.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程1.png","path":"img/TCP协议通讯过程1.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程2.png","path":"img/TCP协议通讯过程2.png","modified":1,"renderable":0},{"_id":"source/img/ThreadLocal内部存储.png","path":"img/ThreadLocal内部存储.png","modified":1,"renderable":0},{"_id":"source/img/UDPHeader.png","path":"img/UDPHeader.png","modified":1,"renderable":0},{"_id":"source/img/UDPTCPcompare.png","path":"img/UDPTCPcompare.png","modified":1,"renderable":0},{"_id":"source/img/Wechat.jpg","path":"img/Wechat.jpg","modified":1,"renderable":0},{"_id":"source/img/Yarn.png","path":"img/Yarn.png","modified":1,"renderable":0},{"_id":"source/img/a4ef9088f117a8766ac65c4a1a630a34.png","path":"img/a4ef9088f117a8766ac65c4a1a630a34.png","modified":1,"renderable":0},{"_id":"source/img/a7ade6c9.png","path":"img/a7ade6c9.png","modified":1,"renderable":0},{"_id":"source/img/a7d8693f0dccfde8ecd0919cff94ff88.png","path":"img/a7d8693f0dccfde8ecd0919cff94ff88.png","modified":1,"renderable":0},{"_id":"source/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png","path":"img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime.png","path":"img/agent-costtime.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime2.png","path":"img/agent-costtime2.png","modified":1,"renderable":0},{"_id":"source/img/alipay.jpg","path":"img/alipay.jpg","modified":1,"renderable":0},{"_id":"source/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png","path":"img/b126a2a46dcd9ec3380c1b5bd4992bb3.png","modified":1,"renderable":0},{"_id":"source/img/b235f114.png","path":"img/b235f114.png","modified":1,"renderable":0},{"_id":"source/img/b4d68165.png","path":"img/b4d68165.png","modified":1,"renderable":0},{"_id":"source/img/b5a900fe460481976d0ae062ab18dd61.png","path":"img/b5a900fe460481976d0ae062ab18dd61.png","modified":1,"renderable":0},{"_id":"source/img/ce0bdff067bf19a083c5d26b1266bb44.png","path":"img/ce0bdff067bf19a083c5d26b1266bb44.png","modified":1,"renderable":0},{"_id":"source/img/chartype.png","path":"img/chartype.png","modified":1,"renderable":0},{"_id":"source/img/clip_image002.png","path":"img/clip_image002.png","modified":1,"renderable":0},{"_id":"source/img/clip_image004.png","path":"img/clip_image004.png","modified":1,"renderable":0},{"_id":"source/img/d133e6d9b982b320594ea62168dbe462.png","path":"img/d133e6d9b982b320594ea62168dbe462.png","modified":1,"renderable":0},{"_id":"source/img/d1a8d5d473881fffebc5baccd896258d.png","path":"img/d1a8d5d473881fffebc5baccd896258d.png","modified":1,"renderable":0},{"_id":"source/img/d234cadc40e81da676a8b541d9cb6ab4.png","path":"img/d234cadc40e81da676a8b541d9cb6ab4.png","modified":1,"renderable":0},{"_id":"source/img/d6f2bab64eddad3ee5d092672e8fac50.png","path":"img/d6f2bab64eddad3ee5d092672e8fac50.png","modified":1,"renderable":0},{"_id":"source/img/d325d29a.png","path":"img/d325d29a.png","modified":1,"renderable":0},{"_id":"source/img/d94cd34e.png","path":"img/d94cd34e.png","modified":1,"renderable":0},{"_id":"source/img/d9710d39e895a6eb8210645c6e0b85e3.png","path":"img/d9710d39e895a6eb8210645c6e0b85e3.png","modified":1,"renderable":0},{"_id":"source/img/data-collect-analysis.png","path":"img/data-collect-analysis.png","modified":1,"renderable":0},{"_id":"source/img/data-collect.png","path":"img/data-collect.png","modified":1,"renderable":0},{"_id":"source/img/dataSource-behaviour-relative.png","path":"img/dataSource-behaviour-relative.png","modified":1,"renderable":0},{"_id":"source/img/df56c6c619c3e1efb6f6140d56f67bcd.png","path":"img/df56c6c619c3e1efb6f6140d56f67bcd.png","modified":1,"renderable":0},{"_id":"source/img/e13f637bd584b1563442aeaece8cf1e4.png","path":"img/e13f637bd584b1563442aeaece8cf1e4.png","modified":1,"renderable":0},{"_id":"source/img/e18c9709.jpg","path":"img/e18c9709.jpg","modified":1,"renderable":0},{"_id":"source/img/ea179a09.png","path":"img/ea179a09.png","modified":1,"renderable":0},{"_id":"source/img/eac62f3b0d93e90dd8f19789d013c1ec.png","path":"img/eac62f3b0d93e90dd8f19789d013c1ec.png","modified":1,"renderable":0},{"_id":"source/img/edf97c87de27aae8adafce7f255f0870.png","path":"img/edf97c87de27aae8adafce7f255f0870.png","modified":1,"renderable":0},{"_id":"source/img/es1.png","path":"img/es1.png","modified":1,"renderable":0},{"_id":"source/img/es2.png","path":"img/es2.png","modified":1,"renderable":0},{"_id":"source/img/f785ade6f77521bce18fc9f8825d4d9b.png","path":"img/f785ade6f77521bce18fc9f8825d4d9b.png","modified":1,"renderable":0},{"_id":"source/img/f8f4bde6cb9aeac56366a2f3853f24f3.png","path":"img/f8f4bde6cb9aeac56366a2f3853f24f3.png","modified":1,"renderable":0},{"_id":"source/img/fac717a5.png","path":"img/fac717a5.png","modified":1,"renderable":0},{"_id":"source/img/fb523dd6.png","path":"img/fb523dd6.png","modified":1,"renderable":0},{"_id":"source/img/ffd661fb15bf746863a6e57d7aca04c4.png","path":"img/ffd661fb15bf746863a6e57d7aca04c4.png","modified":1,"renderable":0},{"_id":"source/img/floattype.png","path":"img/floattype.png","modified":1,"renderable":0},{"_id":"source/img/hdfs-read-file.png","path":"img/hdfs-read-file.png","modified":1,"renderable":0},{"_id":"source/img/hdfs-write-file.png","path":"img/hdfs-write-file.png","modified":1,"renderable":0},{"_id":"source/img/hdfs.png","path":"img/hdfs.png","modified":1,"renderable":0},{"_id":"source/img/hdfs创建文件夹.png","path":"img/hdfs创建文件夹.png","modified":1,"renderable":0},{"_id":"source/img/hive-partition.png","path":"img/hive-partition.png","modified":1,"renderable":0},{"_id":"source/img/hive-数学函数.png","path":"img/hive-数学函数.png","modified":1,"renderable":0},{"_id":"source/img/hive-算数运算符.png","path":"img/hive-算数运算符.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合1.png","path":"img/hive-聚合1.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合2.png","path":"img/hive-聚合2.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合3.png","path":"img/hive-聚合3.png","modified":1,"renderable":0},{"_id":"source/img/hive-表生成函数.png","path":"img/hive-表生成函数.png","modified":1,"renderable":0},{"_id":"source/img/hive-运算3.png","path":"img/hive-运算3.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构.png","path":"img/hive数据结构.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构1.png","path":"img/hive数据结构1.png","modified":1,"renderable":0},{"_id":"source/img/hive文本文件数据编码.png","path":"img/hive文本文件数据编码.png","modified":1,"renderable":0},{"_id":"source/img/hive运算1.png","path":"img/hive运算1.png","modified":1,"renderable":0},{"_id":"source/img/hive集合数据类型.png","path":"img/hive集合数据类型.png","modified":1,"renderable":0},{"_id":"source/img/https-intro.png","path":"img/https-intro.png","modified":1,"renderable":0},{"_id":"source/img/image-20200117113506023.png","path":"img/image-20200117113506023.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115459406.png","path":"img/image-20201206115459406.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115600801.png","path":"img/image-20201206115600801.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115653987.png","path":"img/image-20201206115653987.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115738831.png","path":"img/image-20201206115738831.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115832025.png","path":"img/image-20201206115832025.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121627187.png","path":"img/image-20201206121627187.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121650739.png","path":"img/image-20201206121650739.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121711716.png","path":"img/image-20201206121711716.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121829515.png","path":"img/image-20201206121829515.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206122126739.png","path":"img/image-20201206122126739.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214114013294.png","path":"img/image-20201214114013294.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121332594.png","path":"img/image-20201214121332594.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121613146.png","path":"img/image-20201214121613146.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121645728.png","path":"img/image-20201214121645728.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121721100.png","path":"img/image-20201214121721100.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214122255720.png","path":"img/image-20201214122255720.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214123508379.png","path":"img/image-20201214123508379.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214124300675.png","path":"img/image-20201214124300675.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214124603810.png","path":"img/image-20201214124603810.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125020584.png","path":"img/image-20201214125020584.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125032862.png","path":"img/image-20201214125032862.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125402694.png","path":"img/image-20201214125402694.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125453643.png","path":"img/image-20201214125453643.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125504552.png","path":"img/image-20201214125504552.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125529023.png","path":"img/image-20201214125529023.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214130757812.png","path":"img/image-20201214130757812.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131527522.png","path":"img/image-20201214131527522.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131543066.png","path":"img/image-20201214131543066.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131734289.png","path":"img/image-20201214131734289.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131814678.png","path":"img/image-20201214131814678.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131847691.png","path":"img/image-20201214131847691.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131948506.png","path":"img/image-20201214131948506.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132014480.png","path":"img/image-20201214132014480.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132059091.png","path":"img/image-20201214132059091.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132127621.png","path":"img/image-20201214132127621.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132215125.png","path":"img/image-20201214132215125.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132254636.png","path":"img/image-20201214132254636.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132309723.png","path":"img/image-20201214132309723.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132326043.png","path":"img/image-20201214132326043.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132343909.png","path":"img/image-20201214132343909.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132401207.png","path":"img/image-20201214132401207.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133601256.png","path":"img/image-20201214133601256.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132416329.png","path":"img/image-20201214132416329.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133612299.png","path":"img/image-20201214133612299.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133830361.png","path":"img/image-20201214133830361.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133955605.png","path":"img/image-20201214133955605.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133935704.png","path":"img/image-20201214133935704.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134132982.png","path":"img/image-20201214134132982.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134208507.png","path":"img/image-20201214134208507.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134235050.png","path":"img/image-20201214134235050.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134902337.png","path":"img/image-20201214134902337.png","modified":1,"renderable":0},{"_id":"source/img/image-20210415193556837.png","path":"img/image-20210415193556837.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214135053884.png","path":"img/image-20201214135053884.png","modified":1,"renderable":0},{"_id":"source/img/image-20210430152414489.png","path":"img/image-20210430152414489.png","modified":1,"renderable":0},{"_id":"source/img/image-20210430152457125.png","path":"img/image-20210430152457125.png","modified":1,"renderable":0},{"_id":"source/img/image-20210430152537381.png","path":"img/image-20210430152537381.png","modified":1,"renderable":0},{"_id":"source/img/image-20210430152325572.png","path":"img/image-20210430152325572.png","modified":1,"renderable":0},{"_id":"source/img/image-20210430152636683.png","path":"img/image-20210430152636683.png","modified":1,"renderable":0},{"_id":"source/img/image-20210718193314796.png","path":"img/image-20210718193314796.png","modified":1,"renderable":0},{"_id":"source/img/image-20210718193618402.png","path":"img/image-20210718193618402.png","modified":1,"renderable":0},{"_id":"source/img/image-20210802000052243.png","path":"img/image-20210802000052243.png","modified":1,"renderable":0},{"_id":"source/img/image-20210802000308665.png","path":"img/image-20210802000308665.png","modified":1,"renderable":0},{"_id":"source/img/image-20210802000916321.png","path":"img/image-20210802000916321.png","modified":1,"renderable":0},{"_id":"source/img/image-20210807123832574.png","path":"img/image-20210807123832574.png","modified":1,"renderable":0},{"_id":"source/img/image-20220122144100590.png","path":"img/image-20220122144100590.png","modified":1,"renderable":0},{"_id":"source/img/image-20211014175135307.png","path":"img/image-20211014175135307.png","modified":1,"renderable":0},{"_id":"source/img/image-20220123235957505.png","path":"img/image-20220123235957505.png","modified":1,"renderable":0},{"_id":"source/img/image-20220322221741703.png","path":"img/image-20220322221741703.png","modified":1,"renderable":0},{"_id":"source/img/image-20220404214514222.png","path":"img/image-20220404214514222.png","modified":1,"renderable":0},{"_id":"source/img/image-20220411212506831.png","path":"img/image-20220411212506831.png","modified":1,"renderable":0},{"_id":"source/img/image-20220404214709099.png","path":"img/image-20220404214709099.png","modified":1,"renderable":0},{"_id":"source/img/image-20220411220512656.png","path":"img/image-20220411220512656.png","modified":1,"renderable":0},{"_id":"source/img/image-20220411221525635.png","path":"img/image-20220411221525635.png","modified":1,"renderable":0},{"_id":"source/img/image-20220411222247494.png","path":"img/image-20220411222247494.png","modified":1,"renderable":0},{"_id":"source/img/javaagent1.png","path":"img/javaagent1.png","modified":1,"renderable":0},{"_id":"source/img/inttype.png","path":"img/inttype.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储.png","path":"img/java对象存储.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储2.png","path":"img/java对象存储2.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储3.png","path":"img/java对象存储3.png","modified":1,"renderable":0},{"_id":"source/img/jvm1.png","path":"img/jvm1.png","modified":1,"renderable":0},{"_id":"source/img/jvm2.png","path":"img/jvm2.png","modified":1,"renderable":0},{"_id":"source/img/jvmHeapStructure.png","path":"img/jvmHeapStructure.png","modified":1,"renderable":0},{"_id":"source/img/jvm3.png","path":"img/jvm3.png","modified":1,"renderable":0},{"_id":"source/img/loc.png","path":"img/loc.png","modified":1,"renderable":0},{"_id":"source/img/maven配置.png","path":"img/maven配置.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序1.png","path":"img/mysql排序1.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序2.png","path":"img/mysql排序2.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序3.png","path":"img/mysql排序3.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序4.png","path":"img/mysql排序4.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序5.png","path":"img/mysql排序5.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序6.png","path":"img/mysql排序6.png","modified":1,"renderable":0},{"_id":"source/img/mysql时间存储.png","path":"img/mysql时间存储.png","modified":1,"renderable":0},{"_id":"source/img/mysql的ip存储.png","path":"img/mysql的ip存储.png","modified":1,"renderable":0},{"_id":"source/img/nacos-producer.png","path":"img/nacos-producer.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud1.png","path":"img/nacos-springCloud1.png","modified":1,"renderable":0},{"_id":"source/img/nacos1.png","path":"img/nacos1.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud2.png","path":"img/nacos-springCloud2.png","modified":1,"renderable":0},{"_id":"source/img/output_11_0.png","path":"img/output_11_0.png","modified":1,"renderable":0},{"_id":"source/img/output_11_1.png","path":"img/output_11_1.png","modified":1,"renderable":0},{"_id":"source/img/output_11_111.png","path":"img/output_11_111.png","modified":1,"renderable":0},{"_id":"source/img/output_13_011.png","path":"img/output_13_011.png","modified":1,"renderable":0},{"_id":"source/img/output_13_0.png","path":"img/output_13_0.png","modified":1,"renderable":0},{"_id":"source/img/output_13_1.png","path":"img/output_13_1.png","modified":1,"renderable":0},{"_id":"source/img/output_13_111.png","path":"img/output_13_111.png","modified":1,"renderable":0},{"_id":"source/img/output_14_0.png","path":"img/output_14_0.png","modified":1,"renderable":0},{"_id":"source/img/output_15_0.png","path":"img/output_15_0.png","modified":1,"renderable":0},{"_id":"source/img/output_15_011.png","path":"img/output_15_011.png","modified":1,"renderable":0},{"_id":"source/img/output_17_0.png","path":"img/output_17_0.png","modified":1,"renderable":0},{"_id":"source/img/output_19_0.png","path":"img/output_19_0.png","modified":1,"renderable":0},{"_id":"source/img/output_22_0.png","path":"img/output_22_0.png","modified":1,"renderable":0},{"_id":"source/img/output_20_1.png","path":"img/output_20_1.png","modified":1,"renderable":0},{"_id":"source/img/output_26_1.png","path":"img/output_26_1.png","modified":1,"renderable":0},{"_id":"source/img/output_26_111.png","path":"img/output_26_111.png","modified":1,"renderable":0},{"_id":"source/img/output_29_0.png","path":"img/output_29_0.png","modified":1,"renderable":0},{"_id":"source/img/output_30_12.png","path":"img/output_30_12.png","modified":1,"renderable":0},{"_id":"source/img/output_30_0111.png","path":"img/output_30_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_31_0.png","path":"img/output_31_0.png","modified":1,"renderable":0},{"_id":"source/img/output_32_02.png","path":"img/output_32_02.png","modified":1,"renderable":0},{"_id":"source/img/output_32_1.png","path":"img/output_32_1.png","modified":1,"renderable":0},{"_id":"source/img/output_33_1.png","path":"img/output_33_1.png","modified":1,"renderable":0},{"_id":"source/img/output_32_111.png","path":"img/output_32_111.png","modified":1,"renderable":0},{"_id":"source/img/output_33_111.png","path":"img/output_33_111.png","modified":1,"renderable":0},{"_id":"source/img/output_34_0111.png","path":"img/output_34_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_34_1.png","path":"img/output_34_1.png","modified":1,"renderable":0},{"_id":"source/img/output_35_02.png","path":"img/output_35_02.png","modified":1,"renderable":0},{"_id":"source/img/output_36_1.png","path":"img/output_36_1.png","modified":1,"renderable":0},{"_id":"source/img/output_37_0111.png","path":"img/output_37_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_38_02.png","path":"img/output_38_02.png","modified":1,"renderable":0},{"_id":"source/img/output_40_02.png","path":"img/output_40_02.png","modified":1,"renderable":0},{"_id":"source/img/output_40_0.png","path":"img/output_40_0.png","modified":1,"renderable":0},{"_id":"source/img/output_42_0.png","path":"img/output_42_0.png","modified":1,"renderable":0},{"_id":"source/img/output_42_0111.png","path":"img/output_42_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_44_0.png","path":"img/output_44_0.png","modified":1,"renderable":0},{"_id":"source/img/output_44_0111.png","path":"img/output_44_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_46_0.png","path":"img/output_46_0.png","modified":1,"renderable":0},{"_id":"source/img/output_47_0111.png","path":"img/output_47_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_49_12.png","path":"img/output_49_12.png","modified":1,"renderable":0},{"_id":"source/img/output_50_0111.png","path":"img/output_50_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_51_12.png","path":"img/output_51_12.png","modified":1,"renderable":0},{"_id":"source/img/output_52_0.png","path":"img/output_52_0.png","modified":1,"renderable":0},{"_id":"source/img/output_54_0.png","path":"img/output_54_0.png","modified":1,"renderable":0},{"_id":"source/img/output_55_0111.png","path":"img/output_55_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_55_12.png","path":"img/output_55_12.png","modified":1,"renderable":0},{"_id":"source/img/output_57_0111.png","path":"img/output_57_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_57_12.png","path":"img/output_57_12.png","modified":1,"renderable":0},{"_id":"source/img/output_59_0111.png","path":"img/output_59_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_63_12.png","path":"img/output_63_12.png","modified":1,"renderable":0},{"_id":"source/img/output_60_12.png","path":"img/output_60_12.png","modified":1,"renderable":0},{"_id":"source/img/output_65_0111.png","path":"img/output_65_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_65_12.png","path":"img/output_65_12.png","modified":1,"renderable":0},{"_id":"source/img/output_67_0111.png","path":"img/output_67_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_68_12.png","path":"img/output_68_12.png","modified":1,"renderable":0},{"_id":"source/img/output_70_12.png","path":"img/output_70_12.png","modified":1,"renderable":0},{"_id":"source/img/output_71_0111.png","path":"img/output_71_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_72_12.png","path":"img/output_72_12.png","modified":1,"renderable":0},{"_id":"source/img/output_74_12.png","path":"img/output_74_12.png","modified":1,"renderable":0},{"_id":"source/img/output_75_0111.png","path":"img/output_75_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_79_0111.png","path":"img/output_79_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_79_02.png","path":"img/output_79_02.png","modified":1,"renderable":0},{"_id":"source/img/output_79_12.png","path":"img/output_79_12.png","modified":1,"renderable":0},{"_id":"source/img/output_81_0111.png","path":"img/output_81_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_85_1111.png","path":"img/output_85_1111.png","modified":1,"renderable":0},{"_id":"source/img/output_87_0111.png","path":"img/output_87_0111.png","modified":1,"renderable":0},{"_id":"source/img/output_81_02.png","path":"img/output_81_02.png","modified":1,"renderable":0},{"_id":"source/img/output_89_02.png","path":"img/output_89_02.png","modified":1,"renderable":0},{"_id":"source/img/output_8_0.png","path":"img/output_8_0.png","modified":1,"renderable":0},{"_id":"source/img/output_8_1.png","path":"img/output_8_1.png","modified":1,"renderable":0},{"_id":"source/img/output_8_111.png","path":"img/output_8_111.png","modified":1,"renderable":0},{"_id":"source/img/pasted-0.png","path":"img/pasted-0.png","modified":1,"renderable":0},{"_id":"source/img/redis分布式锁实现原理.jpg","path":"img/redis分布式锁实现原理.jpg","modified":1,"renderable":0},{"_id":"source/img/secondaryNameNode.jpg","path":"img/secondaryNameNode.jpg","modified":1,"renderable":0},{"_id":"source/img/select、epoll模型对比.png","path":"img/select、epoll模型对比.png","modified":1,"renderable":0},{"_id":"source/img/simpleDateFormat-alibaba.png","path":"img/simpleDateFormat-alibaba.png","modified":1,"renderable":0},{"_id":"source/img/spark+hdfs.png","path":"img/spark+hdfs.png","modified":1,"renderable":0},{"_id":"source/img/spark-all.png","path":"img/spark-all.png","modified":1,"renderable":0},{"_id":"source/img/transaction.png","path":"img/transaction.png","modified":1,"renderable":0},{"_id":"source/img/user-behaviour.png","path":"img/user-behaviour.png","modified":1,"renderable":0},{"_id":"source/img/typetrans.png","path":"img/typetrans.png","modified":1,"renderable":0},{"_id":"source/img/user-log.png","path":"img/user-log.png","modified":1,"renderable":0},{"_id":"source/img/weixin.jpg","path":"img/weixin.jpg","modified":1,"renderable":0},{"_id":"source/img/wordcount-map.png","path":"img/wordcount-map.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-reduce.png","path":"img/wordcount-reduce.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-split.png","path":"img/wordcount-split.png","modified":1,"renderable":0},{"_id":"source/img/wordcount.png","path":"img/wordcount.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议1.png","path":"img/三次握手协议1.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议2.png","path":"img/三次握手协议2.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议3.png","path":"img/三次握手协议3.png","modified":1,"renderable":0},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","path":"img/主动免疫可信架构信任链传递示意图.png","modified":1,"renderable":0},{"_id":"source/img/使用协议进行通讯.png","path":"img/使用协议进行通讯.png","modified":1,"renderable":0},{"_id":"source/img/信任链.png","path":"img/信任链.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥1.png","path":"img/公钥私钥1.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥10.png","path":"img/公钥私钥10.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥11.png","path":"img/公钥私钥11.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥12.png","path":"img/公钥私钥12.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥13.png","path":"img/公钥私钥13.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥2.png","path":"img/公钥私钥2.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥3.png","path":"img/公钥私钥3.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥4.png","path":"img/公钥私钥4.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥5.png","path":"img/公钥私钥5.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥6.png","path":"img/公钥私钥6.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥8.png","path":"img/公钥私钥8.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥7.png","path":"img/公钥私钥7.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥9.png","path":"img/公钥私钥9.png","modified":1,"renderable":0},{"_id":"source/img/加密算法.png","path":"img/加密算法.png","modified":1,"renderable":0},{"_id":"source/img/可信在云平台的基础架构.png","path":"img/可信在云平台的基础架构.png","modified":1,"renderable":0},{"_id":"source/img/可信根.png","path":"img/可信根.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议.png","path":"img/四次挥手协议.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议2.png","path":"img/四次挥手协议2.png","modified":1,"renderable":0},{"_id":"source/img/基于ETCD实现分布式锁分析.png","path":"img/基于ETCD实现分布式锁分析.png","modified":1,"renderable":0},{"_id":"source/img/基于Zookeeper分布式锁.png","path":"img/基于Zookeeper分布式锁.png","modified":1,"renderable":0},{"_id":"source/img/对称加密算法.png","path":"img/对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/对象存储1.png","path":"img/对象存储1.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩1.png","path":"img/指针压缩1.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩2.png","path":"img/指针压缩2.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩3.png","path":"img/指针压缩3.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩4.png","path":"img/指针压缩4.png","modified":1,"renderable":0},{"_id":"source/img/混合加密的方式.png","path":"img/混合加密的方式.png","modified":1,"renderable":0},{"_id":"source/img/椭圆曲线算法的基本原理.png","path":"img/椭圆曲线算法的基本原理.png","modified":1,"renderable":0},{"_id":"source/img/数据库分布式ID生成.png","path":"img/数据库分布式ID生成.png","modified":1,"renderable":0},{"_id":"source/img/线程相关1.jpg","path":"img/线程相关1.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关4.jpg","path":"img/线程相关4.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关3.jpg","path":"img/线程相关3.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关2.jpg","path":"img/线程相关2.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关5.jpg","path":"img/线程相关5.jpg","modified":1,"renderable":0},{"_id":"source/img/网络连接模型.png","path":"img/网络连接模型.png","modified":1,"renderable":0},{"_id":"source/img/读写锁.png","path":"img/读写锁.png","modified":1,"renderable":0},{"_id":"source/img/锁的创建.png","path":"img/锁的创建.png","modified":1,"renderable":0},{"_id":"source/img/锁的创建2.png","path":"img/锁的创建2.png","modified":1,"renderable":0},{"_id":"source/img/阻塞IO.png","path":"img/阻塞IO.png","modified":1,"renderable":0},{"_id":"source/img/阻塞与非阻塞调用对比.png","path":"img/阻塞与非阻塞调用对比.png","modified":1,"renderable":0},{"_id":"source/img/非对称加密算法.png","path":"img/非对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/雪花算法.png","path":"img/雪花算法.png","modified":1,"renderable":0},{"_id":"source/img/非阻塞IO.png","path":"img/非阻塞IO.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_discarded/hello-world.md","hash":"aaa8bfb04d442da88828d4dfd36d2ace6806d777","modified":1649061160132},{"_id":"source/_drafts/信息系统项目管理师-信息化.md","hash":"232e4854291552bb30925fc583e601c134da6dfe","modified":1649061160133},{"_id":"source/_drafts/原子性，有序性，可见性.md","hash":"689a4d46179518105944c0d02e452d3077565e91","modified":1649061160134},{"_id":"source/_drafts/基于JavaAgent的全链路监控（4）.md","hash":"4985613a8bf29680907d75e2ac3c0f773271ee6a","modified":1649061160134},{"_id":"source/_drafts/排序方法.md","hash":"000f674677cc679c81c688c8cad4cfb9dfc90a52","modified":1649061160134},{"_id":"source/_posts/AtomicInteger.md","hash":"92a115403d614029219c1991d110ed68dfb39e73","modified":1649061160135},{"_id":"source/_posts/CountDownLatch.md","hash":"0e645fcdbea8a3d6ee47a74c8edbd1a0849ee364","modified":1649426688373},{"_id":"source/_posts/DNS.md","hash":"1db61f5be48a72bebbe948c6aa4469d14a4400d0","modified":1649061160136},{"_id":"source/_posts/Disruptor中发布事件相关类.md","hash":"8fa2ff7f2ad486e9145107441c13f4b3905536d1","modified":1649061160137},{"_id":"source/_posts/Docker入门.md","hash":"a692dee9bb107af526567772298d4acc9e59d84a","modified":1649061160137},{"_id":"source/_posts/ElasticSearch客户端.md","hash":"71ed111e0271018f92f9cc3316e29f634588f89a","modified":1649061160137},{"_id":"source/_posts/ElasticSearch近实时性介绍.md","hash":"83f9aa323b7b434e2a8962d9dd47e857eb0aaaab","modified":1649061160138},{"_id":"source/_posts/GET与POST区别.md","hash":"1ec63b6b4377239895bffed0e35f55f72bbef4ff","modified":1649061160138},{"_id":"source/_posts/Disruptor.md","hash":"a532dda9c1e8c7d110e4c17dfd7b2ca1aac108b6","modified":1649061160136},{"_id":"source/_posts/HDFS-shell操作（1）.md","hash":"e47f2f1c17e46ce0033c8842e05d1641738a36aa","modified":1649061160139},{"_id":"source/_posts/HDFS-shell操作（2）.md","hash":"2fa2347d8d162404b5e4b036bef6e50cca22b6a7","modified":1649061160139},{"_id":"source/_posts/Git梳理.md","hash":"c4e1ca0d8b78e06c2be620771146c340deb11cc6","modified":1649061160138},{"_id":"source/_posts/HDFS文件操作.md","hash":"ad563809292d15003ce1f6c4dca37c34faa174c1","modified":1649061160139},{"_id":"source/_posts/HDFS概述.md","hash":"414625fc2a3b2a7799f16b6eac209b9ab0c1f026","modified":1649061160140},{"_id":"source/_posts/Histogram（直方图）-KDE（密度图.md","hash":"0fd3cf120ce798e6fae8f6e4fcfd3de7385e49ab","modified":1649061160143},{"_id":"source/_posts/Hash解决冲突的方法.md","hash":"48b10362df65394c3344329780173d5f26268f64","modified":1649061160140},{"_id":"source/_posts/Hive数据定义.md","hash":"50f17c99059d304c7aa129faf847ef5221e55f89","modified":1649061160144},{"_id":"source/_posts/HiveQL视图.md","hash":"d849b79413a84d9d938fb67c364f87966285c200","modified":1649061160143},{"_id":"source/_posts/Hive数据操作.md","hash":"6ce31bf063fea42e058bfb7eb942174871d6bbba","modified":1649061160144},{"_id":"source/_posts/Hive数据操作（2）.md","hash":"5f8d8d18fd1012130162ee7fcf480737166fc0a5","modified":1649061160144},{"_id":"source/_posts/Hive数据操作（3）.md","hash":"d598c199e53c8b10c071e76bb69658d8f1122b06","modified":1649061160145},{"_id":"source/_posts/Hive数据类型和文件格式.md","hash":"feb1e48e382f2c4303e038650291e65e706c5cff","modified":1649061160145},{"_id":"source/_posts/Hive模式设计.md","hash":"77aa7351b6472a05a71821dc9cc6af0824d50a42","modified":1649061160145},{"_id":"source/_posts/Hive索引.md","hash":"f4b37f00427515e00369a49ffbee675ce246ef0d","modified":1649061160145},{"_id":"source/_posts/Hive调优.md","hash":"7be24d37fbfbcbdfbce547741dee3421aed1d3cc","modified":1649061160146},{"_id":"source/_posts/HttpClient.md","hash":"c210257e869a028465f5c3d5b4a60d720eac7980","modified":1649061160146},{"_id":"source/_posts/Http和Https的区别.md","hash":"57c9eb5b58a5d18a8c8e0985515ade73323afdc7","modified":1649061160146},{"_id":"source/_posts/IOC中的基本反射步骤.md","hash":"09d85030bca7783812dd7aadba30d54a5a587996","modified":1649061160147},{"_id":"source/_posts/JAVA数据类型.md","hash":"29e6f6dd4ff0879b7be91fe32000a8bf0be49188","modified":1649061160147},{"_id":"source/_posts/JDK并发包常用类.md","hash":"57eb145fce8038d3a18ed540a80f0b9fff9030ab","modified":1649427669048},{"_id":"source/_posts/JVM内存结构.md","hash":"70307fe04fa78bd46d0647cb053bb723a908682b","modified":1649061160147},{"_id":"source/_posts/JVM垃圾回收算法.md","hash":"434d054e06bc0061a27f347a7f41ceb388e66ade","modified":1649061160148},{"_id":"source/_posts/JVM性能优化整理.md","hash":"b35f1b693b4678a0e5a68f6baabf229fa34845e3","modified":1649061160148},{"_id":"source/_posts/JVM类加载过程.md","hash":"49fab51d21e0b5b8ba78f64d9d6dc2410d6bd17b","modified":1649061160148},{"_id":"source/_posts/Kafka的简单使用.md","hash":"43a0286262f02be772c8f1ea576fdd1b03872eb5","modified":1649061160148},{"_id":"source/_posts/MapReduce概述.md","hash":"13a50dc5458d8651951d8f9fb92e17c6ca6ac9b3","modified":1649061160149},{"_id":"source/_posts/MinIO单机安装以及使用.md","hash":"f506d3f99bf1ab6ee14a3d6af08c2d7bb7be5923","modified":1649426260601},{"_id":"source/_posts/MapReduce平均数计算.md","hash":"9ad530e07a195a0a2517422e07544bcdf89b4f6e","modified":1649061160149},{"_id":"source/_posts/Redis数据结构与对象（一）-链表.md","hash":"4ad061dc2b23ed3f1baf41812a042cda416695eb","modified":1649061160151},{"_id":"source/_posts/Nacos配置中心使用.md","hash":"0b33a681cb95b56bcd7ee69c10c7e0c2a0a34149","modified":1649061160151},{"_id":"source/_posts/Redis数据结构与对象（三）-字典.md","hash":"9720f00c10973f33e1e7ec8b5604d48aa5a25066","modified":1649061160151},{"_id":"source/_posts/SimpleDateFormat引发的线程安全问题.md","hash":"cbd7b10d6b350bce12bc4a185ec47e723a9b1fc4","modified":1649061160152},{"_id":"source/_posts/Redis设计与实现.md","hash":"c7277d9ca66857d0aacb8f3fd6497325c17f2235","modified":1649061160152},{"_id":"source/_posts/Spark相关概述.md","hash":"dddf8fbafad5a344e215bf3ed4ec90697e93152c","modified":1649061160152},{"_id":"source/_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","hash":"780622132fcc3a1570344a0e6b4d33b146499630","modified":1649061160153},{"_id":"source/_posts/Spring-Bean生命周期.md","hash":"f935d2ca6addb9c6ba7da2468bfd19bd94328a1b","modified":1649061160153},{"_id":"source/_posts/SpringCloud-Hystrix参数配置.md","hash":"0ad02b1a42dd17dfe2a52e893a8abf75eb79af6e","modified":1649061160153},{"_id":"source/_posts/SpringCloud-Ribbon参数配置.md","hash":"9eaf5c7a88fa5155febee2f28ed543f97d702b77","modified":1649061160154},{"_id":"source/_posts/SpringCloud-client配置.md","hash":"ee07eefffb6366bc87c9ba88b6ddf6fe7213ff34","modified":1649061160154},{"_id":"source/_posts/SpringCloud使用Feign-Ribbon-Hystrix.md","hash":"5996e2ec00a285a6a8445e9e635a050611f9c5ca","modified":1649061160154},{"_id":"source/_posts/SpringCloud使用RestTemplate.md","hash":"2021f39350755742495fa1beadf5d15044dbbfa8","modified":1649061160155},{"_id":"source/_posts/SpringCloud使用Feign-Ribbon.md","hash":"99fe585e530d09716dc75dae26fbdedfe0355dca","modified":1649061160155},{"_id":"source/_posts/SpringCloud使用Feign.md","hash":"0c2549b2cfaeb83ccf2c0fca350f2dd63ddd602f","modified":1649061160155},{"_id":"source/_posts/SpringCloud健康检查.md","hash":"124fcd66a2c34f011070cbae073fd7d4f4597e5b","modified":1649061160156},{"_id":"source/_posts/SpringCloud异常配置.md","hash":"6c54a6d60547ea1159575d223c64c4913b781804","modified":1649061160156},{"_id":"source/_posts/SpringCloud服务构建.md","hash":"aa4e65cad984915dae7ac3b5f6bea33aa092c313","modified":1649061160156},{"_id":"source/_posts/SpringCloud服务注册.md","hash":"d269dd665e46e0b2fccd34349c3e58a651e28ae3","modified":1649061160157},{"_id":"source/_posts/SpringCloud服务消费.md","hash":"d80f06a0538aa1351303acc6bed04895ab2dc1e3","modified":1649061160157},{"_id":"source/_posts/SpringCloud管理配置页面.md","hash":"04ac9191a890ba912287e5f110c1f391d5671a6c","modified":1649061160157},{"_id":"source/_posts/SpringCloud运维接口.md","hash":"436bf23185775e463bbcd2b8287b7edad576fcac","modified":1649061160157},{"_id":"source/_posts/TCP-IP四层网络模型.md","hash":"0f9107353f89c88aa2a53878ef0b0cb050f2f4c4","modified":1649061160158},{"_id":"source/_posts/TCP与UDP的区别.md","hash":"7f61caf4c6b5f8c2a120d0b684000eea2dff8130","modified":1649061160158},{"_id":"source/_posts/TCP握手、挥手协议.md","hash":"8476f6d45b59d1d38589aaa88eea8d5ac224950c","modified":1649061160158},{"_id":"source/_posts/ThreadLocal.md","hash":"13dfd861619726f1a12dfb4f57738afbb0f0f769","modified":1649061160159},{"_id":"source/_posts/Tomcat性能优化整理.md","hash":"65ddf40dd1cd06d6f05a6545eb084cd80d07b68e","modified":1649061160159},{"_id":"source/_posts/Yarn概述.md","hash":"bb2c8fcae4592c1d73955821c9f7f5f9b92c123e","modified":1649061160159},{"_id":"source/_posts/WordCount简析.md","hash":"9a303573d24163422cc05eb83fa9578b6fd593e2","modified":1649061160159},{"_id":"source/_posts/docker20-10-7本机开启2375配置.md","hash":"40bd7816d143c2486a5e8174ddca2cff0070c366","modified":1649061160159},{"_id":"source/_posts/docker镜像部署到k8s集群.md","hash":"15565b9032cd29af7398ae45823ba9ea8e8466ab","modified":1649689402723},{"_id":"source/_posts/java8新特性.md","hash":"66f6f65cdcd6ff4b39e8d120a746c6042ee86f32","modified":1649061160159},{"_id":"source/_posts/java中的Queue队列.md","hash":"65dd9d89abfb7049264c76727653a59f18a2fdd7","modified":1649061160160},{"_id":"source/_posts/k8s集群搭建.md","hash":"0c0d270a1502956bd4be254f2d42c8f42ff38493","modified":1649080268877},{"_id":"source/_posts/k8s构建ELK日志平台.md","hash":"ddfa12405dedaec6ea21fbd1ef0f087de02f00d1","modified":1649061160160},{"_id":"source/_posts/l-String、-StringBuilder、StringBuffer区别.md","hash":"d2c1e999489d47ed3cce60ba581b4085a92e61fe","modified":1649061160160},{"_id":"source/_posts/maven梳理.md","hash":"69e0486290668fea0626d7fc4b47f5ffcc6be24a","modified":1649061160161},{"_id":"source/_posts/mysql事务.md","hash":"760c135d354981de10ef0f5d1f318467df9ea385","modified":1649061160161},{"_id":"source/_posts/mysql排序.md","hash":"6c0da8aa6a75f03509161f8d4ce46f3f9203d3c1","modified":1649061160161},{"_id":"source/_posts/mysql突然变慢排查.md","hash":"67a175b544b966c2ffce1e71032777436afc1bc9","modified":1649061160161},{"_id":"source/_posts/mysql简单主从搭建过程.md","hash":"da250f19b5f0fab3af97768747b08a106af8f631","modified":1649719925164},{"_id":"source/_posts/mysql表设计及优化.md","hash":"a68f2344b408a4fc9721f9686025756fc8cdfc0b","modified":1649061160162},{"_id":"source/_posts/python可视化基础.md","hash":"ef72bfa65e30f056c02bfedb145df6c978c74d82","modified":1649061160164},{"_id":"source/_posts/互斥锁.md","hash":"8194753854138e516f43c85e3b4bb5b5f4214ef7","modified":1649061160164},{"_id":"source/_posts/伪共享.md","hash":"745020116389a4b34ded5481239c441e061b1113","modified":1649061160165},{"_id":"source/_posts/信息系统项目管理师-信息化-1.md","hash":"89f4c8eb71087a501e32bcca24fc2d8b79c06873","modified":1649061160165},{"_id":"source/_posts/偏向锁.md","hash":"151361f66ce460d7831e3e384ecdf97d78a58f07","modified":1649061160165},{"_id":"source/_posts/公平锁、非公平锁.md","hash":"e20eeb629537ad7c25f418b1f37627a7d357accb","modified":1649061160166},{"_id":"source/_posts/分布式CAP概念.md","hash":"1f7e0a8fbc0d8df9f07d5d480c1c2eaf7a92264b","modified":1649061160166},{"_id":"source/_posts/分布式全局唯一ID生成策略.md","hash":"105dc8c1538b9b0df100c788c54919be0d7182b9","modified":1649061160167},{"_id":"source/_posts/分布式锁.md","hash":"2b4fc5dfea215ee9b6b00a7b44ec0386680cca03","modified":1649061160168},{"_id":"source/_posts/初识redis（1）.md","hash":"447ee49789c453a92673bd8ea1839fabe798dfae","modified":1649061160168},{"_id":"source/_posts/初识redis（3）-持久化.md","hash":"b97e084351505a9e253e804d1a1daa4b48372796","modified":1649061160169},{"_id":"source/_posts/初识redis（2）.md","hash":"fff2d43b801adbaf6a06b20148c2c39a9827c898","modified":1649061160169},{"_id":"source/_posts/初识redis（4）-主从架构.md","hash":"6cec35b176a1366e3a9ba82b94ccd99dd4ea69b3","modified":1649061160169},{"_id":"source/_posts/初识redis（5）-内存调优.md","hash":"fe4a6ca65f786e0b856b8b2ef6f6ad6e0e2e5057","modified":1649061160170},{"_id":"source/_posts/加密解密.md","hash":"85977a0aca26c6b170708e66e48b31629fc42160","modified":1649061160170},{"_id":"source/_posts/单例模式.md","hash":"edd5a1e5e1030770bc54a54ec6fa7e3bf71da920","modified":1649061160170},{"_id":"source/_posts/可信与可信计算.md","hash":"49d3e8ac43aedf374606b117eec168d75c70ca7b","modified":1649061160171},{"_id":"source/_posts/可信基本概念.md","hash":"1fc4362a6f79fb75f1e79cc1cfa5953398745803","modified":1649061160171},{"_id":"source/_posts/可重入锁.md","hash":"e80ffae652225ea5c66479348b5de1ef979e8185","modified":1649061160171},{"_id":"source/_posts/可靠性和容错技术.md","hash":"2e2663804621d8bc58ef8107a1a1ea6aac8a739f","modified":1649061160172},{"_id":"source/_posts/图解公钥与私钥.md","hash":"4c25b208eb034a4e22d4e095ae984cf95631320c","modified":1649061160172},{"_id":"source/_posts/基于JavaAgent的全链路监控（1）.md","hash":"aa4e4654cc6b53cc6dcaa9f0877bf3aa4a818d87","modified":1649061160173},{"_id":"source/_posts/基于JavaAgent的全链路监控（2）.md","hash":"985180d9609c0658ce67b2048ecf97fd1de84d43","modified":1649061160173},{"_id":"source/_posts/基于JavaAgent的全链路监控（3）.md","hash":"6d6aab6f11199f94c429f02ed62523e74387a573","modified":1649061160173},{"_id":"source/_posts/对象存储与指针压缩.md","hash":"78e08599d74ddc0c0b4acb1b957148a84f9830fd","modified":1649061160174},{"_id":"source/_posts/并发编程总结.md","hash":"d6f497e17947ad01b8d0172bd35b3d960f39bf33","modified":1649061160174},{"_id":"source/_posts/悲观锁、乐观锁.md","hash":"f2c85723388fd67c3ada8405016e08158d55eb26","modified":1649061160174},{"_id":"source/_posts/手写一个简单Autowired.md","hash":"427350ab20d1bb47ba878c97caf304b8dbb6541e","modified":1649061160175},{"_id":"source/_posts/排序之比较器.md","hash":"fed592ce5abe74e3f7ff0f8109aeda35dee018e8","modified":1649061160175},{"_id":"source/_posts/排序之比较器Comparator-T.md","hash":"32f1bf13ea9212d0365031839ef76993e9b1fded","modified":1649061160175},{"_id":"source/_posts/散点图.md","hash":"e9c7177cc4379aee079ee1eacbdf833695cfcf58","modified":1649061160176},{"_id":"source/_posts/数字签名.md","hash":"1779c679ab381e0077c154b3d00035c5b067133e","modified":1649061160176},{"_id":"source/_posts/文件上传.md","hash":"c8e8ce226b9d3b2eb12bf2e9cedd51e30037ed3b","modified":1649061160177},{"_id":"source/_posts/池化之线程池.md","hash":"7794aad4fa34e0b45eccfe01bbca0f4cb87123f1","modified":1649429132793},{"_id":"source/_posts/特征提取-简单流程.md","hash":"a026e32b49c5376f0c250284cff1d3b850ff3fda","modified":1649061160177},{"_id":"source/_posts/理解IO阻塞与非阻塞.md","hash":"f16878fb18f62d0d87cc5a92c55bceaab1f115e6","modified":1649061160177},{"_id":"source/_posts/直方图.md","hash":"f78d5e166b278de6b0f36f8f33dfd3d37e499d25","modified":1649061160178},{"_id":"source/_posts/线程相关的知识.md","hash":"9e056964de587840cdb4dc4a8e5d67ff3a06059f","modified":1649061160178},{"_id":"source/_posts/自旋锁.md","hash":"73546d9722c8932c73cc6253ffd9d8d177e3d2de","modified":1649061160179},{"_id":"source/_posts/读写锁.md","hash":"bfe69f589a6ea906059242ed58460d093a619f3c","modified":1649061160179},{"_id":"source/_posts/责任链模式.md","hash":"a99981fa6d8911c8877d28d416f67109fc084b22","modified":1649061160180},{"_id":"source/_posts/重放攻击.md","hash":"511c53e6371e23a5934da65d800fc87e07960e58","modified":1649061160181},{"_id":"source/_posts/运算符.md","hash":"8b94ff7e3662a23f1099a7b5ba84528ecdd5b29c","modified":1649061160180},{"_id":"source/_posts/轻量级锁.md","hash":"9fcee07d99c757153c92f87389336b460eed4924","modified":1649061160180},{"_id":"source/_posts/重构.md","hash":"1d9d4b9398bed3859187ee80b13c975d994ccef6","modified":1649061160181},{"_id":"source/_posts/锁粗化.md","hash":"268fea80049696e0defd4791d0e75928592c8fb7","modified":1649061160181},{"_id":"source/_posts/阻塞锁.md","hash":"c53b76d97b45cec8d1550c3373164e1c36df5ace","modified":1649061160182},{"_id":"source/_posts/饼图.md","hash":"10eea9e90f7f53ebcba7c1c153ddfb4df799e5d4","modified":1649061160182},{"_id":"source/img/108cd6b414ab2dbb30126c0fb0700e23.png","hash":"67bf717f106c76aafabc07a5c2601d3d873f0d39","modified":1649061160183},{"_id":"source/img/1112dfc7b11a214351a67103cd58a5c8.png","hash":"20595180bd1af121b4f8e6ba17001e7145471c42","modified":1649061160184},{"_id":"source/img/145d5664.png","hash":"5de6263d58ac723c22250323e760162284317a71","modified":1649061160186},{"_id":"source/img/157e6450653a0fc457342b5794658fc9.png","hash":"1e396fc40a3ed2f6ee336158b619362541a614be","modified":1649061160186},{"_id":"source/img/1618244110010.png","hash":"947439297da765710fa815ea74e0920af546e46b","modified":1649061160189},{"_id":"source/img/1618244166891.png","hash":"5fbb1c37f8bc0d5a9d12e931dbf7b24da9d180ce","modified":1649061160190},{"_id":"source/img/1618244251013.png","hash":"a4a3edf1be8118db34c7d6ccd951dbcfe29ad50c","modified":1649061160192},{"_id":"source/img/1618244289269.png","hash":"3505425f677bf63c3351d9bb723193659be5f0f1","modified":1649061160192},{"_id":"source/img/1618244387386.png","hash":"f57bf41422b093082b4b2f4b42afa9488f0bb8de","modified":1649061160194},{"_id":"source/img/1618244497271.png","hash":"6e3f32bbe2c044ddbdfde51fda828ed6eb4190c4","modified":1649061160194},{"_id":"source/img/1618244522945.png","hash":"3484b30a26d62ae602e3b7a6faf8b7b49bab0295","modified":1649061160195},{"_id":"source/img/1618244566899.png","hash":"7a5274a43d170806b63e4f3457c2f0af774a86cd","modified":1649061160197},{"_id":"source/img/1618675966507.png","hash":"34016aea55982c639b85e635abdd8bdc5c08e6de","modified":1649061160201},{"_id":"source/img/227d0458.png","hash":"2840cba30e73d089195ec8d60e2bd567e96485e6","modified":1649061160220},{"_id":"source/img/2b97031c92882db452d126852b74cfb2.png","hash":"fe9164297c77477054de20f56ada187a7397dde6","modified":1649061160220},{"_id":"source/img/2cfc91e2dc6104321827e210ccfc595c.png","hash":"372aa64130efc06a4afca46e7fe19608cb8c7b1b","modified":1649061160221},{"_id":"source/img/41d2985c3545a456341af1e4f5b8231a.png","hash":"d542f333e70286bdc5e113ad1e6182f8476e96de","modified":1649061160221},{"_id":"source/img/480c47d9.png","hash":"d1b8b8bd0eb74760f8c54dee986e6fef466d6b6e","modified":1649061160222},{"_id":"source/img/4e12cc06.png","hash":"750185f3c87805cafe374b9ed6c6b4d6909c8d3a","modified":1649061160222},{"_id":"source/img/57c9d55222f65769c0ab0844bc5b6432.png","hash":"8ca28dcb12a065333f54d2e8df334877e6348dd8","modified":1649061160223},{"_id":"source/img/591a3d39db7558c3fd1db79821abec5e.png","hash":"9589587ae012ed1578f656ca3d55548f058ca56b","modified":1649061160223},{"_id":"source/img/5a377b3b.png","hash":"7a8b1a4998362f443f0989fd88d2424a93adb05e","modified":1649061160224},{"_id":"source/img/5a824e2f.png","hash":"7a187c1968dcd312f4480515f5ddb7b8a0815774","modified":1649061160224},{"_id":"source/img/70db7f87.jpg","hash":"354e54d0f10bf2d4bb92ad58e51a542d66095e2c","modified":1649061160225},{"_id":"source/img/743e6cec.jpg","hash":"eda743b32da786d5287d69c54f66600b02dfb48a","modified":1649061160225},{"_id":"source/img/79c1c0a4746f50074f0e44491d3cb8b3.png","hash":"c24cebcf4fd76df87c818fed8e67d1b24f5d8f6b","modified":1649061160226},{"_id":"source/img/7fdd593ac3e140d5cd2e9328b51110c9.png","hash":"e99fae23f53ca43700b92e510c770dad33efa64f","modified":1649061160227},{"_id":"source/img/879bfdfc7cab85a54bed5a33275c6dc0.png","hash":"b43a21590a27b6812486d939e3f69b101908ef88","modified":1649061160228},{"_id":"source/img/8fb0cd0c.png","hash":"3656d384f8348607e72d6ddb69f23ec7e3e1801b","modified":1649061160230},{"_id":"source/img/9b9139fd46f7419279ba6346b090ddc7.png","hash":"7aef4036bed44b2eca4a3a440a8e91178f80bfff","modified":1649061160231},{"_id":"source/img/9c9e497f4dc73587aa9d620a4149101c.png","hash":"b248c9780fb66208c64500e0014e9c52698f4f68","modified":1649061160231},{"_id":"source/img/Git工作流程.png","hash":"13f5329292c5f0b42cb969b2981141981bc62c12","modified":1649061160237},{"_id":"source/img/JVMMemory.png","hash":"053217d209af791dd65e97e321268cf17df92275","modified":1649061160248},{"_id":"source/img/JVM类加载过程.png","hash":"aa87f840b42869db7b76bd2789d1eaa8dcda2d0f","modified":1649061160249},{"_id":"source/img/StringBuilder.png","hash":"eaa519848fcd7948c1820e5e2a2f2ad719277708","modified":1649061160256},{"_id":"source/img/StringStringBuilderStringBuffer.png","hash":"d836b560a0cf006cfa6d8d833c3453f131db7c12","modified":1649061160257},{"_id":"source/img/TCPIP服务器接收请求.png","hash":"dd6986ec9df5f3ffe5ff743e97408b53147743fc","modified":1649061160259},{"_id":"source/img/TCPIP用户发送请求.png","hash":"f4428ca4bf21e4d067da5b944bb742892939d131","modified":1649061160260},{"_id":"source/img/UDPHeader.png","hash":"03991c09cb8a800bc68acc974430c1d6df3073d2","modified":1649061160267},{"_id":"source/img/UDPTCPcompare.png","hash":"fc2c1380a717ad5e1ac5a85250d26d21e56a9f92","modified":1649061160268},{"_id":"source/img/Wechat.jpg","hash":"5944a56bb7506847a6362acee8a54aa268db2919","modified":1649061160269},{"_id":"source/img/a4ef9088f117a8766ac65c4a1a630a34.png","hash":"bef96d7494c6d4fdf5efaa53ea7e7c0f0f3d3257","modified":1649061160271},{"_id":"source/img/a7ade6c9.png","hash":"f3c6bb2c3cf9073b99b71d5ed8c48bf7d9f85d77","modified":1649061160271},{"_id":"source/img/a7d8693f0dccfde8ecd0919cff94ff88.png","hash":"19650ca1c919b99de580339632ae0933ad83c623","modified":1649061160272},{"_id":"source/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png","hash":"be20a8b0190f360f7cb12cb14d0aff0544bab46f","modified":1649061160272},{"_id":"source/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1649061160274},{"_id":"source/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png","hash":"9430376a898992c9a6c09e9251f3db4bab3d7993","modified":1649061160275},{"_id":"source/img/b235f114.png","hash":"605d8626ed4d395ea711cb82a141c2ce1e36f4d8","modified":1649061160276},{"_id":"source/img/b4d68165.png","hash":"5d7c542053cb704983fde9b21bc7cd74c9053628","modified":1649061160276},{"_id":"source/img/b5a900fe460481976d0ae062ab18dd61.png","hash":"b16db3668f5827827935b826a4b71cb4b5fb497e","modified":1649061160277},{"_id":"source/img/ce0bdff067bf19a083c5d26b1266bb44.png","hash":"eb065c1639deba3fc3ea14639534090780cfa7ec","modified":1649061160277},{"_id":"source/img/clip_image002.png","hash":"773f0b16657a294e5b51f44c9adc7cf0d4813728","modified":1649061160278},{"_id":"source/img/d133e6d9b982b320594ea62168dbe462.png","hash":"2a8692dcfbac84a0f4c5c46f667f8a1a79f6dfbf","modified":1649061160280},{"_id":"source/img/d1a8d5d473881fffebc5baccd896258d.png","hash":"0e9c768626f3b23d79ed70766537e50b8d5b5b75","modified":1649061160280},{"_id":"source/img/d234cadc40e81da676a8b541d9cb6ab4.png","hash":"a068b49bba0ce76d72f9cb4f0de4a467cd19c2dc","modified":1649061160281},{"_id":"source/img/d6f2bab64eddad3ee5d092672e8fac50.png","hash":"fdae43fdb7a90fb5f79eb90db1b721e0e52e7485","modified":1649061160281},{"_id":"source/img/d325d29a.png","hash":"962a746a65c34d0e4ea2b5a3e9d23a30f1d35201","modified":1649061160281},{"_id":"source/img/d94cd34e.png","hash":"d29a3d8cbf07390c741d3b0a1706461fc85e6590","modified":1649061160282},{"_id":"source/img/d9710d39e895a6eb8210645c6e0b85e3.png","hash":"f7d1620e8c38aaa0e04eac4f7d2094446140da6f","modified":1649061160282},{"_id":"source/img/dataSource-behaviour-relative.png","hash":"702859e0f00906daf67b67515b0bd683148f8109","modified":1649061160286},{"_id":"source/img/df56c6c619c3e1efb6f6140d56f67bcd.png","hash":"8bfd7c4e974653323164bce9b25ceae8eeec2418","modified":1649061160286},{"_id":"source/img/e13f637bd584b1563442aeaece8cf1e4.png","hash":"8f5f563fc6ab2d276868eacfbed49e1427ba4af3","modified":1649061160287},{"_id":"source/img/e18c9709.jpg","hash":"06375b48ff834c994e894405ee00742bb12d962d","modified":1649061160287},{"_id":"source/img/ea179a09.png","hash":"145556ada619dcda0eb7f503930ea65ef73b3465","modified":1649061160288},{"_id":"source/img/eac62f3b0d93e90dd8f19789d013c1ec.png","hash":"4cc347bbb4a3ef2f28e03627654a09e2bede58b4","modified":1649061160289},{"_id":"source/img/edf97c87de27aae8adafce7f255f0870.png","hash":"b9134008d1a4b333d2d8d40cec0f6bacf90aa777","modified":1649061160289},{"_id":"source/img/es1.png","hash":"89281b1bef0d51742b32b70b774525c6ae08c95f","modified":1649061160290},{"_id":"source/img/es2.png","hash":"07f92f31d703b5659cf537a5b31b2ff9c55cbdc1","modified":1649061160290},{"_id":"source/img/f785ade6f77521bce18fc9f8825d4d9b.png","hash":"c412daa2e0104ffb4bccde4c3982bf0cfaf82d43","modified":1649061160291},{"_id":"source/img/f8f4bde6cb9aeac56366a2f3853f24f3.png","hash":"08664ed857a79c157500b7ceb85f7ed08bff0a15","modified":1649061160291},{"_id":"source/img/fac717a5.png","hash":"793349e13d727eef0ec51f496e78116cdb7604ca","modified":1649061160292},{"_id":"source/img/fb523dd6.png","hash":"cf674b0aa3f9fedd73a0e9501121aaeadce2f834","modified":1649061160292},{"_id":"source/img/ffd661fb15bf746863a6e57d7aca04c4.png","hash":"8fb35d52fae7bb70ccdea766c8478a73db870ea0","modified":1649061160293},{"_id":"source/img/floattype.png","hash":"a7c29f3c863dfe4b4f3ea2ccea6b35002b7eb7ba","modified":1649061160293},{"_id":"source/img/hdfs创建文件夹.png","hash":"27daebbd4cf39ca74ffa88aa9865257de46beeb2","modified":1649061160298},{"_id":"source/img/hive-聚合2.png","hash":"69ab607095185fb63a23851f371ac09c3261728a","modified":1649061160335},{"_id":"source/img/https-intro.png","hash":"14850fa8463ff0892e5258b5884b4e92cb66f481","modified":1649061160352},{"_id":"source/img/image-20201206115459406.png","hash":"b459dcbaa97aba5f099acae0f39145ad3daa61e5","modified":1649061160354},{"_id":"source/img/image-20201206115832025.png","hash":"83d8dac7f6b2b8c02031d46c5d9a8748d2242228","modified":1649061160357},{"_id":"source/img/image-20201206121627187.png","hash":"071a741e5d52a45d3ca56dccb5af33a190597f7f","modified":1649061160358},{"_id":"source/img/image-20201206121650739.png","hash":"2ad0274ed10bb42df9bddc574b430e023858e661","modified":1649061160359},{"_id":"source/img/image-20201206121711716.png","hash":"e70f981b0e2661a084fe730e068bc889b4efb7ac","modified":1649061160359},{"_id":"source/img/image-20201214114013294.png","hash":"b88b3083b0bf5a8ca307695fc265dd07fd2e9bd9","modified":1649061160363},{"_id":"source/img/image-20201214121332594.png","hash":"4ab43a767fa96e721de32a3084870e7ef785bc8a","modified":1649061160363},{"_id":"source/img/image-20201214121613146.png","hash":"0855b48f6f372b5f20905d273cc9d780ea2c19dc","modified":1649061160363},{"_id":"source/img/image-20201214121645728.png","hash":"fdea15c1fff089408871ff3150b2c8b6c107173a","modified":1649061160364},{"_id":"source/img/image-20201214121721100.png","hash":"feade0b22e88fe8bdf2d97cf7d939f39b3466735","modified":1649061160364},{"_id":"source/img/image-20201214122255720.png","hash":"0238a48f3dcd45892e5263bda2e67d455ecdc631","modified":1649061160365},{"_id":"source/img/image-20201214124300675.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1649061160366},{"_id":"source/img/image-20201214124603810.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1649061160367},{"_id":"source/img/image-20201214125020584.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1649061160368},{"_id":"source/img/image-20201214125032862.png","hash":"2d9a0431305510a118e7136455247f2918f68708","modified":1649061160369},{"_id":"source/img/image-20201214125453643.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1649061160370},{"_id":"source/img/image-20201214125504552.png","hash":"0a1ac24eef9da3dbdb800c4d46175abc60eba89a","modified":1649061160371},{"_id":"source/img/image-20201214125529023.png","hash":"82d57e9f373789317595d49aa2e379260acb5a9f","modified":1649061160371},{"_id":"source/img/image-20201214130757812.png","hash":"5d935f2e74081b23dff7c75ce43b3f088906ece6","modified":1649061160388},{"_id":"source/img/image-20201214131734289.png","hash":"f63b13d6700ce20d9e476e880b0ef1d7f230e249","modified":1649061160390},{"_id":"source/img/image-20201214131814678.png","hash":"e86681a9d954b4ab9ceeccef85949166532d22d7","modified":1649061160391},{"_id":"source/img/image-20201214131847691.png","hash":"8c841533dd0f0e9d2965695be830eac48c571f04","modified":1649061160391},{"_id":"source/img/image-20201214131948506.png","hash":"8eb5e9f7c9d9f5f47e19509c702632a8ee881711","modified":1649061160392},{"_id":"source/img/image-20201214132014480.png","hash":"692652f4e430312e6d6a3414304b0c64e46d6d36","modified":1649061160392},{"_id":"source/img/image-20201214132059091.png","hash":"0e9f8c4f95304e59cc8af6adff897b0656925bea","modified":1649061160392},{"_id":"source/img/image-20201214132127621.png","hash":"5e48bbea9baa60cb25cdff8a3b5ce28a0f0b935f","modified":1649061160393},{"_id":"source/img/image-20201214132215125.png","hash":"fedaacf9919b879a2539c2faacb748456b0b3e62","modified":1649061160393},{"_id":"source/img/image-20201214132309723.png","hash":"9ee9276322c6160b294b49127b0ccfdd31006d6a","modified":1649061160394},{"_id":"source/img/image-20201214132326043.png","hash":"e2fcc5164dcfdd9125fd0f2b2a785de9c8668c3c","modified":1649061160395},{"_id":"source/img/image-20201214132343909.png","hash":"90f9e923863dcdeddfe91f1794c42e904011d33d","modified":1649061160395},{"_id":"source/img/image-20201214132401207.png","hash":"29faa2ade6819a751e21d2ed734326369ba67afb","modified":1649061160395},{"_id":"source/img/image-20201214133601256.png","hash":"30359512fcf46fca69e040fd4446ea477b5c2646","modified":1649061160396},{"_id":"source/img/image-20201214132416329.png","hash":"06b6ecf11a65a2616a7ba9b317bc4efb0dd2fb9c","modified":1649061160396},{"_id":"source/img/image-20201214133955605.png","hash":"df38a2d88ed44358447df314b1b05cb762fb3e34","modified":1649061160400},{"_id":"source/img/image-20201214133935704.png","hash":"53cb6b400f9247b609d3d6df67a79f65eabe1f55","modified":1649061160399},{"_id":"source/img/image-20201214134132982.png","hash":"4f48f02bee07e0e6be8cd93a463a70750abf12f6","modified":1649061160401},{"_id":"source/img/image-20201214134208507.png","hash":"a2ad991b50538e8a070fd5b390cdafd800c3affd","modified":1649061160402},{"_id":"source/img/image-20201214135053884.png","hash":"f6c523e663d41c1df848333892389670db4b14e5","modified":1649061160404},{"_id":"source/img/image-20210430152414489.png","hash":"49aa3d7b1aeed9550e2eb2ea42a6bbb1ebac3b3e","modified":1649061160416},{"_id":"source/img/image-20210430152457125.png","hash":"36f6d86933771011c5adfcb2b098ad6893a71830","modified":1649061160418},{"_id":"source/img/image-20210430152537381.png","hash":"5ff6454780ab7bae49d550bb647c431b4e5234b5","modified":1649061160418},{"_id":"source/img/image-20210430152325572.png","hash":"2f5f4e5e6710ec4872e5091bb3aad4889afa2e88","modified":1649061160415},{"_id":"source/img/image-20210430152636683.png","hash":"905ed38af2aacba8e19191aa58526ebd809743fc","modified":1649061160419},{"_id":"source/img/image-20210718193314796.png","hash":"95d9d80eb4dfbc2ec4d7b5ca5714fcbb42211d2c","modified":1649061160420},{"_id":"source/img/image-20210718193618402.png","hash":"ad5bc245550044192daf0a3ac8ebcddd3eabdf63","modified":1649061160421},{"_id":"source/img/image-20210802000052243.png","hash":"773cc994b809af6b4c300fd9918e4804cb37c711","modified":1649061160421},{"_id":"source/img/image-20210802000308665.png","hash":"88ad18bf715e5340956b8e9a4d7b08a804cb1456","modified":1649061160422},{"_id":"source/img/image-20210802000916321.png","hash":"7a5d83a364c36cd420e825f6983a3ccafdd48ca9","modified":1649061160422},{"_id":"source/img/image-20220122144100590.png","hash":"ff791b62112e65c9d9546fe2032b8dfc9c9a21f7","modified":1649061160426},{"_id":"source/img/image-20211014175135307.png","hash":"6dd7a087cb13f69679d104c05b38ee6dd160c920","modified":1649061160425},{"_id":"source/img/image-20220404214709099.png","hash":"4b44b8a89ce9c00c456864af41182eca6c229182","modified":1649080035713},{"_id":"source/img/image-20220411221525635.png","hash":"dd9ba20d3024b2b81f44a7ae06be90f09d1d9767","modified":1649686529161},{"_id":"source/img/image-20220411222247494.png","hash":"4b98ed28d32456a22fcca21800d178bb01d3429a","modified":1649686971081},{"_id":"source/img/javaagent1.png","hash":"93c2b34bbbbb8ce1fccb0d0eb0daff97a7a2bdac","modified":1649061160430},{"_id":"source/img/java对象存储.png","hash":"78231e4a1c56b2078cdd4454d20593ba3e2b48e3","modified":1649061160431},{"_id":"source/img/java对象存储2.png","hash":"a5e540b6699183dcad89a9912c25c6a0c5eed9f3","modified":1649061160431},{"_id":"source/img/java对象存储3.png","hash":"4b83f6c1accd8ea2753e0dc16f52820d52abb700","modified":1649061160432},{"_id":"source/img/loc.png","hash":"55d7bc698c3f0394701a798ef22eb664c5fce7ed","modified":1649061160439},{"_id":"source/img/maven配置.png","hash":"c966dcfb8e36138b51cbaa08a8e15bc73a615778","modified":1649061160439},{"_id":"source/img/mysql排序1.png","hash":"09e8d6d507cd2b06ed4e6e9a76c832e226981ed0","modified":1649061160440},{"_id":"source/img/mysql排序2.png","hash":"0f0e1c1417f85b8e5f7bf1bb929ad63223569faf","modified":1649061160440},{"_id":"source/img/mysql排序4.png","hash":"9ef66bb2173d553fb58106a4959b3c0a5751d482","modified":1649061160442},{"_id":"source/img/mysql排序5.png","hash":"044b6bc8c3b4d33f3a25598b9b99771efe75bccf","modified":1649061160443},{"_id":"source/img/mysql排序6.png","hash":"254e269dc8da6b5358e883e82a16c52243a4286e","modified":1649061160443},{"_id":"source/img/nacos-producer.png","hash":"36eefd319d40a5a11e9343d3d817d66098d9d043","modified":1649061160445},{"_id":"source/img/nacos-springCloud1.png","hash":"afd0806eb1d72ed4988811e08d1c2e7cb621682b","modified":1649061160445},{"_id":"source/img/nacos1.png","hash":"07c9cfee060b4090c04d5a9d422b97e2dd1cbbd2","modified":1649061160446},{"_id":"source/img/nacos-springCloud2.png","hash":"6ce7c90123f1a1eb0e3ebbe93ae50eded7392603","modified":1649061160446},{"_id":"source/img/output_11_0.png","hash":"feae0728c34b976d209edf511796c8d86b374266","modified":1649061160447},{"_id":"source/img/output_11_1.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1649061160447},{"_id":"source/img/output_11_111.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1649061160448},{"_id":"source/img/output_13_011.png","hash":"00d6df80990d980af9b8d244faba51db58904ebc","modified":1649061160449},{"_id":"source/img/output_13_0.png","hash":"74537a7efb66b8f35c874c0b69c1d0282ab92eec","modified":1649061160448},{"_id":"source/img/output_13_1.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1649061160450},{"_id":"source/img/output_14_0.png","hash":"ec71387d1c4c05e7a86a33c13aa3a08568e75827","modified":1649061160451},{"_id":"source/img/output_13_111.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1649061160451},{"_id":"source/img/output_15_0.png","hash":"cc3a6ab17830f239b1e82500c967f6a3b40093f4","modified":1649061160451},{"_id":"source/img/output_15_011.png","hash":"d699a9e55c1d99653f37a36ab91be019a53e4bbf","modified":1649061160452},{"_id":"source/img/output_17_0.png","hash":"e4f13d81304ba2ac471c18b70558fc4cea417666","modified":1649061160452},{"_id":"source/img/output_19_0.png","hash":"d278d46ba923bcc6dfdbe29ca0d8547898ab1f2a","modified":1649061160453},{"_id":"source/img/output_22_0.png","hash":"9b8f87f0342ee9093359c92047f1438a4c474dff","modified":1649061160454},{"_id":"source/img/output_20_1.png","hash":"65f2cbd977d34618d64be151e40aa7c5dbe9ec9f","modified":1649061160453},{"_id":"source/img/output_26_1.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1649061160454},{"_id":"source/img/output_26_111.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1649061160455},{"_id":"source/img/output_29_0.png","hash":"e4c9e227e39a0e97a133e1b96b9577534ff3cb4c","modified":1649061160456},{"_id":"source/img/output_30_12.png","hash":"71900ff594f876e60656e61ec55b8550de208eea","modified":1649061160456},{"_id":"source/img/output_30_0111.png","hash":"03859e3478c86b4b495f49826e6a4f21f0507f52","modified":1649061160456},{"_id":"source/img/output_31_0.png","hash":"0850ffa53150038f984d9861c87bebcc127662c2","modified":1649061160457},{"_id":"source/img/output_32_02.png","hash":"45e2ff2fd81678595b7c33f27ce4efdb1727db53","modified":1649061160457},{"_id":"source/img/output_32_1.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1649061160458},{"_id":"source/img/output_33_1.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1649061160458},{"_id":"source/img/output_32_111.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1649061160458},{"_id":"source/img/output_33_111.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1649061160459},{"_id":"source/img/output_34_0111.png","hash":"e23bcc316f3646a1fec22cff049ce868c051e972","modified":1649061160459},{"_id":"source/img/output_34_1.png","hash":"219b0f0516b12045f827b0461e1be52eed5afa35","modified":1649061160460},{"_id":"source/img/output_35_02.png","hash":"02a51de4cf25a2a71bb9d9ecc7fecdcabb8cfa8f","modified":1649061160460},{"_id":"source/img/output_36_1.png","hash":"14241a13199df5a0b3daeb5ab3c628f9bca15763","modified":1649061160460},{"_id":"source/img/output_37_0111.png","hash":"753bede65e90d7c3b820c71b15f4a82160032400","modified":1649061160461},{"_id":"source/img/output_38_02.png","hash":"8fa952d47ced59bb033ed0c42177c4d52981f458","modified":1649061160461},{"_id":"source/img/output_40_02.png","hash":"13f4139b8d41242bb7cf7a0818676a6171d640f6","modified":1649061160462},{"_id":"source/img/output_40_0.png","hash":"a13785823254893b0bf7a2be92b5347507c44e01","modified":1649061160462},{"_id":"source/img/output_42_0.png","hash":"5f66e78fbf687a4e955d043ca3c949036f445847","modified":1649061160463},{"_id":"source/img/output_42_0111.png","hash":"5010a66d219e9f12a4817f5cd23a1fdac5004471","modified":1649061160463},{"_id":"source/img/output_44_0.png","hash":"8d3277c861b982ddcea78e95020eed54f412af3d","modified":1649061160464},{"_id":"source/img/output_44_0111.png","hash":"5901f5f0cfb80a132ab923a832ac5d524708d08a","modified":1649061160464},{"_id":"source/img/output_46_0.png","hash":"26fbd45ab4c82c99b073d1c940767c4732aa6de1","modified":1649061160465},{"_id":"source/img/output_47_0111.png","hash":"8d7ca8b0fb8554f174aa3d310c088ba8a584c86a","modified":1649061160465},{"_id":"source/img/output_49_12.png","hash":"4049cbcb428cf1f2578782f77aa1cfd01d79eb26","modified":1649061160465},{"_id":"source/img/output_50_0111.png","hash":"19ab454f3b4ff2130be35d478314093697cc09d7","modified":1649061160466},{"_id":"source/img/output_51_12.png","hash":"4fe6ba1d674d7706b8ffa7bdf05e333910977cbd","modified":1649061160466},{"_id":"source/img/output_52_0.png","hash":"0abbeb362227d28f260cef8812606514f6530b9c","modified":1649061160467},{"_id":"source/img/output_54_0.png","hash":"e889c6a42c4956042d69b1fa6c6bf75e60a9b41c","modified":1649061160468},{"_id":"source/img/output_55_0111.png","hash":"43f870cd8c8c6467b8aafa62ea3a77638cf59d99","modified":1649061160468},{"_id":"source/img/output_55_12.png","hash":"c87f337c935f80907db1fd33372939164a8bf857","modified":1649061160469},{"_id":"source/img/output_57_0111.png","hash":"43f870cd8c8c6467b8aafa62ea3a77638cf59d99","modified":1649061160469},{"_id":"source/img/output_57_12.png","hash":"5bd256fe115a1b86475088c8a3ce017342f4ef93","modified":1649061160469},{"_id":"source/img/output_59_0111.png","hash":"3f57a86a9bd422deb093e0fa348eca3fc77b7df6","modified":1649061160470},{"_id":"source/img/output_63_12.png","hash":"0dcfa074599c6faea3b7ef78585510c161149ace","modified":1649061160471},{"_id":"source/img/output_60_12.png","hash":"b27e851a7d17aa9228f12f573d66c2c37949217d","modified":1649061160471},{"_id":"source/img/output_65_0111.png","hash":"fd70b1f86f887a6903c2951cdd2e9b1bc7fa84e4","modified":1649061160472},{"_id":"source/img/output_65_12.png","hash":"7a6bef5a90b57fadd9db6f31597c6c35681df70d","modified":1649061160472},{"_id":"source/img/output_67_0111.png","hash":"778f9da3a8ffc41c71296abb4bff4a908ef43f3d","modified":1649061160472},{"_id":"source/img/output_68_12.png","hash":"a14c63cbb8d830f3d5915804e79c3d34042740d6","modified":1649061160473},{"_id":"source/img/output_70_12.png","hash":"166a28b2edc7a59fdfa924ea6153b8c3d9d7a7cf","modified":1649061160473},{"_id":"source/img/output_71_0111.png","hash":"5dc1be414946db6eee3ff4a77c93c8f3ec9a3d03","modified":1649061160474},{"_id":"source/img/output_72_12.png","hash":"9eab845425a9b5b82d2753eb247aaa3117304992","modified":1649061160474},{"_id":"source/img/output_74_12.png","hash":"9b5d3beb0d801e210800a7f9638fd6956760d20d","modified":1649061160475},{"_id":"source/img/output_75_0111.png","hash":"c1509330ce2fff34cef75b953b9bf54d873da3ce","modified":1649061160475},{"_id":"source/img/output_79_0111.png","hash":"56aa7a5ebb280611469a02e677e9bd22505175af","modified":1649061160476},{"_id":"source/img/output_79_02.png","hash":"8ff18b2f1a6243935a6b6acab1aa16a662f073e4","modified":1649061160476},{"_id":"source/img/output_81_0111.png","hash":"d2918818943b407b83cb8af7283f2f6465f91e85","modified":1649061160477},{"_id":"source/img/output_79_12.png","hash":"69276815c0942893eb4389a3aefd07e9811878be","modified":1649061160477},{"_id":"source/img/output_85_1111.png","hash":"5b19eb29e1fba305a9320bcb9cafe68724f7f8b5","modified":1649061160478},{"_id":"source/img/output_87_0111.png","hash":"afc5437df08acf2a7395626b2000a92f61f7eff7","modified":1649061160478},{"_id":"source/img/output_81_02.png","hash":"146b692db59101420b38903bb44bfced79689d8f","modified":1649061160477},{"_id":"source/img/output_89_02.png","hash":"ecd199e8ed0d919bf49d18a34ffc0fceaebbd8f8","modified":1649061160479},{"_id":"source/img/output_8_0.png","hash":"0bcb929ddd78eab93836703fc4f859dfeef6c3ae","modified":1649061160479},{"_id":"source/img/output_8_1.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1649061160480},{"_id":"source/img/output_8_111.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1649061160480},{"_id":"source/img/pasted-0.png","hash":"d4f20b5880ebe1cd4114af86e182d3a8042f29b6","modified":1649061160480},{"_id":"source/img/redis分布式锁实现原理.jpg","hash":"08e0920c4df0d033672d5900ec24235aa03bade6","modified":1649061160481},{"_id":"source/img/secondaryNameNode.jpg","hash":"6186b1ee9e43435bafb7abefde1824d5f66e8f8b","modified":1649061160482},{"_id":"source/img/user-log.png","hash":"f7bd7155b8a465ad32afb87f8ae386768833c64f","modified":1649061160495},{"_id":"source/img/wordcount.png","hash":"78dbf6fe038d27891bb860321f1087fa9f642e3c","modified":1649061160502},{"_id":"source/img/三次握手协议1.png","hash":"b7cf27e8ba8b7e299c4e02519394600d0269ae84","modified":1649061160503},{"_id":"source/img/三次握手协议2.png","hash":"149fa2d315b1471325a89b68ee659409c709c10c","modified":1649061160503},{"_id":"source/img/使用协议进行通讯.png","hash":"fe93735a4d871a544feb31b0b713f98ab4c715fb","modified":1649061160509},{"_id":"source/img/信任链.png","hash":"23517850fc98c07ad66d80bac4556b77e0688214","modified":1649061160509},{"_id":"source/img/公钥私钥6.png","hash":"c7104a322604a0b51de2d988bd9522b5deab2045","modified":1649061160520},{"_id":"source/img/公钥私钥8.png","hash":"6b13644b98068f7f44ddb51532f17030906031c0","modified":1649061160521},{"_id":"source/img/加密算法.png","hash":"5a0a4d8fbdc43f929dd5aa86aded45086ba38cbb","modified":1649061160523},{"_id":"source/img/可信根.png","hash":"875a1cdae0d77993f07a1df1dd968f907a2c1d1e","modified":1649061160525},{"_id":"source/img/对称加密算法.png","hash":"6a7ed179fb8b48e9054caba56308b1691b6ebe92","modified":1649061160533},{"_id":"source/img/数据库分布式ID生成.png","hash":"7376ea76ee62cc47d2e0389f90ca9fa1a3173f74","modified":1649061160541},{"_id":"source/img/线程相关1.jpg","hash":"cd2dfeb2b0a367d208e46d1fec4ed9241164256e","modified":1649061160544},{"_id":"source/img/线程相关4.jpg","hash":"3268689c6020136cda73d95d1553dfa6816aa60b","modified":1649061160545},{"_id":"source/img/线程相关3.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1649061160545},{"_id":"source/img/线程相关2.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1649061160545},{"_id":"source/img/线程相关5.jpg","hash":"54f4bd928b107b4da2faaa87fa74484b76900c0b","modified":1649061160546},{"_id":"source/img/锁的创建.png","hash":"ac72c945c263be025d83970d2cfdb6c240c38bbc","modified":1649061160553},{"_id":"source/img/锁的创建2.png","hash":"ebf05f8b81c02d9c880b9081e9ccc0b2b6c5e4ae","modified":1649061160553},{"_id":"source/img/阻塞IO.png","hash":"f5cfd961b871078b9b202d1e8d4810f3f126aaef","modified":1649061160554},{"_id":"source/img/雪花算法.png","hash":"522b23e1a5bc4bdf46b1d285233d815004847271","modified":1649061160557},{"_id":"source/img/1337b059.png","hash":"934fedad21bc55fd4848e8478221aa8de7c30ee7","modified":1649061160185},{"_id":"source/img/1618244073306.png","hash":"c23f3b75c0e00674c191e398da326fbabfdc1155","modified":1649061160188},{"_id":"source/img/1618244002479.png","hash":"476a48c86fe9853e6e3814d62967932180e87aac","modified":1649061160187},{"_id":"source/img/1618244193984.png","hash":"5f54d9d4ca1822d23caf1b86f07d11775d996d3c","modified":1649061160191},{"_id":"source/img/1618244361109.png","hash":"565b7cfe75ec1b5d599bdaa2be09166c5ffb7275","modified":1649061160193},{"_id":"source/img/1618244592782.png","hash":"4f2779efd3450377236256ecf7cc6c7525fcf57c","modified":1649061160197},{"_id":"source/img/1618677203153.png","hash":"c7704c40b99c71e1d8060502a7bab46eda7bb7a6","modified":1649061160219},{"_id":"source/img/7f3f75ca.png","hash":"3af248ce70e75081d255e4eaa66dd9e2aba6b5bb","modified":1649061160227},{"_id":"source/img/9cb319ab.png","hash":"7a7cd79948bc7091586b5b20a61aa70de5ce1827","modified":1649061160232},{"_id":"source/img/GETPOST.png","hash":"2a4f00d2d7cf57f885f347c1f6cdd5d0f477408c","modified":1649061160237},{"_id":"source/img/HDFS-liucheng.png","hash":"b37eef3d1e61aeaf650ea9860b40acff7c5fc1e4","modified":1649061160238},{"_id":"source/img/HDFS上传.png","hash":"7ca87724851bc20365821ed02ffcbe9de0768a0c","modified":1649061160239},{"_id":"source/img/HTTPS2.png","hash":"b792dee35f602f38fce149164dad4593c64a9650","modified":1649061160240},{"_id":"source/img/HTTPS3.png","hash":"e3e58b8e4961a4533ac2a63154aee3c49322e379","modified":1649061160241},{"_id":"source/img/HTTPS4.png","hash":"98c925d3de7c588950cb1ce2be36b3346a0ee848","modified":1649061160242},{"_id":"source/img/HTTPS5.png","hash":"7f96023634200693631288ec5047f024ee53f91c","modified":1649061160243},{"_id":"source/img/IO复用select模型.png","hash":"9f0c021b9b258025552f261e3ab8cbb2f96093ad","modified":1649061160247},{"_id":"source/img/QQ.jpg","hash":"6f463db364ede8033b209c855f15a4d7a15bca97","modified":1649061160250},{"_id":"source/img/Spark.png","hash":"ab45bdfcdd54f893ab046360fea5ccd51b887b3f","modified":1649061160254},{"_id":"source/img/StringUpdate.png","hash":"1cad86bc5aa015dfba1e0929e4dd98c708401011","modified":1649061160258},{"_id":"source/img/TCPIP模型.png","hash":"69c031664c0698c88a8b5d40b7b8e7e7afc53748","modified":1649061160260},{"_id":"source/img/TCP协议通讯过程.png","hash":"46efae085ca09e1445b9eac506c02c3462e2a16b","modified":1649061160261},{"_id":"source/img/ThreadLocal内部存储.png","hash":"1bcf081078acb1a514dd19ef48b740663837e899","modified":1649061160266},{"_id":"source/img/agent-costtime.png","hash":"1659bf6791dac773e8eb6d5ee3357b79d8702c12","modified":1649061160273},{"_id":"source/img/agent-costtime2.png","hash":"657b28cdec63d882c44e6010a7a78fdffbd6c186","modified":1649061160274},{"_id":"source/img/chartype.png","hash":"965a1655bf9364c0ec0b7a8773c74d8ce9d91805","modified":1649061160278},{"_id":"source/img/clip_image004.png","hash":"762418fea78372f2c8b67fc6f4dffb7045e9d449","modified":1649061160279},{"_id":"source/img/data-collect-analysis.png","hash":"19e4706ad65dfa32bf285624af839d4437db17df","modified":1649061160284},{"_id":"source/img/data-collect.png","hash":"f46c55acbc3e188b8c9cce341fb61d2db1027f31","modified":1649061160285},{"_id":"source/img/hdfs-read-file.png","hash":"861f90f65c5c1eac3ffea16430c75833aff5e089","modified":1649061160295},{"_id":"source/img/hive数据结构.png","hash":"6d0a423af256a0aecb19cd1ef7d34f4eb327efde","modified":1649061160344},{"_id":"source/img/hive数据结构1.png","hash":"1251b48e87a11c926b56969b3de0db1077627a3a","modified":1649061160345},{"_id":"source/img/hive文本文件数据编码.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1649061160346},{"_id":"source/img/image-20200117113506023.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1649061160353},{"_id":"source/img/image-20201206115600801.png","hash":"793cd10108ad7b33685387becb060e215a2eb045","modified":1649061160355},{"_id":"source/img/image-20201206115738831.png","hash":"03a84a42d3f64d15ac10a3d3449e554dc184be28","modified":1649061160357},{"_id":"source/img/image-20201206122126739.png","hash":"57cc146ead1c09f0fb6d2e54ad6f3bec553db218","modified":1649061160362},{"_id":"source/img/image-20201214123508379.png","hash":"36aa63f8051ab7294256ea771bdde1930d350db3","modified":1649061160366},{"_id":"source/img/image-20201214125402694.png","hash":"c5023ea858552e39671ae6d0f4aeef4189cc1801","modified":1649061160370},{"_id":"source/img/image-20201214131527522.png","hash":"9cb523d61f7f7178a933e7cee84a538d729a4026","modified":1649061160389},{"_id":"source/img/image-20201214131543066.png","hash":"00c552d3184a69128eb6b263119cb1c5cf6976ed","modified":1649061160390},{"_id":"source/img/image-20201214132254636.png","hash":"c580027a831986c0f1ada1ec7cf5e47726a29d99","modified":1649061160394},{"_id":"source/img/image-20201214133612299.png","hash":"c8f1d908f463400ae615da444654759a79c9511b","modified":1649061160397},{"_id":"source/img/image-20201214133830361.png","hash":"eb52ab258a8bf959e9402b5abf071c9d3db1a2cf","modified":1649061160398},{"_id":"source/img/image-20201214134235050.png","hash":"da065bb97d038439d13aca2178dc7f204d397c9f","modified":1649061160403},{"_id":"source/img/image-20220404214514222.png","hash":"19fdd9c15030ead0bf3a34a60cdf87496b26f3ec","modified":1649079924942},{"_id":"source/img/image-20220411212506831.png","hash":"a3299283b9a40bc1f6d0de15ae04c7c6b82334bc","modified":1649683518016},{"_id":"source/img/image-20220411220512656.png","hash":"46ca2fb0784c19b31370ef0ed8867bd0e7930e04","modified":1649685977306},{"_id":"source/img/inttype.png","hash":"e01f1199570a67cd2ae26c3012227718e2575056","modified":1649061160429},{"_id":"source/img/jvm1.png","hash":"3cd098688ff2de0e071bc49739e257c18d0046af","modified":1649061160433},{"_id":"source/img/jvm2.png","hash":"6839ef36387dfa9daccb9d6395f486f9c6e73a4f","modified":1649061160434},{"_id":"source/img/jvm3.png","hash":"e3b1fd453e531b527b569c4e28eaa04776f28fd5","modified":1649061160435},{"_id":"source/img/mysql时间存储.png","hash":"1db17e59e7b6dad480009153803c159d5a97cbfe","modified":1649061160444},{"_id":"source/img/mysql的ip存储.png","hash":"d06d3619438f57be43aa1236ec9ace4e1bf3d126","modified":1649061160444},{"_id":"source/img/transaction.png","hash":"787175d99cc453a5ff1692029e2043060b1a7514","modified":1649061160491},{"_id":"source/img/user-behaviour.png","hash":"b0ce3ff640e61461a545a39c56d01a3b6447a68a","modified":1649061160494},{"_id":"source/img/typetrans.png","hash":"485490a772062455bc238936c0607d48d4aa93b7","modified":1649061160493},{"_id":"source/img/weixin.jpg","hash":"697f60350a05f53228bbb88505e79007e97b2fbd","modified":1649061160496},{"_id":"source/img/wordcount-map.png","hash":"a1c10dc70e0fdef09e3db7f305e16cd2fdf631a1","modified":1649061160497},{"_id":"source/img/wordcount-split.png","hash":"27610016538c3dca06409dcfbd57f8a805f86624","modified":1649061160501},{"_id":"source/img/公钥私钥1.png","hash":"498ea1f36ce52b0ff2964c9423bf5eb5ab0ae804","modified":1649061160510},{"_id":"source/img/公钥私钥10.png","hash":"767005c089b3be3b91213d12fab4e462718a7787","modified":1649061160511},{"_id":"source/img/公钥私钥11.png","hash":"10755e3bcfc39d9bf3c8b9b5060ec5682a18cad3","modified":1649061160512},{"_id":"source/img/公钥私钥13.png","hash":"afd0297e43e61b9193b6dcbf9a71da610938dd2f","modified":1649061160515},{"_id":"source/img/公钥私钥2.png","hash":"c60ac4f1a39665d9a2b923f3a78541fc663b0e6b","modified":1649061160516},{"_id":"source/img/公钥私钥3.png","hash":"63b5f3e415343c2a65d3274fe8d6e4ce4c26f490","modified":1649061160517},{"_id":"source/img/公钥私钥4.png","hash":"683543865f578ea89be62b1af374df4de0f42069","modified":1649061160518},{"_id":"source/img/公钥私钥5.png","hash":"b2721631bbbac0306b12e70466fa49531f8bae30","modified":1649061160519},{"_id":"source/img/公钥私钥7.png","hash":"e7f712acb8d5e47d717f18494d4c5e22ea9b5468","modified":1649061160521},{"_id":"source/img/公钥私钥9.png","hash":"4c4d25e78aea05b34ccf9cd0c5025cfcb7bf94e3","modified":1649061160523},{"_id":"source/img/基于Zookeeper分布式锁.png","hash":"897ebe02592f747426bbcfd26748deb812c06845","modified":1649061160532},{"_id":"source/img/指针压缩2.png","hash":"2db8b3b15563eedb0a35b192c37760fec8f9c8fa","modified":1649061160538},{"_id":"source/img/指针压缩3.png","hash":"f142f7d0e15d58e03d02d99c0eba541e51e30a7f","modified":1649061160539},{"_id":"source/img/指针压缩4.png","hash":"945b359ce34488105eab6422d8f397d8c76b4ea0","modified":1649061160540},{"_id":"source/img/非对称加密算法.png","hash":"260798ec83168388a70e55c60b1a7e5a47e78246","modified":1649061160557},{"_id":"source/img/1618244137853.png","hash":"90a6d21417b376515ad5d1652f85eba0d61545c2","modified":1649061160190},{"_id":"source/img/1618244544994.png","hash":"92f81e311f76ad0ac43170b8629248563e2af10f","modified":1649061160196},{"_id":"source/img/1618244623527.png","hash":"813c175d735bb6274abc069750ff9bda641937b7","modified":1649061160199},{"_id":"source/img/1618676461026.png","hash":"a240beb2b863869fb5effabc3bdbb15318e3dedf","modified":1649061160215},{"_id":"source/img/1618676526483.png","hash":"b69356b7ddec7edff861a27275eb442d86ef569f","modified":1649061160219},{"_id":"source/img/DNS2.png","hash":"0e8ca00be831d63e2ba6f36efc36dac00b590330","modified":1649061160236},{"_id":"source/img/HTTPS1.png","hash":"2cc9b05768fb05bd25da12dd3428e6f02dfe50ec","modified":1649061160240},{"_id":"source/img/SpringBean3.png","hash":"2b96172eecc1cbd1e41ab9756c0a12836ebad948","modified":1649061160256},{"_id":"source/img/hdfs.png","hash":"c3be7dde1cdd24c4765a87ddf07c75c70148e970","modified":1649061160298},{"_id":"source/img/hive-算数运算符.png","hash":"54c7db33bbd7b5b8484727d7a988869609539d4c","modified":1649061160304},{"_id":"source/img/hive-聚合3.png","hash":"e3efe8d25521a72b1180deab85b38de9ecda0f5f","modified":1649061160337},{"_id":"source/img/image-20220123235957505.png","hash":"2a916b7c8f4af37631784e40598772055eb1bba1","modified":1649061160427},{"_id":"source/img/image-20220322221741703.png","hash":"d8f0c9fb1fbfeb3eaf3b4e95c1655d1fef071d22","modified":1649061160429},{"_id":"source/img/mysql排序3.png","hash":"a91f727d4c99fb6289678a86f64ddd1c246568c7","modified":1649061160442},{"_id":"source/img/spark+hdfs.png","hash":"5ea801d62e8a34b8f08c627826d7f71369c2b749","modified":1649061160488},{"_id":"source/img/可信在云平台的基础架构.png","hash":"b507acb0ecbe5f39f783b7a82340a30bb6fd42d4","modified":1649061160525},{"_id":"source/img/椭圆曲线算法的基本原理.png","hash":"5f848a66630c6ba515286fa96dc36ed3a9a11106","modified":1649061160542},{"_id":"source/img/非阻塞IO.png","hash":"58573268ff130afca209099ecda84d1c4eeff17f","modified":1649061160559},{"_id":"source/img/1618675987731.png","hash":"d60ff3303b0726d472ec4315f02b2c483fc9b5c4","modified":1649061160203},{"_id":"source/img/1618676025236.png","hash":"a5b39d633936bf8ef0298c709a41e84585edfe36","modified":1649061160205},{"_id":"source/img/1618676280257.png","hash":"03a393d5e007b461c6d4e1b15519f14ce9809b5c","modified":1649061160210},{"_id":"source/img/1618676358941.png","hash":"38fca4b60911868d060fd5de519bddbf83bcc047","modified":1649061160212},{"_id":"source/img/1618676411155.png","hash":"9ce76fc1e2821c68a6a46a587604e6bcd470b85a","modified":1649061160213},{"_id":"source/img/1618676506490.png","hash":"c7d7ddfeebc07dd8c245f27161f90d894d5b1e30","modified":1649061160217},{"_id":"source/img/HTTPS6.png","hash":"be9aa6a1578dd12e1970db4731f7c9727f9bc17e","modified":1649061160245},{"_id":"source/img/Hive-运算2.png","hash":"1422e43a9e0aba7dac5ea4b6eecddbf6adc6bfe7","modified":1649061160247},{"_id":"source/img/TCP协议通讯过程2.png","hash":"9ae8696d16fd4ffdc46b6c005b598999dec3f9c1","modified":1649061160265},{"_id":"source/img/Yarn.png","hash":"8b15f23ee56b0f7e2e5aa00157c1de50a0979da7","modified":1649061160270},{"_id":"source/img/image-20201206115653987.png","hash":"6ecd4c60466dc5e4278f30761e2354568150992a","modified":1649061160356},{"_id":"source/img/image-20201214134902337.png","hash":"5d56cc52e66348ca9f9cc41c1b7b93fa30659f29","modified":1649061160403},{"_id":"source/img/simpleDateFormat-alibaba.png","hash":"f45845a64160deda103dcca59d212712bfe544e6","modified":1649061160486},{"_id":"source/img/spark-all.png","hash":"d52bb136b90c8f63acaba9398811dbf1642de23f","modified":1649061160490},{"_id":"source/img/公钥私钥12.png","hash":"fdadf38dfcfd50d4afb6939f7403946ebcf1217b","modified":1649061160514},{"_id":"source/img/8ab72b98.png","hash":"04732de500f3fc10dada0c0c2b2f35004c87624b","modified":1649061160230},{"_id":"source/img/DNS1.png","hash":"6ab8d7e53c1f65db2993710c9974276b7c6724fe","modified":1649061160234},{"_id":"source/img/hdfs-write-file.png","hash":"3482163375f44f53c6ff7791e005d5ebc42bf0f8","modified":1649061160296},{"_id":"source/img/hive-数学函数.png","hash":"e15b0006e6290c17b2348d0840272b52be732e0e","modified":1649061160303},{"_id":"source/img/hive集合数据类型.png","hash":"3803153d332cc20f22be920a84a9e371617099f3","modified":1649061160352},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","hash":"ec45ff26518a046bd0f6274a8f7317222f05ea81","modified":1649061160508},{"_id":"source/img/基于ETCD实现分布式锁分析.png","hash":"77d619b33b524e8e014ec8c0a74413493a05313e","modified":1649061160531},{"_id":"source/img/指针压缩1.png","hash":"d26943d292b23b4cf544cee8b1a73aa4172a8920","modified":1649061160538},{"_id":"source/img/混合加密的方式.png","hash":"45b8a92df2e3c57f6af2d483c444a96e59929843","modified":1649061160544},{"_id":"source/img/读写锁.png","hash":"f5c76702b3a276fc48835d2ad1a02859a4767e08","modified":1649061160553},{"_id":"source/img/1618676232016.png","hash":"54b768ddac2305c283ed547ed5fcc1e1e5620d0d","modified":1649061160209},{"_id":"source/img/jvmHeapStructure.png","hash":"002db89529955be1ab4647f6628bcb41474c9ec6","modified":1649061160438},{"_id":"source/img/四次挥手协议.png","hash":"112d408966bbd5f77373bda95aaa07fd952e23d4","modified":1649061160526},{"_id":"source/img/阻塞与非阻塞调用对比.png","hash":"cdee42373e7940d8f09575bf844ce08f49b4d1af","modified":1649061160556},{"_id":"source/img/1618295759372.png","hash":"369d456db024815c115664ca926d0356e0d9af2d","modified":1649061160201},{"_id":"source/img/TCP协议通讯过程1.png","hash":"100f6ea49b8b585a10e0ac88e2486a902e221a5c","modified":1649061160263},{"_id":"source/img/hive-partition.png","hash":"c587e39d656360af68418b82172cf91d1b0e7cd2","modified":1649061160301},{"_id":"source/img/hive-运算3.png","hash":"5bc45de4ce61992130c1c3636d2031969dd81cb1","modified":1649061160343},{"_id":"source/img/wordcount-reduce.png","hash":"99f7334d973e7a4ffaf5d34e7870b55d0016858b","modified":1649061160500},{"_id":"source/img/hive-表生成函数.png","hash":"cf8bb24a3e4d1263993cc3df54d7777a3931eb64","modified":1649061160340},{"_id":"source/img/image-20201206121829515.png","hash":"c5523dda003b29efcb8231211c4d2045d61b0dec","modified":1649061160362},{"_id":"source/img/1618676190337.png","hash":"e6122eb7c6daa185cec1e8ad3a5e96b8ca2b7c2a","modified":1649061160207},{"_id":"source/img/hive运算1.png","hash":"472120109358e0ec58e980b9d0fc9ab533764d4c","modified":1649061160349},{"_id":"themes/hexo-theme-3-hexo/LICENSE","hash":"b04140c5f682db2b300428f97bb164fd7f5f18bd","modified":1649061160560},{"_id":"themes/hexo-theme-3-hexo/.gitignore","hash":"46eca80fe689a00cbe4d015c094702af54119021","modified":1649061160560},{"_id":"source/img/对象存储1.png","hash":"281b0b0c7d4c55df4000800b3f00888663e5ee25","modified":1649061160536},{"_id":"themes/hexo-theme-3-hexo/README.md","hash":"19b8cfe6690c28427492f342e74dda5ed49a1664","modified":1649061160560},{"_id":"themes/hexo-theme-3-hexo/languages/en.yml","hash":"616e02c035c86033ab4a97c5ae9e0a9e5f0b8ea3","modified":1649061160561},{"_id":"themes/hexo-theme-3-hexo/_config.yml","hash":"2552ccd708619e8130de946314a42167dac83d45","modified":1649061160561},{"_id":"themes/hexo-theme-3-hexo/languages/zh-CN.yml","hash":"83633d45420c96dfac41333aeac3f3616dca5286","modified":1649061160561},{"_id":"themes/hexo-theme-3-hexo/layout/index.ejs","hash":"1c185288c2925a652d577965626718e12df07f65","modified":1649061160569},{"_id":"themes/hexo-theme-3-hexo/layout/indexs.md","hash":"b03a3f2ca61eeadb6d02c949f64fc65dd2f7c99a","modified":1649061160569},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/article_copyright.ejs","hash":"9e1cdec49d5b9b44399348d96ecd7331f3ee7d85","modified":1649061160563},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/article.ejs","hash":"9e5afcc26f47f93c165072b0a2b5cbf72efb7ef9","modified":1649061160562},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comment.ejs","hash":"d18f94e04ef0cf7abb432a8e707ccb3abc7fe435","modified":1649061160563},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/dashang.ejs","hash":"b2a01cc1f0326965f0a186ce3c9b3c991fd4e2c9","modified":1649061160565},{"_id":"themes/hexo-theme-3-hexo/layout/post.ejs","hash":"a0eaba41e7ec9db5843af482470a45531049b457","modified":1649061160569},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/copyright.ejs","hash":"4c09f47e899fe36bfe36d92b12996219c2b5f622","modified":1649061160565},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/footer.ejs","hash":"9087af9647a87c3fa9ef87632de5427ba4abe9c4","modified":1649061160566},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/friends.ejs","hash":"e6dd90be668195016d6e1c51a6baefb50676e6ab","modified":1649061160566},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/full-toc.ejs","hash":"60a085fab3165ea1fc6370abac0bd6ab1b2f2510","modified":1649061160566},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/header.ejs","hash":"1e04b617fe38acca8b3d3774c5dbfcb74a02db6b","modified":1649061160566},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/mathjax.ejs","hash":"e2be0e37f3d48e63e65a47d819bfb800b9aa3784","modified":1649061160567},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/meta.ejs","hash":"ab6329ddd908b0567c18f39ac6a8553c6fec67c5","modified":1649061160567},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/nav-right.ejs","hash":"7942c661b48e15fced4a97acf86fac7fea013378","modified":1649061160568},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/nav-left.ejs","hash":"f3cc395fbb4e308776a38e369faefbc9e5891807","modified":1649061160567},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/toc-ref.ejs","hash":"33f7a4bfca1bb9835ec8f0d1e73188d1f56cc8b9","modified":1649061160568},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/tag.ejs","hash":"8704e6bd833d270cc6a494d4e7cf1dfeddedba40","modified":1649061160568},{"_id":"themes/hexo-theme-3-hexo/source/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1649061160585},{"_id":"themes/hexo-theme-3-hexo/source/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1649061160587},{"_id":"themes/hexo-theme-3-hexo/source/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1649061160587},{"_id":"themes/hexo-theme-3-hexo/source/css/mobile.styl","hash":"1c2f8b7d7cf46f219adb3a628bdf380f29ff4a6b","modified":1649061160584},{"_id":"themes/hexo-theme-3-hexo/source/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1649061160588},{"_id":"themes/hexo-theme-3-hexo/source/css/gitalk.css","hash":"3dc58e9a3fd63a3144d5fe850eb55e3dc885c9fb","modified":1649061160578},{"_id":"themes/hexo-theme-3-hexo/source/css/style.styl","hash":"29fa7f6619519c2dcfec4efac4314c5af659a92a","modified":1649061160584},{"_id":"themes/hexo-theme-3-hexo/source/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1649061160595},{"_id":"themes/hexo-theme-3-hexo/source/js/jquery.pjax.js","hash":"8c2a4f10a4da3d9615a3a81542494c6d21479b3d","modified":1649061160596},{"_id":"themes/hexo-theme-3-hexo/source/js/search.js","hash":"788c610149a5f9361295f9f0207c8523f37ddb8b","modified":1649061160597},{"_id":"themes/hexo-theme-3-hexo/source/js/script.js","hash":"36275888d57fecb6afd2c6f9291c46c2b3894ac5","modified":1649061160596},{"_id":"themes/hexo-theme-3-hexo/source/js/titleTip.js","hash":"7299ac046ddd6e6a4267d435f7b4c8198baaaccc","modified":1649061160597},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/click2show.ejs","hash":"05b09c45b379ffeb4f48c1604044d88829f90799","modified":1649061160563},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/disqus.ejs","hash":"32ce7b48d366b9c888ff2ceb911a3cd82f864537","modified":1649061160564},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/gitalk.ejs","hash":"01567e010cf4f2dd141fe2019490d3f0d5aa2529","modified":1649061160564},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/livere.ejs","hash":"2d115e79cadedc2d5d8f4b5618559640d986e01f","modified":1649061160565},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/gitment.ejs","hash":"eaf2b6f297282606b630ad55fb9e38af7e2829dc","modified":1649061160564},{"_id":"themes/hexo-theme-3-hexo/layout/_partial/comments/utteranc.ejs","hash":"8f2d4f42fbad351677c82e72420224587a5bd666","modified":1649061160565},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/comment.styl","hash":"d5fa333970a2eac66937d42eeb16fdb362e121ed","modified":1649061160570},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/fade.styl","hash":"02c7510a26f306e240f23ddbf772a69be2c890dd","modified":1649061160571},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/dashang.styl","hash":"f0eac1dc1f5dbed1769d032bb5fd5f002faaee26","modified":1649061160571},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/font.styl","hash":"3db01e603985e6dbcacb6b0f13dbd804f5849e3c","modified":1649061160571},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/full-toc.styl","hash":"9a732af065d0a80c9e420934be0f3582bf0129dc","modified":1649061160572},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/nav-left.styl","hash":"0a067ced25025000aa33c8f5017c87fff0971378","modified":1649061160572},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/nprogress.styl","hash":"2620a02169a6aeb75137fd368eac2c36423d8498","modified":1649061160573},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/post.styl","hash":"f1251e2a3b5334af3a22b51fc0293c2456568b50","modified":1649061160573},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/num-load.styl","hash":"f7ef35459ece22e1da950b86126be1c2bfe97fcf","modified":1649061160573},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1649061160574},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.svg","hash":"b5e7562c8494b0ddb3a70ecc5545ef7340d8e971","modified":1649061160574},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1649061160575},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1649061160575},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1649061160575},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.svg","hash":"3630aabf2f9c0417f483ebd03d9e429dbc2594e0","modified":1649061160576},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1649061160576},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1649061160577},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1649061160577},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/atom-dark.styl","hash":"f3eb4e5feda9cbd6242ccf44ca064e2979b5d719","modified":1649061160578},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/atom-light.styl","hash":"553987211d3323a7dfc0b08786b183a3435978c9","modified":1649061160579},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/brown-paper.styl","hash":"03af387edcc1cf8c18d12e9c440fd51b6cf425b6","modified":1649061160579},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/darcula.styl","hash":"2bfc14f27ccca108b4b3755782de8366e8bd001e","modified":1649061160579},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/github-gist.styl","hash":"5e05b19832c1099bd9d284bc3ed00dc8a3d7ee23","modified":1649061160580},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/github.styl","hash":"53276ff1f224f691dfe811e82c0af7f4476abf5d","modified":1649061160580},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/gruvbox-dark.styl","hash":"315ad610d303caba9eac80a7d51002193a15478a","modified":1649061160581},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/gruvbox-light.styl","hash":"1bece084b1dbbbd4af064f05feffd8c332b96a48","modified":1649061160581},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/kimbie-dark.styl","hash":"e9c190f9ffc37a13cac430512e4e0c760205be4a","modified":1649061160581},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/kimbie-light.styl","hash":"0c3ccd0d64e7504c7061d246dc32737f502f64e4","modified":1649061160582},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/railscasts.styl","hash":"a6e8cfd2202afd7893f5268f3437421e35066e7b","modified":1649061160582},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/rainbow.styl","hash":"e5c37646a9d9c1094f9aab7a7c65a4b242e8db00","modified":1649061160582},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/sublime.styl","hash":"501d75ef0f4385bea24d9b9b4cc434ba68d4be27","modified":1649061160583},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/school-book.styl","hash":"51659351b391a2be5c68728bb51b7ad467c5e0db","modified":1649061160583},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/sunburst.styl","hash":"2aa9817e68fb2ed216781ea04b733039ebe18214","modified":1649061160583},{"_id":"themes/hexo-theme-3-hexo/source/css/hl_theme/zenbum.styl","hash":"92941a6ae73b74f44ad7c559c5548c44073c644a","modified":1649061160584},{"_id":"source/img/hive-聚合1.png","hash":"070d7feb1ef4626223019a5b205d43b24116dc4a","modified":1649061160334},{"_id":"source/img/select、epoll模型对比.png","hash":"cdc93cacbedcf65fb68878bd36da3929e6333e2b","modified":1649061160485},{"_id":"themes/hexo-theme-3-hexo/source/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1649061160586},{"_id":"themes/hexo-theme-3-hexo/source/img/avatar.jpg","hash":"525dc2b6ef38fee9d4c66e554f928934eadd9117","modified":1649061160587},{"_id":"themes/hexo-theme-3-hexo/source/css/_partial/nav-right.styl","hash":"588b75e3b83ed95e526154bf3c0336c6f33e2be7","modified":1649061160572},{"_id":"themes/hexo-theme-3-hexo/source/css/fonts/selection.json","hash":"b6456a4eabcffd95e822d1d7adce96da524d481a","modified":1649061160577},{"_id":"themes/hexo-theme-3-hexo/source/img/weixin.jpg","hash":"59cf33d0f6ce9324dabe183f3d4551620959e987","modified":1649061160589},{"_id":"source/img/网络连接模型.png","hash":"f2f48effb8cb134f597011fda639932ea0d2fac2","modified":1649061160551},{"_id":"source/img/Socket通讯模型.png","hash":"2e260e5f30b7e17b7dba072e65ac7860e8d7e6ce","modified":1649061160253},{"_id":"themes/hexo-theme-3-hexo/source/js/gitment.js","hash":"67984b83cd46ff4300d4fd959bf6c17dd66b4136","modified":1649061160595},{"_id":"source/img/image-20210807123832574.png","hash":"789320ecd88502d6113ee4955a1e3fcdaf5b2cc8","modified":1649061160425},{"_id":"source/img/三次握手协议3.png","hash":"e6fe59d8473f94be2b361f4bda5be7d791022eec","modified":1649061160506},{"_id":"source/img/四次挥手协议2.png","hash":"1a4890549075feacfa4010df20911537fc726e4b","modified":1649061160529},{"_id":"themes/hexo-theme-3-hexo/source/js/gitalk.js","hash":"a95b598d998c4723f978ed21614127150075bf40","modified":1649061160593},{"_id":"source/img/image-20210415193556837.png","hash":"56b313e3e79284892d99656549cc4b7f0335e296","modified":1649061160414},{"_id":"public/2022/04/11/mysql简单主从搭建过程/index.html","hash":"953d75c9b3d1b444545afff0fe1abddc84aa800c","modified":1649719938072},{"_id":"public/2022/04/11/docker镜像部署到k8s集群/index.html","hash":"46fada183ea4d738ca063ffaea353818e0bb73b7","modified":1649719938072},{"_id":"public/2022/04/08/JDK并发包常用类/index.html","hash":"fa09ea4578660a565e52498a365a88700e839dd8","modified":1649719938072},{"_id":"public/2022/04/04/k8s集群搭建/index.html","hash":"b97ac1e538d1759fc01ea48ead0d3c2b51d997a3","modified":1649719938072},{"_id":"public/2022/03/22/MinIO单机安装以及使用/index.html","hash":"86cd2ea7bbe9016a4a0b5dbe6e6aabe8a71e146c","modified":1649719938072},{"_id":"public/2022/01/25/Redis数据结构与对象（三）-字典/index.html","hash":"47e42afac08285b580a9f0ebf2601d1da8dbab89","modified":1649719938072},{"_id":"public/2022/01/23/Redis数据结构与对象（一）-链表/index.html","hash":"b17a9b8396575ab2b78551df27a68aa25a3491ae","modified":1649719938072},{"_id":"public/2022/01/22/Redis设计与实现/index.html","hash":"ac661c2058ec9682a4527d9164d20b3ad60fbd90","modified":1649719938072},{"_id":"public/2021/10/16/mysql突然变慢排查/index.html","hash":"e230a9a01a422fcd8ac302c26e735d8d4fb281c6","modified":1649719938072},{"_id":"public/2021/08/07/CountDownLatch/index.html","hash":"5f4a7fb3cb7cbb0d06be148c808855201c379f9c","modified":1649719938072},{"_id":"public/2021/08/02/初识redis（5）-内存调优/index.html","hash":"a4a86171a10b8618061f918398e5c4b85ad4ae95","modified":1649719938072},{"_id":"public/2021/08/02/初识redis（4）-主从架构/index.html","hash":"977497f65f2c99da366b3b26ec87994a5eb80753","modified":1649719938072},{"_id":"public/2021/08/01/初识redis（3）-持久化/index.html","hash":"da4a1599fd890d29e7ea106cba8dd821ac3091fa","modified":1649719938072},{"_id":"public/2021/08/01/初识redis（2）/index.html","hash":"a1ff44424a604541daa6940a0b7058fdcde0fb3c","modified":1649719938072},{"_id":"public/2021/07/31/初识redis（1）/index.html","hash":"d966ed5257432427d50d5a90f40f8d79d867ab77","modified":1649719938072},{"_id":"public/2021/07/18/docker20-10-7本机开启2375配置/index.html","hash":"ab397ded1ec12ab5d19ec552e18e660c02daecdb","modified":1649719938072},{"_id":"public/2021/04/30/饼图/index.html","hash":"e52e6773d46162b0062fc154d5266ab41ad75447","modified":1649719938072},{"_id":"public/2021/04/18/分布式锁/index.html","hash":"56d41b52c6aafe42a37b2f9decc38b0694223f6f","modified":1649719938072},{"_id":"public/2021/04/18/重构/index.html","hash":"017a2e80f550993c728fb59714eafb5f1e5f73a8","modified":1649719938072},{"_id":"public/2021/04/17/Histogram（直方图）-KDE（密度图/index.html","hash":"4d4e97a19957e306221f92645f70041074503598","modified":1649719938072},{"_id":"public/2021/04/17/python可视化基础/index.html","hash":"d3f3d59f1c147e21585e04f28113f9e0b8195939","modified":1649719938072},{"_id":"public/2021/04/17/直方图/index.html","hash":"5daee8269a056d4f68872cdb4a333ecf374ee9f5","modified":1649719938072},{"_id":"public/2021/04/17/散点图/index.html","hash":"dbdb007a5671538ff80bca0f82d388a3e29599fc","modified":1649719938072},{"_id":"public/2021/04/15/信息系统项目管理师-信息化-1/index.html","hash":"893ace08345a9c61696fb9705855b620809e5444","modified":1649719938072},{"_id":"public/2021/04/13/k8s构建ELK日志平台/index.html","hash":"33708035a47f4bdd6cfd6b5877e1897ab19b3a22","modified":1649719938072},{"_id":"public/2021/04/13/Disruptor中发布事件相关类/index.html","hash":"a7fb71c2b50dbf655e63895b75d7641553c24b82","modified":1649719938072},{"_id":"public/2021/04/13/Disruptor/index.html","hash":"c00c1c5e423c720ac2db1b6a9950a865f5648a55","modified":1649719938072},{"_id":"public/2020/12/14/伪共享/index.html","hash":"797efdb8e894a59a08f63be5ef24c34e1465120d","modified":1649719938072},{"_id":"public/2020/12/14/分布式CAP概念/index.html","hash":"f4ff7e91dd54d33f22544a03adc5e540570cfded","modified":1649719938072},{"_id":"public/2020/12/14/Docker入门/index.html","hash":"1507c50d1d6e74829773c8972a159642082ee41f","modified":1649719938072},{"_id":"public/2020/12/14/java中的Queue队列/index.html","hash":"3ed59212c71302392a1755bf93fdcab5078d1995","modified":1649719938072},{"_id":"public/2020/12/14/Kafka的简单使用/index.html","hash":"81911608a1ddc62d73836b07e300783b3a9d8986","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud使用Feign-Ribbon-Hystrix/index.html","hash":"15859bd8b163b4e2b4d366f378cc2188ff4b034e","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud使用Feign-Ribbon/index.html","hash":"0bdbfaa4659575249ad4a90256563697a6d7ebc5","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud使用Feign/index.html","hash":"fc537f80bbbb4f3ac557e5c0bed39a9e44dfe9d5","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud使用RestTemplate/index.html","hash":"8c151a75e70b46e62405c811c2d7ba95107e8c9e","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud异常配置/index.html","hash":"ac0895b452a196e2dca55afae2e92d11eeb58ce2","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud-Hystrix参数配置/index.html","hash":"7fb3acda6b3d3521f49aa9ed81d9e92ce178f040","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud-Ribbon参数配置/index.html","hash":"892e10e07142f4d60b8798ff45860864a55f19ed","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud-client配置/index.html","hash":"84cd49dd133995586dd07946c2c85cfc77c17400","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud服务注册/index.html","hash":"879e97cbdb96c2dfe6e856cff88ed3c3c174b234","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud服务消费/index.html","hash":"342f91c21d30fb72b0a5000b44bda33a5d3c872e","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud健康检查/index.html","hash":"55c8c3751f949bb0c4641e7cc2c172daa5ce0fde","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud管理配置页面/index.html","hash":"b2bc339ccf7941e113288e144cc52970c5cfc781","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud运维接口/index.html","hash":"6770b1514d874c4049fabb94fea286ecdcd45a8d","modified":1649719938072},{"_id":"public/2020/12/14/SpringCloud服务构建/index.html","hash":"41d829c5972c4e2640fdb508fbf2da401bab06e6","modified":1649719938072},{"_id":"public/2020/12/06/MapReduce平均数计算/index.html","hash":"64d1c56636466b1a8efdd4c789c78f8d55b07862","modified":1649719938072},{"_id":"public/2020/12/06/HDFS-shell操作（2）/index.html","hash":"8fb7c641625e4b8b42f8f813c0b7cea4d0ad5a52","modified":1649719938072},{"_id":"public/2020/12/06/HDFS-shell操作（1）/index.html","hash":"54ddde422ba054f03935596658fc952fc441d9ac","modified":1649719938072},{"_id":"public/2020/11/17/并发编程总结/index.html","hash":"4a54a7b7c021fe7c42859ff47e7558558143f707","modified":1649719938072},{"_id":"public/2020/11/17/JVM性能优化整理/index.html","hash":"fcb3b07b02d33c3ba55f60e37aa963e6d0bef6f3","modified":1649719938072},{"_id":"public/2020/09/27/JVM内存结构/index.html","hash":"082e74d1e4d4f28d33678237700718b6f7810e62","modified":1649719938072},{"_id":"public/2020/11/17/Tomcat性能优化整理/index.html","hash":"7bbbe61eedf6c10ddeb68e86e5f2fbd65f1c757c","modified":1649719938072},{"_id":"public/2020/09/27/JVM类加载过程/index.html","hash":"ff1db4f0f22e59d887cc02c13bde890c63154acd","modified":1649719938072},{"_id":"public/2020/09/18/Hash解决冲突的方法/index.html","hash":"fce9e49a5d8fffec3f7ed703bcf44271c1d0157e","modified":1649719938072},{"_id":"public/2020/09/18/JVM垃圾回收算法/index.html","hash":"b047c8dad8e24d72d01825b7fe3a2c5d056fc55b","modified":1649719938072},{"_id":"public/2020/09/13/IOC中的基本反射步骤/index.html","hash":"5453c6616440533bfabf34a17e3166025bfd52f2","modified":1649719938072},{"_id":"public/2020/09/11/Http和Https的区别/index.html","hash":"55ade8c77231047153a8fe9771aab03767220b18","modified":1649719938072},{"_id":"public/2020/09/11/手写一个简单Autowired/index.html","hash":"b88a4fba043668f96268e47f181b58de9cd2e0c2","modified":1649719938072},{"_id":"public/2020/09/01/运算符/index.html","hash":"7fdc9fe9e4fea565e60bef94a6f4e18aefac175b","modified":1649719938072},{"_id":"public/2020/07/24/AtomicInteger/index.html","hash":"3eb43d709be3aca58f4e31c39d194ef858c96bd4","modified":1649719938072},{"_id":"public/2020/07/21/DNS/index.html","hash":"e9b1dc014d610cf69ff1a622f9caa8e2bc040e3d","modified":1649719938072},{"_id":"public/2020/07/21/l-String、-StringBuilder、StringBuffer区别/index.html","hash":"4877d7e15b916c883a6d69fa5667a4f8381d7da3","modified":1649719938072},{"_id":"public/2020/07/21/mysql事务/index.html","hash":"f06ce923ba459412a2b4301f25f8f787a3d4dd52","modified":1649719938072},{"_id":"public/2020/07/21/GET与POST区别/index.html","hash":"5b6fae6f19f9740fe7023c96d11b26a8b4c20729","modified":1649719938072},{"_id":"public/2020/07/21/TCP与UDP的区别/index.html","hash":"c87b27c95d51cdc289ebaf0e7ea5b1a6052f17f7","modified":1649719938072},{"_id":"public/2020/07/19/基于JavaAgent的全链路监控（3）/index.html","hash":"62773de2111d96e0856c51c7195d75291618558d","modified":1649719938072},{"_id":"public/2020/07/19/基于JavaAgent的全链路监控（2）/index.html","hash":"e177ab97fc1fb5c880f73dd9c9acd6e6fdf193aa","modified":1649719938072},{"_id":"public/2020/07/17/基于JavaAgent的全链路监控（1）/index.html","hash":"fa86a59b6921ecb4048158fde95a87af90eaacb7","modified":1649719938072},{"_id":"public/2020/07/15/HttpClient/index.html","hash":"aeffa9a9f23298572ce40895af021cbb6b251b1a","modified":1649719938072},{"_id":"public/2020/07/15/ElasticSearch客户端/index.html","hash":"bc54544092f5c19a3bff45891741f0b767906b76","modified":1649719938072},{"_id":"public/2020/07/15/ElasticSearch近实时性介绍/index.html","hash":"21e8e59eff8116a524a211ac69c62eb68183e273","modified":1649719938072},{"_id":"public/2020/03/28/ThreadLocal/index.html","hash":"323330e12892d43e347738bd909051ff0f42d561","modified":1649719938072},{"_id":"public/2020/01/21/Hive调优/index.html","hash":"6231eecc601253c2fcd7b6d83d11ad0717904e12","modified":1649719938072},{"_id":"public/2020/01/21/Hive模式设计/index.html","hash":"6db6003602134375ddf6180608c3d895df0b650e","modified":1649719938072},{"_id":"public/2020/01/21/Hive索引/index.html","hash":"88b32dfb03813c783ed832bf7aa35117269cfcec","modified":1649719938072},{"_id":"public/2020/01/20/HiveQL视图/index.html","hash":"4194dc6301852d97493cd6491cc8daf2ab286667","modified":1649719938072},{"_id":"public/2020/01/20/Hive数据操作（3）/index.html","hash":"d03e9193bcce0e14c155df4b08286be72d606336","modified":1649719938072},{"_id":"public/2020/01/19/Hive数据操作（2）/index.html","hash":"cb766589e4379f54a6d11a679b2cedd997c6e581","modified":1649719938072},{"_id":"public/2020/01/17/Hive数据操作/index.html","hash":"fc8eafd8d2dca4772c91d468d85f1c4f2e268487","modified":1649719938072},{"_id":"public/2020/01/17/Hive数据定义/index.html","hash":"9db69cb73fc6129c945d06751971858a3460f8ff","modified":1649719938072},{"_id":"public/2020/01/17/Hive数据类型和文件格式/index.html","hash":"59406e7b8a16564ace6ee47a6aef613abff5199f","modified":1649719938072},{"_id":"public/2020/01/06/JAVA数据类型/index.html","hash":"841530b862504c7338ac9875797de13b3fdcb1fc","modified":1649719938072},{"_id":"public/2020/01/03/单例模式/index.html","hash":"b57d296011c553252354a58901fd4c804db175dc","modified":1649719938072},{"_id":"public/2020/01/03/责任链模式/index.html","hash":"057c55b3be5b7095039c9c546b6032dc6a3b5832","modified":1649719938072},{"_id":"public/2020/01/02/排序之比较器Comparator-T/index.html","hash":"a7c7973451964091b7d0cfacb59fa7311953748c","modified":1649719938072},{"_id":"public/2020/01/02/排序之比较器/index.html","hash":"4703d64791113d4e43add2805edffe7cd37c3d53","modified":1649719938072},{"_id":"public/2019/12/23/特征提取-简单流程/index.html","hash":"828974071bb23f62f900b71de5b22c5ce70d6019","modified":1649719938072},{"_id":"public/2019/12/18/Spark相关概述/index.html","hash":"676f9c37644546b7aaf863ec38fc5c40d5ee5e23","modified":1649719938072},{"_id":"public/2019/12/18/WordCount简析/index.html","hash":"8f64fefe40b5bb49b5238d4f8d8af2fdfe1637bf","modified":1649719938072},{"_id":"public/2019/12/18/Yarn概述/index.html","hash":"e1a48587f6d00f3bcf05d7fe218d5d567409ccab","modified":1649719938072},{"_id":"public/2019/12/16/MapReduce概述/index.html","hash":"811522294c33637e73ff96810d59ba5930e11916","modified":1649719938072},{"_id":"public/2019/12/16/HDFS文件操作/index.html","hash":"5380d091817e3447b21bc6952fd2fe9f885974c2","modified":1649719938072},{"_id":"public/2019/12/16/HDFS概述/index.html","hash":"a0bf4114666517484c2a7961df15b0b64d670c2d","modified":1649719938072},{"_id":"public/2019/12/03/SpringCloud-Alibaba整合Nacos服务注册发现/index.html","hash":"9c257522389f68a2d6335a5c6949aaca8faf66b7","modified":1649719938072},{"_id":"public/2019/11/25/Nacos配置中心使用/index.html","hash":"f68709bfa85a1c86d577d6718e0ec9831a919300","modified":1649719938072},{"_id":"public/2019/11/20/mysql排序/index.html","hash":"6b7b1569d9ee8c68b0f56a4a95022cf027c60785","modified":1649719938072},{"_id":"public/2019/11/20/对象存储与指针压缩/index.html","hash":"fcdb5e09c04df69653b5724a8aeee68e0b2ce9f8","modified":1649719938072},{"_id":"public/2019/11/20/线程相关的知识/index.html","hash":"6f1c7941251f96799cbe5d16943655213aeb70e9","modified":1649719938072},{"_id":"public/2019/11/14/Spring-Bean生命周期/index.html","hash":"1fb8e6411bc9c0e0abdb39789cc4c71f1833c5b2","modified":1649719938072},{"_id":"public/2019/11/14/java8新特性/index.html","hash":"5cb4416cc1eea197bb668c3ad34b0a8de5b7ff01","modified":1649719938072},{"_id":"public/2019/10/12/SimpleDateFormat引发的线程安全问题/index.html","hash":"032b8b59dd89215668e3f61180b1bb0a923f453c","modified":1649719938072},{"_id":"public/2019/10/09/分布式全局唯一ID生成策略/index.html","hash":"3be091f91f5345b3909b041fecebdb202d3e030f","modified":1649719938072},{"_id":"public/2019/10/02/可靠性和容错技术/index.html","hash":"154a61e367c87fe28558f5cfcdaf0aab8b6d4cb8","modified":1649719938072},{"_id":"public/2019/09/28/可信与可信计算/index.html","hash":"d2c19a9ade39beb9c33c6d817f08dc0db9b7ae03","modified":1649719938072},{"_id":"public/2019/09/24/图解公钥与私钥/index.html","hash":"7ee852fc854767e040ac457d2b6177089abfdd2b","modified":1649719938072},{"_id":"public/2019/09/04/重放攻击/index.html","hash":"e1e14cd2fb6eb4d9c9d78222a0930086a09b48cd","modified":1649719938072},{"_id":"public/2019/09/02/加密解密/index.html","hash":"957739fe9c8ce2d628d50b9205bf424d2a512ff5","modified":1649719938072},{"_id":"public/2019/09/01/数字签名/index.html","hash":"f01eb7d45a6428e007efb5aaccbd2c1bae8d7b78","modified":1649719938072},{"_id":"public/2019/09/01/可信基本概念/index.html","hash":"8946a708f59924f85cbab6b94ea94f5e89c6f5f0","modified":1649719938072},{"_id":"public/2019/08/31/文件上传/index.html","hash":"e70cdeba72042ee101a7574f7c15913c32f93630","modified":1649719938072},{"_id":"public/2019/09/01/池化之线程池/index.html","hash":"65aa067f6b1f4a4bb83b1f2ed3b57c8b2659944f","modified":1649719938072},{"_id":"public/2019/08/31/mysql表设计及优化/index.html","hash":"fa1572521fe16e301782b36d01f7c209904697dd","modified":1649719938072},{"_id":"public/2019/08/31/轻量级锁/index.html","hash":"8801def3c701e09adffa89552df0bc5262ca2f69","modified":1649719938072},{"_id":"public/2019/08/31/锁粗化/index.html","hash":"da4aad9977d502885ff8d392071dfedabff43247","modified":1649719938072},{"_id":"public/2019/08/31/偏向锁/index.html","hash":"bdbc564b26fd1f09707b12f791451129da139ae6","modified":1649719938072},{"_id":"public/2019/08/31/公平锁、非公平锁/index.html","hash":"1d69c8cecee103c1881920126304b75fb09e214a","modified":1649719938072},{"_id":"public/2019/08/31/悲观锁、乐观锁/index.html","hash":"e5cd5741688e9eb4142bf879af462dfbaf6c472d","modified":1649719938072},{"_id":"public/2019/08/31/互斥锁/index.html","hash":"a6515b561ed9192ebf77db046c2581ed920bd708","modified":1649719938072},{"_id":"public/2019/08/31/读写锁/index.html","hash":"82e212657351db41cea1bd5cc1ccb5bfa3256a6c","modified":1649719938072},{"_id":"public/2019/08/31/可重入锁/index.html","hash":"a4e4948218ffa0fe3d571ee04a47adaa5afdf484","modified":1649719938072},{"_id":"public/2019/08/31/阻塞锁/index.html","hash":"973129f2dc190857d0391d4c9c23e79be45dee02","modified":1649719938072},{"_id":"public/2019/08/31/自旋锁/index.html","hash":"540162618700410dbeb6a42564a682e8c7b7d036","modified":1649719938072},{"_id":"public/2019/08/30/理解IO阻塞与非阻塞/index.html","hash":"5000266ce9b6949f14df82d9387ccc8510b08269","modified":1649719938072},{"_id":"public/2019/08/30/TCP握手、挥手协议/index.html","hash":"13585e9fe86f1543d71f53938230bad4100fa0e8","modified":1649719938072},{"_id":"public/2019/08/30/TCP-IP四层网络模型/index.html","hash":"e4d1c1359c95c8832fabdf331db87e1ec9ff5910","modified":1649719938072},{"_id":"public/2019/08/29/Git梳理/index.html","hash":"58727d09ab2fd92697fdfd69179050e94cc1b9f8","modified":1649719938072},{"_id":"public/2019/08/28/maven梳理/index.html","hash":"7c57743844c2689983faf61945aff0644575579c","modified":1649719938072},{"_id":"public/archives/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/4/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/5/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/6/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/8/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/7/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/9/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/10/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/11/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/12/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/page/13/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/page/4/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/page/5/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/08/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/08/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/09/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/10/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/11/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2019/12/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/page/4/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/page/5/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/page/6/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/01/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/01/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/03/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/07/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/07/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/09/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/11/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/12/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/12/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2020/12/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/04/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/04/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/07/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/08/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2021/10/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2022/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2022/01/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2022/03/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/archives/2022/04/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/java基础/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/java基础/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/java基础/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/面试/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/网络/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/分布式/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/CICD/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/软件管理/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/大数据/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/大数据/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/spring/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/spring/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/数据库/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/数据库/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/操作系统/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/可信/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/设计模式/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/javaagent/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/categories/python基础/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/多线程/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/java基础/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/java/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/DNS/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/Disruptor/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/docker/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/es/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/GET-POST/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/git/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/hadoop/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/HDFS/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/HADOOP/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/哈希表/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/hive/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/httpclient/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/https/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/spring/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/JVM/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/SpringCloud/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/SpringCloud/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/分布式存储/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/nacos-config/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/redis/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/并发/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/线程安全/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/Spark/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/TCP-IP/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/UDP/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/tomcat/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/k8s/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/JDK1-8新特性/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/elk/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/maven/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/String/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/StringBuilder/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/StringBuffer/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/mysql/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/锁/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/伪共享/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/CAP/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/信息系统项目管理师/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/分布式/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/可信/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/密码学/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/设计模式/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/可信计算/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/可靠/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/容错/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/内存模型/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/数据结构/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/加密算法/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/文件上传/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/IO/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/阻塞与非阻塞/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/python/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/网络安全/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/线程/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/运算符/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/分布式锁/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/tags/重构/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/2/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/3/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/4/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/5/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/6/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/8/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/9/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/7/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/10/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/11/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/12/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/page/13/index.html","hash":"312ae2de0ef0cd7182baf81b893c30826c7d6a4d","modified":1649719938072},{"_id":"public/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1649719938072},{"_id":"public/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1649719938072},{"_id":"public/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1649719938072},{"_id":"public/css/fonts/icomoon.svg","hash":"b5e7562c8494b0ddb3a70ecc5545ef7340d8e971","modified":1649719938072},{"_id":"public/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1649719938072},{"_id":"public/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1649719938072},{"_id":"public/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1649719938072},{"_id":"public/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1649719938072},{"_id":"public/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1649719938072},{"_id":"public/css/fonts/iconfont.svg","hash":"3630aabf2f9c0417f483ebd03d9e429dbc2594e0","modified":1649719938072},{"_id":"public/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1649719938072},{"_id":"public/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1649719938072},{"_id":"public/img/108cd6b414ab2dbb30126c0fb0700e23.png","hash":"67bf717f106c76aafabc07a5c2601d3d873f0d39","modified":1649719938072},{"_id":"public/img/1112dfc7b11a214351a67103cd58a5c8.png","hash":"20595180bd1af121b4f8e6ba17001e7145471c42","modified":1649719938072},{"_id":"public/img/145d5664.png","hash":"5de6263d58ac723c22250323e760162284317a71","modified":1649719938072},{"_id":"public/img/157e6450653a0fc457342b5794658fc9.png","hash":"1e396fc40a3ed2f6ee336158b619362541a614be","modified":1649719938072},{"_id":"public/img/1618244110010.png","hash":"947439297da765710fa815ea74e0920af546e46b","modified":1649719938072},{"_id":"public/img/1618244289269.png","hash":"3505425f677bf63c3351d9bb723193659be5f0f1","modified":1649719938072},{"_id":"public/img/1618244387386.png","hash":"f57bf41422b093082b4b2f4b42afa9488f0bb8de","modified":1649719938072},{"_id":"public/img/1618244497271.png","hash":"6e3f32bbe2c044ddbdfde51fda828ed6eb4190c4","modified":1649719938072},{"_id":"public/img/1618244566899.png","hash":"7a5274a43d170806b63e4f3457c2f0af774a86cd","modified":1649719938072},{"_id":"public/img/1618675966507.png","hash":"34016aea55982c639b85e635abdd8bdc5c08e6de","modified":1649719938072},{"_id":"public/img/2b97031c92882db452d126852b74cfb2.png","hash":"fe9164297c77477054de20f56ada187a7397dde6","modified":1649719938072},{"_id":"public/img/227d0458.png","hash":"2840cba30e73d089195ec8d60e2bd567e96485e6","modified":1649719938072},{"_id":"public/img/2cfc91e2dc6104321827e210ccfc595c.png","hash":"372aa64130efc06a4afca46e7fe19608cb8c7b1b","modified":1649719938072},{"_id":"public/img/41d2985c3545a456341af1e4f5b8231a.png","hash":"d542f333e70286bdc5e113ad1e6182f8476e96de","modified":1649719938072},{"_id":"public/img/57c9d55222f65769c0ab0844bc5b6432.png","hash":"8ca28dcb12a065333f54d2e8df334877e6348dd8","modified":1649719938072},{"_id":"public/img/480c47d9.png","hash":"d1b8b8bd0eb74760f8c54dee986e6fef466d6b6e","modified":1649719938072},{"_id":"public/img/591a3d39db7558c3fd1db79821abec5e.png","hash":"9589587ae012ed1578f656ca3d55548f058ca56b","modified":1649719938072},{"_id":"public/img/5a824e2f.png","hash":"7a187c1968dcd312f4480515f5ddb7b8a0815774","modified":1649719938072},{"_id":"public/img/743e6cec.jpg","hash":"eda743b32da786d5287d69c54f66600b02dfb48a","modified":1649719938072},{"_id":"public/img/70db7f87.jpg","hash":"354e54d0f10bf2d4bb92ad58e51a542d66095e2c","modified":1649719938072},{"_id":"public/img/79c1c0a4746f50074f0e44491d3cb8b3.png","hash":"c24cebcf4fd76df87c818fed8e67d1b24f5d8f6b","modified":1649719938072},{"_id":"public/img/879bfdfc7cab85a54bed5a33275c6dc0.png","hash":"b43a21590a27b6812486d939e3f69b101908ef88","modified":1649719938072},{"_id":"public/img/8fb0cd0c.png","hash":"3656d384f8348607e72d6ddb69f23ec7e3e1801b","modified":1649719938072},{"_id":"public/img/9b9139fd46f7419279ba6346b090ddc7.png","hash":"7aef4036bed44b2eca4a3a440a8e91178f80bfff","modified":1649719938072},{"_id":"public/img/9c9e497f4dc73587aa9d620a4149101c.png","hash":"b248c9780fb66208c64500e0014e9c52698f4f68","modified":1649719938072},{"_id":"public/img/JVMMemory.png","hash":"053217d209af791dd65e97e321268cf17df92275","modified":1649719938072},{"_id":"public/img/JVM类加载过程.png","hash":"aa87f840b42869db7b76bd2789d1eaa8dcda2d0f","modified":1649719938072},{"_id":"public/img/StringBuilder.png","hash":"eaa519848fcd7948c1820e5e2a2f2ad719277708","modified":1649719938072},{"_id":"public/img/StringStringBuilderStringBuffer.png","hash":"d836b560a0cf006cfa6d8d833c3453f131db7c12","modified":1649719938072},{"_id":"public/img/TCPIP用户发送请求.png","hash":"f4428ca4bf21e4d067da5b944bb742892939d131","modified":1649719938072},{"_id":"public/img/TCPIP服务器接收请求.png","hash":"dd6986ec9df5f3ffe5ff743e97408b53147743fc","modified":1649719938072},{"_id":"public/img/UDPHeader.png","hash":"03991c09cb8a800bc68acc974430c1d6df3073d2","modified":1649719938072},{"_id":"public/img/Wechat.jpg","hash":"5944a56bb7506847a6362acee8a54aa268db2919","modified":1649719938072},{"_id":"public/img/UDPTCPcompare.png","hash":"fc2c1380a717ad5e1ac5a85250d26d21e56a9f92","modified":1649719938072},{"_id":"public/img/a4ef9088f117a8766ac65c4a1a630a34.png","hash":"bef96d7494c6d4fdf5efaa53ea7e7c0f0f3d3257","modified":1649719938072},{"_id":"public/img/a7d8693f0dccfde8ecd0919cff94ff88.png","hash":"19650ca1c919b99de580339632ae0933ad83c623","modified":1649719938072},{"_id":"public/img/a7ade6c9.png","hash":"f3c6bb2c3cf9073b99b71d5ed8c48bf7d9f85d77","modified":1649719938072},{"_id":"public/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png","hash":"be20a8b0190f360f7cb12cb14d0aff0544bab46f","modified":1649719938072},{"_id":"public/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png","hash":"9430376a898992c9a6c09e9251f3db4bab3d7993","modified":1649719938072},{"_id":"public/img/b235f114.png","hash":"605d8626ed4d395ea711cb82a141c2ce1e36f4d8","modified":1649719938072},{"_id":"public/img/b5a900fe460481976d0ae062ab18dd61.png","hash":"b16db3668f5827827935b826a4b71cb4b5fb497e","modified":1649719938072},{"_id":"public/img/ce0bdff067bf19a083c5d26b1266bb44.png","hash":"eb065c1639deba3fc3ea14639534090780cfa7ec","modified":1649719938072},{"_id":"public/img/b4d68165.png","hash":"5d7c542053cb704983fde9b21bc7cd74c9053628","modified":1649719938072},{"_id":"public/img/clip_image002.png","hash":"773f0b16657a294e5b51f44c9adc7cf0d4813728","modified":1649719938072},{"_id":"public/img/d1a8d5d473881fffebc5baccd896258d.png","hash":"0e9c768626f3b23d79ed70766537e50b8d5b5b75","modified":1649719938072},{"_id":"public/img/d6f2bab64eddad3ee5d092672e8fac50.png","hash":"fdae43fdb7a90fb5f79eb90db1b721e0e52e7485","modified":1649719938072},{"_id":"public/img/d133e6d9b982b320594ea62168dbe462.png","hash":"2a8692dcfbac84a0f4c5c46f667f8a1a79f6dfbf","modified":1649719938072},{"_id":"public/img/d234cadc40e81da676a8b541d9cb6ab4.png","hash":"a068b49bba0ce76d72f9cb4f0de4a467cd19c2dc","modified":1649719938072},{"_id":"public/img/d325d29a.png","hash":"962a746a65c34d0e4ea2b5a3e9d23a30f1d35201","modified":1649719938072},{"_id":"public/img/d94cd34e.png","hash":"d29a3d8cbf07390c741d3b0a1706461fc85e6590","modified":1649719938072},{"_id":"public/img/d9710d39e895a6eb8210645c6e0b85e3.png","hash":"f7d1620e8c38aaa0e04eac4f7d2094446140da6f","modified":1649719938072},{"_id":"public/img/df56c6c619c3e1efb6f6140d56f67bcd.png","hash":"8bfd7c4e974653323164bce9b25ceae8eeec2418","modified":1649719938072},{"_id":"public/img/dataSource-behaviour-relative.png","hash":"702859e0f00906daf67b67515b0bd683148f8109","modified":1649719938072},{"_id":"public/img/e18c9709.jpg","hash":"06375b48ff834c994e894405ee00742bb12d962d","modified":1649719938072},{"_id":"public/img/e13f637bd584b1563442aeaece8cf1e4.png","hash":"8f5f563fc6ab2d276868eacfbed49e1427ba4af3","modified":1649719938072},{"_id":"public/img/ea179a09.png","hash":"145556ada619dcda0eb7f503930ea65ef73b3465","modified":1649719938072},{"_id":"public/img/edf97c87de27aae8adafce7f255f0870.png","hash":"b9134008d1a4b333d2d8d40cec0f6bacf90aa777","modified":1649719938072},{"_id":"public/img/f785ade6f77521bce18fc9f8825d4d9b.png","hash":"c412daa2e0104ffb4bccde4c3982bf0cfaf82d43","modified":1649719938072},{"_id":"public/img/es1.png","hash":"89281b1bef0d51742b32b70b774525c6ae08c95f","modified":1649719938072},{"_id":"public/img/es2.png","hash":"07f92f31d703b5659cf537a5b31b2ff9c55cbdc1","modified":1649719938072},{"_id":"public/img/fac717a5.png","hash":"793349e13d727eef0ec51f496e78116cdb7604ca","modified":1649719938072},{"_id":"public/img/f8f4bde6cb9aeac56366a2f3853f24f3.png","hash":"08664ed857a79c157500b7ceb85f7ed08bff0a15","modified":1649719938072},{"_id":"public/img/ffd661fb15bf746863a6e57d7aca04c4.png","hash":"8fb35d52fae7bb70ccdea766c8478a73db870ea0","modified":1649719938072},{"_id":"public/img/fb523dd6.png","hash":"cf674b0aa3f9fedd73a0e9501121aaeadce2f834","modified":1649719938072},{"_id":"public/img/hdfs创建文件夹.png","hash":"27daebbd4cf39ca74ffa88aa9865257de46beeb2","modified":1649719938072},{"_id":"public/img/hive-聚合2.png","hash":"69ab607095185fb63a23851f371ac09c3261728a","modified":1649719938072},{"_id":"public/img/https-intro.png","hash":"14850fa8463ff0892e5258b5884b4e92cb66f481","modified":1649719938072},{"_id":"public/img/image-20201206115459406.png","hash":"b459dcbaa97aba5f099acae0f39145ad3daa61e5","modified":1649719938072},{"_id":"public/img/image-20201206115832025.png","hash":"83d8dac7f6b2b8c02031d46c5d9a8748d2242228","modified":1649719938072},{"_id":"public/img/image-20201206121627187.png","hash":"071a741e5d52a45d3ca56dccb5af33a190597f7f","modified":1649719938072},{"_id":"public/img/image-20201206121650739.png","hash":"2ad0274ed10bb42df9bddc574b430e023858e661","modified":1649719938072},{"_id":"public/img/image-20201214121332594.png","hash":"4ab43a767fa96e721de32a3084870e7ef785bc8a","modified":1649719938072},{"_id":"public/img/image-20201214114013294.png","hash":"b88b3083b0bf5a8ca307695fc265dd07fd2e9bd9","modified":1649719938072},{"_id":"public/img/image-20201214121613146.png","hash":"0855b48f6f372b5f20905d273cc9d780ea2c19dc","modified":1649719938072},{"_id":"public/img/image-20201214121645728.png","hash":"fdea15c1fff089408871ff3150b2c8b6c107173a","modified":1649719938072},{"_id":"public/img/image-20201214121721100.png","hash":"feade0b22e88fe8bdf2d97cf7d939f39b3466735","modified":1649719938072},{"_id":"public/img/image-20201206121711716.png","hash":"e70f981b0e2661a084fe730e068bc889b4efb7ac","modified":1649719938072},{"_id":"public/img/image-20201214122255720.png","hash":"0238a48f3dcd45892e5263bda2e67d455ecdc631","modified":1649719938072},{"_id":"public/img/image-20201214124300675.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1649719938072},{"_id":"public/img/image-20201214124603810.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1649719938072},{"_id":"public/img/image-20201214125020584.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1649719938072},{"_id":"public/img/image-20201214125032862.png","hash":"2d9a0431305510a118e7136455247f2918f68708","modified":1649719938072},{"_id":"public/img/image-20201214125453643.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1649719938072},{"_id":"public/img/image-20201214125504552.png","hash":"0a1ac24eef9da3dbdb800c4d46175abc60eba89a","modified":1649719938072},{"_id":"public/img/image-20201214125529023.png","hash":"82d57e9f373789317595d49aa2e379260acb5a9f","modified":1649719938072},{"_id":"public/img/image-20201214130757812.png","hash":"5d935f2e74081b23dff7c75ce43b3f088906ece6","modified":1649719938072},{"_id":"public/img/image-20201214131847691.png","hash":"8c841533dd0f0e9d2965695be830eac48c571f04","modified":1649719938072},{"_id":"public/img/image-20201214131734289.png","hash":"f63b13d6700ce20d9e476e880b0ef1d7f230e249","modified":1649719938072},{"_id":"public/img/image-20201214131814678.png","hash":"e86681a9d954b4ab9ceeccef85949166532d22d7","modified":1649719938072},{"_id":"public/img/image-20201214132014480.png","hash":"692652f4e430312e6d6a3414304b0c64e46d6d36","modified":1649719938072},{"_id":"public/img/image-20201214131948506.png","hash":"8eb5e9f7c9d9f5f47e19509c702632a8ee881711","modified":1649719938072},{"_id":"public/img/image-20201214132059091.png","hash":"0e9f8c4f95304e59cc8af6adff897b0656925bea","modified":1649719938072},{"_id":"public/img/image-20201214132127621.png","hash":"5e48bbea9baa60cb25cdff8a3b5ce28a0f0b935f","modified":1649719938072},{"_id":"public/img/image-20201214132215125.png","hash":"fedaacf9919b879a2539c2faacb748456b0b3e62","modified":1649719938072},{"_id":"public/img/image-20201214132326043.png","hash":"e2fcc5164dcfdd9125fd0f2b2a785de9c8668c3c","modified":1649719938072},{"_id":"public/img/image-20201214132343909.png","hash":"90f9e923863dcdeddfe91f1794c42e904011d33d","modified":1649719938072},{"_id":"public/img/image-20201214132309723.png","hash":"9ee9276322c6160b294b49127b0ccfdd31006d6a","modified":1649719938072},{"_id":"public/img/image-20201214132401207.png","hash":"29faa2ade6819a751e21d2ed734326369ba67afb","modified":1649719938072},{"_id":"public/img/image-20201214132416329.png","hash":"06b6ecf11a65a2616a7ba9b317bc4efb0dd2fb9c","modified":1649719938072},{"_id":"public/img/image-20201214133601256.png","hash":"30359512fcf46fca69e040fd4446ea477b5c2646","modified":1649719938072},{"_id":"public/img/image-20201214133955605.png","hash":"df38a2d88ed44358447df314b1b05cb762fb3e34","modified":1649719938072},{"_id":"public/img/image-20201214134132982.png","hash":"4f48f02bee07e0e6be8cd93a463a70750abf12f6","modified":1649719938072},{"_id":"public/img/image-20201214134208507.png","hash":"a2ad991b50538e8a070fd5b390cdafd800c3affd","modified":1649719938072},{"_id":"public/img/image-20201214135053884.png","hash":"f6c523e663d41c1df848333892389670db4b14e5","modified":1649719938072},{"_id":"public/img/image-20210430152537381.png","hash":"5ff6454780ab7bae49d550bb647c431b4e5234b5","modified":1649719938072},{"_id":"public/img/image-20210430152457125.png","hash":"36f6d86933771011c5adfcb2b098ad6893a71830","modified":1649719938072},{"_id":"public/img/image-20210430152414489.png","hash":"49aa3d7b1aeed9550e2eb2ea42a6bbb1ebac3b3e","modified":1649719938072},{"_id":"public/img/image-20210430152325572.png","hash":"2f5f4e5e6710ec4872e5091bb3aad4889afa2e88","modified":1649719938072},{"_id":"public/img/image-20210430152636683.png","hash":"905ed38af2aacba8e19191aa58526ebd809743fc","modified":1649719938072},{"_id":"public/img/image-20210718193314796.png","hash":"95d9d80eb4dfbc2ec4d7b5ca5714fcbb42211d2c","modified":1649719938072},{"_id":"public/img/image-20210802000308665.png","hash":"88ad18bf715e5340956b8e9a4d7b08a804cb1456","modified":1649719938072},{"_id":"public/img/image-20210718193618402.png","hash":"ad5bc245550044192daf0a3ac8ebcddd3eabdf63","modified":1649719938072},{"_id":"public/img/image-20210802000052243.png","hash":"773cc994b809af6b4c300fd9918e4804cb37c711","modified":1649719938072},{"_id":"public/img/image-20210802000916321.png","hash":"7a5d83a364c36cd420e825f6983a3ccafdd48ca9","modified":1649719938072},{"_id":"public/img/image-20220122144100590.png","hash":"ff791b62112e65c9d9546fe2032b8dfc9c9a21f7","modified":1649719938072},{"_id":"public/img/image-20211014175135307.png","hash":"6dd7a087cb13f69679d104c05b38ee6dd160c920","modified":1649719938072},{"_id":"public/img/image-20220404214709099.png","hash":"4b44b8a89ce9c00c456864af41182eca6c229182","modified":1649719938072},{"_id":"public/img/image-20220411221525635.png","hash":"dd9ba20d3024b2b81f44a7ae06be90f09d1d9767","modified":1649719938072},{"_id":"public/img/image-20220411222247494.png","hash":"4b98ed28d32456a22fcca21800d178bb01d3429a","modified":1649719938072},{"_id":"public/img/javaagent1.png","hash":"93c2b34bbbbb8ce1fccb0d0eb0daff97a7a2bdac","modified":1649719938072},{"_id":"public/img/java对象存储3.png","hash":"4b83f6c1accd8ea2753e0dc16f52820d52abb700","modified":1649719938072},{"_id":"public/img/java对象存储.png","hash":"78231e4a1c56b2078cdd4454d20593ba3e2b48e3","modified":1649719938072},{"_id":"public/img/java对象存储2.png","hash":"a5e540b6699183dcad89a9912c25c6a0c5eed9f3","modified":1649719938072},{"_id":"public/img/loc.png","hash":"55d7bc698c3f0394701a798ef22eb664c5fce7ed","modified":1649719938072},{"_id":"public/img/maven配置.png","hash":"c966dcfb8e36138b51cbaa08a8e15bc73a615778","modified":1649719938072},{"_id":"public/img/mysql排序1.png","hash":"09e8d6d507cd2b06ed4e6e9a76c832e226981ed0","modified":1649719938072},{"_id":"public/img/mysql排序2.png","hash":"0f0e1c1417f85b8e5f7bf1bb929ad63223569faf","modified":1649719938072},{"_id":"public/img/mysql排序4.png","hash":"9ef66bb2173d553fb58106a4959b3c0a5751d482","modified":1649719938072},{"_id":"public/img/mysql排序6.png","hash":"254e269dc8da6b5358e883e82a16c52243a4286e","modified":1649719938072},{"_id":"public/img/mysql排序5.png","hash":"044b6bc8c3b4d33f3a25598b9b99771efe75bccf","modified":1649719938072},{"_id":"public/img/nacos-springCloud1.png","hash":"afd0806eb1d72ed4988811e08d1c2e7cb621682b","modified":1649719938072},{"_id":"public/img/nacos1.png","hash":"07c9cfee060b4090c04d5a9d422b97e2dd1cbbd2","modified":1649719938072},{"_id":"public/img/nacos-springCloud2.png","hash":"6ce7c90123f1a1eb0e3ebbe93ae50eded7392603","modified":1649719938072},{"_id":"public/img/output_11_0.png","hash":"feae0728c34b976d209edf511796c8d86b374266","modified":1649719938072},{"_id":"public/img/nacos-producer.png","hash":"36eefd319d40a5a11e9343d3d817d66098d9d043","modified":1649719938072},{"_id":"public/img/output_11_1.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1649719938072},{"_id":"public/img/output_11_111.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1649719938072},{"_id":"public/img/output_13_011.png","hash":"00d6df80990d980af9b8d244faba51db58904ebc","modified":1649719938072},{"_id":"public/img/output_13_0.png","hash":"74537a7efb66b8f35c874c0b69c1d0282ab92eec","modified":1649719938072},{"_id":"public/img/output_13_1.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1649719938072},{"_id":"public/img/output_14_0.png","hash":"ec71387d1c4c05e7a86a33c13aa3a08568e75827","modified":1649719938072},{"_id":"public/img/output_13_111.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1649719938072},{"_id":"public/img/output_15_0.png","hash":"cc3a6ab17830f239b1e82500c967f6a3b40093f4","modified":1649719938072},{"_id":"public/img/output_17_0.png","hash":"e4f13d81304ba2ac471c18b70558fc4cea417666","modified":1649719938072},{"_id":"public/img/output_15_011.png","hash":"d699a9e55c1d99653f37a36ab91be019a53e4bbf","modified":1649719938072},{"_id":"public/img/output_19_0.png","hash":"d278d46ba923bcc6dfdbe29ca0d8547898ab1f2a","modified":1649719938072},{"_id":"public/img/output_20_1.png","hash":"65f2cbd977d34618d64be151e40aa7c5dbe9ec9f","modified":1649719938072},{"_id":"public/img/output_22_0.png","hash":"9b8f87f0342ee9093359c92047f1438a4c474dff","modified":1649719938072},{"_id":"public/img/output_26_111.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1649719938072},{"_id":"public/img/output_26_1.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1649719938072},{"_id":"public/img/output_29_0.png","hash":"e4c9e227e39a0e97a133e1b96b9577534ff3cb4c","modified":1649719938072},{"_id":"public/img/output_30_12.png","hash":"71900ff594f876e60656e61ec55b8550de208eea","modified":1649719938072},{"_id":"public/img/output_30_0111.png","hash":"03859e3478c86b4b495f49826e6a4f21f0507f52","modified":1649719938072},{"_id":"public/img/output_31_0.png","hash":"0850ffa53150038f984d9861c87bebcc127662c2","modified":1649719938072},{"_id":"public/img/output_32_02.png","hash":"45e2ff2fd81678595b7c33f27ce4efdb1727db53","modified":1649719938072},{"_id":"public/img/output_32_1.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1649719938072},{"_id":"public/img/output_33_1.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1649719938072},{"_id":"public/img/output_34_1.png","hash":"219b0f0516b12045f827b0461e1be52eed5afa35","modified":1649719938072},{"_id":"public/img/output_32_111.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1649719938072},{"_id":"public/img/output_33_111.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1649719938072},{"_id":"public/img/output_35_02.png","hash":"02a51de4cf25a2a71bb9d9ecc7fecdcabb8cfa8f","modified":1649719938072},{"_id":"public/img/output_34_0111.png","hash":"e23bcc316f3646a1fec22cff049ce868c051e972","modified":1649719938072},{"_id":"public/img/output_36_1.png","hash":"14241a13199df5a0b3daeb5ab3c628f9bca15763","modified":1649719938072},{"_id":"public/img/output_37_0111.png","hash":"753bede65e90d7c3b820c71b15f4a82160032400","modified":1649719938072},{"_id":"public/img/output_38_02.png","hash":"8fa952d47ced59bb033ed0c42177c4d52981f458","modified":1649719938072},{"_id":"public/img/output_40_02.png","hash":"13f4139b8d41242bb7cf7a0818676a6171d640f6","modified":1649719938072},{"_id":"public/img/output_44_0111.png","hash":"5901f5f0cfb80a132ab923a832ac5d524708d08a","modified":1649719938072},{"_id":"public/img/output_42_0.png","hash":"5f66e78fbf687a4e955d043ca3c949036f445847","modified":1649719938072},{"_id":"public/img/output_42_0111.png","hash":"5010a66d219e9f12a4817f5cd23a1fdac5004471","modified":1649719938072},{"_id":"public/img/output_46_0.png","hash":"26fbd45ab4c82c99b073d1c940767c4732aa6de1","modified":1649719938072},{"_id":"public/img/output_44_0.png","hash":"8d3277c861b982ddcea78e95020eed54f412af3d","modified":1649719938072},{"_id":"public/img/output_40_0.png","hash":"a13785823254893b0bf7a2be92b5347507c44e01","modified":1649719938072},{"_id":"public/img/output_52_0.png","hash":"0abbeb362227d28f260cef8812606514f6530b9c","modified":1649719938072},{"_id":"public/img/output_49_12.png","hash":"4049cbcb428cf1f2578782f77aa1cfd01d79eb26","modified":1649719938072},{"_id":"public/img/output_47_0111.png","hash":"8d7ca8b0fb8554f174aa3d310c088ba8a584c86a","modified":1649719938072},{"_id":"public/img/output_50_0111.png","hash":"19ab454f3b4ff2130be35d478314093697cc09d7","modified":1649719938072},{"_id":"public/img/output_54_0.png","hash":"e889c6a42c4956042d69b1fa6c6bf75e60a9b41c","modified":1649719938072},{"_id":"public/img/output_55_0111.png","hash":"43f870cd8c8c6467b8aafa62ea3a77638cf59d99","modified":1649719938072},{"_id":"public/img/output_57_0111.png","hash":"43f870cd8c8c6467b8aafa62ea3a77638cf59d99","modified":1649719938072},{"_id":"public/img/output_55_12.png","hash":"c87f337c935f80907db1fd33372939164a8bf857","modified":1649719938072},{"_id":"public/img/output_51_12.png","hash":"4fe6ba1d674d7706b8ffa7bdf05e333910977cbd","modified":1649719938072},{"_id":"public/img/output_63_12.png","hash":"0dcfa074599c6faea3b7ef78585510c161149ace","modified":1649719938072},{"_id":"public/img/output_59_0111.png","hash":"3f57a86a9bd422deb093e0fa348eca3fc77b7df6","modified":1649719938072},{"_id":"public/img/output_57_12.png","hash":"5bd256fe115a1b86475088c8a3ce017342f4ef93","modified":1649719938072},{"_id":"public/img/output_60_12.png","hash":"b27e851a7d17aa9228f12f573d66c2c37949217d","modified":1649719938072},{"_id":"public/img/output_65_0111.png","hash":"fd70b1f86f887a6903c2951cdd2e9b1bc7fa84e4","modified":1649719938072},{"_id":"public/img/output_67_0111.png","hash":"778f9da3a8ffc41c71296abb4bff4a908ef43f3d","modified":1649719938072},{"_id":"public/img/output_65_12.png","hash":"7a6bef5a90b57fadd9db6f31597c6c35681df70d","modified":1649719938072},{"_id":"public/img/output_68_12.png","hash":"a14c63cbb8d830f3d5915804e79c3d34042740d6","modified":1649719938072},{"_id":"public/img/output_70_12.png","hash":"166a28b2edc7a59fdfa924ea6153b8c3d9d7a7cf","modified":1649719938072},{"_id":"public/img/output_71_0111.png","hash":"5dc1be414946db6eee3ff4a77c93c8f3ec9a3d03","modified":1649719938072},{"_id":"public/img/output_72_12.png","hash":"9eab845425a9b5b82d2753eb247aaa3117304992","modified":1649719938072},{"_id":"public/img/output_74_12.png","hash":"9b5d3beb0d801e210800a7f9638fd6956760d20d","modified":1649719938072},{"_id":"public/img/output_75_0111.png","hash":"c1509330ce2fff34cef75b953b9bf54d873da3ce","modified":1649719938072},{"_id":"public/img/output_79_02.png","hash":"8ff18b2f1a6243935a6b6acab1aa16a662f073e4","modified":1649719938072},{"_id":"public/img/output_79_0111.png","hash":"56aa7a5ebb280611469a02e677e9bd22505175af","modified":1649719938072},{"_id":"public/img/output_79_12.png","hash":"69276815c0942893eb4389a3aefd07e9811878be","modified":1649719938072},{"_id":"public/img/output_85_1111.png","hash":"5b19eb29e1fba305a9320bcb9cafe68724f7f8b5","modified":1649719938072},{"_id":"public/img/output_81_0111.png","hash":"d2918818943b407b83cb8af7283f2f6465f91e85","modified":1649719938072},{"_id":"public/img/output_89_02.png","hash":"ecd199e8ed0d919bf49d18a34ffc0fceaebbd8f8","modified":1649719938072},{"_id":"public/img/output_87_0111.png","hash":"afc5437df08acf2a7395626b2000a92f61f7eff7","modified":1649719938072},{"_id":"public/img/output_81_02.png","hash":"146b692db59101420b38903bb44bfced79689d8f","modified":1649719938072},{"_id":"public/img/output_8_0.png","hash":"0bcb929ddd78eab93836703fc4f859dfeef6c3ae","modified":1649719938072},{"_id":"public/img/output_8_1.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1649719938072},{"_id":"public/img/pasted-0.png","hash":"d4f20b5880ebe1cd4114af86e182d3a8042f29b6","modified":1649719938072},{"_id":"public/img/output_8_111.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1649719938072},{"_id":"public/img/secondaryNameNode.jpg","hash":"6186b1ee9e43435bafb7abefde1824d5f66e8f8b","modified":1649719938072},{"_id":"public/img/redis分布式锁实现原理.jpg","hash":"08e0920c4df0d033672d5900ec24235aa03bade6","modified":1649719938072},{"_id":"public/img/user-log.png","hash":"f7bd7155b8a465ad32afb87f8ae386768833c64f","modified":1649719938072},{"_id":"public/img/wordcount.png","hash":"78dbf6fe038d27891bb860321f1087fa9f642e3c","modified":1649719938072},{"_id":"public/img/三次握手协议1.png","hash":"b7cf27e8ba8b7e299c4e02519394600d0269ae84","modified":1649719938072},{"_id":"public/img/三次握手协议2.png","hash":"149fa2d315b1471325a89b68ee659409c709c10c","modified":1649719938072},{"_id":"public/img/使用协议进行通讯.png","hash":"fe93735a4d871a544feb31b0b713f98ab4c715fb","modified":1649719938072},{"_id":"public/img/信任链.png","hash":"23517850fc98c07ad66d80bac4556b77e0688214","modified":1649719938072},{"_id":"public/img/公钥私钥6.png","hash":"c7104a322604a0b51de2d988bd9522b5deab2045","modified":1649719938072},{"_id":"public/img/公钥私钥8.png","hash":"6b13644b98068f7f44ddb51532f17030906031c0","modified":1649719938072},{"_id":"public/img/加密算法.png","hash":"5a0a4d8fbdc43f929dd5aa86aded45086ba38cbb","modified":1649719938072},{"_id":"public/img/可信根.png","hash":"875a1cdae0d77993f07a1df1dd968f907a2c1d1e","modified":1649719938072},{"_id":"public/img/对称加密算法.png","hash":"6a7ed179fb8b48e9054caba56308b1691b6ebe92","modified":1649719938072},{"_id":"public/img/线程相关1.jpg","hash":"cd2dfeb2b0a367d208e46d1fec4ed9241164256e","modified":1649719938072},{"_id":"public/img/线程相关3.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1649719938072},{"_id":"public/img/数据库分布式ID生成.png","hash":"7376ea76ee62cc47d2e0389f90ca9fa1a3173f74","modified":1649719938072},{"_id":"public/img/线程相关4.jpg","hash":"3268689c6020136cda73d95d1553dfa6816aa60b","modified":1649719938072},{"_id":"public/img/线程相关5.jpg","hash":"54f4bd928b107b4da2faaa87fa74484b76900c0b","modified":1649719938072},{"_id":"public/img/线程相关2.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1649719938072},{"_id":"public/img/锁的创建.png","hash":"ac72c945c263be025d83970d2cfdb6c240c38bbc","modified":1649719938072},{"_id":"public/img/阻塞IO.png","hash":"f5cfd961b871078b9b202d1e8d4810f3f126aaef","modified":1649719938072},{"_id":"public/img/锁的创建2.png","hash":"ebf05f8b81c02d9c880b9081e9ccc0b2b6c5e4ae","modified":1649719938072},{"_id":"public/img/雪花算法.png","hash":"522b23e1a5bc4bdf46b1d285233d815004847271","modified":1649719938072},{"_id":"public/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1649719938072},{"_id":"public/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1649719938072},{"_id":"public/img/avatar.jpg","hash":"525dc2b6ef38fee9d4c66e554f928934eadd9117","modified":1649719938072},{"_id":"public/img/weixin.jpg","hash":"697f60350a05f53228bbb88505e79007e97b2fbd","modified":1649719938072},{"_id":"public/img/1337b059.png","hash":"934fedad21bc55fd4848e8478221aa8de7c30ee7","modified":1649719938072},{"_id":"public/img/1618244002479.png","hash":"476a48c86fe9853e6e3814d62967932180e87aac","modified":1649719938072},{"_id":"public/img/1618244251013.png","hash":"a4a3edf1be8118db34c7d6ccd951dbcfe29ad50c","modified":1649719938072},{"_id":"public/img/1618244166891.png","hash":"5fbb1c37f8bc0d5a9d12e931dbf7b24da9d180ce","modified":1649719938072},{"_id":"public/img/1618244193984.png","hash":"5f54d9d4ca1822d23caf1b86f07d11775d996d3c","modified":1649719938072},{"_id":"public/img/1618244361109.png","hash":"565b7cfe75ec1b5d599bdaa2be09166c5ffb7275","modified":1649719938072},{"_id":"public/img/1618244522945.png","hash":"3484b30a26d62ae602e3b7a6faf8b7b49bab0295","modified":1649719938072},{"_id":"public/img/1618244592782.png","hash":"4f2779efd3450377236256ecf7cc6c7525fcf57c","modified":1649719938072},{"_id":"public/img/1618677203153.png","hash":"c7704c40b99c71e1d8060502a7bab46eda7bb7a6","modified":1649719938072},{"_id":"public/img/4e12cc06.png","hash":"750185f3c87805cafe374b9ed6c6b4d6909c8d3a","modified":1649719938072},{"_id":"public/img/5a377b3b.png","hash":"7a8b1a4998362f443f0989fd88d2424a93adb05e","modified":1649719938072},{"_id":"public/img/7fdd593ac3e140d5cd2e9328b51110c9.png","hash":"e99fae23f53ca43700b92e510c770dad33efa64f","modified":1649719938072},{"_id":"public/img/9cb319ab.png","hash":"7a7cd79948bc7091586b5b20a61aa70de5ce1827","modified":1649719938072},{"_id":"public/img/Git工作流程.png","hash":"13f5329292c5f0b42cb969b2981141981bc62c12","modified":1649719938072},{"_id":"public/img/HDFS-liucheng.png","hash":"b37eef3d1e61aeaf650ea9860b40acff7c5fc1e4","modified":1649719938072},{"_id":"public/img/HDFS上传.png","hash":"7ca87724851bc20365821ed02ffcbe9de0768a0c","modified":1649719938072},{"_id":"public/img/HTTPS2.png","hash":"b792dee35f602f38fce149164dad4593c64a9650","modified":1649719938072},{"_id":"public/img/HTTPS4.png","hash":"98c925d3de7c588950cb1ce2be36b3346a0ee848","modified":1649719938072},{"_id":"public/img/HTTPS5.png","hash":"7f96023634200693631288ec5047f024ee53f91c","modified":1649719938072},{"_id":"public/img/IO复用select模型.png","hash":"9f0c021b9b258025552f261e3ab8cbb2f96093ad","modified":1649719938072},{"_id":"public/img/Spark.png","hash":"ab45bdfcdd54f893ab046360fea5ccd51b887b3f","modified":1649719938072},{"_id":"public/img/TCPIP模型.png","hash":"69c031664c0698c88a8b5d40b7b8e7e7afc53748","modified":1649719938072},{"_id":"public/img/TCP协议通讯过程.png","hash":"46efae085ca09e1445b9eac506c02c3462e2a16b","modified":1649719938072},{"_id":"public/img/ThreadLocal内部存储.png","hash":"1bcf081078acb1a514dd19ef48b740663837e899","modified":1649719938072},{"_id":"public/img/agent-costtime.png","hash":"1659bf6791dac773e8eb6d5ee3357b79d8702c12","modified":1649719938072},{"_id":"public/img/agent-costtime2.png","hash":"657b28cdec63d882c44e6010a7a78fdffbd6c186","modified":1649719938072},{"_id":"public/img/chartype.png","hash":"965a1655bf9364c0ec0b7a8773c74d8ce9d91805","modified":1649719938072},{"_id":"public/img/clip_image004.png","hash":"762418fea78372f2c8b67fc6f4dffb7045e9d449","modified":1649719938072},{"_id":"public/img/eac62f3b0d93e90dd8f19789d013c1ec.png","hash":"4cc347bbb4a3ef2f28e03627654a09e2bede58b4","modified":1649719938072},{"_id":"public/img/floattype.png","hash":"a7c29f3c863dfe4b4f3ea2ccea6b35002b7eb7ba","modified":1649719938072},{"_id":"public/img/hive数据结构.png","hash":"6d0a423af256a0aecb19cd1ef7d34f4eb327efde","modified":1649719938072},{"_id":"public/img/hive数据结构1.png","hash":"1251b48e87a11c926b56969b3de0db1077627a3a","modified":1649719938072},{"_id":"public/img/hive文本文件数据编码.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1649719938072},{"_id":"public/img/image-20200117113506023.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1649719938072},{"_id":"public/img/image-20201206115600801.png","hash":"793cd10108ad7b33685387becb060e215a2eb045","modified":1649719938072},{"_id":"public/img/image-20201206115738831.png","hash":"03a84a42d3f64d15ac10a3d3449e554dc184be28","modified":1649719938072},{"_id":"public/img/image-20201206122126739.png","hash":"57cc146ead1c09f0fb6d2e54ad6f3bec553db218","modified":1649719938072},{"_id":"public/img/image-20201214123508379.png","hash":"36aa63f8051ab7294256ea771bdde1930d350db3","modified":1649719938072},{"_id":"public/img/image-20201214125402694.png","hash":"c5023ea858552e39671ae6d0f4aeef4189cc1801","modified":1649719938072},{"_id":"public/img/image-20201214131527522.png","hash":"9cb523d61f7f7178a933e7cee84a538d729a4026","modified":1649719938072},{"_id":"public/img/image-20201214131543066.png","hash":"00c552d3184a69128eb6b263119cb1c5cf6976ed","modified":1649719938072},{"_id":"public/img/image-20201214132254636.png","hash":"c580027a831986c0f1ada1ec7cf5e47726a29d99","modified":1649719938072},{"_id":"public/img/image-20201214133612299.png","hash":"c8f1d908f463400ae615da444654759a79c9511b","modified":1649719938072},{"_id":"public/img/image-20201214133830361.png","hash":"eb52ab258a8bf959e9402b5abf071c9d3db1a2cf","modified":1649719938072},{"_id":"public/img/image-20201214133935704.png","hash":"53cb6b400f9247b609d3d6df67a79f65eabe1f55","modified":1649719938072},{"_id":"public/img/image-20201214134235050.png","hash":"da065bb97d038439d13aca2178dc7f204d397c9f","modified":1649719938072},{"_id":"public/img/image-20220404214514222.png","hash":"19fdd9c15030ead0bf3a34a60cdf87496b26f3ec","modified":1649719938072},{"_id":"public/img/image-20220411212506831.png","hash":"a3299283b9a40bc1f6d0de15ae04c7c6b82334bc","modified":1649719938072},{"_id":"public/img/image-20220411220512656.png","hash":"46ca2fb0784c19b31370ef0ed8867bd0e7930e04","modified":1649719938072},{"_id":"public/img/inttype.png","hash":"e01f1199570a67cd2ae26c3012227718e2575056","modified":1649719938072},{"_id":"public/img/jvm1.png","hash":"3cd098688ff2de0e071bc49739e257c18d0046af","modified":1649719938072},{"_id":"public/img/jvm3.png","hash":"e3b1fd453e531b527b569c4e28eaa04776f28fd5","modified":1649719938072},{"_id":"public/img/jvm2.png","hash":"6839ef36387dfa9daccb9d6395f486f9c6e73a4f","modified":1649719938072},{"_id":"public/img/mysql时间存储.png","hash":"1db17e59e7b6dad480009153803c159d5a97cbfe","modified":1649719938072},{"_id":"public/img/mysql的ip存储.png","hash":"d06d3619438f57be43aa1236ec9ace4e1bf3d126","modified":1649719938072},{"_id":"public/img/typetrans.png","hash":"485490a772062455bc238936c0607d48d4aa93b7","modified":1649719938072},{"_id":"public/img/transaction.png","hash":"787175d99cc453a5ff1692029e2043060b1a7514","modified":1649719938072},{"_id":"public/img/user-behaviour.png","hash":"b0ce3ff640e61461a545a39c56d01a3b6447a68a","modified":1649719938072},{"_id":"public/img/wordcount-map.png","hash":"a1c10dc70e0fdef09e3db7f305e16cd2fdf631a1","modified":1649719938072},{"_id":"public/img/wordcount-split.png","hash":"27610016538c3dca06409dcfbd57f8a805f86624","modified":1649719938072},{"_id":"public/img/公钥私钥1.png","hash":"498ea1f36ce52b0ff2964c9423bf5eb5ab0ae804","modified":1649719938072},{"_id":"public/img/公钥私钥10.png","hash":"767005c089b3be3b91213d12fab4e462718a7787","modified":1649719938072},{"_id":"public/img/公钥私钥11.png","hash":"10755e3bcfc39d9bf3c8b9b5060ec5682a18cad3","modified":1649719938072},{"_id":"public/img/公钥私钥2.png","hash":"c60ac4f1a39665d9a2b923f3a78541fc663b0e6b","modified":1649719938072},{"_id":"public/img/公钥私钥13.png","hash":"afd0297e43e61b9193b6dcbf9a71da610938dd2f","modified":1649719938072},{"_id":"public/img/公钥私钥3.png","hash":"63b5f3e415343c2a65d3274fe8d6e4ce4c26f490","modified":1649719938072},{"_id":"public/img/公钥私钥4.png","hash":"683543865f578ea89be62b1af374df4de0f42069","modified":1649719938072},{"_id":"public/img/公钥私钥5.png","hash":"b2721631bbbac0306b12e70466fa49531f8bae30","modified":1649719938072},{"_id":"public/img/公钥私钥9.png","hash":"4c4d25e78aea05b34ccf9cd0c5025cfcb7bf94e3","modified":1649719938072},{"_id":"public/img/公钥私钥7.png","hash":"e7f712acb8d5e47d717f18494d4c5e22ea9b5468","modified":1649719938072},{"_id":"public/img/基于Zookeeper分布式锁.png","hash":"897ebe02592f747426bbcfd26748deb812c06845","modified":1649719938072},{"_id":"public/img/指针压缩2.png","hash":"2db8b3b15563eedb0a35b192c37760fec8f9c8fa","modified":1649719938072},{"_id":"public/img/指针压缩3.png","hash":"f142f7d0e15d58e03d02d99c0eba541e51e30a7f","modified":1649719938072},{"_id":"public/img/指针压缩4.png","hash":"945b359ce34488105eab6422d8f397d8c76b4ea0","modified":1649719938072},{"_id":"public/img/非对称加密算法.png","hash":"260798ec83168388a70e55c60b1a7e5a47e78246","modified":1649719938072},{"_id":"public/css/mobile.css","hash":"5998f6fc27998596beb1e40e4bc3c43be2ed764c","modified":1649719938072},{"_id":"public/js/titleTip.js","hash":"81dca549063e29ba3a4a278f0f4388eba8a2167b","modified":1649719938072},{"_id":"public/js/search.js","hash":"c80c9a231ee040c7adc07a477793873fb85ce8bc","modified":1649719938072},{"_id":"public/css/hl_theme/atom-light.css","hash":"d31edb9816dae6b01410028bceb91757a962f780","modified":1649719938072},{"_id":"public/css/hl_theme/atom-dark.css","hash":"88d11052a24e8100af6248eb4dbe1ce7b0e96408","modified":1649719938072},{"_id":"public/css/hl_theme/darcula.css","hash":"4341074bae4bc9f0b86e32b623e27babc0159b6e","modified":1649719938072},{"_id":"public/css/hl_theme/brown-paper.css","hash":"500c8e750373f6656ff49a7857c871ceedcf8777","modified":1649719938072},{"_id":"public/css/hl_theme/github-gist.css","hash":"7a41c1c479d09df875f99f1f6d94aac42e9e2ad0","modified":1649719938072},{"_id":"public/css/hl_theme/gruvbox-dark.css","hash":"8c440d9b4ee19ac03eaee3c6af78ba52e5ba5535","modified":1649719938072},{"_id":"public/css/hl_theme/github.css","hash":"e05a0806a508a26b9f3f3794b6b588ec6504ad3f","modified":1649719938072},{"_id":"public/css/style.css","hash":"db3b1cc156de2bd7563399cf74eeabf9abde50a7","modified":1649719938072},{"_id":"public/img/1618244073306.png","hash":"c23f3b75c0e00674c191e398da326fbabfdc1155","modified":1649719938072},{"_id":"public/img/GETPOST.png","hash":"2a4f00d2d7cf57f885f347c1f6cdd5d0f477408c","modified":1649719938072},{"_id":"public/img/HTTPS3.png","hash":"e3e58b8e4961a4533ac2a63154aee3c49322e379","modified":1649719938072},{"_id":"public/img/SpringBean3.png","hash":"2b96172eecc1cbd1e41ab9756c0a12836ebad948","modified":1649719938072},{"_id":"public/img/StringUpdate.png","hash":"1cad86bc5aa015dfba1e0929e4dd98c708401011","modified":1649719938072},{"_id":"public/img/data-collect-analysis.png","hash":"19e4706ad65dfa32bf285624af839d4437db17df","modified":1649719938072},{"_id":"public/img/hdfs-write-file.png","hash":"3482163375f44f53c6ff7791e005d5ebc42bf0f8","modified":1649719938072},{"_id":"public/img/hdfs.png","hash":"c3be7dde1cdd24c4765a87ddf07c75c70148e970","modified":1649719938072},{"_id":"public/img/hive-聚合3.png","hash":"e3efe8d25521a72b1180deab85b38de9ecda0f5f","modified":1649719938072},{"_id":"public/img/image-20201214134902337.png","hash":"5d56cc52e66348ca9f9cc41c1b7b93fa30659f29","modified":1649719938072},{"_id":"public/img/image-20220123235957505.png","hash":"2a916b7c8f4af37631784e40598772055eb1bba1","modified":1649719938072},{"_id":"public/img/mysql排序3.png","hash":"a91f727d4c99fb6289678a86f64ddd1c246568c7","modified":1649719938072},{"_id":"public/img/simpleDateFormat-alibaba.png","hash":"f45845a64160deda103dcca59d212712bfe544e6","modified":1649719938072},{"_id":"public/img/可信在云平台的基础架构.png","hash":"b507acb0ecbe5f39f783b7a82340a30bb6fd42d4","modified":1649719938072},{"_id":"public/img/非阻塞IO.png","hash":"58573268ff130afca209099ecda84d1c4eeff17f","modified":1649719938072},{"_id":"public/css/hl_theme/gruvbox-light.css","hash":"30514aaa242a34647aa666cfca4fc74c595ea8f2","modified":1649719938072},{"_id":"public/css/hl_theme/kimbie-dark.css","hash":"728527fcc308da454722c119b89e6da3025bd1e3","modified":1649719938072},{"_id":"public/css/hl_theme/kimbie-light.css","hash":"0c61926c989163faefb031d27bce3e287d6e10f2","modified":1649719938072},{"_id":"public/css/hl_theme/railscasts.css","hash":"511f2fd2a84d426e5da5cb17880cc08f73beb002","modified":1649719938072},{"_id":"public/css/hl_theme/sublime.css","hash":"f65c5b116d9213afb9c324384a2f3bc86cb71121","modified":1649719938072},{"_id":"public/css/hl_theme/rainbow.css","hash":"7ff4251938076ddb7e4e49413db82653e5b61321","modified":1649719938072},{"_id":"public/css/hl_theme/school-book.css","hash":"ffbbcd13a74ac2404262c50b7a43053dfd0096ff","modified":1649719938072},{"_id":"public/css/hl_theme/sunburst.css","hash":"8a135abac1512cf430d1d1ad2304b79afa1a4d6e","modified":1649719938072},{"_id":"public/css/hl_theme/zenbum.css","hash":"0a78f74a93568e20b32ca7427c719e9bae9a0b55","modified":1649719938072},{"_id":"public/css/gitalk.css","hash":"58177ce227c50ee359fbf99a4fdd26058887afc5","modified":1649719938072},{"_id":"public/js/jquery.pjax.js","hash":"191c49fdb40dff115a49cfd2b30dffb888d86550","modified":1649719938072},{"_id":"public/css/fonts/selection.json","hash":"047b615ea32dc48dae5b964061427d41feaaafdf","modified":1649719938072},{"_id":"public/img/1618244623527.png","hash":"813c175d735bb6274abc069750ff9bda641937b7","modified":1649719938072},{"_id":"public/img/1618675987731.png","hash":"d60ff3303b0726d472ec4315f02b2c483fc9b5c4","modified":1649719938072},{"_id":"public/img/1618676411155.png","hash":"9ce76fc1e2821c68a6a46a587604e6bcd470b85a","modified":1649719938072},{"_id":"public/img/1618676461026.png","hash":"a240beb2b863869fb5effabc3bdbb15318e3dedf","modified":1649719938072},{"_id":"public/img/1618676526483.png","hash":"b69356b7ddec7edff861a27275eb442d86ef569f","modified":1649719938072},{"_id":"public/img/7f3f75ca.png","hash":"3af248ce70e75081d255e4eaa66dd9e2aba6b5bb","modified":1649719938072},{"_id":"public/img/HTTPS1.png","hash":"2cc9b05768fb05bd25da12dd3428e6f02dfe50ec","modified":1649719938072},{"_id":"public/img/QQ.jpg","hash":"6f463db364ede8033b209c855f15a4d7a15bca97","modified":1649719938072},{"_id":"public/img/data-collect.png","hash":"f46c55acbc3e188b8c9cce341fb61d2db1027f31","modified":1649719938072},{"_id":"public/img/hdfs-read-file.png","hash":"861f90f65c5c1eac3ffea16430c75833aff5e089","modified":1649719938072},{"_id":"public/img/hive-数学函数.png","hash":"e15b0006e6290c17b2348d0840272b52be732e0e","modified":1649719938072},{"_id":"public/img/hive-算数运算符.png","hash":"54c7db33bbd7b5b8484727d7a988869609539d4c","modified":1649719938072},{"_id":"public/img/image-20201206115653987.png","hash":"6ecd4c60466dc5e4278f30761e2354568150992a","modified":1649719938072},{"_id":"public/img/image-20220322221741703.png","hash":"d8f0c9fb1fbfeb3eaf3b4e95c1655d1fef071d22","modified":1649719938072},{"_id":"public/img/spark+hdfs.png","hash":"5ea801d62e8a34b8f08c627826d7f71369c2b749","modified":1649719938072},{"_id":"public/img/公钥私钥12.png","hash":"fdadf38dfcfd50d4afb6939f7403946ebcf1217b","modified":1649719938072},{"_id":"public/img/四次挥手协议.png","hash":"112d408966bbd5f77373bda95aaa07fd952e23d4","modified":1649719938072},{"_id":"public/img/椭圆曲线算法的基本原理.png","hash":"5f848a66630c6ba515286fa96dc36ed3a9a11106","modified":1649719938072},{"_id":"public/img/混合加密的方式.png","hash":"45b8a92df2e3c57f6af2d483c444a96e59929843","modified":1649719938072},{"_id":"public/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1649719938072},{"_id":"public/js/script.js","hash":"d7efd27ade371c6e50d0d7481ffc0ec47018bad2","modified":1649719938072},{"_id":"public/img/1618244544994.png","hash":"92f81e311f76ad0ac43170b8629248563e2af10f","modified":1649719938072},{"_id":"public/img/1618676025236.png","hash":"a5b39d633936bf8ef0298c709a41e84585edfe36","modified":1649719938072},{"_id":"public/img/8ab72b98.png","hash":"04732de500f3fc10dada0c0c2b2f35004c87624b","modified":1649719938072},{"_id":"public/img/DNS2.png","hash":"0e8ca00be831d63e2ba6f36efc36dac00b590330","modified":1649719938072},{"_id":"public/img/TCP协议通讯过程2.png","hash":"9ae8696d16fd4ffdc46b6c005b598999dec3f9c1","modified":1649719938072},{"_id":"public/img/Yarn.png","hash":"8b15f23ee56b0f7e2e5aa00157c1de50a0979da7","modified":1649719938072},{"_id":"public/img/1618676280257.png","hash":"03a393d5e007b461c6d4e1b15519f14ce9809b5c","modified":1649719938072},{"_id":"public/img/hive-partition.png","hash":"c587e39d656360af68418b82172cf91d1b0e7cd2","modified":1649719938072},{"_id":"public/img/hive集合数据类型.png","hash":"3803153d332cc20f22be920a84a9e371617099f3","modified":1649719938072},{"_id":"public/img/spark-all.png","hash":"d52bb136b90c8f63acaba9398811dbf1642de23f","modified":1649719938072},{"_id":"public/img/主动免疫可信架构信任链传递示意图.png","hash":"ec45ff26518a046bd0f6274a8f7317222f05ea81","modified":1649719938072},{"_id":"public/img/指针压缩1.png","hash":"d26943d292b23b4cf544cee8b1a73aa4172a8920","modified":1649719938072},{"_id":"public/img/读写锁.png","hash":"f5c76702b3a276fc48835d2ad1a02859a4767e08","modified":1649719938072},{"_id":"public/img/阻塞与非阻塞调用对比.png","hash":"cdee42373e7940d8f09575bf844ce08f49b4d1af","modified":1649719938072},{"_id":"public/img/1618676358941.png","hash":"38fca4b60911868d060fd5de519bddbf83bcc047","modified":1649719938072},{"_id":"public/img/DNS1.png","hash":"6ab8d7e53c1f65db2993710c9974276b7c6724fe","modified":1649719938072},{"_id":"public/img/hive-运算3.png","hash":"5bc45de4ce61992130c1c3636d2031969dd81cb1","modified":1649719938072},{"_id":"public/img/jvmHeapStructure.png","hash":"002db89529955be1ab4647f6628bcb41474c9ec6","modified":1649719938072},{"_id":"public/img/1618244137853.png","hash":"90a6d21417b376515ad5d1652f85eba0d61545c2","modified":1649719938072},{"_id":"public/img/1618295759372.png","hash":"369d456db024815c115664ca926d0356e0d9af2d","modified":1649719938072},{"_id":"public/img/1618676232016.png","hash":"54b768ddac2305c283ed547ed5fcc1e1e5620d0d","modified":1649719938072},{"_id":"public/img/TCP协议通讯过程1.png","hash":"100f6ea49b8b585a10e0ac88e2486a902e221a5c","modified":1649719938072},{"_id":"public/img/hive运算1.png","hash":"472120109358e0ec58e980b9d0fc9ab533764d4c","modified":1649719938072},{"_id":"public/img/wordcount-reduce.png","hash":"99f7334d973e7a4ffaf5d34e7870b55d0016858b","modified":1649719938072},{"_id":"public/img/1618676506490.png","hash":"c7d7ddfeebc07dd8c245f27161f90d894d5b1e30","modified":1649719938072},{"_id":"public/img/HTTPS6.png","hash":"be9aa6a1578dd12e1970db4731f7c9727f9bc17e","modified":1649719938072},{"_id":"public/img/基于ETCD实现分布式锁分析.png","hash":"77d619b33b524e8e014ec8c0a74413493a05313e","modified":1649719938072},{"_id":"public/img/Hive-运算2.png","hash":"1422e43a9e0aba7dac5ea4b6eecddbf6adc6bfe7","modified":1649719938072},{"_id":"public/img/select、epoll模型对比.png","hash":"cdc93cacbedcf65fb68878bd36da3929e6333e2b","modified":1649719938072},{"_id":"public/img/对象存储1.png","hash":"281b0b0c7d4c55df4000800b3f00888663e5ee25","modified":1649719938072},{"_id":"public/img/1618676190337.png","hash":"e6122eb7c6daa185cec1e8ad3a5e96b8ca2b7c2a","modified":1649719938072},{"_id":"public/img/hive-表生成函数.png","hash":"cf8bb24a3e4d1263993cc3df54d7777a3931eb64","modified":1649719938072},{"_id":"public/img/image-20201206121829515.png","hash":"c5523dda003b29efcb8231211c4d2045d61b0dec","modified":1649719938072},{"_id":"public/img/image-20210807123832574.png","hash":"789320ecd88502d6113ee4955a1e3fcdaf5b2cc8","modified":1649719938072},{"_id":"public/img/Socket通讯模型.png","hash":"2e260e5f30b7e17b7dba072e65ac7860e8d7e6ce","modified":1649719938072},{"_id":"public/img/网络连接模型.png","hash":"f2f48effb8cb134f597011fda639932ea0d2fac2","modified":1649719938072},{"_id":"public/img/三次握手协议3.png","hash":"e6fe59d8473f94be2b361f4bda5be7d791022eec","modified":1649719938072},{"_id":"public/js/gitment.js","hash":"59a1e03f2b0ce61dd9bd405d3c52d3e07cc10dec","modified":1649719938072},{"_id":"public/img/hive-聚合1.png","hash":"070d7feb1ef4626223019a5b205d43b24116dc4a","modified":1649719938072},{"_id":"public/img/四次挥手协议2.png","hash":"1a4890549075feacfa4010df20911537fc726e4b","modified":1649719938072},{"_id":"public/img/image-20210415193556837.png","hash":"56b313e3e79284892d99656549cc4b7f0335e296","modified":1649719938072},{"_id":"public/js/gitalk.js","hash":"a75ead28e6a1fab2a006cc7332ca2d2e868ce8e1","modified":1649719938072}],"Category":[{"name":"java基础","_id":"cl1vcnnyb00037kt92f1xc3pn"},{"name":"面试","_id":"cl1vcnnyg000d7kt93yqcd3kt"},{"name":"网络","_id":"cl1vcnnyl000q7kt95058e27c"},{"name":"分布式","_id":"cl1vcnnyn000x7kt9ga2y5hdw"},{"name":"CICD","_id":"cl1vcnnyo00137kt9bqyl98up"},{"name":"软件管理","_id":"cl1vcnnyx001t7kt97fme2mu3"},{"name":"大数据","_id":"cl1vcnnyz00207kt9dl0105gr"},{"name":"spring","_id":"cl1vcnnzn004b7kt9hr086xan"},{"name":"数据库","_id":"cl1vcnnzq004p7kt94gy73a3x"},{"name":"操作系统","_id":"cl1vcno0h007j7kt917c91q45"},{"name":"可信","_id":"cl1vcno0s008j7kt9f6qjeahs"},{"name":"设计模式","_id":"cl1vcno0u008s7kt98dmf08ix"},{"name":"python基础","_id":"cl1vcno1b00am7kt90ixz088t"}],"Data":[],"Page":[],"Post":[{"title":"信息系统项目管理师-信息化","author":"ztq","_content":"","source":"_drafts/信息系统项目管理师-信息化.md","raw":"---\ntitle: 信息系统项目管理师-信息化\nauthor: ztq\ntags:\n---\n","slug":"信息系统项目管理师-信息化","published":0,"date":"2022-04-04T08:32:40.133Z","updated":"2022-04-04T08:32:40.133Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnny300007kt9c5sg8wt9","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"基于JavaAgent的全链路监控（4）","author":"郑天祺","date":"2020-07-19T14:55:00.000Z","_content":"","source":"_drafts/基于JavaAgent的全链路监控（4）.md","raw":"title: 基于JavaAgent的全链路监控（4）\nauthor: 郑天祺\ntags:\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 22:55:00\n---\n","slug":"基于JavaAgent的全链路监控（4）","published":0,"updated":"2022-04-04T08:32:40.134Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnny900017kt95ktv7ni8","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"原子性，有序性，可见性","author":"郑天祺","date":"2020-08-11T03:04:00.000Z","_content":"","source":"_drafts/原子性，有序性，可见性.md","raw":"title: 原子性，有序性，可见性\nauthor: 郑天祺\ntags:\n  - 多线程\ncategories:\n  - java基础\ndate: 2020-08-11 11:04:00\n---\n","slug":"原子性，有序性，可见性","published":0,"updated":"2022-04-04T08:32:40.134Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnya00027kt9dedk6eaq","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"AtomicInteger","author":"郑天祺","date":"2020-07-24T07:52:00.000Z","_content":"\n# 1、介绍\t\t\n\nAtomicInteger属于JUC并发包下的原子类，继承关系如下：\n\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable\n```\n\njava 的并发机制中有三个特性：原子性、可见性和有序性。\n\nsynchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。\n\n# 2、先看一个例子：\n\n```java\npackage cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main {\n    private static volatile int a = 1;\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n        System.out.println(a);\n    }\n}\n```\n\n结果：\n\n```java\n1\n4\n4\n3\n5\n2\n```\n\n定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：\n\n（1）每个线程从内存中读取a的值\n\n（2）对a进行+1操作\n\n（3）把a重新刷新回内存\n\n当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。\n\n# 3、再看一个例子：\n\n```java\npackage cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main {\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n\t\tSystem.out.println(a);\n    }\n}\n\n```\n\n结果：\n\n```java\n2\n4\n1\n3\n5\n5\n```\n\n利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：\n\n# 4、AtomicInteger如何保证原子性\n\n## （1）、源码分析\n\nAtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）\n\n```java\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n```\n\n底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE\n\n### （1）U\n\n```java\nprivate static final Unsafe U = Unsafe.getUnsafe();\n```\n\n利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。\n\nCAS操作包含三个操作数：内存位置、预期原值及新值。\n\n执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。\n\nUnsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。\n\n### （2）VALUE\n\n```java\n private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n```\n\nVALUE是 long 类型的，代表的含义就是对象的地址的偏移量。\n\n```java\nU.getAndAddInt(this, VALUE, 1) + 1;\n```\n\nU 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；\n\n那么怎么保证 getAndAddInt() 方法是最新的值呢？\n\n```java\n@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) {\n    int v;\n    do {\n        v = getIntVolatile(o, offset);\n    } while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n}\n```\n\n底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：\n\nparam1：o 是当前对象\n\nparam2：offset 表示内存地址的偏移量\n\nparam3：v + delta 表示要增加的值\n\nAtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。\n\nCAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。\n\n\n\n## （2）、源码注释\n\n```java\npackage java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable {\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) {\n        value = initialValue;\n    }\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() {\n    }\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() {\n        return value;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) {\n        value = newValue;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) {\n        U.putIntRelease(this, VALUE, newValue);\n    }\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) {\n        return U.getAndSetInt(this, VALUE, newValue);\n    }\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) {\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue}\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) {\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() {\n        return U.getAndAddInt(this, VALUE, 1);\n    }\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() {\n        return U.getAndAddInt(this, VALUE, -1);\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) {\n        return U.getAndAddInt(this, VALUE, delta);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() {\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) {\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\t.................\n}\n```\n\n","source":"_posts/AtomicInteger.md","raw":"title: AtomicInteger\nauthor: 郑天祺\ntags:\n\n  - java基础\ncategories:\n  - 面试\ndate: 2020-07-24 15:52:00\n\n---\n\n# 1、介绍\t\t\n\nAtomicInteger属于JUC并发包下的原子类，继承关系如下：\n\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable\n```\n\njava 的并发机制中有三个特性：原子性、可见性和有序性。\n\nsynchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。\n\n# 2、先看一个例子：\n\n```java\npackage cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main {\n    private static volatile int a = 1;\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n        System.out.println(a);\n    }\n}\n```\n\n结果：\n\n```java\n1\n4\n4\n3\n5\n2\n```\n\n定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：\n\n（1）每个线程从内存中读取a的值\n\n（2）对a进行+1操作\n\n（3）把a重新刷新回内存\n\n当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。\n\n# 3、再看一个例子：\n\n```java\npackage cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main {\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n\t\tSystem.out.println(a);\n    }\n}\n\n```\n\n结果：\n\n```java\n2\n4\n1\n3\n5\n5\n```\n\n利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：\n\n# 4、AtomicInteger如何保证原子性\n\n## （1）、源码分析\n\nAtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）\n\n```java\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n```\n\n底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE\n\n### （1）U\n\n```java\nprivate static final Unsafe U = Unsafe.getUnsafe();\n```\n\n利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。\n\nCAS操作包含三个操作数：内存位置、预期原值及新值。\n\n执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。\n\nUnsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。\n\n### （2）VALUE\n\n```java\n private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n```\n\nVALUE是 long 类型的，代表的含义就是对象的地址的偏移量。\n\n```java\nU.getAndAddInt(this, VALUE, 1) + 1;\n```\n\nU 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；\n\n那么怎么保证 getAndAddInt() 方法是最新的值呢？\n\n```java\n@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) {\n    int v;\n    do {\n        v = getIntVolatile(o, offset);\n    } while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n}\n```\n\n底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：\n\nparam1：o 是当前对象\n\nparam2：offset 表示内存地址的偏移量\n\nparam3：v + delta 表示要增加的值\n\nAtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。\n\nCAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。\n\n\n\n## （2）、源码注释\n\n```java\npackage java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable {\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) {\n        value = initialValue;\n    }\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() {\n    }\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() {\n        return value;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) {\n        value = newValue;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) {\n        U.putIntRelease(this, VALUE, newValue);\n    }\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) {\n        return U.getAndSetInt(this, VALUE, newValue);\n    }\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) {\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue}\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) {\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() {\n        return U.getAndAddInt(this, VALUE, 1);\n    }\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() {\n        return U.getAndAddInt(this, VALUE, -1);\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) {\n        return U.getAndAddInt(this, VALUE, delta);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() {\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) {\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\t.................\n}\n```\n\n","slug":"AtomicInteger","published":1,"updated":"2022-04-04T08:32:40.135Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyc00057kt912i74ybb","content":"<h1>1、介绍</h1>\n<p>AtomicInteger属于JUC并发包下的原子类，继承关系如下：</p>\n<pre><code class=\"language-java\">public class AtomicInteger extends Number implements java.io.Serializable\n</code></pre>\n<p>java 的并发机制中有三个特性：原子性、可见性和有序性。</p>\n<p>synchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。</p>\n<h1>2、先看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main &#123;\n    private static volatile int a = 1;\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n        System.out.println(a);\n    &#125;\n&#125;\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">1\n4\n4\n3\n5\n2\n</code></pre>\n<p>定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：</p>\n<p>（1）每个线程从内存中读取a的值</p>\n<p>（2）对a进行+1操作</p>\n<p>（3）把a重新刷新回内存</p>\n<p>当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。</p>\n<h1>3、再看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main &#123;\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n\t\tSystem.out.println(a);\n    &#125;\n&#125;\n\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">2\n4\n1\n3\n5\n5\n</code></pre>\n<p>利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：</p>\n<h1>4、AtomicInteger如何保证原子性</h1>\n<h2 id=\"（1）、源码分析\">（1）、源码分析</h2>\n<p>AtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）</p>\n<pre><code class=\"language-java\">    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n</code></pre>\n<p>底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE</p>\n<h3 id=\"（1）U\">（1）U</h3>\n<pre><code class=\"language-java\">private static final Unsafe U = Unsafe.getUnsafe();\n</code></pre>\n<p>利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。</p>\n<p>CAS操作包含三个操作数：内存位置、预期原值及新值。</p>\n<p>执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。</p>\n<p>Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。</p>\n<h3 id=\"（2）VALUE\">（2）VALUE</h3>\n<pre><code class=\"language-java\"> private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n</code></pre>\n<p>VALUE是 long 类型的，代表的含义就是对象的地址的偏移量。</p>\n<pre><code class=\"language-java\">U.getAndAddInt(this, VALUE, 1) + 1;\n</code></pre>\n<p>U 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；</p>\n<p>那么怎么保证 getAndAddInt() 方法是最新的值呢？</p>\n<pre><code class=\"language-java\">@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) &#123;\n    int v;\n    do &#123;\n        v = getIntVolatile(o, offset);\n    &#125; while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n&#125;\n</code></pre>\n<p>底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：</p>\n<p>param1：o 是当前对象</p>\n<p>param2：offset 表示内存地址的偏移量</p>\n<p>param3：v + delta 表示要增加的值</p>\n<p>AtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。</p>\n<p>CAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。</p>\n<h2 id=\"（2）、源码注释\">（2）、源码注释</h2>\n<pre><code class=\"language-java\">package java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable &#123;\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) &#123;\n        value = initialValue;\n    &#125;\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() &#123;\n    &#125;\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() &#123;\n        return value;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) &#123;\n        value = newValue;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) &#123;\n        U.putIntRelease(this, VALUE, newValue);\n    &#125;\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) &#123;\n        return U.getAndSetInt(this, VALUE, newValue);\n    &#125;\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) &#123;\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue&#125;\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) &#123;\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() &#123;\n        return U.getAndAddInt(this, VALUE, 1);\n    &#125;\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() &#123;\n        return U.getAndAddInt(this, VALUE, -1);\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\t.................\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>AtomicInteger属于JUC并发包下的原子类，继承关系如下：</p>\n<pre><code class=\"language-java\">public class AtomicInteger extends Number implements java.io.Serializable\n</code></pre>\n<p>java 的并发机制中有三个特性：原子性、可见性和有序性。</p>\n<p>synchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。</p>\n<h1>2、先看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main &#123;\n    private static volatile int a = 1;\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n        System.out.println(a);\n    &#125;\n&#125;\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">1\n4\n4\n3\n5\n2\n</code></pre>\n<p>定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：</p>\n<p>（1）每个线程从内存中读取a的值</p>\n<p>（2）对a进行+1操作</p>\n<p>（3）把a重新刷新回内存</p>\n<p>当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。</p>\n<h1>3、再看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main &#123;\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n\t\tSystem.out.println(a);\n    &#125;\n&#125;\n\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">2\n4\n1\n3\n5\n5\n</code></pre>\n<p>利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：</p>\n<h1>4、AtomicInteger如何保证原子性</h1>\n<h2 id=\"（1）、源码分析\">（1）、源码分析</h2>\n<p>AtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）</p>\n<pre><code class=\"language-java\">    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n</code></pre>\n<p>底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE</p>\n<h3 id=\"（1）U\">（1）U</h3>\n<pre><code class=\"language-java\">private static final Unsafe U = Unsafe.getUnsafe();\n</code></pre>\n<p>利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。</p>\n<p>CAS操作包含三个操作数：内存位置、预期原值及新值。</p>\n<p>执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。</p>\n<p>Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。</p>\n<h3 id=\"（2）VALUE\">（2）VALUE</h3>\n<pre><code class=\"language-java\"> private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n</code></pre>\n<p>VALUE是 long 类型的，代表的含义就是对象的地址的偏移量。</p>\n<pre><code class=\"language-java\">U.getAndAddInt(this, VALUE, 1) + 1;\n</code></pre>\n<p>U 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；</p>\n<p>那么怎么保证 getAndAddInt() 方法是最新的值呢？</p>\n<pre><code class=\"language-java\">@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) &#123;\n    int v;\n    do &#123;\n        v = getIntVolatile(o, offset);\n    &#125; while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n&#125;\n</code></pre>\n<p>底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：</p>\n<p>param1：o 是当前对象</p>\n<p>param2：offset 表示内存地址的偏移量</p>\n<p>param3：v + delta 表示要增加的值</p>\n<p>AtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。</p>\n<p>CAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。</p>\n<h2 id=\"（2）、源码注释\">（2）、源码注释</h2>\n<pre><code class=\"language-java\">package java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable &#123;\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) &#123;\n        value = initialValue;\n    &#125;\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() &#123;\n    &#125;\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() &#123;\n        return value;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) &#123;\n        value = newValue;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) &#123;\n        U.putIntRelease(this, VALUE, newValue);\n    &#125;\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) &#123;\n        return U.getAndSetInt(this, VALUE, newValue);\n    &#125;\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) &#123;\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue&#125;\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) &#123;\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() &#123;\n        return U.getAndAddInt(this, VALUE, 1);\n    &#125;\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() &#123;\n        return U.getAndAddInt(this, VALUE, -1);\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\t.................\n&#125;\n</code></pre>\n"},{"title":"排序方法","author":"郑天祺","date":"2020-10-23T07:49:00.000Z","_content":"","source":"_drafts/排序方法.md","raw":"title: 排序方法\nauthor: 郑天祺\ntags:\n  - 排序\ncategories:\n  - 面试\ndate: 2020-10-23 15:49:00\n---\n","slug":"排序方法","published":0,"updated":"2022-04-04T08:32:40.134Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyc00067kt9ftwka5m2","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"CountDownLatch","author":"ztq","date":"2021-08-07T05:25:00.000Z","_content":"\n# 1、CountDownLatch简介\n\n​\t\tCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。\n\n​\t\t类似的任务可以使用线程的  join()  方法实现：在等待时间点调用其他线程的  join()  方法，当前线程就会等待join线程执行完之后才继续执行，但 CountDownLatch 实现更加简单，并且比 join 的功能更多。\n\nCountDownLatch函数列表\n\n```java\nCountDownLatch(int count)\n构造一个用给定计数初始化的 CountDownLatch。\n\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。\nvoid await()\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。\nboolean await(long timeout, TimeUnit unit)\n// 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。\nvoid countDown()\n// 返回当前计数。\nlong getCount()\n// 返回标识此锁存器及其状态的字符串。\nString toString()\n```\n\nCountDownLatch和CyclicBarrier的区别：\n\n- CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。\n- CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。\n\n# 2、CountDownLatch使用示例\n\n```java\npackage com.ztq.task;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class CountDownLatchTest {\n    public static void main(String[] args) throws Exception {\n\n        /*创建CountDownLatch实例,计数器的值初始化为3*/\n        final CountDownLatch downLatch = new CountDownLatch(7);\n\n        /*创建三个线程,每个线程等待1s,表示执行比较耗时的任务*/\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(2, 20, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        pool.execute(new MyTask(\"选手1到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手2到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手3到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手4到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手5到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手6到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手7到达终点\", downLatch));\n\n        /*主线程调用await()方法,等到其他三个线程执行完后才继续执行*/\n        downLatch.await();\n        System.out.println(\"赛跑结束\");\n        pool.shutdown();\n\n    }\n\n\n    static class MyTask extends Thread {\n        private String name;\n        private CountDownLatch downLatch;\n\n        public MyTask(String name, CountDownLatch downLatch) {\n            this.name = name;\n            this.downLatch = downLatch;\n        }\n\n        @Override\n        public void run() {\n            try {\n                Thread.sleep(1000);\n\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n\n            }\n            System.out.println(name);\n            /*任务完成后调用CountDownLatch的countDown()方法*/\n            downLatch.countDown();\n        }\n    }\n}\n\n```\n\n# 3、CountDownLatch源码分析\n\n​\t\t分析CountDownLatch的源码我们可以知道，它是使用了一个内部同步器AQS来实现屏蔽功能的。只有当count的值为零时，同步器的tryAcquireShared的结果为1，其他时候都是-1\n\n```java\n     private static final class Sync extends AbstractQueuedSynchronizer {\n        private static final long serialVersionUID = 4982264981922014374L;\n\n        Sync(int count) {\n            setState(count);\n        }\n\n        int getCount() {\n            return getState();\n        }\n\n        protected int tryAcquireShared(int acquires) {\n            return (getState() == 0) ? 1 : -1;\n        }\n\n        protected boolean tryReleaseShared(int releases) {\n            // Decrement count; signal when transition to zero\n            for (;;) {\n                int c = getState();\n                if (c == 0)\n                    return false;\n                int nextc = c-1;\n                if (compareAndSetState(c, nextc))\n                    return nextc == 0;\n            }\n        }\n    }\n    public CountDownLatch(int count) {\n        if (count < 0) throw new IllegalArgumentException(\"count < 0\");\n        this.sync = new Sync(count);\n    }\n\n```\n\n# 四、CountDownLatch 的不足\n\nCountDownLatch是一次性的，不可能重新初始化或者修改其内部计数器的值，当CountDownLatch使用完毕后，它不能再次被使用\n\n# 五、CountDownLatch与CyclicBarrier区别\n\ncountDownLatch：\n计数器：计数器只能使用一次。\n等待：一个线程或多个等待另外n个线程完成之后才能执行。\n\nCyclicBarrier：\n计数器：计数器可以重置(通过reset()方法)。\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。\n","source":"_posts/CountDownLatch.md","raw":"title: CountDownLatch\nauthor: ztq\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2021-08-07 13:25:00\n\n---\n\n# 1、CountDownLatch简介\n\n​\t\tCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。\n\n​\t\t类似的任务可以使用线程的  join()  方法实现：在等待时间点调用其他线程的  join()  方法，当前线程就会等待join线程执行完之后才继续执行，但 CountDownLatch 实现更加简单，并且比 join 的功能更多。\n\nCountDownLatch函数列表\n\n```java\nCountDownLatch(int count)\n构造一个用给定计数初始化的 CountDownLatch。\n\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。\nvoid await()\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。\nboolean await(long timeout, TimeUnit unit)\n// 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。\nvoid countDown()\n// 返回当前计数。\nlong getCount()\n// 返回标识此锁存器及其状态的字符串。\nString toString()\n```\n\nCountDownLatch和CyclicBarrier的区别：\n\n- CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。\n- CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。\n\n# 2、CountDownLatch使用示例\n\n```java\npackage com.ztq.task;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class CountDownLatchTest {\n    public static void main(String[] args) throws Exception {\n\n        /*创建CountDownLatch实例,计数器的值初始化为3*/\n        final CountDownLatch downLatch = new CountDownLatch(7);\n\n        /*创建三个线程,每个线程等待1s,表示执行比较耗时的任务*/\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(2, 20, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        pool.execute(new MyTask(\"选手1到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手2到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手3到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手4到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手5到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手6到达终点\", downLatch));\n        pool.execute(new MyTask(\"选手7到达终点\", downLatch));\n\n        /*主线程调用await()方法,等到其他三个线程执行完后才继续执行*/\n        downLatch.await();\n        System.out.println(\"赛跑结束\");\n        pool.shutdown();\n\n    }\n\n\n    static class MyTask extends Thread {\n        private String name;\n        private CountDownLatch downLatch;\n\n        public MyTask(String name, CountDownLatch downLatch) {\n            this.name = name;\n            this.downLatch = downLatch;\n        }\n\n        @Override\n        public void run() {\n            try {\n                Thread.sleep(1000);\n\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n\n            }\n            System.out.println(name);\n            /*任务完成后调用CountDownLatch的countDown()方法*/\n            downLatch.countDown();\n        }\n    }\n}\n\n```\n\n# 3、CountDownLatch源码分析\n\n​\t\t分析CountDownLatch的源码我们可以知道，它是使用了一个内部同步器AQS来实现屏蔽功能的。只有当count的值为零时，同步器的tryAcquireShared的结果为1，其他时候都是-1\n\n```java\n     private static final class Sync extends AbstractQueuedSynchronizer {\n        private static final long serialVersionUID = 4982264981922014374L;\n\n        Sync(int count) {\n            setState(count);\n        }\n\n        int getCount() {\n            return getState();\n        }\n\n        protected int tryAcquireShared(int acquires) {\n            return (getState() == 0) ? 1 : -1;\n        }\n\n        protected boolean tryReleaseShared(int releases) {\n            // Decrement count; signal when transition to zero\n            for (;;) {\n                int c = getState();\n                if (c == 0)\n                    return false;\n                int nextc = c-1;\n                if (compareAndSetState(c, nextc))\n                    return nextc == 0;\n            }\n        }\n    }\n    public CountDownLatch(int count) {\n        if (count < 0) throw new IllegalArgumentException(\"count < 0\");\n        this.sync = new Sync(count);\n    }\n\n```\n\n# 四、CountDownLatch 的不足\n\nCountDownLatch是一次性的，不可能重新初始化或者修改其内部计数器的值，当CountDownLatch使用完毕后，它不能再次被使用\n\n# 五、CountDownLatch与CyclicBarrier区别\n\ncountDownLatch：\n计数器：计数器只能使用一次。\n等待：一个线程或多个等待另外n个线程完成之后才能执行。\n\nCyclicBarrier：\n计数器：计数器可以重置(通过reset()方法)。\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。\n","slug":"CountDownLatch","published":1,"updated":"2022-04-08T14:04:48.373Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyd00077kt97hca36ba","content":"<h1>1、CountDownLatch简介</h1>\n<p>​\t\tCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。</p>\n<p>​\t\t类似的任务可以使用线程的  join()  方法实现：在等待时间点调用其他线程的  join()  方法，当前线程就会等待join线程执行完之后才继续执行，但 CountDownLatch 实现更加简单，并且比 join 的功能更多。</p>\n<p>CountDownLatch函数列表</p>\n<pre><code class=\"language-java\">CountDownLatch(int count)\n构造一个用给定计数初始化的 CountDownLatch。\n\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。\nvoid await()\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。\nboolean await(long timeout, TimeUnit unit)\n// 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。\nvoid countDown()\n// 返回当前计数。\nlong getCount()\n// 返回标识此锁存器及其状态的字符串。\nString toString()\n</code></pre>\n<p>CountDownLatch和CyclicBarrier的区别：</p>\n<ul>\n<li>CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。</li>\n<li>CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。</li>\n</ul>\n<h1>2、CountDownLatch使用示例</h1>\n<pre><code class=\"language-java\">package com.ztq.task;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class CountDownLatchTest &#123;\n    public static void main(String[] args) throws Exception &#123;\n\n        /*创建CountDownLatch实例,计数器的值初始化为3*/\n        final CountDownLatch downLatch = new CountDownLatch(7);\n\n        /*创建三个线程,每个线程等待1s,表示执行比较耗时的任务*/\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(2, 20, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        pool.execute(new MyTask(&quot;选手1到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手2到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手3到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手4到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手5到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手6到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手7到达终点&quot;, downLatch));\n\n        /*主线程调用await()方法,等到其他三个线程执行完后才继续执行*/\n        downLatch.await();\n        System.out.println(&quot;赛跑结束&quot;);\n        pool.shutdown();\n\n    &#125;\n\n\n    static class MyTask extends Thread &#123;\n        private String name;\n        private CountDownLatch downLatch;\n\n        public MyTask(String name, CountDownLatch downLatch) &#123;\n            this.name = name;\n            this.downLatch = downLatch;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                Thread.sleep(1000);\n\n            &#125; catch (InterruptedException e) &#123;\n                e.printStackTrace();\n\n            &#125;\n            System.out.println(name);\n            /*任务完成后调用CountDownLatch的countDown()方法*/\n            downLatch.countDown();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、CountDownLatch源码分析</h1>\n<p>​\t\t分析CountDownLatch的源码我们可以知道，它是使用了一个内部同步器AQS来实现屏蔽功能的。只有当count的值为零时，同步器的tryAcquireShared的结果为1，其他时候都是-1</p>\n<pre><code class=\"language-java\">     private static final class Sync extends AbstractQueuedSynchronizer &#123;\n        private static final long serialVersionUID = 4982264981922014374L;\n\n        Sync(int count) &#123;\n            setState(count);\n        &#125;\n\n        int getCount() &#123;\n            return getState();\n        &#125;\n\n        protected int tryAcquireShared(int acquires) &#123;\n            return (getState() == 0) ? 1 : -1;\n        &#125;\n\n        protected boolean tryReleaseShared(int releases) &#123;\n            // Decrement count; signal when transition to zero\n            for (;;) &#123;\n                int c = getState();\n                if (c == 0)\n                    return false;\n                int nextc = c-1;\n                if (compareAndSetState(c, nextc))\n                    return nextc == 0;\n            &#125;\n        &#125;\n    &#125;\n    public CountDownLatch(int count) &#123;\n        if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;);\n        this.sync = new Sync(count);\n    &#125;\n\n</code></pre>\n<h1>四、CountDownLatch 的不足</h1>\n<p>CountDownLatch是一次性的，不可能重新初始化或者修改其内部计数器的值，当CountDownLatch使用完毕后，它不能再次被使用</p>\n<h1>五、CountDownLatch与CyclicBarrier区别</h1>\n<p>countDownLatch：<br>\n计数器：计数器只能使用一次。<br>\n等待：一个线程或多个等待另外n个线程完成之后才能执行。</p>\n<p>CyclicBarrier：<br>\n计数器：计数器可以重置(通过reset()方法)。<br>\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、CountDownLatch简介</h1>\n<p>​\t\tCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。</p>\n<p>​\t\t类似的任务可以使用线程的  join()  方法实现：在等待时间点调用其他线程的  join()  方法，当前线程就会等待join线程执行完之后才继续执行，但 CountDownLatch 实现更加简单，并且比 join 的功能更多。</p>\n<p>CountDownLatch函数列表</p>\n<pre><code class=\"language-java\">CountDownLatch(int count)\n构造一个用给定计数初始化的 CountDownLatch。\n\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。\nvoid await()\n// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。\nboolean await(long timeout, TimeUnit unit)\n// 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。\nvoid countDown()\n// 返回当前计数。\nlong getCount()\n// 返回标识此锁存器及其状态的字符串。\nString toString()\n</code></pre>\n<p>CountDownLatch和CyclicBarrier的区别：</p>\n<ul>\n<li>CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。</li>\n<li>CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。</li>\n</ul>\n<h1>2、CountDownLatch使用示例</h1>\n<pre><code class=\"language-java\">package com.ztq.task;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class CountDownLatchTest &#123;\n    public static void main(String[] args) throws Exception &#123;\n\n        /*创建CountDownLatch实例,计数器的值初始化为3*/\n        final CountDownLatch downLatch = new CountDownLatch(7);\n\n        /*创建三个线程,每个线程等待1s,表示执行比较耗时的任务*/\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(2, 20, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        pool.execute(new MyTask(&quot;选手1到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手2到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手3到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手4到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手5到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手6到达终点&quot;, downLatch));\n        pool.execute(new MyTask(&quot;选手7到达终点&quot;, downLatch));\n\n        /*主线程调用await()方法,等到其他三个线程执行完后才继续执行*/\n        downLatch.await();\n        System.out.println(&quot;赛跑结束&quot;);\n        pool.shutdown();\n\n    &#125;\n\n\n    static class MyTask extends Thread &#123;\n        private String name;\n        private CountDownLatch downLatch;\n\n        public MyTask(String name, CountDownLatch downLatch) &#123;\n            this.name = name;\n            this.downLatch = downLatch;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                Thread.sleep(1000);\n\n            &#125; catch (InterruptedException e) &#123;\n                e.printStackTrace();\n\n            &#125;\n            System.out.println(name);\n            /*任务完成后调用CountDownLatch的countDown()方法*/\n            downLatch.countDown();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、CountDownLatch源码分析</h1>\n<p>​\t\t分析CountDownLatch的源码我们可以知道，它是使用了一个内部同步器AQS来实现屏蔽功能的。只有当count的值为零时，同步器的tryAcquireShared的结果为1，其他时候都是-1</p>\n<pre><code class=\"language-java\">     private static final class Sync extends AbstractQueuedSynchronizer &#123;\n        private static final long serialVersionUID = 4982264981922014374L;\n\n        Sync(int count) &#123;\n            setState(count);\n        &#125;\n\n        int getCount() &#123;\n            return getState();\n        &#125;\n\n        protected int tryAcquireShared(int acquires) &#123;\n            return (getState() == 0) ? 1 : -1;\n        &#125;\n\n        protected boolean tryReleaseShared(int releases) &#123;\n            // Decrement count; signal when transition to zero\n            for (;;) &#123;\n                int c = getState();\n                if (c == 0)\n                    return false;\n                int nextc = c-1;\n                if (compareAndSetState(c, nextc))\n                    return nextc == 0;\n            &#125;\n        &#125;\n    &#125;\n    public CountDownLatch(int count) &#123;\n        if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;);\n        this.sync = new Sync(count);\n    &#125;\n\n</code></pre>\n<h1>四、CountDownLatch 的不足</h1>\n<p>CountDownLatch是一次性的，不可能重新初始化或者修改其内部计数器的值，当CountDownLatch使用完毕后，它不能再次被使用</p>\n<h1>五、CountDownLatch与CyclicBarrier区别</h1>\n<p>countDownLatch：<br>\n计数器：计数器只能使用一次。<br>\n等待：一个线程或多个等待另外n个线程完成之后才能执行。</p>\n<p>CyclicBarrier：<br>\n计数器：计数器可以重置(通过reset()方法)。<br>\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。</p>\n"},{"title":"DNS","author":"郑天祺","date":"2020-07-21T07:57:00.000Z","_content":"\n# 1、介绍\n\n​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。\n\n​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。\n\n​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。\n\n​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(如.com .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。\n\n​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。\n\n​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：\n\n​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;\n\n​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;\n\n​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。\n\n![image-20200721195559652](/img/DNS1.png)\n\n# 2、DNS解析原理\n\n![image-20200721195702373](/img/DNS2.png)\n\n\n\n通过9步来诠释DNS解析过程：\n\n（1）用户请求通过浏览器输入要访问网站的地址，例如：www.toutiao.com。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。\n\n（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步\n\n（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步\n\n（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址\n\n（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的\n\n（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL\n\n- A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。\n- CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。\n- TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server\n\n（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server\n\n​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。\n\n​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。\n\n（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步\n\n（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上\n\n\n\n\n\n\n\n借鉴于：https://network.51cto.com/art/202003/613009.htm \n\n作者：崔皓来源：[51CTO技术栈](https://www.toutiao.com/i6807234747488535054/)|2020-03-23 15:08","source":"_posts/DNS.md","raw":"title: DNS\nauthor: 郑天祺\ntags:\n\n  - DNS\ncategories:\n  - 网络\ndate: 2020-07-21 15:57:00\n\n---\n\n# 1、介绍\n\n​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。\n\n​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。\n\n​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。\n\n​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(如.com .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。\n\n​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。\n\n​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：\n\n​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;\n\n​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;\n\n​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。\n\n![image-20200721195559652](/img/DNS1.png)\n\n# 2、DNS解析原理\n\n![image-20200721195702373](/img/DNS2.png)\n\n\n\n通过9步来诠释DNS解析过程：\n\n（1）用户请求通过浏览器输入要访问网站的地址，例如：www.toutiao.com。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。\n\n（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步\n\n（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步\n\n（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址\n\n（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的\n\n（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL\n\n- A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。\n- CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。\n- TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server\n\n（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server\n\n​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。\n\n​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。\n\n（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步\n\n（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上\n\n\n\n\n\n\n\n借鉴于：https://network.51cto.com/art/202003/613009.htm \n\n作者：崔皓来源：[51CTO技术栈](https://www.toutiao.com/i6807234747488535054/)|2020-03-23 15:08","slug":"DNS","published":1,"updated":"2022-04-04T08:32:40.136Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyf000b7kt9bks41sxs","content":"<h1>1、介绍</h1>\n<p>​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。</p>\n<p>​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。</p>\n<p>​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。</p>\n<p>​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(<a href=\"http://xn--bvs.com\">如.com</a> .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。</p>\n<p>​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。</p>\n<p>​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：</p>\n<p>​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;</p>\n<p>​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;</p>\n<p>​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。</p>\n<p><img src=\"/img/DNS1.png\" alt=\"image-20200721195559652\"></p>\n<h1>2、DNS解析原理</h1>\n<p><img src=\"/img/DNS2.png\" alt=\"image-20200721195702373\"></p>\n<p>通过9步来诠释DNS解析过程：</p>\n<p>（1）用户请求通过浏览器输入要访问网站的地址，例如：<a href=\"http://www.toutiao.com\">www.toutiao.com</a>。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。</p>\n<p>（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步</p>\n<p>（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步</p>\n<p>（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址</p>\n<p>（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的</p>\n<p>（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL</p>\n<ul>\n<li>A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。</li>\n<li>CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“<a href=\"http://host.mydomain.com\">host.mydomain.com</a>”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。</li>\n<li>TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server</li>\n</ul>\n<p>（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server</p>\n<p>​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。</p>\n<p>​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。</p>\n<p>（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步</p>\n<p>（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上</p>\n<p>借鉴于：<a href=\"https://network.51cto.com/art/202003/613009.htm\">https://network.51cto.com/art/202003/613009.htm</a></p>\n<p>作者：崔皓来源：<a href=\"https://www.toutiao.com/i6807234747488535054/\">51CTO技术栈</a>|2020-03-23 15:08</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。</p>\n<p>​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。</p>\n<p>​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。</p>\n<p>​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(<a href=\"http://xn--bvs.com\">如.com</a> .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。</p>\n<p>​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。</p>\n<p>​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：</p>\n<p>​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;</p>\n<p>​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;</p>\n<p>​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。</p>\n<p><img src=\"/img/DNS1.png\" alt=\"image-20200721195559652\"></p>\n<h1>2、DNS解析原理</h1>\n<p><img src=\"/img/DNS2.png\" alt=\"image-20200721195702373\"></p>\n<p>通过9步来诠释DNS解析过程：</p>\n<p>（1）用户请求通过浏览器输入要访问网站的地址，例如：<a href=\"http://www.toutiao.com\">www.toutiao.com</a>。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。</p>\n<p>（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步</p>\n<p>（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步</p>\n<p>（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址</p>\n<p>（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的</p>\n<p>（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL</p>\n<ul>\n<li>A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。</li>\n<li>CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“<a href=\"http://host.mydomain.com\">host.mydomain.com</a>”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。</li>\n<li>TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server</li>\n</ul>\n<p>（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server</p>\n<p>​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。</p>\n<p>​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。</p>\n<p>（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步</p>\n<p>（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上</p>\n<p>借鉴于：<a href=\"https://network.51cto.com/art/202003/613009.htm\">https://network.51cto.com/art/202003/613009.htm</a></p>\n<p>作者：崔皓来源：<a href=\"https://www.toutiao.com/i6807234747488535054/\">51CTO技术栈</a>|2020-03-23 15:08</p>\n"},{"title":"Disruptor中发布事件相关类","author":"ztq","date":"2021-04-13T06:27:00.000Z","_content":"\n \n\n### Disruptor中发布事件相关类\n\n#### RingBuffer、EventFactory\n\nEventFactory：提供给RingBuffer做事件预填充\n\nEvent事件：\n\n1、从生产者到消费者过程中所处理的数据单元；\n\n2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；\n\n \n\n### Disruptor中的等待策略\n\n#### WaitStrategy\n\n等待策略的接口\n\n#### BlockingWaitStrategy\n\nBlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略\n\n \n\n#### BusySpinWaitStrategy\n\nBusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。\n\n \n\n#### LiteBlockingWaitStrategy\n\n试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。\n\n从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。\n\n \n\n#### LiteTimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒\n\n \n\n#### PhasedBackoffWaitStrategy\n\nPhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。\n\n \n\n#### SleepingWaitStrategy\n\nSleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。\n\n \n\n#### TimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。\n\n \n\n#### YieldingWaitStrategy\n\nYielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。\n\n \n\n### 总结\n\n| 等待策略  所在包com.Imax.disruptor    | 描述                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| Class BlockingWaitStrategy            | 阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。 |\n| Class BusySpinWaitStrategy            | 自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。 |\n| Class LiteBlockingWaitStrategy        | 阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。 |\n| Class TimeoutBlockingWaitStrategy     | 阻塞给定的时间，超过时间的话会抛出超时异常。                 |\n| Class LiteTimeoutBlockingWaitStrategy | TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。 |\n| Class PhasedBackoffWaitStrategy       | 先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 |\n| Class SleepingWaitStrategy            | 先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。 |\n| Class YieldingWaitStrategy            | 在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。 |\n| Interface WaitStrategy                | 上述等待策略实现接口                                         |\n\n\n\n| 工具类 所在包com.imax.disruptor.util          | 描述                                                         |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| Enum DaemonThreadFactory                      | 访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程 |\n| Class ThreadHints                             | 用于运行时提高代码性能的提示，                               |\n| Class Util                                    | 主要用于计算的工具类                                         |\n| Enum BasicExecutor （com.lmax.disruptor.dsl） | 只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象 |\n| Class BasicExecutor（com.lmax.disruptor.dsl） | 默认提供的线程池对象                                         |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n\n\n\n| 序列类 所在包com.imax.disruptor         | 描述                                                         |\n| --------------------------------------- | ------------------------------------------------------------ |\n| Class Sequence                          | 环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作 |\n| Class SequenceGroup                     | 继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。 |\n| Class SequenceGroups                    | 用于管理SequenceGroup对象的静态方法                          |\n| Class FixedSequenceGroup                | 包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的\"写\"方法都不支持。 |\n| Interface Sequencer                     | 通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier |\n| Interface SequenceBarrier               | 消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。 |\n| Interface ProcessingSequenceBarrier     | SequenceBarrier的具体实现                                    |\n| Class AbstractSequencer                 | AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列 |\n| Class SingleProducerSequencer           | 申请序列，发布序列，唤醒消费者                               |\n| Class MultiProducerSequencer            | 适用于多线程的消费者，申请序列，发布序列，唤醒消费者         |\n| Interface Sequenced                     | Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的 |\n| Interface Cursored                      | Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。 |\n| Interface SequenceReportingEventHandler | 在完成消费事件时通知并设置回调                               |\n\n\n\n| 队列类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface DataProvider          | 提供了按序列值来获取数据的功能                               |\n| Interface EventSink             | EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。 |\n| Class RingBuffer                | 数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。 |\n| Class DataProvider              | DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 * |\n|                                 |                                                              |\n|                                 |                                                              |\n|                                 |                                                              |\n\n \n\n| 异常处理类 所在包com.imax.disruptor                     |                                                              |\n| ------------------------------------------------------- | ------------------------------------------------------------ |\n| Interface ExceptionHandler                              | 事件处理周期中未捕获异常的回调处理程序的接口                 |\n| Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)   | 异常处理的包装类                                             |\n| Class IgnoreExceptionHandler                            | INFO的异常处理程序的便捷实现                                 |\n| Class FatalExceptionHandler                             | SEVERE(严重)的异常处理程序的便捷实                           |\n| Class InsufficientCapacityException                     | 如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常 |\n| Class ExceptionHandlerSetting（com.lmax.disruptor.dsl） | 为特定消费者设置异常处理程序的支持类                         |\n|                                                         |                                                              |\n\n\n\n| 事件类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Inetface EventSink              | 这个类主要是提供发布事件(就是往队列上放数据)的功能           |\n| Interface EventFactory          | 由RingBuffer调用，以预先调用所有事件以填充RingBuffer         |\n| Interface EventHandler          | 回调接口，用于处理RingBuffer中可用的事件                     |\n| Class EventPoller               | 用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller |\n| Interface EventProcessor        | 事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件 |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface EventTranslator       | 在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。 |\n\n \n\n时序图\n\n![img](/img/8ab72b98.png)\n\n 类图\n\n![1618295759372](/img/1618295759372.png)\n\n参考资料：https://brokendreams.iteye.com/blog/2255720\n\nhttp://www.ibigdata.io/?p=92\n\n ","source":"_posts/Disruptor中发布事件相关类.md","raw":"title: Disruptor中发布事件相关类\nauthor: ztq\ntags:\n\n  - Disruptor\ncategories:\n  - 分布式\ndate: 2021-04-13 14:27:00\n\n---\n\n \n\n### Disruptor中发布事件相关类\n\n#### RingBuffer、EventFactory\n\nEventFactory：提供给RingBuffer做事件预填充\n\nEvent事件：\n\n1、从生产者到消费者过程中所处理的数据单元；\n\n2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；\n\n \n\n### Disruptor中的等待策略\n\n#### WaitStrategy\n\n等待策略的接口\n\n#### BlockingWaitStrategy\n\nBlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略\n\n \n\n#### BusySpinWaitStrategy\n\nBusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。\n\n \n\n#### LiteBlockingWaitStrategy\n\n试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。\n\n从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。\n\n \n\n#### LiteTimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒\n\n \n\n#### PhasedBackoffWaitStrategy\n\nPhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。\n\n \n\n#### SleepingWaitStrategy\n\nSleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。\n\n \n\n#### TimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。\n\n \n\n#### YieldingWaitStrategy\n\nYielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。\n\n \n\n### 总结\n\n| 等待策略  所在包com.Imax.disruptor    | 描述                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| Class BlockingWaitStrategy            | 阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。 |\n| Class BusySpinWaitStrategy            | 自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。 |\n| Class LiteBlockingWaitStrategy        | 阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。 |\n| Class TimeoutBlockingWaitStrategy     | 阻塞给定的时间，超过时间的话会抛出超时异常。                 |\n| Class LiteTimeoutBlockingWaitStrategy | TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。 |\n| Class PhasedBackoffWaitStrategy       | 先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 |\n| Class SleepingWaitStrategy            | 先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。 |\n| Class YieldingWaitStrategy            | 在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。 |\n| Interface WaitStrategy                | 上述等待策略实现接口                                         |\n\n\n\n| 工具类 所在包com.imax.disruptor.util          | 描述                                                         |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| Enum DaemonThreadFactory                      | 访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程 |\n| Class ThreadHints                             | 用于运行时提高代码性能的提示，                               |\n| Class Util                                    | 主要用于计算的工具类                                         |\n| Enum BasicExecutor （com.lmax.disruptor.dsl） | 只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象 |\n| Class BasicExecutor（com.lmax.disruptor.dsl） | 默认提供的线程池对象                                         |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n\n\n\n| 序列类 所在包com.imax.disruptor         | 描述                                                         |\n| --------------------------------------- | ------------------------------------------------------------ |\n| Class Sequence                          | 环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作 |\n| Class SequenceGroup                     | 继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。 |\n| Class SequenceGroups                    | 用于管理SequenceGroup对象的静态方法                          |\n| Class FixedSequenceGroup                | 包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的\"写\"方法都不支持。 |\n| Interface Sequencer                     | 通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier |\n| Interface SequenceBarrier               | 消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。 |\n| Interface ProcessingSequenceBarrier     | SequenceBarrier的具体实现                                    |\n| Class AbstractSequencer                 | AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列 |\n| Class SingleProducerSequencer           | 申请序列，发布序列，唤醒消费者                               |\n| Class MultiProducerSequencer            | 适用于多线程的消费者，申请序列，发布序列，唤醒消费者         |\n| Interface Sequenced                     | Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的 |\n| Interface Cursored                      | Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。 |\n| Interface SequenceReportingEventHandler | 在完成消费事件时通知并设置回调                               |\n\n\n\n| 队列类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface DataProvider          | 提供了按序列值来获取数据的功能                               |\n| Interface EventSink             | EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。 |\n| Class RingBuffer                | 数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。 |\n| Class DataProvider              | DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 * |\n|                                 |                                                              |\n|                                 |                                                              |\n|                                 |                                                              |\n\n \n\n| 异常处理类 所在包com.imax.disruptor                     |                                                              |\n| ------------------------------------------------------- | ------------------------------------------------------------ |\n| Interface ExceptionHandler                              | 事件处理周期中未捕获异常的回调处理程序的接口                 |\n| Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)   | 异常处理的包装类                                             |\n| Class IgnoreExceptionHandler                            | INFO的异常处理程序的便捷实现                                 |\n| Class FatalExceptionHandler                             | SEVERE(严重)的异常处理程序的便捷实                           |\n| Class InsufficientCapacityException                     | 如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常 |\n| Class ExceptionHandlerSetting（com.lmax.disruptor.dsl） | 为特定消费者设置异常处理程序的支持类                         |\n|                                                         |                                                              |\n\n\n\n| 事件类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Inetface EventSink              | 这个类主要是提供发布事件(就是往队列上放数据)的功能           |\n| Interface EventFactory          | 由RingBuffer调用，以预先调用所有事件以填充RingBuffer         |\n| Interface EventHandler          | 回调接口，用于处理RingBuffer中可用的事件                     |\n| Class EventPoller               | 用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller |\n| Interface EventProcessor        | 事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件 |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface EventTranslator       | 在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。 |\n\n \n\n时序图\n\n![img](/img/8ab72b98.png)\n\n 类图\n\n![1618295759372](/img/1618295759372.png)\n\n参考资料：https://brokendreams.iteye.com/blog/2255720\n\nhttp://www.ibigdata.io/?p=92\n\n ","slug":"Disruptor中发布事件相关类","published":1,"updated":"2022-04-04T08:32:40.137Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyg000c7kt99ach53aa","content":"<h3 id=\"Disruptor中发布事件相关类\">Disruptor中发布事件相关类</h3>\n<h4 id=\"RingBuffer、EventFactory\">RingBuffer、EventFactory</h4>\n<p>EventFactory：提供给RingBuffer做事件预填充</p>\n<p>Event事件：</p>\n<p>1、从生产者到消费者过程中所处理的数据单元；</p>\n<p>2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；</p>\n<h3 id=\"Disruptor中的等待策略\">Disruptor中的等待策略</h3>\n<h4 id=\"WaitStrategy\">WaitStrategy</h4>\n<p>等待策略的接口</p>\n<h4 id=\"BlockingWaitStrategy\">BlockingWaitStrategy</h4>\n<p>BlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略</p>\n<h4 id=\"BusySpinWaitStrategy\">BusySpinWaitStrategy</h4>\n<p>BusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</p>\n<h4 id=\"LiteBlockingWaitStrategy\">LiteBlockingWaitStrategy</h4>\n<p>试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。</p>\n<p>从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</p>\n<h4 id=\"LiteTimeoutBlockingWaitStrategy\">LiteTimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒</p>\n<h4 id=\"PhasedBackoffWaitStrategy\">PhasedBackoffWaitStrategy</h4>\n<p>PhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<h4 id=\"SleepingWaitStrategy\">SleepingWaitStrategy</h4>\n<p>SleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。<br>\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</p>\n<h4 id=\"TimeoutBlockingWaitStrategy\">TimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。</p>\n<h4 id=\"YieldingWaitStrategy\">YieldingWaitStrategy</h4>\n<p>Yielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。</p>\n<h3 id=\"总结\">总结</h3>\n<table>\n<thead>\n<tr>\n<th>等待策略  所在包com.Imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class BlockingWaitStrategy</td>\n<td>阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。</td>\n</tr>\n<tr>\n<td>Class BusySpinWaitStrategy</td>\n<td>自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</td>\n</tr>\n<tr>\n<td>Class LiteBlockingWaitStrategy</td>\n<td>阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</td>\n</tr>\n<tr>\n<td>Class TimeoutBlockingWaitStrategy</td>\n<td>阻塞给定的时间，超过时间的话会抛出超时异常。</td>\n</tr>\n<tr>\n<td>Class LiteTimeoutBlockingWaitStrategy</td>\n<td>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。</td>\n</tr>\n<tr>\n<td>Class PhasedBackoffWaitStrategy</td>\n<td>先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</td>\n</tr>\n<tr>\n<td>Class SleepingWaitStrategy</td>\n<td>先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</td>\n</tr>\n<tr>\n<td>Class YieldingWaitStrategy</td>\n<td>在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。</td>\n</tr>\n<tr>\n<td>Interface WaitStrategy</td>\n<td>上述等待策略实现接口</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>工具类 所在包com.imax.disruptor.util</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Enum DaemonThreadFactory</td>\n<td>访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程</td>\n</tr>\n<tr>\n<td>Class ThreadHints</td>\n<td>用于运行时提高代码性能的提示，</td>\n</tr>\n<tr>\n<td>Class Util</td>\n<td>主要用于计算的工具类</td>\n</tr>\n<tr>\n<td>Enum BasicExecutor （com.lmax.disruptor.dsl）</td>\n<td>只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象</td>\n</tr>\n<tr>\n<td>Class BasicExecutor（com.lmax.disruptor.dsl）</td>\n<td>默认提供的线程池对象</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>序列类 所在包com.imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class Sequence</td>\n<td>环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作</td>\n</tr>\n<tr>\n<td>Class SequenceGroup</td>\n<td>继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。</td>\n</tr>\n<tr>\n<td>Class SequenceGroups</td>\n<td>用于管理SequenceGroup对象的静态方法</td>\n</tr>\n<tr>\n<td>Class FixedSequenceGroup</td>\n<td>包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的&quot;写&quot;方法都不支持。</td>\n</tr>\n<tr>\n<td>Interface Sequencer</td>\n<td>通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier</td>\n</tr>\n<tr>\n<td>Interface SequenceBarrier</td>\n<td>消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。</td>\n</tr>\n<tr>\n<td>Interface ProcessingSequenceBarrier</td>\n<td>SequenceBarrier的具体实现</td>\n</tr>\n<tr>\n<td>Class AbstractSequencer</td>\n<td>AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列</td>\n</tr>\n<tr>\n<td>Class SingleProducerSequencer</td>\n<td>申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Class MultiProducerSequencer</td>\n<td>适用于多线程的消费者，申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Interface Sequenced</td>\n<td>Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的</td>\n</tr>\n<tr>\n<td>Interface Cursored</td>\n<td>Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。</td>\n</tr>\n<tr>\n<td>Interface SequenceReportingEventHandler</td>\n<td>在完成消费事件时通知并设置回调</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>队列类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface DataProvider</td>\n<td>提供了按序列值来获取数据的功能</td>\n</tr>\n<tr>\n<td>Interface EventSink</td>\n<td>EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。</td>\n</tr>\n<tr>\n<td>Class RingBuffer</td>\n<td>数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。</td>\n</tr>\n<tr>\n<td>Class DataProvider</td>\n<td>DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 *</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>异常处理类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface ExceptionHandler</td>\n<td>事件处理周期中未捕获异常的回调处理程序的接口</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)</td>\n<td>异常处理的包装类</td>\n</tr>\n<tr>\n<td>Class IgnoreExceptionHandler</td>\n<td>INFO的异常处理程序的便捷实现</td>\n</tr>\n<tr>\n<td>Class FatalExceptionHandler</td>\n<td>SEVERE(严重)的异常处理程序的便捷实</td>\n</tr>\n<tr>\n<td>Class InsufficientCapacityException</td>\n<td>如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerSetting（com.lmax.disruptor.dsl）</td>\n<td>为特定消费者设置异常处理程序的支持类</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>事件类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Inetface EventSink</td>\n<td>这个类主要是提供发布事件(就是往队列上放数据)的功能</td>\n</tr>\n<tr>\n<td>Interface EventFactory</td>\n<td>由RingBuffer调用，以预先调用所有事件以填充RingBuffer</td>\n</tr>\n<tr>\n<td>Interface EventHandler</td>\n<td>回调接口，用于处理RingBuffer中可用的事件</td>\n</tr>\n<tr>\n<td>Class EventPoller</td>\n<td>用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller</td>\n</tr>\n<tr>\n<td>Interface EventProcessor</td>\n<td>事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件</td>\n</tr>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface EventTranslator</td>\n<td>在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。</td>\n</tr>\n</tbody>\n</table>\n<p>时序图</p>\n<p><img src=\"/img/8ab72b98.png\" alt=\"img\"></p>\n<p>类图</p>\n<p><img src=\"/img/1618295759372.png\" alt=\"1618295759372\"></p>\n<p>参考资料：<a href=\"https://brokendreams.iteye.com/blog/2255720\">https://brokendreams.iteye.com/blog/2255720</a></p>\n<p><a href=\"http://www.ibigdata.io/?p=92\">http://www.ibigdata.io/?p=92</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Disruptor中发布事件相关类\">Disruptor中发布事件相关类</h3>\n<h4 id=\"RingBuffer、EventFactory\">RingBuffer、EventFactory</h4>\n<p>EventFactory：提供给RingBuffer做事件预填充</p>\n<p>Event事件：</p>\n<p>1、从生产者到消费者过程中所处理的数据单元；</p>\n<p>2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；</p>\n<h3 id=\"Disruptor中的等待策略\">Disruptor中的等待策略</h3>\n<h4 id=\"WaitStrategy\">WaitStrategy</h4>\n<p>等待策略的接口</p>\n<h4 id=\"BlockingWaitStrategy\">BlockingWaitStrategy</h4>\n<p>BlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略</p>\n<h4 id=\"BusySpinWaitStrategy\">BusySpinWaitStrategy</h4>\n<p>BusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</p>\n<h4 id=\"LiteBlockingWaitStrategy\">LiteBlockingWaitStrategy</h4>\n<p>试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。</p>\n<p>从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</p>\n<h4 id=\"LiteTimeoutBlockingWaitStrategy\">LiteTimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒</p>\n<h4 id=\"PhasedBackoffWaitStrategy\">PhasedBackoffWaitStrategy</h4>\n<p>PhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<h4 id=\"SleepingWaitStrategy\">SleepingWaitStrategy</h4>\n<p>SleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。<br>\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</p>\n<h4 id=\"TimeoutBlockingWaitStrategy\">TimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。</p>\n<h4 id=\"YieldingWaitStrategy\">YieldingWaitStrategy</h4>\n<p>Yielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。</p>\n<h3 id=\"总结\">总结</h3>\n<table>\n<thead>\n<tr>\n<th>等待策略  所在包com.Imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class BlockingWaitStrategy</td>\n<td>阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。</td>\n</tr>\n<tr>\n<td>Class BusySpinWaitStrategy</td>\n<td>自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</td>\n</tr>\n<tr>\n<td>Class LiteBlockingWaitStrategy</td>\n<td>阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</td>\n</tr>\n<tr>\n<td>Class TimeoutBlockingWaitStrategy</td>\n<td>阻塞给定的时间，超过时间的话会抛出超时异常。</td>\n</tr>\n<tr>\n<td>Class LiteTimeoutBlockingWaitStrategy</td>\n<td>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。</td>\n</tr>\n<tr>\n<td>Class PhasedBackoffWaitStrategy</td>\n<td>先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</td>\n</tr>\n<tr>\n<td>Class SleepingWaitStrategy</td>\n<td>先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</td>\n</tr>\n<tr>\n<td>Class YieldingWaitStrategy</td>\n<td>在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。</td>\n</tr>\n<tr>\n<td>Interface WaitStrategy</td>\n<td>上述等待策略实现接口</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>工具类 所在包com.imax.disruptor.util</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Enum DaemonThreadFactory</td>\n<td>访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程</td>\n</tr>\n<tr>\n<td>Class ThreadHints</td>\n<td>用于运行时提高代码性能的提示，</td>\n</tr>\n<tr>\n<td>Class Util</td>\n<td>主要用于计算的工具类</td>\n</tr>\n<tr>\n<td>Enum BasicExecutor （com.lmax.disruptor.dsl）</td>\n<td>只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象</td>\n</tr>\n<tr>\n<td>Class BasicExecutor（com.lmax.disruptor.dsl）</td>\n<td>默认提供的线程池对象</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>序列类 所在包com.imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class Sequence</td>\n<td>环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作</td>\n</tr>\n<tr>\n<td>Class SequenceGroup</td>\n<td>继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。</td>\n</tr>\n<tr>\n<td>Class SequenceGroups</td>\n<td>用于管理SequenceGroup对象的静态方法</td>\n</tr>\n<tr>\n<td>Class FixedSequenceGroup</td>\n<td>包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的&quot;写&quot;方法都不支持。</td>\n</tr>\n<tr>\n<td>Interface Sequencer</td>\n<td>通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier</td>\n</tr>\n<tr>\n<td>Interface SequenceBarrier</td>\n<td>消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。</td>\n</tr>\n<tr>\n<td>Interface ProcessingSequenceBarrier</td>\n<td>SequenceBarrier的具体实现</td>\n</tr>\n<tr>\n<td>Class AbstractSequencer</td>\n<td>AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列</td>\n</tr>\n<tr>\n<td>Class SingleProducerSequencer</td>\n<td>申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Class MultiProducerSequencer</td>\n<td>适用于多线程的消费者，申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Interface Sequenced</td>\n<td>Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的</td>\n</tr>\n<tr>\n<td>Interface Cursored</td>\n<td>Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。</td>\n</tr>\n<tr>\n<td>Interface SequenceReportingEventHandler</td>\n<td>在完成消费事件时通知并设置回调</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>队列类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface DataProvider</td>\n<td>提供了按序列值来获取数据的功能</td>\n</tr>\n<tr>\n<td>Interface EventSink</td>\n<td>EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。</td>\n</tr>\n<tr>\n<td>Class RingBuffer</td>\n<td>数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。</td>\n</tr>\n<tr>\n<td>Class DataProvider</td>\n<td>DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 *</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>异常处理类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface ExceptionHandler</td>\n<td>事件处理周期中未捕获异常的回调处理程序的接口</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)</td>\n<td>异常处理的包装类</td>\n</tr>\n<tr>\n<td>Class IgnoreExceptionHandler</td>\n<td>INFO的异常处理程序的便捷实现</td>\n</tr>\n<tr>\n<td>Class FatalExceptionHandler</td>\n<td>SEVERE(严重)的异常处理程序的便捷实</td>\n</tr>\n<tr>\n<td>Class InsufficientCapacityException</td>\n<td>如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerSetting（com.lmax.disruptor.dsl）</td>\n<td>为特定消费者设置异常处理程序的支持类</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>事件类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Inetface EventSink</td>\n<td>这个类主要是提供发布事件(就是往队列上放数据)的功能</td>\n</tr>\n<tr>\n<td>Interface EventFactory</td>\n<td>由RingBuffer调用，以预先调用所有事件以填充RingBuffer</td>\n</tr>\n<tr>\n<td>Interface EventHandler</td>\n<td>回调接口，用于处理RingBuffer中可用的事件</td>\n</tr>\n<tr>\n<td>Class EventPoller</td>\n<td>用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller</td>\n</tr>\n<tr>\n<td>Interface EventProcessor</td>\n<td>事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件</td>\n</tr>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface EventTranslator</td>\n<td>在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。</td>\n</tr>\n</tbody>\n</table>\n<p>时序图</p>\n<p><img src=\"/img/8ab72b98.png\" alt=\"img\"></p>\n<p>类图</p>\n<p><img src=\"/img/1618295759372.png\" alt=\"1618295759372\"></p>\n<p>参考资料：<a href=\"https://brokendreams.iteye.com/blog/2255720\">https://brokendreams.iteye.com/blog/2255720</a></p>\n<p><a href=\"http://www.ibigdata.io/?p=92\">http://www.ibigdata.io/?p=92</a></p>\n"},{"title":"Docker入门","author":"郑天祺","date":"2020-12-14T05:13:00.000Z","_content":"\n\n\n## docker概念\t\t\n\n​\t\tdocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；\n​\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。\n\n![image-20201214131527522](/img/image-20201214131527522.png)\n\n## docker架构\n\n![image-20201214131543066](/img/image-20201214131543066.png)\n\n## docker的组成元素\n\n•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(http://hub.Docker.com)\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。\n\n​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；\n\n## docker安装\n\n安装 参考[docker官网](http://www.docker.com/products/docker)\n\n查看安装版本 \n\n```java\ndocker version\n```\n\n![image-20201214131734289](/img/image-20201214131734289.png)\n\n## 测试镜像库\n\n为docker 添加国内镜像 \n\n/etc/docker/daemon.json将: \n\n{ \"registry-mirrors\": [\" https://obou6wyb.mirror.aliyuncs.com\"]}\n\n替换为 { \"dns\" : [ \"192.168.101.2\" , \"8.8.8.8\" ], \"registry-mirrors\" : [ \"https://docker.mirrors.ustc.edu.cn\" ] } \n\n## 重启docker \n\n```java\nsystemctl start docker\n```\n\n## 查看资源库有tomcat镜像\n\n```java\ndocker search tomcat\n```\n\n![image-20201214131814678](/img/image-20201214131814678.png)\n\n## 从国内docker镜像库下载tomcat、centos\n\n```java\ndocker pull tomcat/centos/nginx\n```\n\n## 查看有哪些镜像 \n\n```java\ndocker images\n```\n\n![image-20201214131847691](/img/image-20201214131847691.png)\n\n## 启动基于tomcat,centos镜像启动容器 \n\n```java\n  docker run -p 8081:8080 tomcat \n```\n\n​\t若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口\n\n```java\ndocker run -dit -p 4000:4000 centos \n\n-d 以守护态运行 \n-p 宿主机端口映射容器端口 \n-i 允许容器内标准输入 \n-t 新容器内指定一个伪终端 \n```\n\n浏览器查看访问容器tomcat实例http://192.168.6.71:8081/\n\n![image-20201214131948506](/img/image-20201214131948506.png)\n\n第一个容器服务部署成功了！\n\n## 进去伪终端查看 \n\ndocker登录容器 \n\n```java\ndocker exec -it hardcore_edison  \"/bin/bash\"\n```\n\n![image-20201214132014480](/img/image-20201214132014480.png)\n\n## 本地文件复制容器中\n\n```java\ndocker cp localFile containerID:targetAddress\n```\n\n 命令： \n\n```java\ndocker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n```\n\n将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：\n\n![image-20201214132059091](/img/image-20201214132059091.png)\n\n以上就是docker的简单入门操作；\n\n 构建一个docker镜像需要写一个叫做Dockerfile的文件\n 先查看下本地镜像有哪些？\n\n![image-20201214132127621](/img/image-20201214132127621.png)\n\n在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：\n\n```java\nmkdir dockerDemo && cd dockerDemo && touch Dockerfile\n```\n\n接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）\n\n```java\n vim Dockerfile\n```\n\n\n\n```java\n#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module && make && make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n```\n\n这里以编译nginx提供web服务来构建新的镜像\n\n下载nginx源码包到docker_demo这个目录下\n\nwget -c https://nginx.org/download/nginx-1.12.2.tar.gz\n\n![image-20201214132215125](/img/image-20201214132215125.png)\n\n## 构建nginx:v1版本镜像\n\n```java\ndocker build -t centos_nginx:v1 .\n```\n\n## 观察日志\n\n每一个步骤都成功\n\n![image-20201214132254636](/img/image-20201214132254636.png)\n\n## 构建步骤\n\n成功构建centos_nginx:v1\n\n![image-20201214132309723](/img/image-20201214132309723.png)\n\n```java\ndocker images\n```\n\n![image-20201214132326043](/img/image-20201214132326043.png)\n\n## 启动容器\n\n```java\ndocker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g \"daemon off;\"\n```\n\n![image-20201214132343909](/img/image-20201214132343909.png)\n\n## 查看镜像对外暴露端口号\n\n```java\ndocker port containerID\n```\n\n![image-20201214132401207](/img/image-20201214132401207.png)\n\n## 浏览器查看nginx启动状态\n\n![image-20201214132416329](/img/image-20201214132416329.png)\n\n已经完成第一个nginx的镜像构建以及容器启动；","source":"_posts/Docker入门.md","raw":"title: Docker入门\nauthor: 郑天祺\ntags:\n  - docker\ncategories:\n  - CICD\ndate: 2020-12-14 13:13:00\n---\n\n\n\n## docker概念\t\t\n\n​\t\tdocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；\n​\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。\n\n![image-20201214131527522](/img/image-20201214131527522.png)\n\n## docker架构\n\n![image-20201214131543066](/img/image-20201214131543066.png)\n\n## docker的组成元素\n\n•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(http://hub.Docker.com)\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。\n\n​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；\n\n## docker安装\n\n安装 参考[docker官网](http://www.docker.com/products/docker)\n\n查看安装版本 \n\n```java\ndocker version\n```\n\n![image-20201214131734289](/img/image-20201214131734289.png)\n\n## 测试镜像库\n\n为docker 添加国内镜像 \n\n/etc/docker/daemon.json将: \n\n{ \"registry-mirrors\": [\" https://obou6wyb.mirror.aliyuncs.com\"]}\n\n替换为 { \"dns\" : [ \"192.168.101.2\" , \"8.8.8.8\" ], \"registry-mirrors\" : [ \"https://docker.mirrors.ustc.edu.cn\" ] } \n\n## 重启docker \n\n```java\nsystemctl start docker\n```\n\n## 查看资源库有tomcat镜像\n\n```java\ndocker search tomcat\n```\n\n![image-20201214131814678](/img/image-20201214131814678.png)\n\n## 从国内docker镜像库下载tomcat、centos\n\n```java\ndocker pull tomcat/centos/nginx\n```\n\n## 查看有哪些镜像 \n\n```java\ndocker images\n```\n\n![image-20201214131847691](/img/image-20201214131847691.png)\n\n## 启动基于tomcat,centos镜像启动容器 \n\n```java\n  docker run -p 8081:8080 tomcat \n```\n\n​\t若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口\n\n```java\ndocker run -dit -p 4000:4000 centos \n\n-d 以守护态运行 \n-p 宿主机端口映射容器端口 \n-i 允许容器内标准输入 \n-t 新容器内指定一个伪终端 \n```\n\n浏览器查看访问容器tomcat实例http://192.168.6.71:8081/\n\n![image-20201214131948506](/img/image-20201214131948506.png)\n\n第一个容器服务部署成功了！\n\n## 进去伪终端查看 \n\ndocker登录容器 \n\n```java\ndocker exec -it hardcore_edison  \"/bin/bash\"\n```\n\n![image-20201214132014480](/img/image-20201214132014480.png)\n\n## 本地文件复制容器中\n\n```java\ndocker cp localFile containerID:targetAddress\n```\n\n 命令： \n\n```java\ndocker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n```\n\n将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：\n\n![image-20201214132059091](/img/image-20201214132059091.png)\n\n以上就是docker的简单入门操作；\n\n 构建一个docker镜像需要写一个叫做Dockerfile的文件\n 先查看下本地镜像有哪些？\n\n![image-20201214132127621](/img/image-20201214132127621.png)\n\n在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：\n\n```java\nmkdir dockerDemo && cd dockerDemo && touch Dockerfile\n```\n\n接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）\n\n```java\n vim Dockerfile\n```\n\n\n\n```java\n#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module && make && make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n```\n\n这里以编译nginx提供web服务来构建新的镜像\n\n下载nginx源码包到docker_demo这个目录下\n\nwget -c https://nginx.org/download/nginx-1.12.2.tar.gz\n\n![image-20201214132215125](/img/image-20201214132215125.png)\n\n## 构建nginx:v1版本镜像\n\n```java\ndocker build -t centos_nginx:v1 .\n```\n\n## 观察日志\n\n每一个步骤都成功\n\n![image-20201214132254636](/img/image-20201214132254636.png)\n\n## 构建步骤\n\n成功构建centos_nginx:v1\n\n![image-20201214132309723](/img/image-20201214132309723.png)\n\n```java\ndocker images\n```\n\n![image-20201214132326043](/img/image-20201214132326043.png)\n\n## 启动容器\n\n```java\ndocker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g \"daemon off;\"\n```\n\n![image-20201214132343909](/img/image-20201214132343909.png)\n\n## 查看镜像对外暴露端口号\n\n```java\ndocker port containerID\n```\n\n![image-20201214132401207](/img/image-20201214132401207.png)\n\n## 浏览器查看nginx启动状态\n\n![image-20201214132416329](/img/image-20201214132416329.png)\n\n已经完成第一个nginx的镜像构建以及容器启动；","slug":"Docker入门","published":1,"updated":"2022-04-04T08:32:40.137Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyh000g7kt96c594hcn","content":"<h2 id=\"docker概念\">docker概念</h2>\n<p>​\t\tdocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；<br>\n​\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。</p>\n<p><img src=\"/img/image-20201214131527522.png\" alt=\"image-20201214131527522\"></p>\n<h2 id=\"docker架构\">docker架构</h2>\n<p><img src=\"/img/image-20201214131543066.png\" alt=\"image-20201214131543066\"></p>\n<h2 id=\"docker的组成元素\">docker的组成元素</h2>\n<p>•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。<br>\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。<br>\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。<br>\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(<a href=\"http://hub.Docker.com\">http://hub.Docker.com</a>)<br>\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。</p>\n<p>​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；</p>\n<h2 id=\"docker安装\">docker安装</h2>\n<p>安装 参考<a href=\"http://www.docker.com/products/docker\">docker官网</a></p>\n<p>查看安装版本</p>\n<pre><code class=\"language-java\">docker version\n</code></pre>\n<p><img src=\"/img/image-20201214131734289.png\" alt=\"image-20201214131734289\"></p>\n<h2 id=\"测试镜像库\">测试镜像库</h2>\n<p>为docker 添加国内镜像</p>\n<p>/etc/docker/daemon.json将:</p>\n<p>{ “registry-mirrors”: [&quot; <a href=\"https://obou6wyb.mirror.aliyuncs.com\">https://obou6wyb.mirror.aliyuncs.com</a>&quot;]}</p>\n<p>替换为 { “dns” : [ “192.168.101.2” , “8.8.8.8” ], “registry-mirrors” : [ “<a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a>” ] }</p>\n<h2 id=\"重启docker\">重启docker</h2>\n<pre><code class=\"language-java\">systemctl start docker\n</code></pre>\n<h2 id=\"查看资源库有tomcat镜像\">查看资源库有tomcat镜像</h2>\n<pre><code class=\"language-java\">docker search tomcat\n</code></pre>\n<p><img src=\"/img/image-20201214131814678.png\" alt=\"image-20201214131814678\"></p>\n<h2 id=\"从国内docker镜像库下载tomcat、centos\">从国内docker镜像库下载tomcat、centos</h2>\n<pre><code class=\"language-java\">docker pull tomcat/centos/nginx\n</code></pre>\n<h2 id=\"查看有哪些镜像\">查看有哪些镜像</h2>\n<pre><code class=\"language-java\">docker images\n</code></pre>\n<p><img src=\"/img/image-20201214131847691.png\" alt=\"image-20201214131847691\"></p>\n<h2 id=\"启动基于tomcat-centos镜像启动容器\">启动基于tomcat,centos镜像启动容器</h2>\n<pre><code class=\"language-java\">  docker run -p 8081:8080 tomcat \n</code></pre>\n<p>​\t若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口</p>\n<pre><code class=\"language-java\">docker run -dit -p 4000:4000 centos \n\n-d 以守护态运行 \n-p 宿主机端口映射容器端口 \n-i 允许容器内标准输入 \n-t 新容器内指定一个伪终端 \n</code></pre>\n<p>浏览器查看访问容器tomcat实例http://192.168.6.71:8081/</p>\n<p><img src=\"/img/image-20201214131948506.png\" alt=\"image-20201214131948506\"></p>\n<p>第一个容器服务部署成功了！</p>\n<h2 id=\"进去伪终端查看\">进去伪终端查看</h2>\n<p>docker登录容器</p>\n<pre><code class=\"language-java\">docker exec -it hardcore_edison  &quot;/bin/bash&quot;\n</code></pre>\n<p><img src=\"/img/image-20201214132014480.png\" alt=\"image-20201214132014480\"></p>\n<h2 id=\"本地文件复制容器中\">本地文件复制容器中</h2>\n<pre><code class=\"language-java\">docker cp localFile containerID:targetAddress\n</code></pre>\n<p>命令：</p>\n<pre><code class=\"language-java\">docker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n</code></pre>\n<p>将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：</p>\n<p><img src=\"/img/image-20201214132059091.png\" alt=\"image-20201214132059091\"></p>\n<p>以上就是docker的简单入门操作；</p>\n<p>构建一个docker镜像需要写一个叫做Dockerfile的文件<br>\n先查看下本地镜像有哪些？</p>\n<p><img src=\"/img/image-20201214132127621.png\" alt=\"image-20201214132127621\"></p>\n<p>在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：</p>\n<pre><code class=\"language-java\">mkdir dockerDemo &amp;&amp; cd dockerDemo &amp;&amp; touch Dockerfile\n</code></pre>\n<p>接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）</p>\n<pre><code class=\"language-java\"> vim Dockerfile\n</code></pre>\n<pre><code class=\"language-java\">#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module &amp;&amp; make &amp;&amp; make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n</code></pre>\n<p>这里以编译nginx提供web服务来构建新的镜像</p>\n<p>下载nginx源码包到docker_demo这个目录下</p>\n<p>wget -c <a href=\"https://nginx.org/download/nginx-1.12.2.tar.gz\">https://nginx.org/download/nginx-1.12.2.tar.gz</a></p>\n<p><img src=\"/img/image-20201214132215125.png\" alt=\"image-20201214132215125\"></p>\n<h2 id=\"构建nginx-v1版本镜像\">构建nginx:v1版本镜像</h2>\n<pre><code class=\"language-java\">docker build -t centos_nginx:v1 .\n</code></pre>\n<h2 id=\"观察日志\">观察日志</h2>\n<p>每一个步骤都成功</p>\n<p><img src=\"/img/image-20201214132254636.png\" alt=\"image-20201214132254636\"></p>\n<h2 id=\"构建步骤\">构建步骤</h2>\n<p>成功构建centos_nginx:v1</p>\n<p><img src=\"/img/image-20201214132309723.png\" alt=\"image-20201214132309723\"></p>\n<pre><code class=\"language-java\">docker images\n</code></pre>\n<p><img src=\"/img/image-20201214132326043.png\" alt=\"image-20201214132326043\"></p>\n<h2 id=\"启动容器\">启动容器</h2>\n<pre><code class=\"language-java\">docker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g &quot;daemon off;&quot;\n</code></pre>\n<p><img src=\"/img/image-20201214132343909.png\" alt=\"image-20201214132343909\"></p>\n<h2 id=\"查看镜像对外暴露端口号\">查看镜像对外暴露端口号</h2>\n<pre><code class=\"language-java\">docker port containerID\n</code></pre>\n<p><img src=\"/img/image-20201214132401207.png\" alt=\"image-20201214132401207\"></p>\n<h2 id=\"浏览器查看nginx启动状态\">浏览器查看nginx启动状态</h2>\n<p><img src=\"/img/image-20201214132416329.png\" alt=\"image-20201214132416329\"></p>\n<p>已经完成第一个nginx的镜像构建以及容器启动；</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"docker概念\">docker概念</h2>\n<p>​\t\tdocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；<br>\n​\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。</p>\n<p><img src=\"/img/image-20201214131527522.png\" alt=\"image-20201214131527522\"></p>\n<h2 id=\"docker架构\">docker架构</h2>\n<p><img src=\"/img/image-20201214131543066.png\" alt=\"image-20201214131543066\"></p>\n<h2 id=\"docker的组成元素\">docker的组成元素</h2>\n<p>•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。<br>\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。<br>\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。<br>\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(<a href=\"http://hub.Docker.com\">http://hub.Docker.com</a>)<br>\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。</p>\n<p>​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；</p>\n<h2 id=\"docker安装\">docker安装</h2>\n<p>安装 参考<a href=\"http://www.docker.com/products/docker\">docker官网</a></p>\n<p>查看安装版本</p>\n<pre><code class=\"language-java\">docker version\n</code></pre>\n<p><img src=\"/img/image-20201214131734289.png\" alt=\"image-20201214131734289\"></p>\n<h2 id=\"测试镜像库\">测试镜像库</h2>\n<p>为docker 添加国内镜像</p>\n<p>/etc/docker/daemon.json将:</p>\n<p>{ “registry-mirrors”: [&quot; <a href=\"https://obou6wyb.mirror.aliyuncs.com\">https://obou6wyb.mirror.aliyuncs.com</a>&quot;]}</p>\n<p>替换为 { “dns” : [ “192.168.101.2” , “8.8.8.8” ], “registry-mirrors” : [ “<a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a>” ] }</p>\n<h2 id=\"重启docker\">重启docker</h2>\n<pre><code class=\"language-java\">systemctl start docker\n</code></pre>\n<h2 id=\"查看资源库有tomcat镜像\">查看资源库有tomcat镜像</h2>\n<pre><code class=\"language-java\">docker search tomcat\n</code></pre>\n<p><img src=\"/img/image-20201214131814678.png\" alt=\"image-20201214131814678\"></p>\n<h2 id=\"从国内docker镜像库下载tomcat、centos\">从国内docker镜像库下载tomcat、centos</h2>\n<pre><code class=\"language-java\">docker pull tomcat/centos/nginx\n</code></pre>\n<h2 id=\"查看有哪些镜像\">查看有哪些镜像</h2>\n<pre><code class=\"language-java\">docker images\n</code></pre>\n<p><img src=\"/img/image-20201214131847691.png\" alt=\"image-20201214131847691\"></p>\n<h2 id=\"启动基于tomcat-centos镜像启动容器\">启动基于tomcat,centos镜像启动容器</h2>\n<pre><code class=\"language-java\">  docker run -p 8081:8080 tomcat \n</code></pre>\n<p>​\t若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口</p>\n<pre><code class=\"language-java\">docker run -dit -p 4000:4000 centos \n\n-d 以守护态运行 \n-p 宿主机端口映射容器端口 \n-i 允许容器内标准输入 \n-t 新容器内指定一个伪终端 \n</code></pre>\n<p>浏览器查看访问容器tomcat实例http://192.168.6.71:8081/</p>\n<p><img src=\"/img/image-20201214131948506.png\" alt=\"image-20201214131948506\"></p>\n<p>第一个容器服务部署成功了！</p>\n<h2 id=\"进去伪终端查看\">进去伪终端查看</h2>\n<p>docker登录容器</p>\n<pre><code class=\"language-java\">docker exec -it hardcore_edison  &quot;/bin/bash&quot;\n</code></pre>\n<p><img src=\"/img/image-20201214132014480.png\" alt=\"image-20201214132014480\"></p>\n<h2 id=\"本地文件复制容器中\">本地文件复制容器中</h2>\n<pre><code class=\"language-java\">docker cp localFile containerID:targetAddress\n</code></pre>\n<p>命令：</p>\n<pre><code class=\"language-java\">docker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n</code></pre>\n<p>将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：</p>\n<p><img src=\"/img/image-20201214132059091.png\" alt=\"image-20201214132059091\"></p>\n<p>以上就是docker的简单入门操作；</p>\n<p>构建一个docker镜像需要写一个叫做Dockerfile的文件<br>\n先查看下本地镜像有哪些？</p>\n<p><img src=\"/img/image-20201214132127621.png\" alt=\"image-20201214132127621\"></p>\n<p>在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：</p>\n<pre><code class=\"language-java\">mkdir dockerDemo &amp;&amp; cd dockerDemo &amp;&amp; touch Dockerfile\n</code></pre>\n<p>接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）</p>\n<pre><code class=\"language-java\"> vim Dockerfile\n</code></pre>\n<pre><code class=\"language-java\">#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module &amp;&amp; make &amp;&amp; make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n</code></pre>\n<p>这里以编译nginx提供web服务来构建新的镜像</p>\n<p>下载nginx源码包到docker_demo这个目录下</p>\n<p>wget -c <a href=\"https://nginx.org/download/nginx-1.12.2.tar.gz\">https://nginx.org/download/nginx-1.12.2.tar.gz</a></p>\n<p><img src=\"/img/image-20201214132215125.png\" alt=\"image-20201214132215125\"></p>\n<h2 id=\"构建nginx-v1版本镜像\">构建nginx:v1版本镜像</h2>\n<pre><code class=\"language-java\">docker build -t centos_nginx:v1 .\n</code></pre>\n<h2 id=\"观察日志\">观察日志</h2>\n<p>每一个步骤都成功</p>\n<p><img src=\"/img/image-20201214132254636.png\" alt=\"image-20201214132254636\"></p>\n<h2 id=\"构建步骤\">构建步骤</h2>\n<p>成功构建centos_nginx:v1</p>\n<p><img src=\"/img/image-20201214132309723.png\" alt=\"image-20201214132309723\"></p>\n<pre><code class=\"language-java\">docker images\n</code></pre>\n<p><img src=\"/img/image-20201214132326043.png\" alt=\"image-20201214132326043\"></p>\n<h2 id=\"启动容器\">启动容器</h2>\n<pre><code class=\"language-java\">docker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g &quot;daemon off;&quot;\n</code></pre>\n<p><img src=\"/img/image-20201214132343909.png\" alt=\"image-20201214132343909\"></p>\n<h2 id=\"查看镜像对外暴露端口号\">查看镜像对外暴露端口号</h2>\n<pre><code class=\"language-java\">docker port containerID\n</code></pre>\n<p><img src=\"/img/image-20201214132401207.png\" alt=\"image-20201214132401207\"></p>\n<h2 id=\"浏览器查看nginx启动状态\">浏览器查看nginx启动状态</h2>\n<p><img src=\"/img/image-20201214132416329.png\" alt=\"image-20201214132416329\"></p>\n<p>已经完成第一个nginx的镜像构建以及容器启动；</p>\n"},{"title":"ElasticSearch近实时性介绍","author":"郑天祺","date":"2020-07-15T02:51:00.000Z","_content":"\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","source":"_posts/ElasticSearch近实时性介绍.md","raw":"title: ElasticSearch近实时性介绍\nauthor: 郑天祺\ntags:\n  - es\ncategories:\n  - CICD\n  - ''\ndate: 2020-07-15 10:51:00\n---\n\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","slug":"ElasticSearch近实时性介绍","published":1,"updated":"2022-04-04T08:32:40.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyi000j7kt9gqxnemcw","content":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n"},{"title":"ElasticSearch客户端","author":"郑天祺","date":"2020-07-15T03:10:00.000Z","_content":"\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","source":"_posts/ElasticSearch客户端.md","raw":"title: ElasticSearch客户端\nauthor: 郑天祺\ntags:\n  - es\ncategories:\n  - CICD\n  - ''\ndate: 2020-07-15 11:10:00\n---\n\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","slug":"ElasticSearch客户端","published":1,"updated":"2022-04-04T08:32:40.137Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyj000n7kt945y8c2o2","content":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient &#123;\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() &#123;\n    &#125;\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() &#123;\n        return InternalClass.client;\n    &#125;\n\n    private static class InternalClass &#123;\n        private static RestHighLevelClient client;\n\n        static &#123;\n            try &#123;\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) &#123;\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                &#125;\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; &#123;\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                &#125;).setRequestConfigCallback(requestConfigBuilder -&gt; &#123;\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                &#125;);\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            &#125; catch (Exception e) &#123;\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            &#125;\n            if (null == client) &#123;\n                log.severe(&quot;创建ES连接失败!&quot;);\n            &#125;\n        &#125;\n    &#125;\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException &#123;\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) &#123;\n        try &#123;\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) &#123;\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            &#125;\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        &#125; catch (Exception e) &#123;\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        &#125;\n    &#125;\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try &#123;\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        &#125; catch (Throwable e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        &#125;\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try &#123;\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        &#125;\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) &#123;\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) &#123;\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        &#125;\n        return personList;\n    &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient &#123;\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() &#123;\n    &#125;\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() &#123;\n        return InternalClass.client;\n    &#125;\n\n    private static class InternalClass &#123;\n        private static RestHighLevelClient client;\n\n        static &#123;\n            try &#123;\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) &#123;\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                &#125;\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; &#123;\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                &#125;).setRequestConfigCallback(requestConfigBuilder -&gt; &#123;\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                &#125;);\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            &#125; catch (Exception e) &#123;\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            &#125;\n            if (null == client) &#123;\n                log.severe(&quot;创建ES连接失败!&quot;);\n            &#125;\n        &#125;\n    &#125;\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException &#123;\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) &#123;\n        try &#123;\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) &#123;\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            &#125;\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        &#125; catch (Exception e) &#123;\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        &#125;\n    &#125;\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try &#123;\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        &#125; catch (Throwable e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        &#125;\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try &#123;\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        &#125;\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) &#123;\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) &#123;\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        &#125;\n        return personList;\n    &#125;\n</code></pre>\n"},{"title":"Disruptor","author":"ztq","date":"2021-04-13T06:17:00.000Z","_content":"\n# [disruptor](https://github.com/LMAX-Exchange/disruptor)\n\n------高性能的线程间消息传递框架\n\n## 介绍：\n\nDisruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。\n\n但是，Disruptor提供了与队列不同的关键功能：\n\n1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)\n\n2、为事件（events）预先分配内存空间\n\n3、针对极高的性能目标而实现的极度优化和无锁的设计；\n\n​    应用场景：\n\n## 类图：\n\n![img](/img/5a377b3b.png)\n\n## 核心概念：\n\n[RingBuffer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java) 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。\n\n[Sequence](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java) 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。\n\n[Sequencer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java) Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。\n\n [SequenceBarrier](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java) 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。\n\n [WaitStrategy](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java) 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。\n\nEvent 从生产者传递给消费者的数据单位**。**\n\n[EventProcessor](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java) 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。\n\n[EventHandler](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java) 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。\n\nProducer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。\n\n \n\n## DSL图：\n\n![img](/img/7f3f75ca.png)\n\n \n\nDisruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；\n\nRingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；\n\nEventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；\n\nEventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api\n\n## RingBuffer实现：\n\nRingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。\n\n1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）\n\n2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。\n\n3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。\n\n \n\n![img](/img/a7ade6c9.png)\n\n \n\nRingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。\n\n**消费者读取数据：**\n\n​    ![img](/img/4e12cc06.png)\n\n \n\n消费者(*Consumer*)是一个想从*RingBuffer*里读取数据的线程，它可以访问*ConsumerBarrier*对象——这个对象由*RingBuffer*创建并且代表消费者与*RingBuffer*进行交互。就像*RingBuffer*显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了*RingBuffer*里序号*8*之前（包括*8*）的所有数据，那么它期待访问的下一个序号是*9*。\n\n消费者可以调用*ConsumerBarrier*对象的*waitFor()*方法，传递它所需要的下一个序号.\n\nfinal long availableSeq = consumerBarrier.waitFor(nextSequence);\n\n*ConsumerBarrier*返回*RingBuffer*的最大可访问序号——在上面的例子中是*12*。\n\n接下来，消费者会一直原地停留，等待更多数据被写入*RingBuffer*。并且，一旦数据写入后消费者会收到通知——节点*9*，*10*，*11*和*12* 已写入。现在序号*12*到了，消费者可以让*ConsumerBarrier*去拿这些序号节点里的数据了\n\n![img](/img/227d0458.png)\n\n \n\n拿到了数据后，消费者(***Consumer***)会更新自己的标识(***cursor***)。\n\n \n\n## 这样做有助于平缓延迟的峰值？\n\n以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(*Consumer*)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（*RingBuffer*本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(*Consumer)*去读同一个*RingBuffer* ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在*Disruptor*的协调下实现真正的并发数据处理。\n\n## 生产者写入数据：\n\n \n\n 写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。\n\n RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。\n\nProducerBarrier如何防止RingBuffer重叠\n\n![img](/img/d94cd34e.png)\n\n \n\n在这幅图中，我们假设只有一个生产者写入 RingBuffer。\n\n ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。\n\n在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。\n\n现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。\n\n## 申请下一个节点：\n\n![img](/img/145d5664.png)\n\n \n\nProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。\n\n \n\n## 提交新的数据：\n\n![img](/img/b235f114.png)\n\n \n\n当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。\n\nProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。\n\n## ProducerBarrier上的批处理\n\n  Disruptor 可以同时在生产者和消费者两端实现批处理。\n\n![img](/img/b4d68165.png)\n\nProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。\n\n## 多个生产者的场景\n\n![img](/img/9cb319ab.png)\n\n现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。\n\n \n\nRingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。\n\n资料：\n\n官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction\n\n官翻https://www.cnblogs.com/daoqidelv/p/6995888.html\n\n博客http://ifeve.com/dissecting-disruptor-whats-so-special/\n\nhttps://my.oschina.net/u/1765168/blog/1807887\n\nhttps://www.jianshu.com/p/f6d0d0c2a647\n\nhttps://www.jianshu.com/p/4a202ef547cc\n\n补充：\n\n流程简图：\n\n![img](/img/d325d29a.png)\n\n等待策略\n\nBlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高\n\nBusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。\n\nLiteBlockingWaitStrategy实现方法也是阻塞等待\n\nSleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。\n\nTimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。\n\nYieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。\n\nPhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 \n\n新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享\n\n \n\n ","source":"_posts/Disruptor.md","raw":"title: Disruptor\nauthor: ztq\ntags:\n\n  - Disruptor\ncategories:\n  - 分布式\ndate: 2021-04-13 14:17:00\n---\n\n# [disruptor](https://github.com/LMAX-Exchange/disruptor)\n\n------高性能的线程间消息传递框架\n\n## 介绍：\n\nDisruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。\n\n但是，Disruptor提供了与队列不同的关键功能：\n\n1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)\n\n2、为事件（events）预先分配内存空间\n\n3、针对极高的性能目标而实现的极度优化和无锁的设计；\n\n​    应用场景：\n\n## 类图：\n\n![img](/img/5a377b3b.png)\n\n## 核心概念：\n\n[RingBuffer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java) 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。\n\n[Sequence](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java) 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。\n\n[Sequencer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java) Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。\n\n [SequenceBarrier](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java) 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。\n\n [WaitStrategy](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java) 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。\n\nEvent 从生产者传递给消费者的数据单位**。**\n\n[EventProcessor](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java) 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。\n\n[EventHandler](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java) 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。\n\nProducer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。\n\n \n\n## DSL图：\n\n![img](/img/7f3f75ca.png)\n\n \n\nDisruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；\n\nRingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；\n\nEventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；\n\nEventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api\n\n## RingBuffer实现：\n\nRingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。\n\n1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）\n\n2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。\n\n3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。\n\n \n\n![img](/img/a7ade6c9.png)\n\n \n\nRingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。\n\n**消费者读取数据：**\n\n​    ![img](/img/4e12cc06.png)\n\n \n\n消费者(*Consumer*)是一个想从*RingBuffer*里读取数据的线程，它可以访问*ConsumerBarrier*对象——这个对象由*RingBuffer*创建并且代表消费者与*RingBuffer*进行交互。就像*RingBuffer*显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了*RingBuffer*里序号*8*之前（包括*8*）的所有数据，那么它期待访问的下一个序号是*9*。\n\n消费者可以调用*ConsumerBarrier*对象的*waitFor()*方法，传递它所需要的下一个序号.\n\nfinal long availableSeq = consumerBarrier.waitFor(nextSequence);\n\n*ConsumerBarrier*返回*RingBuffer*的最大可访问序号——在上面的例子中是*12*。\n\n接下来，消费者会一直原地停留，等待更多数据被写入*RingBuffer*。并且，一旦数据写入后消费者会收到通知——节点*9*，*10*，*11*和*12* 已写入。现在序号*12*到了，消费者可以让*ConsumerBarrier*去拿这些序号节点里的数据了\n\n![img](/img/227d0458.png)\n\n \n\n拿到了数据后，消费者(***Consumer***)会更新自己的标识(***cursor***)。\n\n \n\n## 这样做有助于平缓延迟的峰值？\n\n以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(*Consumer*)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（*RingBuffer*本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(*Consumer)*去读同一个*RingBuffer* ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在*Disruptor*的协调下实现真正的并发数据处理。\n\n## 生产者写入数据：\n\n \n\n 写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。\n\n RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。\n\nProducerBarrier如何防止RingBuffer重叠\n\n![img](/img/d94cd34e.png)\n\n \n\n在这幅图中，我们假设只有一个生产者写入 RingBuffer。\n\n ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。\n\n在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。\n\n现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。\n\n## 申请下一个节点：\n\n![img](/img/145d5664.png)\n\n \n\nProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。\n\n \n\n## 提交新的数据：\n\n![img](/img/b235f114.png)\n\n \n\n当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。\n\nProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。\n\n## ProducerBarrier上的批处理\n\n  Disruptor 可以同时在生产者和消费者两端实现批处理。\n\n![img](/img/b4d68165.png)\n\nProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。\n\n## 多个生产者的场景\n\n![img](/img/9cb319ab.png)\n\n现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。\n\n \n\nRingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。\n\n资料：\n\n官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction\n\n官翻https://www.cnblogs.com/daoqidelv/p/6995888.html\n\n博客http://ifeve.com/dissecting-disruptor-whats-so-special/\n\nhttps://my.oschina.net/u/1765168/blog/1807887\n\nhttps://www.jianshu.com/p/f6d0d0c2a647\n\nhttps://www.jianshu.com/p/4a202ef547cc\n\n补充：\n\n流程简图：\n\n![img](/img/d325d29a.png)\n\n等待策略\n\nBlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高\n\nBusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。\n\nLiteBlockingWaitStrategy实现方法也是阻塞等待\n\nSleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。\n\nTimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。\n\nYieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。\n\nPhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 \n\n新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享\n\n \n\n ","slug":"Disruptor","published":1,"updated":"2022-04-04T08:32:40.136Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyk000p7kt9606y5gxc","content":"<h1><a href=\"https://github.com/LMAX-Exchange/disruptor\">disruptor</a></h1>\n<p>------高性能的线程间消息传递框架</p>\n<h2 id=\"介绍：\">介绍：</h2>\n<p>Disruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。</p>\n<p>但是，Disruptor提供了与队列不同的关键功能：</p>\n<p>1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)</p>\n<p>2、为事件（events）预先分配内存空间</p>\n<p>3、针对极高的性能目标而实现的极度优化和无锁的设计；</p>\n<p>​    应用场景：</p>\n<h2 id=\"类图：\">类图：</h2>\n<p><img src=\"/img/5a377b3b.png\" alt=\"img\"></p>\n<h2 id=\"核心概念：\">核心概念：</h2>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java\">RingBuffer</a> 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java\">Sequence</a> 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java\">Sequencer</a> Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java\">SequenceBarrier</a> 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java\">WaitStrategy</a> 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。</p>\n<p>Event 从生产者传递给消费者的数据单位**。**</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java\">EventProcessor</a> 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java\">EventHandler</a> 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。</p>\n<p>Producer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。</p>\n<h2 id=\"DSL图：\">DSL图：</h2>\n<p><img src=\"/img/7f3f75ca.png\" alt=\"img\"></p>\n<p>Disruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；</p>\n<p>RingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；</p>\n<p>EventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；</p>\n<p>EventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api</p>\n<h2 id=\"RingBuffer实现：\">RingBuffer实现：</h2>\n<p>RingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。</p>\n<p>1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）</p>\n<p>2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。</p>\n<p>3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。</p>\n<p><img src=\"/img/a7ade6c9.png\" alt=\"img\"></p>\n<p>RingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。</p>\n<p><strong>消费者读取数据：</strong></p>\n<p>​    <img src=\"/img/4e12cc06.png\" alt=\"img\"></p>\n<p>消费者(<em>Consumer</em>)是一个想从<em>RingBuffer</em>里读取数据的线程，它可以访问<em>ConsumerBarrier</em>对象——这个对象由<em>RingBuffer</em>创建并且代表消费者与<em>RingBuffer</em>进行交互。就像<em>RingBuffer</em>显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了<em>RingBuffer</em>里序号<em>8</em>之前（包括<em>8</em>）的所有数据，那么它期待访问的下一个序号是<em>9</em>。</p>\n<p>消费者可以调用<em>ConsumerBarrier</em>对象的*waitFor()*方法，传递它所需要的下一个序号.</p>\n<p>final long availableSeq = consumerBarrier.waitFor(nextSequence);</p>\n<p><em>ConsumerBarrier</em>返回<em>RingBuffer</em>的最大可访问序号——在上面的例子中是<em>12</em>。</p>\n<p>接下来，消费者会一直原地停留，等待更多数据被写入<em>RingBuffer</em>。并且，一旦数据写入后消费者会收到通知——节点<em>9</em>，<em>10</em>，<em>11</em>和<em>12</em> 已写入。现在序号<em>12</em>到了，消费者可以让<em>ConsumerBarrier</em>去拿这些序号节点里的数据了</p>\n<p><img src=\"/img/227d0458.png\" alt=\"img\"></p>\n<p>拿到了数据后，消费者(<em><strong>Consumer</strong></em>)会更新自己的标识(<em><strong>cursor</strong></em>)。</p>\n<h2 id=\"这样做有助于平缓延迟的峰值？\">这样做有助于平缓延迟的峰值？</h2>\n<p>以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(<em>Consumer</em>)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（<em>RingBuffer</em>本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(<em>Consumer)<em>去读同一个</em>RingBuffer</em> ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在<em>Disruptor</em>的协调下实现真正的并发数据处理。</p>\n<h2 id=\"生产者写入数据：\">生产者写入数据：</h2>\n<p>写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。</p>\n<p>RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。</p>\n<p>ProducerBarrier如何防止RingBuffer重叠</p>\n<p><img src=\"/img/d94cd34e.png\" alt=\"img\"></p>\n<p>在这幅图中，我们假设只有一个生产者写入 RingBuffer。</p>\n<p>ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。</p>\n<p>在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。</p>\n<p>现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。</p>\n<h2 id=\"申请下一个节点：\">申请下一个节点：</h2>\n<p><img src=\"/img/145d5664.png\" alt=\"img\"></p>\n<p>ProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。</p>\n<h2 id=\"提交新的数据：\">提交新的数据：</h2>\n<p><img src=\"/img/b235f114.png\" alt=\"img\"></p>\n<p>当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。</p>\n<p>ProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。</p>\n<h2 id=\"ProducerBarrier上的批处理\">ProducerBarrier上的批处理</h2>\n<p>Disruptor 可以同时在生产者和消费者两端实现批处理。</p>\n<p><img src=\"/img/b4d68165.png\" alt=\"img\"></p>\n<p>ProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。</p>\n<h2 id=\"多个生产者的场景\">多个生产者的场景</h2>\n<p><img src=\"/img/9cb319ab.png\" alt=\"img\"></p>\n<p>现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。</p>\n<p>RingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。</p>\n<p>资料：</p>\n<p>官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction</p>\n<p>官翻https://www.cnblogs.com/daoqidelv/p/6995888.html</p>\n<p>博客http://ifeve.com/dissecting-disruptor-whats-so-special/</p>\n<p><a href=\"https://my.oschina.net/u/1765168/blog/1807887\">https://my.oschina.net/u/1765168/blog/1807887</a></p>\n<p><a href=\"https://www.jianshu.com/p/f6d0d0c2a647\">https://www.jianshu.com/p/f6d0d0c2a647</a></p>\n<p><a href=\"https://www.jianshu.com/p/4a202ef547cc\">https://www.jianshu.com/p/4a202ef547cc</a></p>\n<p>补充：</p>\n<p>流程简图：</p>\n<p><img src=\"/img/d325d29a.png\" alt=\"img\"></p>\n<p>等待策略</p>\n<p>BlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高</p>\n<p>BusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。</p>\n<p>LiteBlockingWaitStrategy实现方法也是阻塞等待</p>\n<p>SleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。</p>\n<p>TimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。</p>\n<p>YieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。</p>\n<p>PhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<p>新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享</p>\n","site":{"data":{}},"excerpt":"","more":"<h1><a href=\"https://github.com/LMAX-Exchange/disruptor\">disruptor</a></h1>\n<p>------高性能的线程间消息传递框架</p>\n<h2 id=\"介绍：\">介绍：</h2>\n<p>Disruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。</p>\n<p>但是，Disruptor提供了与队列不同的关键功能：</p>\n<p>1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)</p>\n<p>2、为事件（events）预先分配内存空间</p>\n<p>3、针对极高的性能目标而实现的极度优化和无锁的设计；</p>\n<p>​    应用场景：</p>\n<h2 id=\"类图：\">类图：</h2>\n<p><img src=\"/img/5a377b3b.png\" alt=\"img\"></p>\n<h2 id=\"核心概念：\">核心概念：</h2>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java\">RingBuffer</a> 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java\">Sequence</a> 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java\">Sequencer</a> Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java\">SequenceBarrier</a> 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java\">WaitStrategy</a> 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。</p>\n<p>Event 从生产者传递给消费者的数据单位**。**</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java\">EventProcessor</a> 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java\">EventHandler</a> 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。</p>\n<p>Producer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。</p>\n<h2 id=\"DSL图：\">DSL图：</h2>\n<p><img src=\"/img/7f3f75ca.png\" alt=\"img\"></p>\n<p>Disruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；</p>\n<p>RingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；</p>\n<p>EventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；</p>\n<p>EventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api</p>\n<h2 id=\"RingBuffer实现：\">RingBuffer实现：</h2>\n<p>RingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。</p>\n<p>1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）</p>\n<p>2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。</p>\n<p>3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。</p>\n<p><img src=\"/img/a7ade6c9.png\" alt=\"img\"></p>\n<p>RingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。</p>\n<p><strong>消费者读取数据：</strong></p>\n<p>​    <img src=\"/img/4e12cc06.png\" alt=\"img\"></p>\n<p>消费者(<em>Consumer</em>)是一个想从<em>RingBuffer</em>里读取数据的线程，它可以访问<em>ConsumerBarrier</em>对象——这个对象由<em>RingBuffer</em>创建并且代表消费者与<em>RingBuffer</em>进行交互。就像<em>RingBuffer</em>显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了<em>RingBuffer</em>里序号<em>8</em>之前（包括<em>8</em>）的所有数据，那么它期待访问的下一个序号是<em>9</em>。</p>\n<p>消费者可以调用<em>ConsumerBarrier</em>对象的*waitFor()*方法，传递它所需要的下一个序号.</p>\n<p>final long availableSeq = consumerBarrier.waitFor(nextSequence);</p>\n<p><em>ConsumerBarrier</em>返回<em>RingBuffer</em>的最大可访问序号——在上面的例子中是<em>12</em>。</p>\n<p>接下来，消费者会一直原地停留，等待更多数据被写入<em>RingBuffer</em>。并且，一旦数据写入后消费者会收到通知——节点<em>9</em>，<em>10</em>，<em>11</em>和<em>12</em> 已写入。现在序号<em>12</em>到了，消费者可以让<em>ConsumerBarrier</em>去拿这些序号节点里的数据了</p>\n<p><img src=\"/img/227d0458.png\" alt=\"img\"></p>\n<p>拿到了数据后，消费者(<em><strong>Consumer</strong></em>)会更新自己的标识(<em><strong>cursor</strong></em>)。</p>\n<h2 id=\"这样做有助于平缓延迟的峰值？\">这样做有助于平缓延迟的峰值？</h2>\n<p>以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(<em>Consumer</em>)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（<em>RingBuffer</em>本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(<em>Consumer)<em>去读同一个</em>RingBuffer</em> ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在<em>Disruptor</em>的协调下实现真正的并发数据处理。</p>\n<h2 id=\"生产者写入数据：\">生产者写入数据：</h2>\n<p>写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。</p>\n<p>RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。</p>\n<p>ProducerBarrier如何防止RingBuffer重叠</p>\n<p><img src=\"/img/d94cd34e.png\" alt=\"img\"></p>\n<p>在这幅图中，我们假设只有一个生产者写入 RingBuffer。</p>\n<p>ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。</p>\n<p>在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。</p>\n<p>现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。</p>\n<h2 id=\"申请下一个节点：\">申请下一个节点：</h2>\n<p><img src=\"/img/145d5664.png\" alt=\"img\"></p>\n<p>ProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。</p>\n<h2 id=\"提交新的数据：\">提交新的数据：</h2>\n<p><img src=\"/img/b235f114.png\" alt=\"img\"></p>\n<p>当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。</p>\n<p>ProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。</p>\n<h2 id=\"ProducerBarrier上的批处理\">ProducerBarrier上的批处理</h2>\n<p>Disruptor 可以同时在生产者和消费者两端实现批处理。</p>\n<p><img src=\"/img/b4d68165.png\" alt=\"img\"></p>\n<p>ProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。</p>\n<h2 id=\"多个生产者的场景\">多个生产者的场景</h2>\n<p><img src=\"/img/9cb319ab.png\" alt=\"img\"></p>\n<p>现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。</p>\n<p>RingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。</p>\n<p>资料：</p>\n<p>官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction</p>\n<p>官翻https://www.cnblogs.com/daoqidelv/p/6995888.html</p>\n<p>博客http://ifeve.com/dissecting-disruptor-whats-so-special/</p>\n<p><a href=\"https://my.oschina.net/u/1765168/blog/1807887\">https://my.oschina.net/u/1765168/blog/1807887</a></p>\n<p><a href=\"https://www.jianshu.com/p/f6d0d0c2a647\">https://www.jianshu.com/p/f6d0d0c2a647</a></p>\n<p><a href=\"https://www.jianshu.com/p/4a202ef547cc\">https://www.jianshu.com/p/4a202ef547cc</a></p>\n<p>补充：</p>\n<p>流程简图：</p>\n<p><img src=\"/img/d325d29a.png\" alt=\"img\"></p>\n<p>等待策略</p>\n<p>BlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高</p>\n<p>BusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。</p>\n<p>LiteBlockingWaitStrategy实现方法也是阻塞等待</p>\n<p>SleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。</p>\n<p>TimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。</p>\n<p>YieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。</p>\n<p>PhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<p>新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享</p>\n"},{"title":"GET与POST区别","author":"郑天祺","date":"2020-07-21T00:29:00.000Z","_content":"\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","source":"_posts/GET与POST区别.md","raw":"title: GET与POST区别\nauthor: 郑天祺\ntags:\n  - GET/POST\ncategories:\n  - 网络\ndate: 2020-07-21 08:29:00\n\n---\n\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","slug":"GET与POST区别","published":1,"updated":"2022-04-04T08:32:40.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyl000t7kt90qu6b00x","content":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2 id=\"GET请求\">GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型<br>\nAccept-Language 浏览器支持的语言<br>\nAccept-Encoding 浏览器支持的压缩格式<br>\nHost 请求的主机<br>\nConnection keep-alive 这个是链接一小段时间</p>\n<h2 id=\"GET响应\">GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2 id=\"POST请求\">POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 – 加上128 – 转化为16进制 – 添加%</p>\n<h2 id=\"POST响应\">POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2 id=\"（1）表格对比：\">（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存<br>\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2 id=\"（2）交互对比\">（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2 id=\"（1）GET方法的参数写法是固定的吗？\">（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2 id=\"（2）GET-方法的长度限制是怎么回事？\">（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2 id=\"（3）POST-方法比-GET-方法安全？\">（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：<a href=\"http://www.javanx.cn/20190227/get-post/\">http://www.javanx.cn/20190227/get-post/</a></p>\n<h2 id=\"（4）POST-方法会产生两个-TCP-数据包？\">（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2 id=\"GET请求\">GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型<br>\nAccept-Language 浏览器支持的语言<br>\nAccept-Encoding 浏览器支持的压缩格式<br>\nHost 请求的主机<br>\nConnection keep-alive 这个是链接一小段时间</p>\n<h2 id=\"GET响应\">GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2 id=\"POST请求\">POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 – 加上128 – 转化为16进制 – 添加%</p>\n<h2 id=\"POST响应\">POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2 id=\"（1）表格对比：\">（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存<br>\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2 id=\"（2）交互对比\">（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2 id=\"（1）GET方法的参数写法是固定的吗？\">（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2 id=\"（2）GET-方法的长度限制是怎么回事？\">（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2 id=\"（3）POST-方法比-GET-方法安全？\">（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：<a href=\"http://www.javanx.cn/20190227/get-post/\">http://www.javanx.cn/20190227/get-post/</a></p>\n<h2 id=\"（4）POST-方法会产生两个-TCP-数据包？\">（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n"},{"title":"Git梳理","author":"郑天祺","date":"2019-08-29T02:28:00.000Z","_content":"## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n$ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： \n\n```java\n$ git clone http:/zzzzzzzz.git\n```\n\n查看远程仓库：\n\n```java\n$ git remote -v\n```\n\n添加远程仓库：\n\n```java\n$ git remote add [name] [url]\n```\n\n删除远程仓库：\n\n```java\n$ git remote rm [name]\n```\n\n修改远程仓库：\n\n```java\n$ git remote set-url --push [name] [newUrl]\n```\n\n拉取远程仓库：\n\n```java\n$ git pull [remoteName] [localBranchName]\n```\n\n从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：\n\n```java\n$ git push [remoteName] [localBranchName] \n```\n\n将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：\n\n```java\n$ git branch \n```\n\n列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支\n\n```java\n$ git branch -D new_branch ----就会强制删除叫做new_branch的分支\n```\n\n查看远程分支：\n\n```java\n$ git branch -r\n```\n\n创建本地分支：\n\n```java\n$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n```\n\n切换分支：\n\n```java\n$ git checkout [name]    \n```\n\nGit的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：\n\n```java\n$ git checkout -b [name]\n```\n\n删除分支：\n\n```java\n$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n```\n\n合并分支：\n\n```java\n$ git merge [name] ----将名称为[name]的分支与当前分支合并\n```\n\n创建远程分支(本地分支push到远程)：\n\n```java\n$ git push origin [name]\n```\n\n删除远程分支：\n\n```java\n$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n```\n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n\n```java\n$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx\n```\n\n\n\n####         c）版本(tag)操作相关命令\n\n查看版本：\n\n```java\n$ git tag\n```\n\n创建版本：\n\n```java\n$ git tag [name]\n```\n\n可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：\n\n```java\n$ git tag -d [name]\n```\n\n查看远程版本：\n\n```java\n$ git tag -r\n```\n\n创建远程版本(本地版本push到远程)：\n\n```java\n$ git push origin [name]\n```\n\n删除远程版本：\n\n```java\n$ git push origin  :refs/tags/[name]\n```\n\n合并远程仓库的tag到本地：\n\n```java\n$ git pull origin --tags\n```\n\n上传本地tag到远程仓库：\n\n```java\n$ git push origin --tags\n```\n\n创建带注释的tag：\n\n```java\n$ git tag -a [name] -m 'yourMessage' \n```\n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：\n\n```java\n$ git submodule add [url]       [path]\n\n​如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n```\n\n初始化子模块：\n\n```java\n$ git submodule init ----只在首次检出仓库时运行一次就行\n```\n\n更新子模块：\n\n```java\n$ git submodule update ----每次更新或切换分支后都需要运行一下\n```\n\n删除子模块：（分4步走） \n\n```java\n1)$ git rm --cached [path]\n2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n4)手动删除子模块残留的目录\n```\n\n####         e）补充\n\n更改或者新增的文件：\n\n```java\n$ git add       \n```\n\n是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，\n\n```java\n例如: git add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n```\n\n删除文件：\n\n```java\n$ git rm   \n```\n\n从当前的工作空间中和索引中删除文件，\n\n```java\n例如: git rm app/model/user.rb\n```\n\n查看历史日志：\n\n```java\n$ git log\n```\n\n还原：\n\n```java\n$ git revert \n```\n\n还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：\n\n```java\n$ git commit  \n```\n\n当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\n```java\npush -u [url] \n```\n\n####         f）忽略一些文件、文件夹不提交\n\n在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n```java\nIDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected\n```\n\n","source":"_posts/Git梳理.md","raw":"title: Git梳理\ntags:\n  - git\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-29 10:28:00\n---\n## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n$ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： \n\n```java\n$ git clone http:/zzzzzzzz.git\n```\n\n查看远程仓库：\n\n```java\n$ git remote -v\n```\n\n添加远程仓库：\n\n```java\n$ git remote add [name] [url]\n```\n\n删除远程仓库：\n\n```java\n$ git remote rm [name]\n```\n\n修改远程仓库：\n\n```java\n$ git remote set-url --push [name] [newUrl]\n```\n\n拉取远程仓库：\n\n```java\n$ git pull [remoteName] [localBranchName]\n```\n\n从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：\n\n```java\n$ git push [remoteName] [localBranchName] \n```\n\n将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：\n\n```java\n$ git branch \n```\n\n列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支\n\n```java\n$ git branch -D new_branch ----就会强制删除叫做new_branch的分支\n```\n\n查看远程分支：\n\n```java\n$ git branch -r\n```\n\n创建本地分支：\n\n```java\n$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n```\n\n切换分支：\n\n```java\n$ git checkout [name]    \n```\n\nGit的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：\n\n```java\n$ git checkout -b [name]\n```\n\n删除分支：\n\n```java\n$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n```\n\n合并分支：\n\n```java\n$ git merge [name] ----将名称为[name]的分支与当前分支合并\n```\n\n创建远程分支(本地分支push到远程)：\n\n```java\n$ git push origin [name]\n```\n\n删除远程分支：\n\n```java\n$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n```\n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n\n```java\n$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx\n```\n\n\n\n####         c）版本(tag)操作相关命令\n\n查看版本：\n\n```java\n$ git tag\n```\n\n创建版本：\n\n```java\n$ git tag [name]\n```\n\n可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：\n\n```java\n$ git tag -d [name]\n```\n\n查看远程版本：\n\n```java\n$ git tag -r\n```\n\n创建远程版本(本地版本push到远程)：\n\n```java\n$ git push origin [name]\n```\n\n删除远程版本：\n\n```java\n$ git push origin  :refs/tags/[name]\n```\n\n合并远程仓库的tag到本地：\n\n```java\n$ git pull origin --tags\n```\n\n上传本地tag到远程仓库：\n\n```java\n$ git push origin --tags\n```\n\n创建带注释的tag：\n\n```java\n$ git tag -a [name] -m 'yourMessage' \n```\n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：\n\n```java\n$ git submodule add [url]       [path]\n\n​如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n```\n\n初始化子模块：\n\n```java\n$ git submodule init ----只在首次检出仓库时运行一次就行\n```\n\n更新子模块：\n\n```java\n$ git submodule update ----每次更新或切换分支后都需要运行一下\n```\n\n删除子模块：（分4步走） \n\n```java\n1)$ git rm --cached [path]\n2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n4)手动删除子模块残留的目录\n```\n\n####         e）补充\n\n更改或者新增的文件：\n\n```java\n$ git add       \n```\n\n是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，\n\n```java\n例如: git add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n```\n\n删除文件：\n\n```java\n$ git rm   \n```\n\n从当前的工作空间中和索引中删除文件，\n\n```java\n例如: git rm app/model/user.rb\n```\n\n查看历史日志：\n\n```java\n$ git log\n```\n\n还原：\n\n```java\n$ git revert \n```\n\n还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：\n\n```java\n$ git commit  \n```\n\n当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\n```java\npush -u [url] \n```\n\n####         f）忽略一些文件、文件夹不提交\n\n在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n```java\nIDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected\n```\n\n","slug":"Git梳理","published":1,"updated":"2022-04-04T08:32:40.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnym000v7kt9h23ce7f1","content":"<h2 id=\"1、Git介绍：\">1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2 id=\"2、为什么用GitLab？\">2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确<br>\n​\t清晰的看到产品迭代，为产品研发提供参考<br>\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2 id=\"3、Git-工作区、暂存区和版本库\">3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2 id=\"4、Git工作流程\">4、Git工作流程</h2>\n<h3 id=\"1、一个分支\">1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3 id=\"2、多个分支\">2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h2 id=\"5、Git配置及使用：\">5、Git配置及使用：</h2>\n<h3 id=\"1）配置用户信息\">1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3 id=\"2）查看配置\">2）查看配置</h3>\n<pre><code class=\"language-java\">$ git config --list \n</code></pre>\n<h3 id=\"3）创建仓库\">3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  <a href=\"http://zzzzz.git\">http://zzzzz.git</a>   // ‘<a href=\"http://zzzzzz.git\">http://zzzzzz.git</a>’  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用’git init’命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用’git remote add’命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3 id=\"4）基本操作\">4）基本操作</h3>\n<h4 id=\"a）远程仓库相关命令\">a）远程仓库相关命令</h4>\n<p>检出仓库：</p>\n<pre><code class=\"language-java\">$ git clone http:/zzzzzzzz.git\n</code></pre>\n<p>查看远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote -v\n</code></pre>\n<p>添加远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote add [name] [url]\n</code></pre>\n<p>删除远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote rm [name]\n</code></pre>\n<p>修改远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote set-url --push [name] [newUrl]\n</code></pre>\n<p>拉取远程仓库：</p>\n<pre><code class=\"language-java\">$ git pull [remoteName] [localBranchName]\n</code></pre>\n<p>从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：</p>\n<pre><code class=\"language-java\">$ git push [remoteName] [localBranchName] \n</code></pre>\n<p>将本地commit的代码更新到远程版本库中，例如’git push origin’就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4 id=\"b）分支-branch-操作相关命令\">b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：</p>\n<pre><code class=\"language-java\">$ git branch \n</code></pre>\n<p>列出本地所有的分支 对分支的增、删、查等操作，例如’git branch       new_branch’会从当前的工作版本创建一个叫做new_branch的新分支</p>\n<pre><code class=\"language-java\">$ git branch -D new_branch ----就会强制删除叫做new_branch的分支\n</code></pre>\n<p>查看远程分支：</p>\n<pre><code class=\"language-java\">$ git branch -r\n</code></pre>\n<p>创建本地分支：</p>\n<pre><code class=\"language-java\">$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n</code></pre>\n<p>切换分支：</p>\n<pre><code class=\"language-java\">$ git checkout [name]    \n</code></pre>\n<p>Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如’git checkout       new_branch’就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如’git checkout app/model/user.rb’就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：</p>\n<pre><code class=\"language-java\">$ git checkout -b [name]\n</code></pre>\n<p>删除分支：</p>\n<pre><code class=\"language-java\">$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n</code></pre>\n<p>合并分支：</p>\n<pre><code class=\"language-java\">$ git merge [name] ----将名称为[name]的分支与当前分支合并\n</code></pre>\n<p>创建远程分支(本地分支push到远程)：</p>\n<pre><code class=\"language-java\">$ git push origin [name]\n</code></pre>\n<p>删除远程分支：</p>\n<pre><code class=\"language-java\">$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n</code></pre>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)</p>\n<pre><code class=\"language-java\">$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx\n</code></pre>\n<h4 id=\"c）版本-tag-操作相关命令\">c）版本(tag)操作相关命令</h4>\n<p>查看版本：</p>\n<pre><code class=\"language-java\">$ git tag\n</code></pre>\n<p>创建版本：</p>\n<pre><code class=\"language-java\">$ git tag [name]\n</code></pre>\n<p>可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用’git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20’来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：</p>\n<pre><code class=\"language-java\">$ git tag -d [name]\n</code></pre>\n<p>查看远程版本：</p>\n<pre><code class=\"language-java\">$ git tag -r\n</code></pre>\n<p>创建远程版本(本地版本push到远程)：</p>\n<pre><code class=\"language-java\">$ git push origin [name]\n</code></pre>\n<p>删除远程版本：</p>\n<pre><code class=\"language-java\">$ git push origin  :refs/tags/[name]\n</code></pre>\n<p>合并远程仓库的tag到本地：</p>\n<pre><code class=\"language-java\">$ git pull origin --tags\n</code></pre>\n<p>上传本地tag到远程仓库：</p>\n<pre><code class=\"language-java\">$ git push origin --tags\n</code></pre>\n<p>创建带注释的tag：</p>\n<pre><code class=\"language-java\">$ git tag -a [name] -m 'yourMessage' \n</code></pre>\n<h4 id=\"d）子模块-submodule-相关操作命令\">d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：</p>\n<pre><code class=\"language-java\">$ git submodule add [url]       [path]\n\n​如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n</code></pre>\n<p>初始化子模块：</p>\n<pre><code class=\"language-java\">$ git submodule init ----只在首次检出仓库时运行一次就行\n</code></pre>\n<p>更新子模块：</p>\n<pre><code class=\"language-java\">$ git submodule update ----每次更新或切换分支后都需要运行一下\n</code></pre>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">1)$ git rm --cached [path]\n2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n4)手动删除子模块残留的目录\n</code></pre>\n<h4 id=\"e）补充\">e）补充</h4>\n<p>更改或者新增的文件：</p>\n<pre><code class=\"language-java\">$ git add       \n</code></pre>\n<p>是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，</p>\n<pre><code class=\"language-java\">例如: git add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n</code></pre>\n<p>删除文件：</p>\n<pre><code class=\"language-java\">$ git rm   \n</code></pre>\n<p>从当前的工作空间中和索引中删除文件，</p>\n<pre><code class=\"language-java\">例如: git rm app/model/user.rb\n</code></pre>\n<p>查看历史日志：</p>\n<pre><code class=\"language-java\">$ git log\n</code></pre>\n<p>还原：</p>\n<pre><code class=\"language-java\">$ git revert \n</code></pre>\n<p>还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值</p>\n<p>提交：</p>\n<pre><code class=\"language-java\">$ git commit  \n</code></pre>\n<p>当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<pre><code class=\"language-java\">push -u [url] \n</code></pre>\n<h4 id=\"f）忽略一些文件、文件夹不提交\">f）忽略一些文件、文件夹不提交</h4>\n<p>在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3 id=\"5-解决冲突\">5)解决冲突</h3>\n<pre><code class=\"language-java\">IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、Git介绍：\">1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2 id=\"2、为什么用GitLab？\">2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确<br>\n​\t清晰的看到产品迭代，为产品研发提供参考<br>\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2 id=\"3、Git-工作区、暂存区和版本库\">3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2 id=\"4、Git工作流程\">4、Git工作流程</h2>\n<h3 id=\"1、一个分支\">1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3 id=\"2、多个分支\">2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h2 id=\"5、Git配置及使用：\">5、Git配置及使用：</h2>\n<h3 id=\"1）配置用户信息\">1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3 id=\"2）查看配置\">2）查看配置</h3>\n<pre><code class=\"language-java\">$ git config --list \n</code></pre>\n<h3 id=\"3）创建仓库\">3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  <a href=\"http://zzzzz.git\">http://zzzzz.git</a>   // ‘<a href=\"http://zzzzzz.git\">http://zzzzzz.git</a>’  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用’git init’命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用’git remote add’命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3 id=\"4）基本操作\">4）基本操作</h3>\n<h4 id=\"a）远程仓库相关命令\">a）远程仓库相关命令</h4>\n<p>检出仓库：</p>\n<pre><code class=\"language-java\">$ git clone http:/zzzzzzzz.git\n</code></pre>\n<p>查看远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote -v\n</code></pre>\n<p>添加远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote add [name] [url]\n</code></pre>\n<p>删除远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote rm [name]\n</code></pre>\n<p>修改远程仓库：</p>\n<pre><code class=\"language-java\">$ git remote set-url --push [name] [newUrl]\n</code></pre>\n<p>拉取远程仓库：</p>\n<pre><code class=\"language-java\">$ git pull [remoteName] [localBranchName]\n</code></pre>\n<p>从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：</p>\n<pre><code class=\"language-java\">$ git push [remoteName] [localBranchName] \n</code></pre>\n<p>将本地commit的代码更新到远程版本库中，例如’git push origin’就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4 id=\"b）分支-branch-操作相关命令\">b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：</p>\n<pre><code class=\"language-java\">$ git branch \n</code></pre>\n<p>列出本地所有的分支 对分支的增、删、查等操作，例如’git branch       new_branch’会从当前的工作版本创建一个叫做new_branch的新分支</p>\n<pre><code class=\"language-java\">$ git branch -D new_branch ----就会强制删除叫做new_branch的分支\n</code></pre>\n<p>查看远程分支：</p>\n<pre><code class=\"language-java\">$ git branch -r\n</code></pre>\n<p>创建本地分支：</p>\n<pre><code class=\"language-java\">$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n</code></pre>\n<p>切换分支：</p>\n<pre><code class=\"language-java\">$ git checkout [name]    \n</code></pre>\n<p>Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如’git checkout       new_branch’就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如’git checkout app/model/user.rb’就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：</p>\n<pre><code class=\"language-java\">$ git checkout -b [name]\n</code></pre>\n<p>删除分支：</p>\n<pre><code class=\"language-java\">$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n</code></pre>\n<p>合并分支：</p>\n<pre><code class=\"language-java\">$ git merge [name] ----将名称为[name]的分支与当前分支合并\n</code></pre>\n<p>创建远程分支(本地分支push到远程)：</p>\n<pre><code class=\"language-java\">$ git push origin [name]\n</code></pre>\n<p>删除远程分支：</p>\n<pre><code class=\"language-java\">$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n</code></pre>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)</p>\n<pre><code class=\"language-java\">$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx\n</code></pre>\n<h4 id=\"c）版本-tag-操作相关命令\">c）版本(tag)操作相关命令</h4>\n<p>查看版本：</p>\n<pre><code class=\"language-java\">$ git tag\n</code></pre>\n<p>创建版本：</p>\n<pre><code class=\"language-java\">$ git tag [name]\n</code></pre>\n<p>可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用’git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20’来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：</p>\n<pre><code class=\"language-java\">$ git tag -d [name]\n</code></pre>\n<p>查看远程版本：</p>\n<pre><code class=\"language-java\">$ git tag -r\n</code></pre>\n<p>创建远程版本(本地版本push到远程)：</p>\n<pre><code class=\"language-java\">$ git push origin [name]\n</code></pre>\n<p>删除远程版本：</p>\n<pre><code class=\"language-java\">$ git push origin  :refs/tags/[name]\n</code></pre>\n<p>合并远程仓库的tag到本地：</p>\n<pre><code class=\"language-java\">$ git pull origin --tags\n</code></pre>\n<p>上传本地tag到远程仓库：</p>\n<pre><code class=\"language-java\">$ git push origin --tags\n</code></pre>\n<p>创建带注释的tag：</p>\n<pre><code class=\"language-java\">$ git tag -a [name] -m 'yourMessage' \n</code></pre>\n<h4 id=\"d）子模块-submodule-相关操作命令\">d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：</p>\n<pre><code class=\"language-java\">$ git submodule add [url]       [path]\n\n​如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n</code></pre>\n<p>初始化子模块：</p>\n<pre><code class=\"language-java\">$ git submodule init ----只在首次检出仓库时运行一次就行\n</code></pre>\n<p>更新子模块：</p>\n<pre><code class=\"language-java\">$ git submodule update ----每次更新或切换分支后都需要运行一下\n</code></pre>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">1)$ git rm --cached [path]\n2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n4)手动删除子模块残留的目录\n</code></pre>\n<h4 id=\"e）补充\">e）补充</h4>\n<p>更改或者新增的文件：</p>\n<pre><code class=\"language-java\">$ git add       \n</code></pre>\n<p>是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，</p>\n<pre><code class=\"language-java\">例如: git add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n</code></pre>\n<p>删除文件：</p>\n<pre><code class=\"language-java\">$ git rm   \n</code></pre>\n<p>从当前的工作空间中和索引中删除文件，</p>\n<pre><code class=\"language-java\">例如: git rm app/model/user.rb\n</code></pre>\n<p>查看历史日志：</p>\n<pre><code class=\"language-java\">$ git log\n</code></pre>\n<p>还原：</p>\n<pre><code class=\"language-java\">$ git revert \n</code></pre>\n<p>还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值</p>\n<p>提交：</p>\n<pre><code class=\"language-java\">$ git commit  \n</code></pre>\n<p>当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<pre><code class=\"language-java\">push -u [url] \n</code></pre>\n<h4 id=\"f）忽略一些文件、文件夹不提交\">f）忽略一些文件、文件夹不提交</h4>\n<p>在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3 id=\"5-解决冲突\">5)解决冲突</h3>\n<pre><code class=\"language-java\">IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected\n</code></pre>\n"},{"title":"HDFS shell操作（1）","author":"郑天祺","date":"2020-12-06T03:50:00.000Z","_content":"\n## 1、创建一个HDFS目录\n\n命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115218466](/img/hdfs创建文件夹.png)\n\n## 2、本地文件上传到HDFS\n\n本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir\n\n命令：echo “I am student” > a.txt\n\n​       echo “I am teacher” > c.txt\n\n​       echo “I like hadoop” > b.txt\n\n​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115351845](/img/HDFS上传.png)\n\n## 3、查看/usr/local/hadoop/data/txtdir目录结构\n\n命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir\n\n截图：\n\n![image-20201206115459406](/img/image-20201206115459406.png)\n\n## 4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容\n\n命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt\n\n截图：![image-20201206115600801](/img/image-20201206115600801.png)\n\n## 5、合并本地多个小文件上传到HDFS\n\n命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt\n\n截图：![image-20201206115653987](/img/image-20201206115653987.png)\n\n## 6、 下载a.txt到本地文件系统\n\n命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt\n\n截图：![image-20201206115738831](/img/image-20201206115738831.png)\n\n## 7、删除HDFS上的/usr/local/hadoop/data/txtdir\n\n命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/\n\n截图：![image-20201206115832025](/img/image-20201206115832025.png)","source":"_posts/HDFS-shell操作（1）.md","raw":"title: HDFS shell操作（1）\nauthor: 郑天祺\ntags:\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 11:50:00\n---\n\n## 1、创建一个HDFS目录\n\n命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115218466](/img/hdfs创建文件夹.png)\n\n## 2、本地文件上传到HDFS\n\n本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir\n\n命令：echo “I am student” > a.txt\n\n​       echo “I am teacher” > c.txt\n\n​       echo “I like hadoop” > b.txt\n\n​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115351845](/img/HDFS上传.png)\n\n## 3、查看/usr/local/hadoop/data/txtdir目录结构\n\n命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir\n\n截图：\n\n![image-20201206115459406](/img/image-20201206115459406.png)\n\n## 4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容\n\n命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt\n\n截图：![image-20201206115600801](/img/image-20201206115600801.png)\n\n## 5、合并本地多个小文件上传到HDFS\n\n命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt\n\n截图：![image-20201206115653987](/img/image-20201206115653987.png)\n\n## 6、 下载a.txt到本地文件系统\n\n命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt\n\n截图：![image-20201206115738831](/img/image-20201206115738831.png)\n\n## 7、删除HDFS上的/usr/local/hadoop/data/txtdir\n\n命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/\n\n截图：![image-20201206115832025](/img/image-20201206115832025.png)","slug":"HDFS-shell操作（1）","published":1,"updated":"2022-04-04T08:32:40.139Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyn000y7kt9ftsyfvg2","content":"<h2 id=\"1、创建一个HDFS目录\">1、创建一个HDFS目录</h2>\n<p>命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/hdfs%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9.png\" alt=\"image-20201206115218466\"></p>\n<h2 id=\"2、本地文件上传到HDFS\">2、本地文件上传到HDFS</h2>\n<p>本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir</p>\n<p>命令：echo “I am student” &gt; a.txt</p>\n<p>​       echo “I am teacher” &gt; c.txt</p>\n<p>​       echo “I like hadoop” &gt; b.txt</p>\n<p>​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/HDFS%E4%B8%8A%E4%BC%A0.png\" alt=\"image-20201206115351845\"></p>\n<h2 id=\"3、查看-usr-local-hadoop-data-txtdir目录结构\">3、查看/usr/local/hadoop/data/txtdir目录结构</h2>\n<p>命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir</p>\n<p>截图：</p>\n<p><img src=\"/img/image-20201206115459406.png\" alt=\"image-20201206115459406\"></p>\n<h2 id=\"4、查看HDFS上-usr-local-hadoop-data-txtdir下的文件内容\">4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容</h2>\n<p>命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt</p>\n<p>截图：<img src=\"/img/image-20201206115600801.png\" alt=\"image-20201206115600801\"></p>\n<h2 id=\"5、合并本地多个小文件上传到HDFS\">5、合并本地多个小文件上传到HDFS</h2>\n<p>命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt</p>\n<p>截图：<img src=\"/img/image-20201206115653987.png\" alt=\"image-20201206115653987\"></p>\n<h2 id=\"6、-下载a-txt到本地文件系统\">6、 下载a.txt到本地文件系统</h2>\n<p>命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>截图：<img src=\"/img/image-20201206115738831.png\" alt=\"image-20201206115738831\"></p>\n<h2 id=\"7、删除HDFS上的-usr-local-hadoop-data-txtdir\">7、删除HDFS上的/usr/local/hadoop/data/txtdir</h2>\n<p>命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/</p>\n<p>截图：<img src=\"/img/image-20201206115832025.png\" alt=\"image-20201206115832025\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、创建一个HDFS目录\">1、创建一个HDFS目录</h2>\n<p>命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/hdfs%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9.png\" alt=\"image-20201206115218466\"></p>\n<h2 id=\"2、本地文件上传到HDFS\">2、本地文件上传到HDFS</h2>\n<p>本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir</p>\n<p>命令：echo “I am student” &gt; a.txt</p>\n<p>​       echo “I am teacher” &gt; c.txt</p>\n<p>​       echo “I like hadoop” &gt; b.txt</p>\n<p>​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/HDFS%E4%B8%8A%E4%BC%A0.png\" alt=\"image-20201206115351845\"></p>\n<h2 id=\"3、查看-usr-local-hadoop-data-txtdir目录结构\">3、查看/usr/local/hadoop/data/txtdir目录结构</h2>\n<p>命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir</p>\n<p>截图：</p>\n<p><img src=\"/img/image-20201206115459406.png\" alt=\"image-20201206115459406\"></p>\n<h2 id=\"4、查看HDFS上-usr-local-hadoop-data-txtdir下的文件内容\">4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容</h2>\n<p>命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt</p>\n<p>截图：<img src=\"/img/image-20201206115600801.png\" alt=\"image-20201206115600801\"></p>\n<h2 id=\"5、合并本地多个小文件上传到HDFS\">5、合并本地多个小文件上传到HDFS</h2>\n<p>命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt</p>\n<p>截图：<img src=\"/img/image-20201206115653987.png\" alt=\"image-20201206115653987\"></p>\n<h2 id=\"6、-下载a-txt到本地文件系统\">6、 下载a.txt到本地文件系统</h2>\n<p>命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>截图：<img src=\"/img/image-20201206115738831.png\" alt=\"image-20201206115738831\"></p>\n<h2 id=\"7、删除HDFS上的-usr-local-hadoop-data-txtdir\">7、删除HDFS上的/usr/local/hadoop/data/txtdir</h2>\n<p>命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/</p>\n<p>截图：<img src=\"/img/image-20201206115832025.png\" alt=\"image-20201206115832025\"></p>\n"},{"title":"HBASE shell操作（2）","author":"郑天祺","date":"2020-12-06T04:02:00.000Z","_content":"\n\n\na.创建学生成绩表，结果如下。\n\nRowkey：id\n\n列族：info和course，course包括3个版本数据\n\nb.插入数据\n\n数据包括\n\n| 学生学号 | Info | course |      |              |         |      |         |\n| -------- | ---- | ------ | ---- | ------------ | ------- | ---- | ------- |\n|          | name | age    | sex  | address      | Chinese | math | english |\n| 95001    | Jom  | 20     | 男   | 山东省济南市 | 80      | 85   | 89      |\n| 95002    | Tom  | 19     | 男   | 山东省济南市 | 55，60  | 80   | 71      |\n| 95003    | Lily | 20     | 女   | 北京市       |         | 65   |         |\n\nc.查询数据\n\n- 查找95001的相关数据\n- 查找95002 行、course 列族中 math 列的值\n- 查找成绩为80-90之间的相关数据\n- 查找名字为Jom的相关数据\n- 查找学生地址是山东省的相关数据\n\nd.删除学生95003的相关数据\n\n2、使用HBase Java API（选做）\n\na.查询所有表\n\nb.创建表test，包括列族f1和f2\n\nc.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135\n\nd.插入数据，rk002 ，f1中列name为lisi\n\ne.查看rk001的数据\n\n\n\n创建学生成绩表 \n\ncreate 'student','pratice','info',{NAME=>'course',VERSIONS=>3} \n\n插入数据 \n\nput 'student','95001','info:name','Jom' \n\nput 'student','95001','info:age','20' \n\nput 'student','95001','info:sex','男' \n\nput 'student','95001','info:address','山东省济南市' \n\nput 'student','95001','course:chinese','80' \n\nput 'student','95001','course:math','85' \n\nput 'student','95001','course:english','89' \n\n \n\nput 'student','95002','info:name','Tom' \n\nput 'student','95002','info:age','19' \n\nput 'student','95002','info:sex','男' \n\nput 'student','95002','info:address','山东省济南市' \n\nput 'student','95002','course:chinese','55,60' \n\nput 'student','95002','course:math','80' \n\nput 'student','95002','course:english','71' \n\n \n\nput 'student','95003','info:name','Lily' \n\nput 'student','95003','info:age','20' \n\nput 'student','95003','info:sex','女' \n\nput 'student','95003','info:address','北京市' \n\nput 'student','95003','course:chinese','' \n\nput 'student','95003','course:math','65' \n\nput 'student','95003','course:english','' \n\n\n\n查找95001的相关数据\n\nget 'student','95001' \n\n\n\n查找95002 行、course 列族中 math 列的值 \n\nget 'student','95002',{COLUMN=>'course:math'} \n\n\n\n查找成绩为80-90之间的相关数据 \n\nscan 'student',{COLUMN=>'course', FILTER=>\"ValueFilter(>=,'binary:80') AND ValueFilter(<=,'binary:90')\"}\n\n\n\n查找名字为Jom的相关数据 \n\nscan 'student',{FILTER=>\"ValueFilter(=,'binary:Jom')\"} \n\n\n\n查找学生地址是山东省的相关数据\n\nscan 'student',{FILTER=>\"ValueFilter(=,'substring:山东省')\"} \n\n\n\n删除学生95003的相关数据\n\ndeleteall 'student','95003' ","source":"_posts/HDFS-shell操作（2）.md","raw":"title: HBASE shell操作（2）\nauthor: 郑天祺\ntags:\n\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 12:02:00\n\n---\n\n\n\na.创建学生成绩表，结果如下。\n\nRowkey：id\n\n列族：info和course，course包括3个版本数据\n\nb.插入数据\n\n数据包括\n\n| 学生学号 | Info | course |      |              |         |      |         |\n| -------- | ---- | ------ | ---- | ------------ | ------- | ---- | ------- |\n|          | name | age    | sex  | address      | Chinese | math | english |\n| 95001    | Jom  | 20     | 男   | 山东省济南市 | 80      | 85   | 89      |\n| 95002    | Tom  | 19     | 男   | 山东省济南市 | 55，60  | 80   | 71      |\n| 95003    | Lily | 20     | 女   | 北京市       |         | 65   |         |\n\nc.查询数据\n\n- 查找95001的相关数据\n- 查找95002 行、course 列族中 math 列的值\n- 查找成绩为80-90之间的相关数据\n- 查找名字为Jom的相关数据\n- 查找学生地址是山东省的相关数据\n\nd.删除学生95003的相关数据\n\n2、使用HBase Java API（选做）\n\na.查询所有表\n\nb.创建表test，包括列族f1和f2\n\nc.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135\n\nd.插入数据，rk002 ，f1中列name为lisi\n\ne.查看rk001的数据\n\n\n\n创建学生成绩表 \n\ncreate 'student','pratice','info',{NAME=>'course',VERSIONS=>3} \n\n插入数据 \n\nput 'student','95001','info:name','Jom' \n\nput 'student','95001','info:age','20' \n\nput 'student','95001','info:sex','男' \n\nput 'student','95001','info:address','山东省济南市' \n\nput 'student','95001','course:chinese','80' \n\nput 'student','95001','course:math','85' \n\nput 'student','95001','course:english','89' \n\n \n\nput 'student','95002','info:name','Tom' \n\nput 'student','95002','info:age','19' \n\nput 'student','95002','info:sex','男' \n\nput 'student','95002','info:address','山东省济南市' \n\nput 'student','95002','course:chinese','55,60' \n\nput 'student','95002','course:math','80' \n\nput 'student','95002','course:english','71' \n\n \n\nput 'student','95003','info:name','Lily' \n\nput 'student','95003','info:age','20' \n\nput 'student','95003','info:sex','女' \n\nput 'student','95003','info:address','北京市' \n\nput 'student','95003','course:chinese','' \n\nput 'student','95003','course:math','65' \n\nput 'student','95003','course:english','' \n\n\n\n查找95001的相关数据\n\nget 'student','95001' \n\n\n\n查找95002 行、course 列族中 math 列的值 \n\nget 'student','95002',{COLUMN=>'course:math'} \n\n\n\n查找成绩为80-90之间的相关数据 \n\nscan 'student',{COLUMN=>'course', FILTER=>\"ValueFilter(>=,'binary:80') AND ValueFilter(<=,'binary:90')\"}\n\n\n\n查找名字为Jom的相关数据 \n\nscan 'student',{FILTER=>\"ValueFilter(=,'binary:Jom')\"} \n\n\n\n查找学生地址是山东省的相关数据\n\nscan 'student',{FILTER=>\"ValueFilter(=,'substring:山东省')\"} \n\n\n\n删除学生95003的相关数据\n\ndeleteall 'student','95003' ","slug":"HDFS-shell操作（2）","published":1,"updated":"2022-04-04T08:32:40.139Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyo00117kt9ag3762r3","content":"<p>a.创建学生成绩表，结果如下。</p>\n<p>Rowkey：id</p>\n<p>列族：info和course，course包括3个版本数据</p>\n<p>b.插入数据</p>\n<p>数据包括</p>\n<table>\n<thead>\n<tr>\n<th>学生学号</th>\n<th>Info</th>\n<th>course</th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>name</td>\n<td>age</td>\n<td>sex</td>\n<td>address</td>\n<td>Chinese</td>\n<td>math</td>\n<td>english</td>\n</tr>\n<tr>\n<td>95001</td>\n<td>Jom</td>\n<td>20</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>80</td>\n<td>85</td>\n<td>89</td>\n</tr>\n<tr>\n<td>95002</td>\n<td>Tom</td>\n<td>19</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>55，60</td>\n<td>80</td>\n<td>71</td>\n</tr>\n<tr>\n<td>95003</td>\n<td>Lily</td>\n<td>20</td>\n<td>女</td>\n<td>北京市</td>\n<td></td>\n<td>65</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>c.查询数据</p>\n<ul>\n<li>查找95001的相关数据</li>\n<li>查找95002 行、course 列族中 math 列的值</li>\n<li>查找成绩为80-90之间的相关数据</li>\n<li>查找名字为Jom的相关数据</li>\n<li>查找学生地址是山东省的相关数据</li>\n</ul>\n<p>d.删除学生95003的相关数据</p>\n<p>2、使用HBase Java API（选做）</p>\n<p>a.查询所有表</p>\n<p>b.创建表test，包括列族f1和f2</p>\n<p>c.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135</p>\n<p>d.插入数据，rk002 ，f1中列name为lisi</p>\n<p>e.查看rk001的数据</p>\n<p>创建学生成绩表</p>\n<p>create ‘student’,‘pratice’,‘info’,{NAME=&gt;‘course’,VERSIONS=&gt;3}</p>\n<p>插入数据</p>\n<p>put ‘student’,‘95001’,‘info:name’,‘Jom’</p>\n<p>put ‘student’,‘95001’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95001’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95001’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95001’,‘course:chinese’,‘80’</p>\n<p>put ‘student’,‘95001’,‘course:math’,‘85’</p>\n<p>put ‘student’,‘95001’,‘course:english’,‘89’</p>\n<p>put ‘student’,‘95002’,‘info:name’,‘Tom’</p>\n<p>put ‘student’,‘95002’,‘info:age’,‘19’</p>\n<p>put ‘student’,‘95002’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95002’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95002’,‘course:chinese’,‘55,60’</p>\n<p>put ‘student’,‘95002’,‘course:math’,‘80’</p>\n<p>put ‘student’,‘95002’,‘course:english’,‘71’</p>\n<p>put ‘student’,‘95003’,‘info:name’,‘Lily’</p>\n<p>put ‘student’,‘95003’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95003’,‘info:sex’,‘女’</p>\n<p>put ‘student’,‘95003’,‘info:address’,‘北京市’</p>\n<p>put ‘student’,‘95003’,‘course:chinese’,‘’</p>\n<p>put ‘student’,‘95003’,‘course:math’,‘65’</p>\n<p>put ‘student’,‘95003’,‘course:english’,‘’</p>\n<p>查找95001的相关数据</p>\n<p>get ‘student’,‘95001’</p>\n<p>查找95002 行、course 列族中 math 列的值</p>\n<p>get ‘student’,‘95002’,{COLUMN=&gt;‘course:math’}</p>\n<p>查找成绩为80-90之间的相关数据</p>\n<p>scan ‘student’,{COLUMN=&gt;‘course’, FILTER=&gt;“ValueFilter(&gt;=,‘binary:80’) AND ValueFilter(&lt;=,‘binary:90’)”}</p>\n<p>查找名字为Jom的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘binary:Jom’)”}</p>\n<p>查找学生地址是山东省的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘substring:山东省’)”}</p>\n<p>删除学生95003的相关数据</p>\n<p>deleteall ‘student’,‘95003’</p>\n","site":{"data":{}},"excerpt":"","more":"<p>a.创建学生成绩表，结果如下。</p>\n<p>Rowkey：id</p>\n<p>列族：info和course，course包括3个版本数据</p>\n<p>b.插入数据</p>\n<p>数据包括</p>\n<table>\n<thead>\n<tr>\n<th>学生学号</th>\n<th>Info</th>\n<th>course</th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>name</td>\n<td>age</td>\n<td>sex</td>\n<td>address</td>\n<td>Chinese</td>\n<td>math</td>\n<td>english</td>\n</tr>\n<tr>\n<td>95001</td>\n<td>Jom</td>\n<td>20</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>80</td>\n<td>85</td>\n<td>89</td>\n</tr>\n<tr>\n<td>95002</td>\n<td>Tom</td>\n<td>19</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>55，60</td>\n<td>80</td>\n<td>71</td>\n</tr>\n<tr>\n<td>95003</td>\n<td>Lily</td>\n<td>20</td>\n<td>女</td>\n<td>北京市</td>\n<td></td>\n<td>65</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>c.查询数据</p>\n<ul>\n<li>查找95001的相关数据</li>\n<li>查找95002 行、course 列族中 math 列的值</li>\n<li>查找成绩为80-90之间的相关数据</li>\n<li>查找名字为Jom的相关数据</li>\n<li>查找学生地址是山东省的相关数据</li>\n</ul>\n<p>d.删除学生95003的相关数据</p>\n<p>2、使用HBase Java API（选做）</p>\n<p>a.查询所有表</p>\n<p>b.创建表test，包括列族f1和f2</p>\n<p>c.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135</p>\n<p>d.插入数据，rk002 ，f1中列name为lisi</p>\n<p>e.查看rk001的数据</p>\n<p>创建学生成绩表</p>\n<p>create ‘student’,‘pratice’,‘info’,{NAME=&gt;‘course’,VERSIONS=&gt;3}</p>\n<p>插入数据</p>\n<p>put ‘student’,‘95001’,‘info:name’,‘Jom’</p>\n<p>put ‘student’,‘95001’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95001’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95001’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95001’,‘course:chinese’,‘80’</p>\n<p>put ‘student’,‘95001’,‘course:math’,‘85’</p>\n<p>put ‘student’,‘95001’,‘course:english’,‘89’</p>\n<p>put ‘student’,‘95002’,‘info:name’,‘Tom’</p>\n<p>put ‘student’,‘95002’,‘info:age’,‘19’</p>\n<p>put ‘student’,‘95002’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95002’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95002’,‘course:chinese’,‘55,60’</p>\n<p>put ‘student’,‘95002’,‘course:math’,‘80’</p>\n<p>put ‘student’,‘95002’,‘course:english’,‘71’</p>\n<p>put ‘student’,‘95003’,‘info:name’,‘Lily’</p>\n<p>put ‘student’,‘95003’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95003’,‘info:sex’,‘女’</p>\n<p>put ‘student’,‘95003’,‘info:address’,‘北京市’</p>\n<p>put ‘student’,‘95003’,‘course:chinese’,‘’</p>\n<p>put ‘student’,‘95003’,‘course:math’,‘65’</p>\n<p>put ‘student’,‘95003’,‘course:english’,‘’</p>\n<p>查找95001的相关数据</p>\n<p>get ‘student’,‘95001’</p>\n<p>查找95002 行、course 列族中 math 列的值</p>\n<p>get ‘student’,‘95002’,{COLUMN=&gt;‘course:math’}</p>\n<p>查找成绩为80-90之间的相关数据</p>\n<p>scan ‘student’,{COLUMN=&gt;‘course’, FILTER=&gt;“ValueFilter(&gt;=,‘binary:80’) AND ValueFilter(&lt;=,‘binary:90’)”}</p>\n<p>查找名字为Jom的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘binary:Jom’)”}</p>\n<p>查找学生地址是山东省的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘substring:山东省’)”}</p>\n<p>删除学生95003的相关数据</p>\n<p>deleteall ‘student’,‘95003’</p>\n"},{"title":"HDFS文件操作","author":"郑天祺","date":"2019-12-16T07:47:00.000Z","_content":"\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","source":"_posts/HDFS文件操作.md","raw":"title: HDFS文件操作\nauthor: 郑天祺\ntags:\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 15:47:00\n\n---\n\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","slug":"HDFS文件操作","published":1,"updated":"2022-04-04T08:32:40.139Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyo00147kt9gfj1aoe0","content":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n"},{"title":"HDFS概述","author":"郑天祺","date":"2019-12-16T02:10:00.000Z","_content":"\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","source":"_posts/HDFS概述.md","raw":"title: HDFS概述\nauthor: 郑天祺\ntags:\n\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 10:10:00\n\n---\n\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","slug":"HDFS概述","published":1,"updated":"2022-04-04T08:32:40.140Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyp00177kt9dxyz29ux","content":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2 id=\"1-NameNode\">(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2 id=\"2-DataNode\">(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2 id=\"（3）元信息的持久化\">（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2 id=\"（4）SecondaryNameNode\">（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt=\"\"></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2 id=\"1-NameNode\">(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2 id=\"2-DataNode\">(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2 id=\"（3）元信息的持久化\">（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2 id=\"（4）SecondaryNameNode\">（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt=\"\"></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n"},{"title":"Hash解决冲突的方法","author":"郑天祺","date":"2020-09-18T05:02:00.000Z","_content":"\n# 1、什么是hash表\n\n​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。\n\n​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。\n\n​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。\n\n# 2、hash冲突\n\n​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。\n\n​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。\n\n# 3、常用的hash函数\n\n## \t1、直接定址法\n\n​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b\n\n## \t2、除留余数法\n\n​\t\t假设数组长度为l，value = key % l\n\n​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。\n\n## \t3、数字分析法\n\n​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算\n\n## \t4、平方区中法\n\n取关键字平方后中间几位作为哈希地址\n\n\n\n# 4、处理hash冲突的方法\n\n## \t1、开放定址法\n\n​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 \n\n​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,......,m-1)\n\n## \t2、再哈希法\n\n​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，...... ,等哈希函数\n\n​\t\t计算地址，直到无冲突。\n\n​\t（不易发生聚集，但是增加计算时间）\n\n## \t3、链地址法\n\n​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。\n\n​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。\n\n## \t4、建立公共溢出区\n\n​\t\t将 哈希表 分为 基本表 和 溢出表两部分\n\n​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。\n\n​\t\t","source":"_posts/Hash解决冲突的方法.md","raw":"title: Hash解决冲突的方法\nauthor: 郑天祺\ntags:\n\n  - 哈希表\ncategories:\n  - 面试\ndate: 2020-09-18 13:02:00\n\n---\n\n# 1、什么是hash表\n\n​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。\n\n​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。\n\n​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。\n\n# 2、hash冲突\n\n​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。\n\n​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。\n\n# 3、常用的hash函数\n\n## \t1、直接定址法\n\n​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b\n\n## \t2、除留余数法\n\n​\t\t假设数组长度为l，value = key % l\n\n​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。\n\n## \t3、数字分析法\n\n​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算\n\n## \t4、平方区中法\n\n取关键字平方后中间几位作为哈希地址\n\n\n\n# 4、处理hash冲突的方法\n\n## \t1、开放定址法\n\n​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 \n\n​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,......,m-1)\n\n## \t2、再哈希法\n\n​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，...... ,等哈希函数\n\n​\t\t计算地址，直到无冲突。\n\n​\t（不易发生聚集，但是增加计算时间）\n\n## \t3、链地址法\n\n​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。\n\n​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。\n\n## \t4、建立公共溢出区\n\n​\t\t将 哈希表 分为 基本表 和 溢出表两部分\n\n​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。\n\n​\t\t","slug":"Hash解决冲突的方法","published":1,"updated":"2022-04-04T08:32:40.140Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyq001a7kt9hkoj8lew","content":"<h1>1、什么是hash表</h1>\n<p>​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。</p>\n<p>​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。</p>\n<p>​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。</p>\n<h1>2、hash冲突</h1>\n<p>​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。</p>\n<p>​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。</p>\n<h1>3、常用的hash函数</h1>\n<h2 id=\"1、直接定址法\">1、直接定址法</h2>\n<p>​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b</p>\n<h2 id=\"2、除留余数法\">2、除留余数法</h2>\n<p>​\t\t假设数组长度为l，value = key % l</p>\n<p>​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。</p>\n<h2 id=\"3、数字分析法\">3、数字分析法</h2>\n<p>​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算</p>\n<h2 id=\"4、平方区中法\">4、平方区中法</h2>\n<p>取关键字平方后中间几位作为哈希地址</p>\n<h1>4、处理hash冲突的方法</h1>\n<h2 id=\"1、开放定址法\">1、开放定址法</h2>\n<p>​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</p>\n<p>​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,…,m-1)</p>\n<h2 id=\"2、再哈希法\">2、再哈希法</h2>\n<p>​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，… ,等哈希函数</p>\n<p>​\t\t计算地址，直到无冲突。</p>\n<p>​\t（不易发生聚集，但是增加计算时间）</p>\n<h2 id=\"3、链地址法\">3、链地址法</h2>\n<p>​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。</p>\n<p>​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。</p>\n<h2 id=\"4、建立公共溢出区\">4、建立公共溢出区</h2>\n<p>​\t\t将 哈希表 分为 基本表 和 溢出表两部分</p>\n<p>​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。</p>\n<p>​</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、什么是hash表</h1>\n<p>​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。</p>\n<p>​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。</p>\n<p>​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。</p>\n<h1>2、hash冲突</h1>\n<p>​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。</p>\n<p>​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。</p>\n<h1>3、常用的hash函数</h1>\n<h2 id=\"1、直接定址法\">1、直接定址法</h2>\n<p>​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b</p>\n<h2 id=\"2、除留余数法\">2、除留余数法</h2>\n<p>​\t\t假设数组长度为l，value = key % l</p>\n<p>​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。</p>\n<h2 id=\"3、数字分析法\">3、数字分析法</h2>\n<p>​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算</p>\n<h2 id=\"4、平方区中法\">4、平方区中法</h2>\n<p>取关键字平方后中间几位作为哈希地址</p>\n<h1>4、处理hash冲突的方法</h1>\n<h2 id=\"1、开放定址法\">1、开放定址法</h2>\n<p>​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</p>\n<p>​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,…,m-1)</p>\n<h2 id=\"2、再哈希法\">2、再哈希法</h2>\n<p>​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，… ,等哈希函数</p>\n<p>​\t\t计算地址，直到无冲突。</p>\n<p>​\t（不易发生聚集，但是增加计算时间）</p>\n<h2 id=\"3、链地址法\">3、链地址法</h2>\n<p>​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。</p>\n<p>​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。</p>\n<h2 id=\"4、建立公共溢出区\">4、建立公共溢出区</h2>\n<p>​\t\t将 哈希表 分为 基本表 和 溢出表两部分</p>\n<p>​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。</p>\n<p>​</p>\n"},{"title":"Hive数据定义","author":"郑天祺","date":"2020-01-17T06:18:00.000Z","_content":"\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","source":"_posts/Hive数据定义.md","raw":"title: Hive数据定义\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 14:18:00\n\n---\n\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","slug":"Hive数据定义","published":1,"updated":"2022-04-04T08:32:40.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyt001d7kt924s9ghz4","content":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n"},{"title":"HiveQL视图","author":"郑天祺","date":"2020-01-20T08:27:00.000Z","_content":"\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","source":"_posts/HiveQL视图.md","raw":"title: HiveQL视图\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 16:27:00\n\n---\n\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","slug":"HiveQL视图","published":1,"updated":"2022-04-04T08:32:40.143Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyt001f7kt9h3zt9bz3","content":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n"},{"title":"Hive数据操作（1）","author":"郑天祺","date":"2020-01-17T08:11:00.000Z","_content":"\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","source":"_posts/Hive数据操作.md","raw":"title: Hive数据操作（1）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 16:11:00\n\n---\n\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","slug":"Hive数据操作","published":1,"updated":"2022-04-04T08:32:40.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyu001j7kt92j3d6gqk","content":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '$&#123;env:HOME&#125;/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE … WHEN … THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '$&#123;env:HOME&#125;/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE … WHEN … THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n"},{"title":"Hive数据操作（2）","author":"郑天祺","date":"2020-01-19T07:50:00.000Z","_content":"\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","source":"_posts/Hive数据操作（2）.md","raw":"title: Hive数据操作（2）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-19 15:50:00\n\n---\n\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","slug":"Hive数据操作（2）","published":1,"updated":"2022-04-04T08:32:40.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyv001m7kt92417djj1","content":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN … EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN … EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n"},{"title":"Hive数据操作（3）","author":"郑天祺","date":"2020-01-20T06:27:00.000Z","_content":"\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","source":"_posts/Hive数据操作（3）.md","raw":"title: Hive数据操作（3）\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 14:27:00\n---\n\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","slug":"Hive数据操作（3）","published":1,"updated":"2022-04-04T08:32:40.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyw001p7kt9htw45mxa","content":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n"},{"title":"Hive数据类型和文件格式","author":"郑天祺","date":"2020-01-17T05:41:00.000Z","_content":"\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","source":"_posts/Hive数据类型和文件格式.md","raw":"title: Hive数据类型和文件格式\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 13:41:00\n\n---\n\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","slug":"Hive数据类型和文件格式","published":1,"updated":"2022-04-04T08:32:40.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyx001s7kt988sa6mfy","content":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：… cast(s AS INT) … ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  ‘\\t’ (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：… cast(s AS INT) … ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  ‘\\t’ (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n"},{"title":"Hive模式设计","author":"郑天祺","date":"2020-01-21T06:09:00.000Z","_content":"\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","source":"_posts/Hive模式设计.md","raw":"title: Hive模式设计\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 14:09:00\n\n---\n\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","slug":"Hive模式设计","published":1,"updated":"2022-04-04T08:32:40.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyy001v7kt9fll11q4v","content":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2 id=\"二、同一份数据多种处理\">二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=$&#123;dt&#125;)\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION($&#123;hiveconf:dt&#125;)\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='$&#123;hiveconf:dt&#125;')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2 id=\"二、同一份数据多种处理\">二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=$&#123;dt&#125;)\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION($&#123;hiveconf:dt&#125;)\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='$&#123;hiveconf:dt&#125;')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n"},{"title":"Hive索引","author":"郑天祺","date":"2020-01-21T02:59:00.000Z","_content":"\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","source":"_posts/Hive索引.md","raw":"title: Hive索引\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 10:59:00\n\n---\n\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","slug":"Hive索引","published":1,"updated":"2022-04-04T08:32:40.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyy001y7kt96j6750ry","content":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n"},{"title":"HttpClient","author":"郑天祺","date":"2020-07-15T09:12:00.000Z","_content":"\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","source":"_posts/HttpClient.md","raw":"title: HttpClient\nauthor: 郑天祺\ntags:\n\n  - httpclient\ncategories:\n  - 网络\ndate: 2020-07-15 17:12:00\n\n---\n\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","slug":"HttpClient","published":1,"updated":"2022-04-04T08:32:40.146Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnyz00217kt98y0w51n1","content":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2 id=\"（1）引入依赖\">（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2 id=\"（2）返回实体\">（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable &#123;\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) &#123;\n        this.code = code;\n        this.content = content;\n    &#125;\n\n    public HttpClientResult(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public int getCode() &#123;\n        return code;\n    &#125;\n\n    public void setCode(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public String getContent() &#123;\n        return content;\n    &#125;\n\n    public void setContent(String content) &#123;\n        this.content = content;\n    &#125;\n\n    @Override\n    public String toString() &#123;\n        return &quot;HttpClientResult&#123;&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '&#125;';\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"（3）工具类\">（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils &#123;\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception &#123;\n        return doGet(url, null, null);\n    &#125;\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doGet(url, null, params);\n    &#125;\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception &#123;\n        return doPost(url, null, null);\n    &#125;\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doPost(url, null, params);\n    &#125;\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception &#123;\n        return doPut(url);\n    &#125;\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        if (params == null) &#123;\n            params = new HashMap&lt;String, String&gt;();\n        &#125;\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    &#125;\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) &#123;\n        // 封装请求头\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException &#123;\n        // 封装请求参数\n        if (params != null) &#123;\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            &#125;\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        &#125;\n    &#125;\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception &#123;\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) &#123;\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) &#123;\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            &#125;\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        &#125;\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    &#125;\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException &#123;\n        // 释放资源\n        if (httpResponse != null) &#123;\n            httpResponse.close();\n        &#125;\n        if (httpClient != null) &#123;\n            httpClient.close();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2 id=\"（1）引入依赖\">（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2 id=\"（2）返回实体\">（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable &#123;\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) &#123;\n        this.code = code;\n        this.content = content;\n    &#125;\n\n    public HttpClientResult(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public int getCode() &#123;\n        return code;\n    &#125;\n\n    public void setCode(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public String getContent() &#123;\n        return content;\n    &#125;\n\n    public void setContent(String content) &#123;\n        this.content = content;\n    &#125;\n\n    @Override\n    public String toString() &#123;\n        return &quot;HttpClientResult&#123;&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '&#125;';\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"（3）工具类\">（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils &#123;\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception &#123;\n        return doGet(url, null, null);\n    &#125;\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doGet(url, null, params);\n    &#125;\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception &#123;\n        return doPost(url, null, null);\n    &#125;\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doPost(url, null, params);\n    &#125;\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception &#123;\n        return doPut(url);\n    &#125;\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        if (params == null) &#123;\n            params = new HashMap&lt;String, String&gt;();\n        &#125;\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    &#125;\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) &#123;\n        // 封装请求头\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException &#123;\n        // 封装请求参数\n        if (params != null) &#123;\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            &#125;\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        &#125;\n    &#125;\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception &#123;\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) &#123;\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) &#123;\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            &#125;\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        &#125;\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    &#125;\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException &#123;\n        // 释放资源\n        if (httpResponse != null) &#123;\n            httpResponse.close();\n        &#125;\n        if (httpClient != null) &#123;\n            httpClient.close();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n"},{"title":"Hive调优","author":"郑天祺","date":"2020-01-21T08:48:00.000Z","_content":"\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","source":"_posts/Hive调优.md","raw":"title: Hive调优\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 16:48:00\n\n---\n\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","slug":"Hive调优","published":1,"updated":"2022-04-04T08:32:40.146Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz000247kt949vzc52d","content":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=$&#123;hiveconf.mapred.job.tracker&#125;;\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=$&#123;oldjobtracker&#125;;\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=$&#123;hiveconf.mapred.job.tracker&#125;;\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=$&#123;oldjobtracker&#125;;\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n"},{"title":"Http和Https的区别","author":"郑天祺","date":"2020-09-11T03:11:00.000Z","_content":"\n# 一、基本概念\n\n## 1、HTTP\n\n​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。\n\n​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。\n\n​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。\n\n## 2、HTTPS\n\n​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。\n\n## 3、HTTPS和HTTP的区别\n\n（1）安全性\n\nHTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。\n\n（2）费用\n\n使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。\n\n（3）响应速度\n\nHTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。\n\n（4）端口\n\nhttp 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。\n\n（5）空间消耗\n\nHTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。\n\n# 二、HTTPS的工作流程\n\n![img](/img/https-intro.png)\n\n\n\n1、客户端发起 HTTPS 请求\n\n这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。\n\n2、服务端的配置\n\n采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。\n\n这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。\n\n3、传送证书\n\n这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。\n\n4、客户端解析证书\n\n这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。\n\n如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。\n\n5、传送加密信息\n\n这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。\n\n6、服务端解密信息\n\n服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。\n\n7、传输加密后的信息\n\n这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。\n\n8、客户端解密信息\n\n客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。","source":"_posts/Http和Https的区别.md","raw":"title: Http和Https的区别\nauthor: 郑天祺\ntags:\n  - https\ncategories:\n  - 网络\ndate: 2020-09-11 11:11:00\n\n---\n\n# 一、基本概念\n\n## 1、HTTP\n\n​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。\n\n​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。\n\n​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。\n\n## 2、HTTPS\n\n​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。\n\n## 3、HTTPS和HTTP的区别\n\n（1）安全性\n\nHTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。\n\n（2）费用\n\n使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。\n\n（3）响应速度\n\nHTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。\n\n（4）端口\n\nhttp 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。\n\n（5）空间消耗\n\nHTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。\n\n# 二、HTTPS的工作流程\n\n![img](/img/https-intro.png)\n\n\n\n1、客户端发起 HTTPS 请求\n\n这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。\n\n2、服务端的配置\n\n采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。\n\n这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。\n\n3、传送证书\n\n这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。\n\n4、客户端解析证书\n\n这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。\n\n如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。\n\n5、传送加密信息\n\n这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。\n\n6、服务端解密信息\n\n服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。\n\n7、传输加密后的信息\n\n这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。\n\n8、客户端解密信息\n\n客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。","slug":"Http和Https的区别","published":1,"updated":"2022-04-04T08:32:40.146Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz100277kt94c2p7v9y","content":"<h1>一、基本概念</h1>\n<h2 id=\"1、HTTP\">1、HTTP</h2>\n<p>​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。</p>\n<p>​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。</p>\n<p>​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。</p>\n<h2 id=\"2、HTTPS\">2、HTTPS</h2>\n<p>​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。</p>\n<h2 id=\"3、HTTPS和HTTP的区别\">3、HTTPS和HTTP的区别</h2>\n<p>（1）安全性</p>\n<p>HTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。</p>\n<p>（2）费用</p>\n<p>使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。</p>\n<p>（3）响应速度</p>\n<p>HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。</p>\n<p>（4）端口</p>\n<p>http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。</p>\n<p>（5）空间消耗</p>\n<p>HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。</p>\n<h1>二、HTTPS的工作流程</h1>\n<p><img src=\"/img/https-intro.png\" alt=\"img\"></p>\n<p>1、客户端发起 HTTPS 请求</p>\n<p>这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。</p>\n<p>2、服务端的配置</p>\n<p>采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。</p>\n<p>这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。</p>\n<p>3、传送证书</p>\n<p>这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。</p>\n<p>4、客户端解析证书</p>\n<p>这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。</p>\n<p>如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。</p>\n<p>5、传送加密信息</p>\n<p>这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。</p>\n<p>6、服务端解密信息</p>\n<p>服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。</p>\n<p>7、传输加密后的信息</p>\n<p>这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。</p>\n<p>8、客户端解密信息</p>\n<p>客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本概念</h1>\n<h2 id=\"1、HTTP\">1、HTTP</h2>\n<p>​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。</p>\n<p>​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。</p>\n<p>​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。</p>\n<h2 id=\"2、HTTPS\">2、HTTPS</h2>\n<p>​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。</p>\n<h2 id=\"3、HTTPS和HTTP的区别\">3、HTTPS和HTTP的区别</h2>\n<p>（1）安全性</p>\n<p>HTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。</p>\n<p>（2）费用</p>\n<p>使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。</p>\n<p>（3）响应速度</p>\n<p>HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。</p>\n<p>（4）端口</p>\n<p>http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。</p>\n<p>（5）空间消耗</p>\n<p>HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。</p>\n<h1>二、HTTPS的工作流程</h1>\n<p><img src=\"/img/https-intro.png\" alt=\"img\"></p>\n<p>1、客户端发起 HTTPS 请求</p>\n<p>这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。</p>\n<p>2、服务端的配置</p>\n<p>采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。</p>\n<p>这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。</p>\n<p>3、传送证书</p>\n<p>这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。</p>\n<p>4、客户端解析证书</p>\n<p>这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。</p>\n<p>如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。</p>\n<p>5、传送加密信息</p>\n<p>这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。</p>\n<p>6、服务端解密信息</p>\n<p>服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。</p>\n<p>7、传输加密后的信息</p>\n<p>这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。</p>\n<p>8、客户端解密信息</p>\n<p>客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。</p>\n"},{"title":"IOC中的基本反射步骤","author":"郑天祺","date":"2020-09-13T01:22:00.000Z","_content":"\n# 1、controller\n\n```java\npackage cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController {\n    \n    private UserService userService;\n\n    public UserService getUserService() {\n        return userService;\n    }\n\n    public void setUserService(UserService userService) {\n        this.userService = userService;\n    }\n}\n```\n\n# 2、service\n\n```java\npackage cn.edu.bjut.base.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n```\n\n# 3、TestReflect\n\n```java\npackage cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect {\n    public static void main(String[] args) throws Exception {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(\"userService\");\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = \"set\" + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    }\n}\n```\n\n","source":"_posts/IOC中的基本反射步骤.md","raw":"title: IOC中的基本反射步骤\nauthor: 郑天祺\ntags:\n\n  - spring\ncategories:\n  - spring\ndate: 2020-09-13 09:22:00\n\n---\n\n# 1、controller\n\n```java\npackage cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController {\n    \n    private UserService userService;\n\n    public UserService getUserService() {\n        return userService;\n    }\n\n    public void setUserService(UserService userService) {\n        this.userService = userService;\n    }\n}\n```\n\n# 2、service\n\n```java\npackage cn.edu.bjut.base.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n```\n\n# 3、TestReflect\n\n```java\npackage cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect {\n    public static void main(String[] args) throws Exception {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(\"userService\");\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = \"set\" + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    }\n}\n```\n\n","slug":"IOC中的基本反射步骤","published":1,"updated":"2022-04-04T08:32:40.147Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz200297kt9b7c8116o","content":"<h1>1、controller</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController &#123;\n    \n    private UserService userService;\n\n    public UserService getUserService() &#123;\n        return userService;\n    &#125;\n\n    public void setUserService(UserService userService) &#123;\n        this.userService = userService;\n    &#125;\n&#125;\n</code></pre>\n<h1>2、service</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、TestReflect</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect &#123;\n    public static void main(String[] args) throws Exception &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(&quot;userService&quot;);\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = &quot;set&quot; + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、controller</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController &#123;\n    \n    private UserService userService;\n\n    public UserService getUserService() &#123;\n        return userService;\n    &#125;\n\n    public void setUserService(UserService userService) &#123;\n        this.userService = userService;\n    &#125;\n&#125;\n</code></pre>\n<h1>2、service</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、TestReflect</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect &#123;\n    public static void main(String[] args) throws Exception &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(&quot;userService&quot;);\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = &quot;set&quot; + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"JAVA数据类型易混概念","author":"郑天祺","date":"2020-01-06T06:30:00.000Z","_content":"\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","source":"_posts/JAVA数据类型.md","raw":"title: JAVA数据类型易混概念\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-01-06 14:30:00\n\n---\n\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","slug":"JAVA数据类型","published":1,"updated":"2022-04-04T08:32:40.147Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz3002d7kt9cj5r5rco","content":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    &#125;\n&#125;\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。<br>\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。<br>\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。<br>\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：<br>\n​\t\t\tdouble x * 9.997 ;<br>\n​\t\t\tint nx = ( int ) x ;<br>\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。<br>\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b<br>\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    &#125;\n&#125;\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。<br>\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。<br>\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。<br>\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：<br>\n​\t\t\tdouble x * 9.997 ;<br>\n​\t\t\tint nx = ( int ) x ;<br>\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。<br>\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b<br>\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n"},{"title":"JDK并发包常用类","author":"ztq","date":"2022-04-08T14:05:00.000Z","_content":"\n# 1、工具类\n\n提供并发控制手段: CountDownLatch、CyclicBarrier、Semaphore\n\n线程间数据交换: Exchanger\n\n## CountDownLatch：\n\n允许一个或多个线程等待其他线程完成操作。\n\nCountDownLatch的构造函数接受一个int类型的参数作为计数器，你想等待n个点完成，就传入n。\n\n两个重要的方法:\n\ncountDown()：调用时，n会减1。\n\nawait()：调用会阻塞当前线程，直到n变成0。\n\nawait(long time,TimeUnit unit)：等待特定时间后，就不会继续阻塞当前线程。\n\ntips:计数器必须大于等于0，当为0时，await就不会阻塞当前线程。\n\n不提供重新初始化或修改内部计数器的值的功能。\n\n## CyclicBarrier\n\n可循环使用的屏障。\n\n让一组线程到达一个屏障（也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。\n\nCyclicBarrier默认构造放时CyclicBarrier(int parities) ,其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达屏障，然后当前线程被阻塞。\n\n```java\n// 王者荣耀需要10个玩家\nCyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() {\n        @Override\n        public void run()\n            // 这是玩家都到齐之后会执行的代码\n            System.out.println(\"10人都已到达游戏战场\")\n        }\n    });\n\n// 定义10线程，相当于10个玩家\n  for (int i = 0; i < 10; i++) {\n      final int finalI = i;\n      new Thread(new Runnable() {\n          @Override\n          public void run() {\n              try {\n                  // 模拟每人到游戏战场所需时间 \n                  Thread.sleep((long) (Math.random()*5000));\n              } catch (InterruptedException e) {\n                  e.printStackTrace();\n              }\n              System.out.println(\"第\"+Thread.currentThread().getName()+\"个人到达游戏战场\");\n              try {\n                  // 等待其他人到达游戏战场\n                  cyclicBarrier.await();\n              } catch (InterruptedException e) {\n                  e.printStackTrace();\n              } catch (BrokenBarrierException e) {\n                  e.printStackTrace();\n              }\n              System.out.println(Thread.currentThread().getName()+\"比赛开始\");\n          }\n      }, String.valueOf(finalI)).start();\n  }\n```\n\n\n\n## CountDownLatch和CyclicBarrier区别\n\ncountDownLatch：\n\n计数器：计数器只能使用一次。\n\n等待：一个线程或多个等待另外n个线程完成之后才能执行。\n\nCyclicBarrier：\n\n计数器：计数器可以重置(通过reset()方法)。\n\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。\n\n## Semaphore\n\n用来控制同时访问资源的线程数量，通过协调各个线程，来保证合理的公共资源的访问。\n\n应用场景:流量控制，特别是公共资源有限的应用场景，比如数据链接，限流等。\n\n```java\nSemaphore semaphore = new Semaphore(10,true);\nsemaphore.acquire();\n//do something here\nsemaphore.release();\n```\n\n```java\npublic class TestCar {\n    // 停车场同时容纳的车辆10\n    private  static  Semaphore semaphore=new Semaphore(10);\n    public static void main(String[] args) {\n        // 模拟100辆车进入停车场\n        for(int i=0;i<100;i++){\n            Thread thread=new Thread(new Runnable() {\n                public void run() {\n                    try {\n                        System.out.println(\"====\"+Thread.currentThread().getName()+\"来到停车场\");\n                        if(semaphore.availablePermits()==0){\n                            System.out.println(\"车位不足，请耐心等待\");\n                        }\n                        //获取令牌尝试进入停车场\n                        semaphore.acquire();\n                        System.out.println(Thread.currentThread().getName()+\"成功进入停车场\");\n                        //模拟车辆在停车场停留的时间\n                        Thread.sleep(new Random().nextInt(10000));\n                        System.out.println(Thread.currentThread().getName()+\"驶出停车场\");\n                        //释放令牌，腾出停车场车位\n                        semaphore.release();\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            },i+\"号车\");\n            thread.start();\n        }\n    }\n}\n```\n\n\n\n## Exchanger\n\nExchanger是一个用于线程间协作的工具类，它提供一个同步点，在这个同步点上，两个线程可以交换彼此的数据。\n\n比如第一个线程执行exchange()方法，它会一直等待第二个线程也执行exchange，当两个线程都到同步点，就可以交换数据了。\n\n一般来说为了避免一直等待的情况，可以使用exchange( x,long timeout,TimeUnit unit),设置最大等待时间。Exchanger可以用于遗传算法。\n","source":"_posts/JDK并发包常用类.md","raw":"title: JDK并发包常用类\nauthor: ztq\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2022-04-08 22:05:00\n\n---\n\n# 1、工具类\n\n提供并发控制手段: CountDownLatch、CyclicBarrier、Semaphore\n\n线程间数据交换: Exchanger\n\n## CountDownLatch：\n\n允许一个或多个线程等待其他线程完成操作。\n\nCountDownLatch的构造函数接受一个int类型的参数作为计数器，你想等待n个点完成，就传入n。\n\n两个重要的方法:\n\ncountDown()：调用时，n会减1。\n\nawait()：调用会阻塞当前线程，直到n变成0。\n\nawait(long time,TimeUnit unit)：等待特定时间后，就不会继续阻塞当前线程。\n\ntips:计数器必须大于等于0，当为0时，await就不会阻塞当前线程。\n\n不提供重新初始化或修改内部计数器的值的功能。\n\n## CyclicBarrier\n\n可循环使用的屏障。\n\n让一组线程到达一个屏障（也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。\n\nCyclicBarrier默认构造放时CyclicBarrier(int parities) ,其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达屏障，然后当前线程被阻塞。\n\n```java\n// 王者荣耀需要10个玩家\nCyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() {\n        @Override\n        public void run()\n            // 这是玩家都到齐之后会执行的代码\n            System.out.println(\"10人都已到达游戏战场\")\n        }\n    });\n\n// 定义10线程，相当于10个玩家\n  for (int i = 0; i < 10; i++) {\n      final int finalI = i;\n      new Thread(new Runnable() {\n          @Override\n          public void run() {\n              try {\n                  // 模拟每人到游戏战场所需时间 \n                  Thread.sleep((long) (Math.random()*5000));\n              } catch (InterruptedException e) {\n                  e.printStackTrace();\n              }\n              System.out.println(\"第\"+Thread.currentThread().getName()+\"个人到达游戏战场\");\n              try {\n                  // 等待其他人到达游戏战场\n                  cyclicBarrier.await();\n              } catch (InterruptedException e) {\n                  e.printStackTrace();\n              } catch (BrokenBarrierException e) {\n                  e.printStackTrace();\n              }\n              System.out.println(Thread.currentThread().getName()+\"比赛开始\");\n          }\n      }, String.valueOf(finalI)).start();\n  }\n```\n\n\n\n## CountDownLatch和CyclicBarrier区别\n\ncountDownLatch：\n\n计数器：计数器只能使用一次。\n\n等待：一个线程或多个等待另外n个线程完成之后才能执行。\n\nCyclicBarrier：\n\n计数器：计数器可以重置(通过reset()方法)。\n\n等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。\n\n## Semaphore\n\n用来控制同时访问资源的线程数量，通过协调各个线程，来保证合理的公共资源的访问。\n\n应用场景:流量控制，特别是公共资源有限的应用场景，比如数据链接，限流等。\n\n```java\nSemaphore semaphore = new Semaphore(10,true);\nsemaphore.acquire();\n//do something here\nsemaphore.release();\n```\n\n```java\npublic class TestCar {\n    // 停车场同时容纳的车辆10\n    private  static  Semaphore semaphore=new Semaphore(10);\n    public static void main(String[] args) {\n        // 模拟100辆车进入停车场\n        for(int i=0;i<100;i++){\n            Thread thread=new Thread(new Runnable() {\n                public void run() {\n                    try {\n                        System.out.println(\"====\"+Thread.currentThread().getName()+\"来到停车场\");\n                        if(semaphore.availablePermits()==0){\n                            System.out.println(\"车位不足，请耐心等待\");\n                        }\n                        //获取令牌尝试进入停车场\n                        semaphore.acquire();\n                        System.out.println(Thread.currentThread().getName()+\"成功进入停车场\");\n                        //模拟车辆在停车场停留的时间\n                        Thread.sleep(new Random().nextInt(10000));\n                        System.out.println(Thread.currentThread().getName()+\"驶出停车场\");\n                        //释放令牌，腾出停车场车位\n                        semaphore.release();\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            },i+\"号车\");\n            thread.start();\n        }\n    }\n}\n```\n\n\n\n## Exchanger\n\nExchanger是一个用于线程间协作的工具类，它提供一个同步点，在这个同步点上，两个线程可以交换彼此的数据。\n\n比如第一个线程执行exchange()方法，它会一直等待第二个线程也执行exchange，当两个线程都到同步点，就可以交换数据了。\n\n一般来说为了避免一直等待的情况，可以使用exchange( x,long timeout,TimeUnit unit),设置最大等待时间。Exchanger可以用于遗传算法。\n","slug":"JDK并发包常用类","published":1,"updated":"2022-04-08T14:21:09.048Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz4002g7kt9391hb0oq","content":"<h1>1、工具类</h1>\n<p>提供并发控制手段: CountDownLatch、CyclicBarrier、Semaphore</p>\n<p>线程间数据交换: Exchanger</p>\n<h2 id=\"CountDownLatch：\">CountDownLatch：</h2>\n<p>允许一个或多个线程等待其他线程完成操作。</p>\n<p>CountDownLatch的构造函数接受一个int类型的参数作为计数器，你想等待n个点完成，就传入n。</p>\n<p>两个重要的方法:</p>\n<p>countDown()：调用时，n会减1。</p>\n<p>await()：调用会阻塞当前线程，直到n变成0。</p>\n<p>await(long time,TimeUnit unit)：等待特定时间后，就不会继续阻塞当前线程。</p>\n<p>tips:计数器必须大于等于0，当为0时，await就不会阻塞当前线程。</p>\n<p>不提供重新初始化或修改内部计数器的值的功能。</p>\n<h2 id=\"CyclicBarrier\">CyclicBarrier</h2>\n<p>可循环使用的屏障。</p>\n<p>让一组线程到达一个屏障（也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。</p>\n<p>CyclicBarrier默认构造放时CyclicBarrier(int parities) ,其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达屏障，然后当前线程被阻塞。</p>\n<pre><code class=\"language-java\">// 王者荣耀需要10个玩家\nCyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() &#123;\n        @Override\n        public void run()\n            // 这是玩家都到齐之后会执行的代码\n            System.out.println(&quot;10人都已到达游戏战场&quot;)\n        &#125;\n    &#125;);\n\n// 定义10线程，相当于10个玩家\n  for (int i = 0; i &lt; 10; i++) &#123;\n      final int finalI = i;\n      new Thread(new Runnable() &#123;\n          @Override\n          public void run() &#123;\n              try &#123;\n                  // 模拟每人到游戏战场所需时间 \n                  Thread.sleep((long) (Math.random()*5000));\n              &#125; catch (InterruptedException e) &#123;\n                  e.printStackTrace();\n              &#125;\n              System.out.println(&quot;第&quot;+Thread.currentThread().getName()+&quot;个人到达游戏战场&quot;);\n              try &#123;\n                  // 等待其他人到达游戏战场\n                  cyclicBarrier.await();\n              &#125; catch (InterruptedException e) &#123;\n                  e.printStackTrace();\n              &#125; catch (BrokenBarrierException e) &#123;\n                  e.printStackTrace();\n              &#125;\n              System.out.println(Thread.currentThread().getName()+&quot;比赛开始&quot;);\n          &#125;\n      &#125;, String.valueOf(finalI)).start();\n  &#125;\n</code></pre>\n<h2 id=\"CountDownLatch和CyclicBarrier区别\">CountDownLatch和CyclicBarrier区别</h2>\n<p>countDownLatch：</p>\n<p>计数器：计数器只能使用一次。</p>\n<p>等待：一个线程或多个等待另外n个线程完成之后才能执行。</p>\n<p>CyclicBarrier：</p>\n<p>计数器：计数器可以重置(通过reset()方法)。</p>\n<p>等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。</p>\n<h2 id=\"Semaphore\">Semaphore</h2>\n<p>用来控制同时访问资源的线程数量，通过协调各个线程，来保证合理的公共资源的访问。</p>\n<p>应用场景:流量控制，特别是公共资源有限的应用场景，比如数据链接，限流等。</p>\n<pre><code class=\"language-java\">Semaphore semaphore = new Semaphore(10,true);\nsemaphore.acquire();\n//do something here\nsemaphore.release();\n</code></pre>\n<pre><code class=\"language-java\">public class TestCar &#123;\n    // 停车场同时容纳的车辆10\n    private  static  Semaphore semaphore=new Semaphore(10);\n    public static void main(String[] args) &#123;\n        // 模拟100辆车进入停车场\n        for(int i=0;i&lt;100;i++)&#123;\n            Thread thread=new Thread(new Runnable() &#123;\n                public void run() &#123;\n                    try &#123;\n                        System.out.println(&quot;====&quot;+Thread.currentThread().getName()+&quot;来到停车场&quot;);\n                        if(semaphore.availablePermits()==0)&#123;\n                            System.out.println(&quot;车位不足，请耐心等待&quot;);\n                        &#125;\n                        //获取令牌尝试进入停车场\n                        semaphore.acquire();\n                        System.out.println(Thread.currentThread().getName()+&quot;成功进入停车场&quot;);\n                        //模拟车辆在停车场停留的时间\n                        Thread.sleep(new Random().nextInt(10000));\n                        System.out.println(Thread.currentThread().getName()+&quot;驶出停车场&quot;);\n                        //释放令牌，腾出停车场车位\n                        semaphore.release();\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;,i+&quot;号车&quot;);\n            thread.start();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"Exchanger\">Exchanger</h2>\n<p>Exchanger是一个用于线程间协作的工具类，它提供一个同步点，在这个同步点上，两个线程可以交换彼此的数据。</p>\n<p>比如第一个线程执行exchange()方法，它会一直等待第二个线程也执行exchange，当两个线程都到同步点，就可以交换数据了。</p>\n<p>一般来说为了避免一直等待的情况，可以使用exchange( x,long timeout,TimeUnit unit),设置最大等待时间。Exchanger可以用于遗传算法。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、工具类</h1>\n<p>提供并发控制手段: CountDownLatch、CyclicBarrier、Semaphore</p>\n<p>线程间数据交换: Exchanger</p>\n<h2 id=\"CountDownLatch：\">CountDownLatch：</h2>\n<p>允许一个或多个线程等待其他线程完成操作。</p>\n<p>CountDownLatch的构造函数接受一个int类型的参数作为计数器，你想等待n个点完成，就传入n。</p>\n<p>两个重要的方法:</p>\n<p>countDown()：调用时，n会减1。</p>\n<p>await()：调用会阻塞当前线程，直到n变成0。</p>\n<p>await(long time,TimeUnit unit)：等待特定时间后，就不会继续阻塞当前线程。</p>\n<p>tips:计数器必须大于等于0，当为0时，await就不会阻塞当前线程。</p>\n<p>不提供重新初始化或修改内部计数器的值的功能。</p>\n<h2 id=\"CyclicBarrier\">CyclicBarrier</h2>\n<p>可循环使用的屏障。</p>\n<p>让一组线程到达一个屏障（也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。</p>\n<p>CyclicBarrier默认构造放时CyclicBarrier(int parities) ,其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达屏障，然后当前线程被阻塞。</p>\n<pre><code class=\"language-java\">// 王者荣耀需要10个玩家\nCyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() &#123;\n        @Override\n        public void run()\n            // 这是玩家都到齐之后会执行的代码\n            System.out.println(&quot;10人都已到达游戏战场&quot;)\n        &#125;\n    &#125;);\n\n// 定义10线程，相当于10个玩家\n  for (int i = 0; i &lt; 10; i++) &#123;\n      final int finalI = i;\n      new Thread(new Runnable() &#123;\n          @Override\n          public void run() &#123;\n              try &#123;\n                  // 模拟每人到游戏战场所需时间 \n                  Thread.sleep((long) (Math.random()*5000));\n              &#125; catch (InterruptedException e) &#123;\n                  e.printStackTrace();\n              &#125;\n              System.out.println(&quot;第&quot;+Thread.currentThread().getName()+&quot;个人到达游戏战场&quot;);\n              try &#123;\n                  // 等待其他人到达游戏战场\n                  cyclicBarrier.await();\n              &#125; catch (InterruptedException e) &#123;\n                  e.printStackTrace();\n              &#125; catch (BrokenBarrierException e) &#123;\n                  e.printStackTrace();\n              &#125;\n              System.out.println(Thread.currentThread().getName()+&quot;比赛开始&quot;);\n          &#125;\n      &#125;, String.valueOf(finalI)).start();\n  &#125;\n</code></pre>\n<h2 id=\"CountDownLatch和CyclicBarrier区别\">CountDownLatch和CyclicBarrier区别</h2>\n<p>countDownLatch：</p>\n<p>计数器：计数器只能使用一次。</p>\n<p>等待：一个线程或多个等待另外n个线程完成之后才能执行。</p>\n<p>CyclicBarrier：</p>\n<p>计数器：计数器可以重置(通过reset()方法)。</p>\n<p>等待：n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。</p>\n<h2 id=\"Semaphore\">Semaphore</h2>\n<p>用来控制同时访问资源的线程数量，通过协调各个线程，来保证合理的公共资源的访问。</p>\n<p>应用场景:流量控制，特别是公共资源有限的应用场景，比如数据链接，限流等。</p>\n<pre><code class=\"language-java\">Semaphore semaphore = new Semaphore(10,true);\nsemaphore.acquire();\n//do something here\nsemaphore.release();\n</code></pre>\n<pre><code class=\"language-java\">public class TestCar &#123;\n    // 停车场同时容纳的车辆10\n    private  static  Semaphore semaphore=new Semaphore(10);\n    public static void main(String[] args) &#123;\n        // 模拟100辆车进入停车场\n        for(int i=0;i&lt;100;i++)&#123;\n            Thread thread=new Thread(new Runnable() &#123;\n                public void run() &#123;\n                    try &#123;\n                        System.out.println(&quot;====&quot;+Thread.currentThread().getName()+&quot;来到停车场&quot;);\n                        if(semaphore.availablePermits()==0)&#123;\n                            System.out.println(&quot;车位不足，请耐心等待&quot;);\n                        &#125;\n                        //获取令牌尝试进入停车场\n                        semaphore.acquire();\n                        System.out.println(Thread.currentThread().getName()+&quot;成功进入停车场&quot;);\n                        //模拟车辆在停车场停留的时间\n                        Thread.sleep(new Random().nextInt(10000));\n                        System.out.println(Thread.currentThread().getName()+&quot;驶出停车场&quot;);\n                        //释放令牌，腾出停车场车位\n                        semaphore.release();\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;,i+&quot;号车&quot;);\n            thread.start();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"Exchanger\">Exchanger</h2>\n<p>Exchanger是一个用于线程间协作的工具类，它提供一个同步点，在这个同步点上，两个线程可以交换彼此的数据。</p>\n<p>比如第一个线程执行exchange()方法，它会一直等待第二个线程也执行exchange，当两个线程都到同步点，就可以交换数据了。</p>\n<p>一般来说为了避免一直等待的情况，可以使用exchange( x,long timeout,TimeUnit unit),设置最大等待时间。Exchanger可以用于遗传算法。</p>\n"},{"title":"JVM内存结构","author":"郑天祺","date":"2020-09-27T03:55:00.000Z","_content":"\n![](/img/JVMMemory.png)\n\n# 1、方法区\n\n用于存储虚拟机加载的 类信息，常量，静态变量等数据。\n\n# 2、堆\n\n存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。\n\n# 3、栈\n\nJava 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。\n\n生命周期与线程相同。\n\n# 4、本地方法栈\n\n作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。\n\n# 5、程序计数器\n\n当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。","source":"_posts/JVM内存结构.md","raw":"title: JVM内存结构\nauthor: 郑天祺\ntags:\n\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-27 11:55:00\n\n---\n\n![](/img/JVMMemory.png)\n\n# 1、方法区\n\n用于存储虚拟机加载的 类信息，常量，静态变量等数据。\n\n# 2、堆\n\n存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。\n\n# 3、栈\n\nJava 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。\n\n生命周期与线程相同。\n\n# 4、本地方法栈\n\n作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。\n\n# 5、程序计数器\n\n当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。","slug":"JVM内存结构","published":1,"updated":"2022-04-04T08:32:40.147Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz5002k7kt9fc6rf7oa","content":"<p><img src=\"/img/JVMMemory.png\" alt=\"\"></p>\n<h1>1、方法区</h1>\n<p>用于存储虚拟机加载的 类信息，常量，静态变量等数据。</p>\n<h1>2、堆</h1>\n<p>存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。</p>\n<h1>3、栈</h1>\n<p>Java 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。</p>\n<p>生命周期与线程相同。</p>\n<h1>4、本地方法栈</h1>\n<p>作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。</p>\n<h1>5、程序计数器</h1>\n<p>当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/img/JVMMemory.png\" alt=\"\"></p>\n<h1>1、方法区</h1>\n<p>用于存储虚拟机加载的 类信息，常量，静态变量等数据。</p>\n<h1>2、堆</h1>\n<p>存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。</p>\n<h1>3、栈</h1>\n<p>Java 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。</p>\n<p>生命周期与线程相同。</p>\n<h1>4、本地方法栈</h1>\n<p>作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。</p>\n<h1>5、程序计数器</h1>\n<p>当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。</p>\n"},{"title":"JVM垃圾回收算法","author":"郑天祺","date":"2020-09-18T01:18:00.000Z","_content":"\n# 1、标记 - 清除 算法\n\n​\t标记无用对象，然后进行清除回收。\n\n​\t缺点：效率不高，无法清除垃圾碎片。\n\n![image-20200918092639589](/img/jvm1.png)\n\n# 2、复制 - 清除 算法\n\n​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n​\t缺点：内存使用率不高，只有原来的一半\n\n![image-20200918092729361](/img/jvm2.png)\n\n# 3、标记 - 整理 - 清除 算法\n\n​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存\n\n![image-20200918092801239](/img/jvm3.png)\n\n# 4、分代 - 收集算法\n\n​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；\n\n​\t\t新生代基本采用复制算法;\n\n​\t\t老年代采用标记整理算法。\n\n# 5、分代 - 收集算法详解\n\n​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。\n\n​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。\n\n​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。\n\n![image-20200918094003026](/img/jvmHeapStructure.png)\n\n·································································\n\n堆大小 = 新生代 + 老年代\n\n新生代与老年代的比例 = 1：2\n\n","source":"_posts/JVM垃圾回收算法.md","raw":"title: JVM垃圾回收算法\nauthor: 郑天祺\ntags:\n\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-18 09:18:00\n\n---\n\n# 1、标记 - 清除 算法\n\n​\t标记无用对象，然后进行清除回收。\n\n​\t缺点：效率不高，无法清除垃圾碎片。\n\n![image-20200918092639589](/img/jvm1.png)\n\n# 2、复制 - 清除 算法\n\n​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n​\t缺点：内存使用率不高，只有原来的一半\n\n![image-20200918092729361](/img/jvm2.png)\n\n# 3、标记 - 整理 - 清除 算法\n\n​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存\n\n![image-20200918092801239](/img/jvm3.png)\n\n# 4、分代 - 收集算法\n\n​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；\n\n​\t\t新生代基本采用复制算法;\n\n​\t\t老年代采用标记整理算法。\n\n# 5、分代 - 收集算法详解\n\n​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。\n\n​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。\n\n​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。\n\n![image-20200918094003026](/img/jvmHeapStructure.png)\n\n·································································\n\n堆大小 = 新生代 + 老年代\n\n新生代与老年代的比例 = 1：2\n\n","slug":"JVM垃圾回收算法","published":1,"updated":"2022-04-04T08:32:40.148Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz6002o7kt9cf1ibtoi","content":"<h1>1、标记 - 清除 算法</h1>\n<p>​\t标记无用对象，然后进行清除回收。</p>\n<p>​\t缺点：效率不高，无法清除垃圾碎片。</p>\n<p><img src=\"/img/jvm1.png\" alt=\"image-20200918092639589\"></p>\n<h1>2、复制 - 清除 算法</h1>\n<p>​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p>\n<p>​\t缺点：内存使用率不高，只有原来的一半</p>\n<p><img src=\"/img/jvm2.png\" alt=\"image-20200918092729361\"></p>\n<h1>3、标记 - 整理 - 清除 算法</h1>\n<p>​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</p>\n<p><img src=\"/img/jvm3.png\" alt=\"image-20200918092801239\"></p>\n<h1>4、分代 - 收集算法</h1>\n<p>​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；</p>\n<p>​\t\t新生代基本采用复制算法;</p>\n<p>​\t\t老年代采用标记整理算法。</p>\n<h1>5、分代 - 收集算法详解</h1>\n<p>​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。</p>\n<p>​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。</p>\n<p>​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。</p>\n<p><img src=\"/img/jvmHeapStructure.png\" alt=\"image-20200918094003026\"></p>\n<p>·································································</p>\n<p>堆大小 = 新生代 + 老年代</p>\n<p>新生代与老年代的比例 = 1：2</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、标记 - 清除 算法</h1>\n<p>​\t标记无用对象，然后进行清除回收。</p>\n<p>​\t缺点：效率不高，无法清除垃圾碎片。</p>\n<p><img src=\"/img/jvm1.png\" alt=\"image-20200918092639589\"></p>\n<h1>2、复制 - 清除 算法</h1>\n<p>​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p>\n<p>​\t缺点：内存使用率不高，只有原来的一半</p>\n<p><img src=\"/img/jvm2.png\" alt=\"image-20200918092729361\"></p>\n<h1>3、标记 - 整理 - 清除 算法</h1>\n<p>​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</p>\n<p><img src=\"/img/jvm3.png\" alt=\"image-20200918092801239\"></p>\n<h1>4、分代 - 收集算法</h1>\n<p>​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；</p>\n<p>​\t\t新生代基本采用复制算法;</p>\n<p>​\t\t老年代采用标记整理算法。</p>\n<h1>5、分代 - 收集算法详解</h1>\n<p>​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。</p>\n<p>​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。</p>\n<p>​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。</p>\n<p><img src=\"/img/jvmHeapStructure.png\" alt=\"image-20200918094003026\"></p>\n<p>·································································</p>\n<p>堆大小 = 新生代 + 老年代</p>\n<p>新生代与老年代的比例 = 1：2</p>\n"},{"title":"JVM性能优化整理","author":"郑天祺","date":"2020-11-17T07:14:00.000Z","_content":"\n1、类加载过程\n\n​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。\n\n​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。\n\n7个阶段：加载-验证-准备-解析-初始化-使用-卸载\n\n（1）加载\n\n加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:\n\n​\t\t通过一个类的全限定名获取该类的二进制流。\n\n​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。\n\n​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。\n\n（2）验证\n\n验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:\n\t\t文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.\n\n​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。\n\n​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n（3）解析\n\n​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n（4）初始化\n\n​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。\n\n（6）使用\n\n（7）卸载\n\n2、GC对象的判定方法\n\n（1）引用计数法\n\n​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。\n\n（2）可达性算法（引用链法）\n\n该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）\n\n在Java中可以作为GC Roots 的对象有以下几种:\n\n虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象\n\n3、java内存泄露\n\n​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。\n\n​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。\n\n​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。\n\n​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。\n\n​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。\n\n​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。\n\n4、深拷贝和浅拷贝\n\n​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。\n\n5、类加载器\n\n实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:\n\t\t（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。\n\t\t（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。\n\n​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。\n\n6、类加载器双亲委派模型\n\n​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。","source":"_posts/JVM性能优化整理.md","raw":"title: JVM性能优化整理\nauthor: 郑天祺\ntags: []\ncategories:\n\n  - 面试\ndate: 2020-11-17 15:14:00\n\n---\n\n1、类加载过程\n\n​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。\n\n​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。\n\n7个阶段：加载-验证-准备-解析-初始化-使用-卸载\n\n（1）加载\n\n加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:\n\n​\t\t通过一个类的全限定名获取该类的二进制流。\n\n​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。\n\n​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。\n\n（2）验证\n\n验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:\n\t\t文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.\n\n​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。\n\n​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n（3）解析\n\n​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n（4）初始化\n\n​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。\n\n（6）使用\n\n（7）卸载\n\n2、GC对象的判定方法\n\n（1）引用计数法\n\n​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。\n\n（2）可达性算法（引用链法）\n\n该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）\n\n在Java中可以作为GC Roots 的对象有以下几种:\n\n虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象\n\n3、java内存泄露\n\n​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。\n\n​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。\n\n​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。\n\n​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。\n\n​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。\n\n​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。\n\n4、深拷贝和浅拷贝\n\n​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。\n\n5、类加载器\n\n实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:\n\t\t（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。\n\t\t（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。\n\n​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。\n\n6、类加载器双亲委派模型\n\n​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。","slug":"JVM性能优化整理","published":1,"updated":"2022-04-04T08:32:40.148Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz7002s7kt9by3r4tua","content":"<p>1、类加载过程</p>\n<p>​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。</p>\n<p>​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。</p>\n<p>7个阶段：加载-验证-准备-解析-初始化-使用-卸载</p>\n<p>（1）加载</p>\n<p>加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:</p>\n<p>​\t\t通过一个类的全限定名获取该类的二进制流。</p>\n<p>​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。</p>\n<p>​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。</p>\n<p>（2）验证</p>\n<p>验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:<br>\n文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.</p>\n<p>​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。<br>\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<p>​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<p>（3）解析</p>\n<p>​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<p>（4）初始化</p>\n<p>​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。</p>\n<p>（6）使用</p>\n<p>（7）卸载</p>\n<p>2、GC对象的判定方法</p>\n<p>（1）引用计数法</p>\n<p>​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。</p>\n<p>（2）可达性算法（引用链法）</p>\n<p>该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）</p>\n<p>在Java中可以作为GC Roots 的对象有以下几种:</p>\n<p>虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象</p>\n<p>3、java内存泄露</p>\n<p>​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。</p>\n<p>​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。</p>\n<p>​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。</p>\n<p>​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。</p>\n<p>​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。</p>\n<p>​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。</p>\n<p>4、深拷贝和浅拷贝</p>\n<p>​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。<br>\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。</p>\n<p>5、类加载器</p>\n<p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:<br>\n（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。<br>\n（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。</p>\n<p>​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径<br>\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。<br>\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</p>\n<p>6、类加载器双亲委派模型</p>\n<p>​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、类加载过程</p>\n<p>​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。</p>\n<p>​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。</p>\n<p>7个阶段：加载-验证-准备-解析-初始化-使用-卸载</p>\n<p>（1）加载</p>\n<p>加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:</p>\n<p>​\t\t通过一个类的全限定名获取该类的二进制流。</p>\n<p>​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。</p>\n<p>​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。</p>\n<p>（2）验证</p>\n<p>验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:<br>\n文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.</p>\n<p>​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。<br>\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<p>​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<p>（3）解析</p>\n<p>​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<p>（4）初始化</p>\n<p>​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。</p>\n<p>（6）使用</p>\n<p>（7）卸载</p>\n<p>2、GC对象的判定方法</p>\n<p>（1）引用计数法</p>\n<p>​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。</p>\n<p>（2）可达性算法（引用链法）</p>\n<p>该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）</p>\n<p>在Java中可以作为GC Roots 的对象有以下几种:</p>\n<p>虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象</p>\n<p>3、java内存泄露</p>\n<p>​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。</p>\n<p>​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。</p>\n<p>​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。</p>\n<p>​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。</p>\n<p>​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。</p>\n<p>​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。</p>\n<p>4、深拷贝和浅拷贝</p>\n<p>​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。<br>\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。</p>\n<p>5、类加载器</p>\n<p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:<br>\n（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。<br>\n（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。</p>\n<p>​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径<br>\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。<br>\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</p>\n<p>6、类加载器双亲委派模型</p>\n<p>​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p>\n"},{"title":"JVM类加载过程","author":"郑天祺","date":"2020-09-27T03:08:00.000Z","_content":"\n# 0、图解\n\n![image-20200927113941669](/img/JVM类加载过程.png)\n\n![image-20210807123832574](/img/image-20210807123832574.png)\n\n# 1、加载\n\n类的加载，分为三步：\n\n（1）通过一个类的全限定名获取该类的二进制流\n\n（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构\n\n（3）在内存中生成该类的Class对象，作为该类的数据访问入口\n\n# 2、验证\n\n验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：\n\n## （1）文件格式验证：\n\n验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。\n\n## （2）元数据验证：\n\n对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。\n\n## （3）字节码验证：\n\n是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。\n\n## （4）符号引用验证：\n\n这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n# 3、准备\n\n准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。\n\n```java\npublic static int value = 123; \n```\n\n在准备阶段初始值是0，在初始化阶段才会变成123\n\n# 4、解析\n\n该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n# 5、初始化\n\n初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码\n\n# 6、总结\n\nJava语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。\n\n这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。\n\n任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。\n\n此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。\n\n# 7、例子\n\n```java\npackage com.ztq.clazz;\n\n/**\n * class的加载顺序\n *\n * @author zhengtianqi\n */\npublic class ClazzLoadSequence {\n\n    static class T1 {\n        public static T1 t = new T1(); // step1. count = 0  t = null 只是引用，默认为空\n        static int count = 2; // step3. count = 2\n\t\tprivate int m = 8;\t// step4. m = 8\n        private T1() {\n            count++; // step2. count = 1\n        }\n    }\n\n    static class T2 {\n        static int count = 2; // step1. count = 2\n        public static T2 t = new T2(); // step2. count = 2\n\n        private T2() {\n            count++; // step3. count = 3\n        }\n    }\n\n\n    public static void main(String[] args) {\n        // 2\n        System.out.println(T1.count);\n        // 3\n        System.out.println(T2.count);\n    }\n\n}\n```\n\n","source":"_posts/JVM类加载过程.md","raw":"title: JVM类加载过程\nauthor: 郑天祺\ntags:\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-27 11:08:00\n\n---\n\n# 0、图解\n\n![image-20200927113941669](/img/JVM类加载过程.png)\n\n![image-20210807123832574](/img/image-20210807123832574.png)\n\n# 1、加载\n\n类的加载，分为三步：\n\n（1）通过一个类的全限定名获取该类的二进制流\n\n（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构\n\n（3）在内存中生成该类的Class对象，作为该类的数据访问入口\n\n# 2、验证\n\n验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：\n\n## （1）文件格式验证：\n\n验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。\n\n## （2）元数据验证：\n\n对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。\n\n## （3）字节码验证：\n\n是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。\n\n## （4）符号引用验证：\n\n这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n# 3、准备\n\n准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。\n\n```java\npublic static int value = 123; \n```\n\n在准备阶段初始值是0，在初始化阶段才会变成123\n\n# 4、解析\n\n该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n# 5、初始化\n\n初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码\n\n# 6、总结\n\nJava语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。\n\n这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。\n\n任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。\n\n此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。\n\n# 7、例子\n\n```java\npackage com.ztq.clazz;\n\n/**\n * class的加载顺序\n *\n * @author zhengtianqi\n */\npublic class ClazzLoadSequence {\n\n    static class T1 {\n        public static T1 t = new T1(); // step1. count = 0  t = null 只是引用，默认为空\n        static int count = 2; // step3. count = 2\n\t\tprivate int m = 8;\t// step4. m = 8\n        private T1() {\n            count++; // step2. count = 1\n        }\n    }\n\n    static class T2 {\n        static int count = 2; // step1. count = 2\n        public static T2 t = new T2(); // step2. count = 2\n\n        private T2() {\n            count++; // step3. count = 3\n        }\n    }\n\n\n    public static void main(String[] args) {\n        // 2\n        System.out.println(T1.count);\n        // 3\n        System.out.println(T2.count);\n    }\n\n}\n```\n\n","slug":"JVM类加载过程","published":1,"updated":"2022-04-04T08:32:40.148Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz8002w7kt94b39alfc","content":"<h1>0、图解</h1>\n<p><img src=\"/img/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B.png\" alt=\"image-20200927113941669\"></p>\n<p><img src=\"/img/image-20210807123832574.png\" alt=\"image-20210807123832574\"></p>\n<h1>1、加载</h1>\n<p>类的加载，分为三步：</p>\n<p>（1）通过一个类的全限定名获取该类的二进制流</p>\n<p>（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构</p>\n<p>（3）在内存中生成该类的Class对象，作为该类的数据访问入口</p>\n<h1>2、验证</h1>\n<p>验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：</p>\n<h2 id=\"（1）文件格式验证：\">（1）文件格式验证：</h2>\n<p>验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。</p>\n<h2 id=\"（2）元数据验证：\">（2）元数据验证：</h2>\n<p>对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。</p>\n<h2 id=\"（3）字节码验证：\">（3）字节码验证：</h2>\n<p>是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<h2 id=\"（4）符号引用验证：\">（4）符号引用验证：</h2>\n<p>这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<h1>3、准备</h1>\n<p>准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。</p>\n<pre><code class=\"language-java\">public static int value = 123; \n</code></pre>\n<p>在准备阶段初始值是0，在初始化阶段才会变成123</p>\n<h1>4、解析</h1>\n<p>该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<h1>5、初始化</h1>\n<p>初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码</p>\n<h1>6、总结</h1>\n<p>Java语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。</p>\n<p>这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。</p>\n<p>任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。</p>\n<p>此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。</p>\n<h1>7、例子</h1>\n<pre><code class=\"language-java\">package com.ztq.clazz;\n\n/**\n * class的加载顺序\n *\n * @author zhengtianqi\n */\npublic class ClazzLoadSequence &#123;\n\n    static class T1 &#123;\n        public static T1 t = new T1(); // step1. count = 0  t = null 只是引用，默认为空\n        static int count = 2; // step3. count = 2\n\t\tprivate int m = 8;\t// step4. m = 8\n        private T1() &#123;\n            count++; // step2. count = 1\n        &#125;\n    &#125;\n\n    static class T2 &#123;\n        static int count = 2; // step1. count = 2\n        public static T2 t = new T2(); // step2. count = 2\n\n        private T2() &#123;\n            count++; // step3. count = 3\n        &#125;\n    &#125;\n\n\n    public static void main(String[] args) &#123;\n        // 2\n        System.out.println(T1.count);\n        // 3\n        System.out.println(T2.count);\n    &#125;\n\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>0、图解</h1>\n<p><img src=\"/img/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B.png\" alt=\"image-20200927113941669\"></p>\n<p><img src=\"/img/image-20210807123832574.png\" alt=\"image-20210807123832574\"></p>\n<h1>1、加载</h1>\n<p>类的加载，分为三步：</p>\n<p>（1）通过一个类的全限定名获取该类的二进制流</p>\n<p>（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构</p>\n<p>（3）在内存中生成该类的Class对象，作为该类的数据访问入口</p>\n<h1>2、验证</h1>\n<p>验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：</p>\n<h2 id=\"（1）文件格式验证：\">（1）文件格式验证：</h2>\n<p>验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。</p>\n<h2 id=\"（2）元数据验证：\">（2）元数据验证：</h2>\n<p>对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。</p>\n<h2 id=\"（3）字节码验证：\">（3）字节码验证：</h2>\n<p>是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<h2 id=\"（4）符号引用验证：\">（4）符号引用验证：</h2>\n<p>这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<h1>3、准备</h1>\n<p>准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。</p>\n<pre><code class=\"language-java\">public static int value = 123; \n</code></pre>\n<p>在准备阶段初始值是0，在初始化阶段才会变成123</p>\n<h1>4、解析</h1>\n<p>该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<h1>5、初始化</h1>\n<p>初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码</p>\n<h1>6、总结</h1>\n<p>Java语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。</p>\n<p>这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。</p>\n<p>任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。</p>\n<p>此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。</p>\n<h1>7、例子</h1>\n<pre><code class=\"language-java\">package com.ztq.clazz;\n\n/**\n * class的加载顺序\n *\n * @author zhengtianqi\n */\npublic class ClazzLoadSequence &#123;\n\n    static class T1 &#123;\n        public static T1 t = new T1(); // step1. count = 0  t = null 只是引用，默认为空\n        static int count = 2; // step3. count = 2\n\t\tprivate int m = 8;\t// step4. m = 8\n        private T1() &#123;\n            count++; // step2. count = 1\n        &#125;\n    &#125;\n\n    static class T2 &#123;\n        static int count = 2; // step1. count = 2\n        public static T2 t = new T2(); // step2. count = 2\n\n        private T2() &#123;\n            count++; // step3. count = 3\n        &#125;\n    &#125;\n\n\n    public static void main(String[] args) &#123;\n        // 2\n        System.out.println(T1.count);\n        // 3\n        System.out.println(T2.count);\n    &#125;\n\n&#125;\n</code></pre>\n"},{"title":"Kafka的简单使用","author":"郑天祺","date":"2020-12-14T04:59:00.000Z","_content":"\n# 1、安装kafka\n\nhttp://kafka.apache.org/quickstart  （linux版）\n\nwindows版：\n首先cmd到kafka的bin下\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties \n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties \n\n集群的设置: http://kafka.apache.org/quickstart#quickstart_multibroker\n\n# 2、参数\n\n建立KafkaProperties 类，编写连接kafka所需要的参数\n\n```java\npublic class KafkaProperties {\n    private static final String IP = \"127.0.0.1:9092\";\n    public static final String TOPIC = \"topic_test\";\n    public static Properties initConfig() {\n        Properties properties = new Properties();\n        properties.setProperty(\"bootstrap.servers\", IP);\n        properties.put(\"group.id\", \"group-1\");\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(\"session.timeout.ms\", \"30000\");\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(\"enable.auto.commit\", \"false\");\n        properties.put(\"auto.commit.interval.ms\", \"1000\");\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(\"auto.offset.reset\", \"earliest\");\n        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    }\n}\n```\n\n# 3、消费者\n\n```java\npublic class KafkaConsumerConnection {\n    private static KafkaConsumer<String, String> consumer = null;\n    private KafkaConsumerConnection() {\n    }\n    public static KafkaConsumer<String, String> getConsumer() {\n        if (consumer == null) {\n            consumer = new KafkaConsumer<>(KafkaProperties.initConfig());\n        }\n        return consumer;\n    }\n}\n\n```\n\n# 4、生产者\n\n```java\npublic class KafkaProducerConnection {\n    private static KafkaProducer<String, String> producer = null;\n    private KafkaProducerConnection() {\n    }\n    public static KafkaProducer<String, String> getProducer(){\n        if(producer == null){\n            producer = new KafkaProducer<>(KafkaProperties.initConfig());\n        }\n        return producer;\n    }\n}\n```\n\n# 5、编写生产者main函数\n\n```java\npublic class ProducerTest {\n    public static void main(String[] args) {\n        Producer<String, String> producer = KafkaProducerConnection.getProducer();\n        try {\n            while (true) {\n                String msg = \"Hello,\" + new Random().nextInt(100);\n                ProducerRecord<String, String> record = new ProducerRecord<>(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(\"消息发送成功:\" + msg);\n                Thread.sleep(500);\n            }\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } finally {\n            producer.close();\n        }\n    }\n}\n```\n\n# 6、编写消费者main函数\n\n```java\npublic class ConsumerTest {\n    public static void main(String[] args) {\n        Consumer<String, String> consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(duration);\n            for (ConsumerRecord<String, String> record : records) {\n                System.out.println(String.format(\"topic:%s,offset:%d,消息:%s\", record.topic(), record.offset(), record.value()));\n            }\n        }\n    }\n}\n```\n\n# 7、先执行消费者后执行生产者。","source":"_posts/Kafka的简单使用.md","raw":"title: Kafka的简单使用\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:59:00\n---\n\n# 1、安装kafka\n\nhttp://kafka.apache.org/quickstart  （linux版）\n\nwindows版：\n首先cmd到kafka的bin下\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties \n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties \n\n集群的设置: http://kafka.apache.org/quickstart#quickstart_multibroker\n\n# 2、参数\n\n建立KafkaProperties 类，编写连接kafka所需要的参数\n\n```java\npublic class KafkaProperties {\n    private static final String IP = \"127.0.0.1:9092\";\n    public static final String TOPIC = \"topic_test\";\n    public static Properties initConfig() {\n        Properties properties = new Properties();\n        properties.setProperty(\"bootstrap.servers\", IP);\n        properties.put(\"group.id\", \"group-1\");\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(\"session.timeout.ms\", \"30000\");\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(\"enable.auto.commit\", \"false\");\n        properties.put(\"auto.commit.interval.ms\", \"1000\");\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(\"auto.offset.reset\", \"earliest\");\n        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    }\n}\n```\n\n# 3、消费者\n\n```java\npublic class KafkaConsumerConnection {\n    private static KafkaConsumer<String, String> consumer = null;\n    private KafkaConsumerConnection() {\n    }\n    public static KafkaConsumer<String, String> getConsumer() {\n        if (consumer == null) {\n            consumer = new KafkaConsumer<>(KafkaProperties.initConfig());\n        }\n        return consumer;\n    }\n}\n\n```\n\n# 4、生产者\n\n```java\npublic class KafkaProducerConnection {\n    private static KafkaProducer<String, String> producer = null;\n    private KafkaProducerConnection() {\n    }\n    public static KafkaProducer<String, String> getProducer(){\n        if(producer == null){\n            producer = new KafkaProducer<>(KafkaProperties.initConfig());\n        }\n        return producer;\n    }\n}\n```\n\n# 5、编写生产者main函数\n\n```java\npublic class ProducerTest {\n    public static void main(String[] args) {\n        Producer<String, String> producer = KafkaProducerConnection.getProducer();\n        try {\n            while (true) {\n                String msg = \"Hello,\" + new Random().nextInt(100);\n                ProducerRecord<String, String> record = new ProducerRecord<>(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(\"消息发送成功:\" + msg);\n                Thread.sleep(500);\n            }\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } finally {\n            producer.close();\n        }\n    }\n}\n```\n\n# 6、编写消费者main函数\n\n```java\npublic class ConsumerTest {\n    public static void main(String[] args) {\n        Consumer<String, String> consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(duration);\n            for (ConsumerRecord<String, String> record : records) {\n                System.out.println(String.format(\"topic:%s,offset:%d,消息:%s\", record.topic(), record.offset(), record.value()));\n            }\n        }\n    }\n}\n```\n\n# 7、先执行消费者后执行生产者。","slug":"Kafka的简单使用","published":1,"updated":"2022-04-04T08:32:40.148Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz900307kt9ce8b0ivr","content":"<h1>1、安装kafka</h1>\n<p><a href=\"http://kafka.apache.org/quickstart\">http://kafka.apache.org/quickstart</a>  （linux版）</p>\n<p>windows版：<br>\n首先cmd到kafka的bin下<br>\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties<br>\n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties</p>\n<p>集群的设置: <a href=\"http://kafka.apache.org/quickstart#quickstart_multibroker\">http://kafka.apache.org/quickstart#quickstart_multibroker</a></p>\n<h1>2、参数</h1>\n<p>建立KafkaProperties 类，编写连接kafka所需要的参数</p>\n<pre><code class=\"language-java\">public class KafkaProperties &#123;\n    private static final String IP = &quot;127.0.0.1:9092&quot;;\n    public static final String TOPIC = &quot;topic_test&quot;;\n    public static Properties initConfig() &#123;\n        Properties properties = new Properties();\n        properties.setProperty(&quot;bootstrap.servers&quot;, IP);\n        properties.put(&quot;group.id&quot;, &quot;group-1&quot;);\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);\n        properties.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;);\n        properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、消费者</h1>\n<pre><code class=\"language-java\">public class KafkaConsumerConnection &#123;\n    private static KafkaConsumer&lt;String, String&gt; consumer = null;\n    private KafkaConsumerConnection() &#123;\n    &#125;\n    public static KafkaConsumer&lt;String, String&gt; getConsumer() &#123;\n        if (consumer == null) &#123;\n            consumer = new KafkaConsumer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return consumer;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、生产者</h1>\n<pre><code class=\"language-java\">public class KafkaProducerConnection &#123;\n    private static KafkaProducer&lt;String, String&gt; producer = null;\n    private KafkaProducerConnection() &#123;\n    &#125;\n    public static KafkaProducer&lt;String, String&gt; getProducer()&#123;\n        if(producer == null)&#123;\n            producer = new KafkaProducer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return producer;\n    &#125;\n&#125;\n</code></pre>\n<h1>5、编写生产者main函数</h1>\n<pre><code class=\"language-java\">public class ProducerTest &#123;\n    public static void main(String[] args) &#123;\n        Producer&lt;String, String&gt; producer = KafkaProducerConnection.getProducer();\n        try &#123;\n            while (true) &#123;\n                String msg = &quot;Hello,&quot; + new Random().nextInt(100);\n                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(&quot;消息发送成功:&quot; + msg);\n                Thread.sleep(500);\n            &#125;\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            producer.close();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、编写消费者main函数</h1>\n<pre><code class=\"language-java\">public class ConsumerTest &#123;\n    public static void main(String[] args) &#123;\n        Consumer&lt;String, String&gt; consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) &#123;\n            ConsumerRecords&lt;String, String&gt; records = consumer.poll(duration);\n            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;\n                System.out.println(String.format(&quot;topic:%s,offset:%d,消息:%s&quot;, record.topic(), record.offset(), record.value()));\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>7、先执行消费者后执行生产者。</h1>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、安装kafka</h1>\n<p><a href=\"http://kafka.apache.org/quickstart\">http://kafka.apache.org/quickstart</a>  （linux版）</p>\n<p>windows版：<br>\n首先cmd到kafka的bin下<br>\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties<br>\n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties</p>\n<p>集群的设置: <a href=\"http://kafka.apache.org/quickstart#quickstart_multibroker\">http://kafka.apache.org/quickstart#quickstart_multibroker</a></p>\n<h1>2、参数</h1>\n<p>建立KafkaProperties 类，编写连接kafka所需要的参数</p>\n<pre><code class=\"language-java\">public class KafkaProperties &#123;\n    private static final String IP = &quot;127.0.0.1:9092&quot;;\n    public static final String TOPIC = &quot;topic_test&quot;;\n    public static Properties initConfig() &#123;\n        Properties properties = new Properties();\n        properties.setProperty(&quot;bootstrap.servers&quot;, IP);\n        properties.put(&quot;group.id&quot;, &quot;group-1&quot;);\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);\n        properties.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;);\n        properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、消费者</h1>\n<pre><code class=\"language-java\">public class KafkaConsumerConnection &#123;\n    private static KafkaConsumer&lt;String, String&gt; consumer = null;\n    private KafkaConsumerConnection() &#123;\n    &#125;\n    public static KafkaConsumer&lt;String, String&gt; getConsumer() &#123;\n        if (consumer == null) &#123;\n            consumer = new KafkaConsumer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return consumer;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、生产者</h1>\n<pre><code class=\"language-java\">public class KafkaProducerConnection &#123;\n    private static KafkaProducer&lt;String, String&gt; producer = null;\n    private KafkaProducerConnection() &#123;\n    &#125;\n    public static KafkaProducer&lt;String, String&gt; getProducer()&#123;\n        if(producer == null)&#123;\n            producer = new KafkaProducer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return producer;\n    &#125;\n&#125;\n</code></pre>\n<h1>5、编写生产者main函数</h1>\n<pre><code class=\"language-java\">public class ProducerTest &#123;\n    public static void main(String[] args) &#123;\n        Producer&lt;String, String&gt; producer = KafkaProducerConnection.getProducer();\n        try &#123;\n            while (true) &#123;\n                String msg = &quot;Hello,&quot; + new Random().nextInt(100);\n                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(&quot;消息发送成功:&quot; + msg);\n                Thread.sleep(500);\n            &#125;\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            producer.close();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、编写消费者main函数</h1>\n<pre><code class=\"language-java\">public class ConsumerTest &#123;\n    public static void main(String[] args) &#123;\n        Consumer&lt;String, String&gt; consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) &#123;\n            ConsumerRecords&lt;String, String&gt; records = consumer.poll(duration);\n            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;\n                System.out.println(String.format(&quot;topic:%s,offset:%d,消息:%s&quot;, record.topic(), record.offset(), record.value()));\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>7、先执行消费者后执行生产者。</h1>\n"},{"title":"MapReduce概述","author":"郑天祺","date":"2019-12-16T09:13:00.000Z","_content":"\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","source":"_posts/MapReduce概述.md","raw":"title: MapReduce概述\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 17:13:00\n\n---\n\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","slug":"MapReduce概述","published":1,"updated":"2022-04-04T08:32:40.149Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnz900327kt9ebyge62n","content":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2 id=\"（1）拆分输入数据\">（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2 id=\"（2）执行Map方法\">（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2 id=\"（3）排序与合并处理\">（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2 id=\"（4）Reduce-阶段的排序与合并\">（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3 id=\"（A）Map\">（A）Map</h3>\n<h4 id=\"（a）Read：\">（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4 id=\"（b）Map：\">（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4 id=\"（c）Collect：\">（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4 id=\"（d）Spill：\">（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4 id=\"（e）Combine：\">（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3 id=\"（B）Reduce\">（B）Reduce</h3>\n<h4 id=\"（a）Shuffle：\">（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4 id=\"（b）Merge：\">（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4 id=\"（c）Sort：\">（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4 id=\"（d）Reduce：\">（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4 id=\"（e）Write：\">（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2 id=\"（1）拆分输入数据\">（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2 id=\"（2）执行Map方法\">（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2 id=\"（3）排序与合并处理\">（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2 id=\"（4）Reduce-阶段的排序与合并\">（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3 id=\"（A）Map\">（A）Map</h3>\n<h4 id=\"（a）Read：\">（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4 id=\"（b）Map：\">（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4 id=\"（c）Collect：\">（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4 id=\"（d）Spill：\">（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4 id=\"（e）Combine：\">（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3 id=\"（B）Reduce\">（B）Reduce</h3>\n<h4 id=\"（a）Shuffle：\">（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4 id=\"（b）Merge：\">（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4 id=\"（c）Sort：\">（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4 id=\"（d）Reduce：\">（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4 id=\"（e）Write：\">（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n"},{"title":"MapReduce平均数计算","author":"郑天祺","date":"2020-12-06T04:14:00.000Z","_content":"\n## 1、建立三个文档\n\n![image-20201206121627187](/img/image-20201206121627187.png)\n\n![image-20201206121650739](/img/image-20201206121650739.png)\n\n![image-20201206121711716](/img/image-20201206121711716.png)\n\n上传 到hdfs\n\n![image-20201206121829515](/img/image-20201206121829515.png)\n\n\n\n## 2、代码\n\n```java\npackage cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore {  \n    public static class Map extends Mapper<LongWritable, Text, Text, IntWritable> {  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(\" \");  \n            List<String> list = new ArrayList<String>(2);  \n            for (String nameOrScore : nameAndScore) {  \n                if (!\"\".equals(nameOrScore)) {  \n                    list.add(nameOrScore);  \n                }  \n            }  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        }  \n    }  \n    public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {  \n        @Override  \n        protected void reduce(Text key, Iterable<IntWritable> values,  \n                Reducer<Text, IntWritable, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) {  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            }  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        }  \n    }  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  \n        Configuration conf = new Configuration();  \n        conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\");  \n        String[] othersArgs = new String[] {\"/mapreduce/inputavg\",\"/mapreduce/outputavg\"};  \n        if(othersArgs.length != 2) {  \n            System.err.println(\"Usage:Avgscore<int><out>\");  \n            System.exit(2);  \n        }  \n        Job job = Job.getInstance(conf, \"Avgscore\");  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    }  \n}  \n```\n\n3、查看结果\n\n![image-20201206122126739](/img/image-20201206122126739.png)","source":"_posts/MapReduce平均数计算.md","raw":"title: MapReduce平均数计算\nauthor: 郑天祺\ntags:\n\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 12:14:00\n\n---\n\n## 1、建立三个文档\n\n![image-20201206121627187](/img/image-20201206121627187.png)\n\n![image-20201206121650739](/img/image-20201206121650739.png)\n\n![image-20201206121711716](/img/image-20201206121711716.png)\n\n上传 到hdfs\n\n![image-20201206121829515](/img/image-20201206121829515.png)\n\n\n\n## 2、代码\n\n```java\npackage cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore {  \n    public static class Map extends Mapper<LongWritable, Text, Text, IntWritable> {  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(\" \");  \n            List<String> list = new ArrayList<String>(2);  \n            for (String nameOrScore : nameAndScore) {  \n                if (!\"\".equals(nameOrScore)) {  \n                    list.add(nameOrScore);  \n                }  \n            }  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        }  \n    }  \n    public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {  \n        @Override  \n        protected void reduce(Text key, Iterable<IntWritable> values,  \n                Reducer<Text, IntWritable, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) {  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            }  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        }  \n    }  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  \n        Configuration conf = new Configuration();  \n        conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\");  \n        String[] othersArgs = new String[] {\"/mapreduce/inputavg\",\"/mapreduce/outputavg\"};  \n        if(othersArgs.length != 2) {  \n            System.err.println(\"Usage:Avgscore<int><out>\");  \n            System.exit(2);  \n        }  \n        Job job = Job.getInstance(conf, \"Avgscore\");  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    }  \n}  \n```\n\n3、查看结果\n\n![image-20201206122126739](/img/image-20201206122126739.png)","slug":"MapReduce平均数计算","published":1,"updated":"2022-04-04T08:32:40.149Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnza00367kt91qqd5mvz","content":"<h2 id=\"1、建立三个文档\">1、建立三个文档</h2>\n<p><img src=\"/img/image-20201206121627187.png\" alt=\"image-20201206121627187\"></p>\n<p><img src=\"/img/image-20201206121650739.png\" alt=\"image-20201206121650739\"></p>\n<p><img src=\"/img/image-20201206121711716.png\" alt=\"image-20201206121711716\"></p>\n<p>上传 到hdfs</p>\n<p><img src=\"/img/image-20201206121829515.png\" alt=\"image-20201206121829515\"></p>\n<h2 id=\"2、代码\">2、代码</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore &#123;  \n    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(&quot; &quot;);  \n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(2);  \n            for (String nameOrScore : nameAndScore) &#123;  \n                if (!&quot;&quot;.equals(nameOrScore)) &#123;  \n                    list.add(nameOrScore);  \n                &#125;  \n            &#125;  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        &#125;  \n    &#125;  \n    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  \n                Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) &#123;  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            &#125;  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        &#125;  \n    &#125;  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;  \n        Configuration conf = new Configuration();  \n        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);  \n        String[] othersArgs = new String[] &#123;&quot;/mapreduce/inputavg&quot;,&quot;/mapreduce/outputavg&quot;&#125;;  \n        if(othersArgs.length != 2) &#123;  \n            System.err.println(&quot;Usage:Avgscore&lt;int&gt;&lt;out&gt;&quot;);  \n            System.exit(2);  \n        &#125;  \n        Job job = Job.getInstance(conf, &quot;Avgscore&quot;);  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    &#125;  \n&#125;  \n</code></pre>\n<p>3、查看结果</p>\n<p><img src=\"/img/image-20201206122126739.png\" alt=\"image-20201206122126739\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、建立三个文档\">1、建立三个文档</h2>\n<p><img src=\"/img/image-20201206121627187.png\" alt=\"image-20201206121627187\"></p>\n<p><img src=\"/img/image-20201206121650739.png\" alt=\"image-20201206121650739\"></p>\n<p><img src=\"/img/image-20201206121711716.png\" alt=\"image-20201206121711716\"></p>\n<p>上传 到hdfs</p>\n<p><img src=\"/img/image-20201206121829515.png\" alt=\"image-20201206121829515\"></p>\n<h2 id=\"2、代码\">2、代码</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore &#123;  \n    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(&quot; &quot;);  \n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(2);  \n            for (String nameOrScore : nameAndScore) &#123;  \n                if (!&quot;&quot;.equals(nameOrScore)) &#123;  \n                    list.add(nameOrScore);  \n                &#125;  \n            &#125;  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        &#125;  \n    &#125;  \n    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  \n                Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) &#123;  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            &#125;  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        &#125;  \n    &#125;  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;  \n        Configuration conf = new Configuration();  \n        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);  \n        String[] othersArgs = new String[] &#123;&quot;/mapreduce/inputavg&quot;,&quot;/mapreduce/outputavg&quot;&#125;;  \n        if(othersArgs.length != 2) &#123;  \n            System.err.println(&quot;Usage:Avgscore&lt;int&gt;&lt;out&gt;&quot;);  \n            System.exit(2);  \n        &#125;  \n        Job job = Job.getInstance(conf, &quot;Avgscore&quot;);  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    &#125;  \n&#125;  \n</code></pre>\n<p>3、查看结果</p>\n<p><img src=\"/img/image-20201206122126739.png\" alt=\"image-20201206122126739\"></p>\n"},{"title":"MinIO单机安装以及使用","author":"ztq","date":"2022-03-22T13:55:00.000Z","_content":"\n一、简介\t\n\n​\t\tMinIO 是在 GNU Affero 通用公共许可证 v3.0 下发布的高性能对象存储。它与 Amazon S3 云存储服务 API 兼容。使用 MinIO 为机器学习、分析和应用程序数据工作负载构建高性能基础架构。\n\n相关文档：\n\n官方文档：https://docs.min.io/docs/minio-quickstart-guide.html\n\n官方文档（中文）：http://docs.minio.org.cn/docs/master/minio-monitoring-guide\n\n官方首页（中文）http://www.minio.org.cn/\n\n\n\n二、单机版安装\n\n1、搜索镜像\n\n```java\ndocker search minio\n```\n\n2、拉取镜像\n\n```java\ndocker pull minio/minio\n```\n\n3、启动与安装镜像\n\n```java\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -d --restart=always \\\n  --name minio1 \\\n  -v /home/environment/minio/data:/data \\\n  -v /home/environment/minio/config:/root/.minio \\\n  --privileged=true \\\n  minio/minio server /data --console-address \":9001\"\n```\n\n-it 表示运行参数\n-p 表示暴露端口\n-d 表示后台运行\n-v 卷挂载（容器到主机的映射，避免存容器中丢失数据）\n\n9001是管理页，默认账号密码均为minioadmin，也可以设置密码  9000\n\n```java\n-e \"MINIO_ACCESS_KEY=账号\" \\\n-e \"MINIO_SECRET_KEY=密码\" \\\n```\n\n4、查看运行镜像\n\n```java\ndocker ps\n```\n\n\n\n三、JAVA API demo\n\n1、引入依赖\n\n```java\n<dependency>\n\t<groupId>com.squareup.okhttp3</groupId>\n\t<artifactId>okhttp</artifactId>\n\t<version>4.8.1</version>\n</dependency>\n\t\n<dependency>\n\t<groupId>io.minio</groupId>\n\t<artifactId>minio</artifactId>\n\t<version>8.3.0</version>\n\t<exclusions>\n\t\t<exclusion>\n\t\t\t<groupId>com.squareup.okhttp3</groupId>\n\t\t\t<artifactId>okhttp</artifactId>\n        </exclusion>\n\t</exclusions>\n</dependency>\n```\n\nminio强依赖于okhttp3，低版本okhttp3会报错：https://blog.csdn.net/u014698745/article/details/122025869\n\n若剔除所有的okhttp3还是不能运行，则将minio版本降低。\n\n2、MinIO客户端\n\n```java\nimport io.minio.*;\nimport io.minio.http.Method;\nimport io.minio.messages.Bucket;\nimport io.minio.messages.Item;\nimport org.apache.commons.lang3.StringUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.stereotype.Component;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.net.URLEncoder;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\n\npublic class S3Utils {\n    private static final Logger logger = LoggerFactory.getLogger(S3Utils.class);\n\n    private static MinioClient s3Client;\n\n    static {\n        try {\n            s3Client = MinioClient.builder()\n                    .endpoint(SdkConstant.endpoint)\n                    .credentials(SdkConstant.s3AccessKey, SdkConstant.s3AccessSecret)\n                    .build();\n        } catch (Exception e) {\n            logger.error(\"初始化s3Client时出错!\", e);\n        }\n        if (null == s3Client) {\n            logger.error(\"创建s3Client时出错!\");\n        }\n    }\n\n    /**\n     * 创建一个桶\n     */\n    public static void createBucket(String bucket) throws Exception {\n        boolean found = s3Client.bucketExists(BucketExistsArgs.builder().bucket(bucket).build());\n        if (!found) {\n            s3Client.makeBucket(MakeBucketArgs.builder().bucket(bucket).build());\n        }\n    }\n\n    /**\n     * 上传一个文件\n     */\n    public static void uploadFile(InputStream stream, String bucket, String objectName) throws Exception {\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n    }\n\n    /**\n     * 上传一个文件,并返回文件url\n     */\n    public static String uploadFileReturnUrl(InputStream stream, String bucket, String objectName) throws Exception {\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n        String url = s3Client.getPresignedObjectUrl(\n                GetPresignedObjectUrlArgs.builder()\n                        .method(Method.GET)\n                        .bucket(bucket)\n                        .object(objectName)\n                        .expiry(7, TimeUnit.DAYS)\n                        .build());\n        return StringUtils.split(url, \"?\")[0];\n    }\n\n    /**\n     * 列出所有的桶\n     */\n    public static List<String> listBuckets() throws Exception {\n        List<Bucket> list = s3Client.listBuckets();\n        List<String> names = new ArrayList<>();\n        list.forEach(b -> {\n            names.add(b.name());\n        });\n        return names;\n    }\n\n    /**\n     * 列出一个桶中的所有文件和目录\n     */\n    public static List<S3FileInfo> listFiles(String bucket) throws Exception {\n        Iterable<Result<Item>> results = s3Client.listObjects(\n                ListObjectsArgs.builder().bucket(bucket).recursive(true).build());\n\n        List<S3FileInfo> infos = new ArrayList<>();\n        results.forEach(r -> {\n            S3FileInfo info = new S3FileInfo();\n            try {\n                Item item = r.get();\n                info.setFileName(item.objectName());\n                info.setDirectory(item.isDir());\n                infos.add(info);\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        });\n        return infos;\n    }\n\n    /**\n     * 下载一个文件\n     */\n    public static InputStream download(String bucket, String objectName) throws Exception {\n        InputStream stream = s3Client.getObject(\n                GetObjectArgs.builder().bucket(bucket).object(objectName).build());\n        return stream;\n    }\n\n    /**\n     * 删除一个桶\n     */\n    public static void deleteBucket(String bucket) throws Exception {\n        s3Client.removeBucket(RemoveBucketArgs.builder().bucket(bucket).build());\n    }\n\n    /**\n     * 删除一个对象\n     */\n    public static void deleteObject(String bucket, String objectName) throws Exception {\n        s3Client.removeObject(RemoveObjectArgs.builder().bucket(bucket).object(objectName).build());\n    }\n\n\n    public static void main(String[] args) throws Exception {\n        S3Utils.createBucket(SdkConstant.bucketName);\n        File file = new File(\"C:\\\\Users\\\\27049\\\\Desktop\\\\minio docker.txt\");\n        InputStream inputStream = new FileInputStream(file);\n        System.out.println(S3Utils.uploadFileReturnUrl(inputStream, SdkConstant.bucketName, \"minio docker.txt\"));\n    }\n}\n\n```\n\n其中：S3FileInfo中只有两个字段，fileName和directory\n\n创建连接的参数为：\n\nSdkConstant.s3AccessKey 用户名\n\nSdkConstant.s3AccessSecret 密码\n\nSdkConstant.endpoint MinIO所在地址（ip:9000）\n\n存储位置：指定的data目录/桶名/...文件夹名.../文件名\n\n\n\ngetPresignedObjectUrl()返回的url最大支持7天，若想永久使用，则配置\n\nhttps://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&utm_term=java+%E8%AE%BE%E7%BD%AE+minio%E6%B0%B8%E4%B9%85%E8%AE%BF%E9%97%AE%E9%93%BE%E6%8E%A5&spm=1000.2123.3001.4430\n\n![image-20220322221741703](/img/image-20220322221741703.png)\n\n\n\n3、AmazonS3客户端（DLC）\n\n```java\n<dependency>\n    <artifactId>aws-java-sdk-s3</artifactId>\n    <groupId>com.amazonaws</groupId>\n    <optional>false</optional>\n    <version>1.11.257</version>\n</dependency>\n```\n\n\n\n```java\nimport com.amazonaws.auth.AWSStaticCredentialsProvider;\nimport com.amazonaws.auth.BasicAWSCredentials;\nimport com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration;\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Calendar;\nimport java.util.Date;\nimport java.util.List;\n\n\npublic class AWSS3Util {\n\n    private static String accessKey = \"\";\n    private static String secretKey = \"\";\n    private static String endpointUrl = \"\";\n\n\n    private volatile static AWSS3Util awsS3Util;\n    private volatile static AmazonS3 s3;\n    private static String region = \"default\";\n\n    private AWSS3Util() {\n\n    }\n\n    /**\n     * 获取带签名认证文件的url\n     */\n    public URL getPresignedUrl(String bucket, String key) {\n        URL url = s3.generatePresignedUrl(bucket, key, getDateAfter(1));\n        return url;\n    }\n\n    /**\n     * 获取文件的url\n     */\n    public URL getUrl(String bucket, String key) {\n        URL url = s3.getUrl(bucket, key);\n        return url;\n    }\n\n\n    /**\n     * 获取指定bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List<String> getFileNames(String bucket) {\n        List<String> fileNames = new ArrayList<>();\n        ObjectListing objectListing = s3.listObjects(bucket);\n        List<S3ObjectSummary> objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) {\n            String name = s3ObjectSummary.getKey();\n            fileNames.add(name.substring(name.lastIndexOf(\"/\") + 1));\n        }\n        return fileNames;\n    }\n    /**\n     * 获取指定路径前缀的bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List<String> getFiles(String bucket,String prefix) {\n        List<String> files = new ArrayList<>();\n        ObjectListing objectListing = s3.listObjects(bucket,prefix);\n        List<S3ObjectSummary> objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) {\n            String name = s3ObjectSummary.getKey();\n            files.add(name.substring(name.lastIndexOf(\"/\") + 1));\n        }\n        return files;\n    }\n\n    public void deleteObject(String bucket, String key) {\n        s3.deleteObject(bucket, key);\n    }\n\n    public static AWSS3Util getInstance() {\n        if (awsS3Util == null) {\n            synchronized (AWSS3Util.class) {\n                if (awsS3Util == null) {\n                    awsS3Util = new AWSS3Util();\n                    s3 = AmazonS3ClientBuilder.standard()\n                            .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))\n                            .withEndpointConfiguration(new EndpointConfiguration(endpointUrl, region)).withPathStyleAccessEnabled(true)\n                            .build();\n                }\n            }\n        }\n        return awsS3Util;\n    }\n\n    /**\n     * 得到几天后的时间\n     *\n     * @param day\n     * @return\n     */\n    private Date getDateAfter(int day) {\n        Date date = new Date();\n        Calendar now = Calendar.getInstance();\n        now.setTime(date);\n        now.set(Calendar.DATE, now.get(Calendar.DATE) + day);\n        return now.getTime();\n    }\n}\n```\n\n","source":"_posts/MinIO单机安装以及使用.md","raw":"title: MinIO单机安装以及使用\nauthor: ztq\ntags:\n\n  - 分布式存储\ncategories:\n  - 数据库\ndate: 2022-03-22 21:55:00\n\n---\n\n一、简介\t\n\n​\t\tMinIO 是在 GNU Affero 通用公共许可证 v3.0 下发布的高性能对象存储。它与 Amazon S3 云存储服务 API 兼容。使用 MinIO 为机器学习、分析和应用程序数据工作负载构建高性能基础架构。\n\n相关文档：\n\n官方文档：https://docs.min.io/docs/minio-quickstart-guide.html\n\n官方文档（中文）：http://docs.minio.org.cn/docs/master/minio-monitoring-guide\n\n官方首页（中文）http://www.minio.org.cn/\n\n\n\n二、单机版安装\n\n1、搜索镜像\n\n```java\ndocker search minio\n```\n\n2、拉取镜像\n\n```java\ndocker pull minio/minio\n```\n\n3、启动与安装镜像\n\n```java\ndocker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -d --restart=always \\\n  --name minio1 \\\n  -v /home/environment/minio/data:/data \\\n  -v /home/environment/minio/config:/root/.minio \\\n  --privileged=true \\\n  minio/minio server /data --console-address \":9001\"\n```\n\n-it 表示运行参数\n-p 表示暴露端口\n-d 表示后台运行\n-v 卷挂载（容器到主机的映射，避免存容器中丢失数据）\n\n9001是管理页，默认账号密码均为minioadmin，也可以设置密码  9000\n\n```java\n-e \"MINIO_ACCESS_KEY=账号\" \\\n-e \"MINIO_SECRET_KEY=密码\" \\\n```\n\n4、查看运行镜像\n\n```java\ndocker ps\n```\n\n\n\n三、JAVA API demo\n\n1、引入依赖\n\n```java\n<dependency>\n\t<groupId>com.squareup.okhttp3</groupId>\n\t<artifactId>okhttp</artifactId>\n\t<version>4.8.1</version>\n</dependency>\n\t\n<dependency>\n\t<groupId>io.minio</groupId>\n\t<artifactId>minio</artifactId>\n\t<version>8.3.0</version>\n\t<exclusions>\n\t\t<exclusion>\n\t\t\t<groupId>com.squareup.okhttp3</groupId>\n\t\t\t<artifactId>okhttp</artifactId>\n        </exclusion>\n\t</exclusions>\n</dependency>\n```\n\nminio强依赖于okhttp3，低版本okhttp3会报错：https://blog.csdn.net/u014698745/article/details/122025869\n\n若剔除所有的okhttp3还是不能运行，则将minio版本降低。\n\n2、MinIO客户端\n\n```java\nimport io.minio.*;\nimport io.minio.http.Method;\nimport io.minio.messages.Bucket;\nimport io.minio.messages.Item;\nimport org.apache.commons.lang3.StringUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.stereotype.Component;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.net.URLEncoder;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\n\npublic class S3Utils {\n    private static final Logger logger = LoggerFactory.getLogger(S3Utils.class);\n\n    private static MinioClient s3Client;\n\n    static {\n        try {\n            s3Client = MinioClient.builder()\n                    .endpoint(SdkConstant.endpoint)\n                    .credentials(SdkConstant.s3AccessKey, SdkConstant.s3AccessSecret)\n                    .build();\n        } catch (Exception e) {\n            logger.error(\"初始化s3Client时出错!\", e);\n        }\n        if (null == s3Client) {\n            logger.error(\"创建s3Client时出错!\");\n        }\n    }\n\n    /**\n     * 创建一个桶\n     */\n    public static void createBucket(String bucket) throws Exception {\n        boolean found = s3Client.bucketExists(BucketExistsArgs.builder().bucket(bucket).build());\n        if (!found) {\n            s3Client.makeBucket(MakeBucketArgs.builder().bucket(bucket).build());\n        }\n    }\n\n    /**\n     * 上传一个文件\n     */\n    public static void uploadFile(InputStream stream, String bucket, String objectName) throws Exception {\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n    }\n\n    /**\n     * 上传一个文件,并返回文件url\n     */\n    public static String uploadFileReturnUrl(InputStream stream, String bucket, String objectName) throws Exception {\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n        String url = s3Client.getPresignedObjectUrl(\n                GetPresignedObjectUrlArgs.builder()\n                        .method(Method.GET)\n                        .bucket(bucket)\n                        .object(objectName)\n                        .expiry(7, TimeUnit.DAYS)\n                        .build());\n        return StringUtils.split(url, \"?\")[0];\n    }\n\n    /**\n     * 列出所有的桶\n     */\n    public static List<String> listBuckets() throws Exception {\n        List<Bucket> list = s3Client.listBuckets();\n        List<String> names = new ArrayList<>();\n        list.forEach(b -> {\n            names.add(b.name());\n        });\n        return names;\n    }\n\n    /**\n     * 列出一个桶中的所有文件和目录\n     */\n    public static List<S3FileInfo> listFiles(String bucket) throws Exception {\n        Iterable<Result<Item>> results = s3Client.listObjects(\n                ListObjectsArgs.builder().bucket(bucket).recursive(true).build());\n\n        List<S3FileInfo> infos = new ArrayList<>();\n        results.forEach(r -> {\n            S3FileInfo info = new S3FileInfo();\n            try {\n                Item item = r.get();\n                info.setFileName(item.objectName());\n                info.setDirectory(item.isDir());\n                infos.add(info);\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        });\n        return infos;\n    }\n\n    /**\n     * 下载一个文件\n     */\n    public static InputStream download(String bucket, String objectName) throws Exception {\n        InputStream stream = s3Client.getObject(\n                GetObjectArgs.builder().bucket(bucket).object(objectName).build());\n        return stream;\n    }\n\n    /**\n     * 删除一个桶\n     */\n    public static void deleteBucket(String bucket) throws Exception {\n        s3Client.removeBucket(RemoveBucketArgs.builder().bucket(bucket).build());\n    }\n\n    /**\n     * 删除一个对象\n     */\n    public static void deleteObject(String bucket, String objectName) throws Exception {\n        s3Client.removeObject(RemoveObjectArgs.builder().bucket(bucket).object(objectName).build());\n    }\n\n\n    public static void main(String[] args) throws Exception {\n        S3Utils.createBucket(SdkConstant.bucketName);\n        File file = new File(\"C:\\\\Users\\\\27049\\\\Desktop\\\\minio docker.txt\");\n        InputStream inputStream = new FileInputStream(file);\n        System.out.println(S3Utils.uploadFileReturnUrl(inputStream, SdkConstant.bucketName, \"minio docker.txt\"));\n    }\n}\n\n```\n\n其中：S3FileInfo中只有两个字段，fileName和directory\n\n创建连接的参数为：\n\nSdkConstant.s3AccessKey 用户名\n\nSdkConstant.s3AccessSecret 密码\n\nSdkConstant.endpoint MinIO所在地址（ip:9000）\n\n存储位置：指定的data目录/桶名/...文件夹名.../文件名\n\n\n\ngetPresignedObjectUrl()返回的url最大支持7天，若想永久使用，则配置\n\nhttps://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&utm_term=java+%E8%AE%BE%E7%BD%AE+minio%E6%B0%B8%E4%B9%85%E8%AE%BF%E9%97%AE%E9%93%BE%E6%8E%A5&spm=1000.2123.3001.4430\n\n![image-20220322221741703](/img/image-20220322221741703.png)\n\n\n\n3、AmazonS3客户端（DLC）\n\n```java\n<dependency>\n    <artifactId>aws-java-sdk-s3</artifactId>\n    <groupId>com.amazonaws</groupId>\n    <optional>false</optional>\n    <version>1.11.257</version>\n</dependency>\n```\n\n\n\n```java\nimport com.amazonaws.auth.AWSStaticCredentialsProvider;\nimport com.amazonaws.auth.BasicAWSCredentials;\nimport com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration;\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Calendar;\nimport java.util.Date;\nimport java.util.List;\n\n\npublic class AWSS3Util {\n\n    private static String accessKey = \"\";\n    private static String secretKey = \"\";\n    private static String endpointUrl = \"\";\n\n\n    private volatile static AWSS3Util awsS3Util;\n    private volatile static AmazonS3 s3;\n    private static String region = \"default\";\n\n    private AWSS3Util() {\n\n    }\n\n    /**\n     * 获取带签名认证文件的url\n     */\n    public URL getPresignedUrl(String bucket, String key) {\n        URL url = s3.generatePresignedUrl(bucket, key, getDateAfter(1));\n        return url;\n    }\n\n    /**\n     * 获取文件的url\n     */\n    public URL getUrl(String bucket, String key) {\n        URL url = s3.getUrl(bucket, key);\n        return url;\n    }\n\n\n    /**\n     * 获取指定bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List<String> getFileNames(String bucket) {\n        List<String> fileNames = new ArrayList<>();\n        ObjectListing objectListing = s3.listObjects(bucket);\n        List<S3ObjectSummary> objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) {\n            String name = s3ObjectSummary.getKey();\n            fileNames.add(name.substring(name.lastIndexOf(\"/\") + 1));\n        }\n        return fileNames;\n    }\n    /**\n     * 获取指定路径前缀的bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List<String> getFiles(String bucket,String prefix) {\n        List<String> files = new ArrayList<>();\n        ObjectListing objectListing = s3.listObjects(bucket,prefix);\n        List<S3ObjectSummary> objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) {\n            String name = s3ObjectSummary.getKey();\n            files.add(name.substring(name.lastIndexOf(\"/\") + 1));\n        }\n        return files;\n    }\n\n    public void deleteObject(String bucket, String key) {\n        s3.deleteObject(bucket, key);\n    }\n\n    public static AWSS3Util getInstance() {\n        if (awsS3Util == null) {\n            synchronized (AWSS3Util.class) {\n                if (awsS3Util == null) {\n                    awsS3Util = new AWSS3Util();\n                    s3 = AmazonS3ClientBuilder.standard()\n                            .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))\n                            .withEndpointConfiguration(new EndpointConfiguration(endpointUrl, region)).withPathStyleAccessEnabled(true)\n                            .build();\n                }\n            }\n        }\n        return awsS3Util;\n    }\n\n    /**\n     * 得到几天后的时间\n     *\n     * @param day\n     * @return\n     */\n    private Date getDateAfter(int day) {\n        Date date = new Date();\n        Calendar now = Calendar.getInstance();\n        now.setTime(date);\n        now.set(Calendar.DATE, now.get(Calendar.DATE) + day);\n        return now.getTime();\n    }\n}\n```\n\n","slug":"MinIO单机安装以及使用","published":1,"updated":"2022-04-08T13:57:40.601Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzb00387kt9e8mo6eex","content":"<p>一、简介</p>\n<p>​\t\tMinIO 是在 GNU Affero 通用公共许可证 v3.0 下发布的高性能对象存储。它与 Amazon S3 云存储服务 API 兼容。使用 MinIO 为机器学习、分析和应用程序数据工作负载构建高性能基础架构。</p>\n<p>相关文档：</p>\n<p>官方文档：<a href=\"https://docs.min.io/docs/minio-quickstart-guide.html\">https://docs.min.io/docs/minio-quickstart-guide.html</a></p>\n<p>官方文档（中文）：<a href=\"http://docs.minio.org.cn/docs/master/minio-monitoring-guide\">http://docs.minio.org.cn/docs/master/minio-monitoring-guide</a></p>\n<p>官方首页（中文）<a href=\"http://www.minio.org.cn/\">http://www.minio.org.cn/</a></p>\n<p>二、单机版安装</p>\n<p>1、搜索镜像</p>\n<pre><code class=\"language-java\">docker search minio\n</code></pre>\n<p>2、拉取镜像</p>\n<pre><code class=\"language-java\">docker pull minio/minio\n</code></pre>\n<p>3、启动与安装镜像</p>\n<pre><code class=\"language-java\">docker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -d --restart=always \\\n  --name minio1 \\\n  -v /home/environment/minio/data:/data \\\n  -v /home/environment/minio/config:/root/.minio \\\n  --privileged=true \\\n  minio/minio server /data --console-address &quot;:9001&quot;\n</code></pre>\n<p>-it 表示运行参数<br>\n-p 表示暴露端口<br>\n-d 表示后台运行<br>\n-v 卷挂载（容器到主机的映射，避免存容器中丢失数据）</p>\n<p>9001是管理页，默认账号密码均为minioadmin，也可以设置密码  9000</p>\n<pre><code class=\"language-java\">-e &quot;MINIO_ACCESS_KEY=账号&quot; \\\n-e &quot;MINIO_SECRET_KEY=密码&quot; \\\n</code></pre>\n<p>4、查看运行镜像</p>\n<pre><code class=\"language-java\">docker ps\n</code></pre>\n<p>三、JAVA API demo</p>\n<p>1、引入依赖</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n\t&lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;\n\t&lt;artifactId&gt;okhttp&lt;/artifactId&gt;\n\t&lt;version&gt;4.8.1&lt;/version&gt;\n&lt;/dependency&gt;\n\t\n&lt;dependency&gt;\n\t&lt;groupId&gt;io.minio&lt;/groupId&gt;\n\t&lt;artifactId&gt;minio&lt;/artifactId&gt;\n\t&lt;version&gt;8.3.0&lt;/version&gt;\n\t&lt;exclusions&gt;\n\t\t&lt;exclusion&gt;\n\t\t\t&lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;okhttp&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n\t&lt;/exclusions&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>minio强依赖于okhttp3，低版本okhttp3会报错：<a href=\"https://blog.csdn.net/u014698745/article/details/122025869\">https://blog.csdn.net/u014698745/article/details/122025869</a></p>\n<p>若剔除所有的okhttp3还是不能运行，则将minio版本降低。</p>\n<p>2、MinIO客户端</p>\n<pre><code class=\"language-java\">import io.minio.*;\nimport io.minio.http.Method;\nimport io.minio.messages.Bucket;\nimport io.minio.messages.Item;\nimport org.apache.commons.lang3.StringUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.stereotype.Component;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.net.URLEncoder;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\n\npublic class S3Utils &#123;\n    private static final Logger logger = LoggerFactory.getLogger(S3Utils.class);\n\n    private static MinioClient s3Client;\n\n    static &#123;\n        try &#123;\n            s3Client = MinioClient.builder()\n                    .endpoint(SdkConstant.endpoint)\n                    .credentials(SdkConstant.s3AccessKey, SdkConstant.s3AccessSecret)\n                    .build();\n        &#125; catch (Exception e) &#123;\n            logger.error(&quot;初始化s3Client时出错!&quot;, e);\n        &#125;\n        if (null == s3Client) &#123;\n            logger.error(&quot;创建s3Client时出错!&quot;);\n        &#125;\n    &#125;\n\n    /**\n     * 创建一个桶\n     */\n    public static void createBucket(String bucket) throws Exception &#123;\n        boolean found = s3Client.bucketExists(BucketExistsArgs.builder().bucket(bucket).build());\n        if (!found) &#123;\n            s3Client.makeBucket(MakeBucketArgs.builder().bucket(bucket).build());\n        &#125;\n    &#125;\n\n    /**\n     * 上传一个文件\n     */\n    public static void uploadFile(InputStream stream, String bucket, String objectName) throws Exception &#123;\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n    &#125;\n\n    /**\n     * 上传一个文件,并返回文件url\n     */\n    public static String uploadFileReturnUrl(InputStream stream, String bucket, String objectName) throws Exception &#123;\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n        String url = s3Client.getPresignedObjectUrl(\n                GetPresignedObjectUrlArgs.builder()\n                        .method(Method.GET)\n                        .bucket(bucket)\n                        .object(objectName)\n                        .expiry(7, TimeUnit.DAYS)\n                        .build());\n        return StringUtils.split(url, &quot;?&quot;)[0];\n    &#125;\n\n    /**\n     * 列出所有的桶\n     */\n    public static List&lt;String&gt; listBuckets() throws Exception &#123;\n        List&lt;Bucket&gt; list = s3Client.listBuckets();\n        List&lt;String&gt; names = new ArrayList&lt;&gt;();\n        list.forEach(b -&gt; &#123;\n            names.add(b.name());\n        &#125;);\n        return names;\n    &#125;\n\n    /**\n     * 列出一个桶中的所有文件和目录\n     */\n    public static List&lt;S3FileInfo&gt; listFiles(String bucket) throws Exception &#123;\n        Iterable&lt;Result&lt;Item&gt;&gt; results = s3Client.listObjects(\n                ListObjectsArgs.builder().bucket(bucket).recursive(true).build());\n\n        List&lt;S3FileInfo&gt; infos = new ArrayList&lt;&gt;();\n        results.forEach(r -&gt; &#123;\n            S3FileInfo info = new S3FileInfo();\n            try &#123;\n                Item item = r.get();\n                info.setFileName(item.objectName());\n                info.setDirectory(item.isDir());\n                infos.add(info);\n            &#125; catch (Exception e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;);\n        return infos;\n    &#125;\n\n    /**\n     * 下载一个文件\n     */\n    public static InputStream download(String bucket, String objectName) throws Exception &#123;\n        InputStream stream = s3Client.getObject(\n                GetObjectArgs.builder().bucket(bucket).object(objectName).build());\n        return stream;\n    &#125;\n\n    /**\n     * 删除一个桶\n     */\n    public static void deleteBucket(String bucket) throws Exception &#123;\n        s3Client.removeBucket(RemoveBucketArgs.builder().bucket(bucket).build());\n    &#125;\n\n    /**\n     * 删除一个对象\n     */\n    public static void deleteObject(String bucket, String objectName) throws Exception &#123;\n        s3Client.removeObject(RemoveObjectArgs.builder().bucket(bucket).object(objectName).build());\n    &#125;\n\n\n    public static void main(String[] args) throws Exception &#123;\n        S3Utils.createBucket(SdkConstant.bucketName);\n        File file = new File(&quot;C:\\\\Users\\\\27049\\\\Desktop\\\\minio docker.txt&quot;);\n        InputStream inputStream = new FileInputStream(file);\n        System.out.println(S3Utils.uploadFileReturnUrl(inputStream, SdkConstant.bucketName, &quot;minio docker.txt&quot;));\n    &#125;\n&#125;\n\n</code></pre>\n<p>其中：S3FileInfo中只有两个字段，fileName和directory</p>\n<p>创建连接的参数为：</p>\n<p>SdkConstant.s3AccessKey 用户名</p>\n<p>SdkConstant.s3AccessSecret 密码</p>\n<p>SdkConstant.endpoint MinIO所在地址（ip:9000）</p>\n<p>存储位置：指定的data目录/桶名/…文件夹名…/文件名</p>\n<p>getPresignedObjectUrl()返回的url最大支持7天，若想永久使用，则配置</p>\n<p><a href=\"https://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&amp;utm_term=java+%E8%AE%BE%E7%BD%AE+minio%E6%B0%B8%E4%B9%85%E8%AE%BF%E9%97%AE%E9%93%BE%E6%8E%A5&amp;spm=1000.2123.3001.4430\">https://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&amp;utm_term=java+设置+minio永久访问链接&amp;spm=1000.2123.3001.4430</a></p>\n<p><img src=\"/img/image-20220322221741703.png\" alt=\"image-20220322221741703\"></p>\n<p>3、AmazonS3客户端（DLC）</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt;\n    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n    &lt;optional&gt;false&lt;/optional&gt;\n    &lt;version&gt;1.11.257&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<pre><code class=\"language-java\">import com.amazonaws.auth.AWSStaticCredentialsProvider;\nimport com.amazonaws.auth.BasicAWSCredentials;\nimport com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration;\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Calendar;\nimport java.util.Date;\nimport java.util.List;\n\n\npublic class AWSS3Util &#123;\n\n    private static String accessKey = &quot;&quot;;\n    private static String secretKey = &quot;&quot;;\n    private static String endpointUrl = &quot;&quot;;\n\n\n    private volatile static AWSS3Util awsS3Util;\n    private volatile static AmazonS3 s3;\n    private static String region = &quot;default&quot;;\n\n    private AWSS3Util() &#123;\n\n    &#125;\n\n    /**\n     * 获取带签名认证文件的url\n     */\n    public URL getPresignedUrl(String bucket, String key) &#123;\n        URL url = s3.generatePresignedUrl(bucket, key, getDateAfter(1));\n        return url;\n    &#125;\n\n    /**\n     * 获取文件的url\n     */\n    public URL getUrl(String bucket, String key) &#123;\n        URL url = s3.getUrl(bucket, key);\n        return url;\n    &#125;\n\n\n    /**\n     * 获取指定bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List&lt;String&gt; getFileNames(String bucket) &#123;\n        List&lt;String&gt; fileNames = new ArrayList&lt;&gt;();\n        ObjectListing objectListing = s3.listObjects(bucket);\n        List&lt;S3ObjectSummary&gt; objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) &#123;\n            String name = s3ObjectSummary.getKey();\n            fileNames.add(name.substring(name.lastIndexOf(&quot;/&quot;) + 1));\n        &#125;\n        return fileNames;\n    &#125;\n    /**\n     * 获取指定路径前缀的bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List&lt;String&gt; getFiles(String bucket,String prefix) &#123;\n        List&lt;String&gt; files = new ArrayList&lt;&gt;();\n        ObjectListing objectListing = s3.listObjects(bucket,prefix);\n        List&lt;S3ObjectSummary&gt; objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) &#123;\n            String name = s3ObjectSummary.getKey();\n            files.add(name.substring(name.lastIndexOf(&quot;/&quot;) + 1));\n        &#125;\n        return files;\n    &#125;\n\n    public void deleteObject(String bucket, String key) &#123;\n        s3.deleteObject(bucket, key);\n    &#125;\n\n    public static AWSS3Util getInstance() &#123;\n        if (awsS3Util == null) &#123;\n            synchronized (AWSS3Util.class) &#123;\n                if (awsS3Util == null) &#123;\n                    awsS3Util = new AWSS3Util();\n                    s3 = AmazonS3ClientBuilder.standard()\n                            .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))\n                            .withEndpointConfiguration(new EndpointConfiguration(endpointUrl, region)).withPathStyleAccessEnabled(true)\n                            .build();\n                &#125;\n            &#125;\n        &#125;\n        return awsS3Util;\n    &#125;\n\n    /**\n     * 得到几天后的时间\n     *\n     * @param day\n     * @return\n     */\n    private Date getDateAfter(int day) &#123;\n        Date date = new Date();\n        Calendar now = Calendar.getInstance();\n        now.setTime(date);\n        now.set(Calendar.DATE, now.get(Calendar.DATE) + day);\n        return now.getTime();\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>一、简介</p>\n<p>​\t\tMinIO 是在 GNU Affero 通用公共许可证 v3.0 下发布的高性能对象存储。它与 Amazon S3 云存储服务 API 兼容。使用 MinIO 为机器学习、分析和应用程序数据工作负载构建高性能基础架构。</p>\n<p>相关文档：</p>\n<p>官方文档：<a href=\"https://docs.min.io/docs/minio-quickstart-guide.html\">https://docs.min.io/docs/minio-quickstart-guide.html</a></p>\n<p>官方文档（中文）：<a href=\"http://docs.minio.org.cn/docs/master/minio-monitoring-guide\">http://docs.minio.org.cn/docs/master/minio-monitoring-guide</a></p>\n<p>官方首页（中文）<a href=\"http://www.minio.org.cn/\">http://www.minio.org.cn/</a></p>\n<p>二、单机版安装</p>\n<p>1、搜索镜像</p>\n<pre><code class=\"language-java\">docker search minio\n</code></pre>\n<p>2、拉取镜像</p>\n<pre><code class=\"language-java\">docker pull minio/minio\n</code></pre>\n<p>3、启动与安装镜像</p>\n<pre><code class=\"language-java\">docker run \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -d --restart=always \\\n  --name minio1 \\\n  -v /home/environment/minio/data:/data \\\n  -v /home/environment/minio/config:/root/.minio \\\n  --privileged=true \\\n  minio/minio server /data --console-address &quot;:9001&quot;\n</code></pre>\n<p>-it 表示运行参数<br>\n-p 表示暴露端口<br>\n-d 表示后台运行<br>\n-v 卷挂载（容器到主机的映射，避免存容器中丢失数据）</p>\n<p>9001是管理页，默认账号密码均为minioadmin，也可以设置密码  9000</p>\n<pre><code class=\"language-java\">-e &quot;MINIO_ACCESS_KEY=账号&quot; \\\n-e &quot;MINIO_SECRET_KEY=密码&quot; \\\n</code></pre>\n<p>4、查看运行镜像</p>\n<pre><code class=\"language-java\">docker ps\n</code></pre>\n<p>三、JAVA API demo</p>\n<p>1、引入依赖</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n\t&lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;\n\t&lt;artifactId&gt;okhttp&lt;/artifactId&gt;\n\t&lt;version&gt;4.8.1&lt;/version&gt;\n&lt;/dependency&gt;\n\t\n&lt;dependency&gt;\n\t&lt;groupId&gt;io.minio&lt;/groupId&gt;\n\t&lt;artifactId&gt;minio&lt;/artifactId&gt;\n\t&lt;version&gt;8.3.0&lt;/version&gt;\n\t&lt;exclusions&gt;\n\t\t&lt;exclusion&gt;\n\t\t\t&lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;okhttp&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n\t&lt;/exclusions&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>minio强依赖于okhttp3，低版本okhttp3会报错：<a href=\"https://blog.csdn.net/u014698745/article/details/122025869\">https://blog.csdn.net/u014698745/article/details/122025869</a></p>\n<p>若剔除所有的okhttp3还是不能运行，则将minio版本降低。</p>\n<p>2、MinIO客户端</p>\n<pre><code class=\"language-java\">import io.minio.*;\nimport io.minio.http.Method;\nimport io.minio.messages.Bucket;\nimport io.minio.messages.Item;\nimport org.apache.commons.lang3.StringUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.stereotype.Component;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.net.URLEncoder;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\n\npublic class S3Utils &#123;\n    private static final Logger logger = LoggerFactory.getLogger(S3Utils.class);\n\n    private static MinioClient s3Client;\n\n    static &#123;\n        try &#123;\n            s3Client = MinioClient.builder()\n                    .endpoint(SdkConstant.endpoint)\n                    .credentials(SdkConstant.s3AccessKey, SdkConstant.s3AccessSecret)\n                    .build();\n        &#125; catch (Exception e) &#123;\n            logger.error(&quot;初始化s3Client时出错!&quot;, e);\n        &#125;\n        if (null == s3Client) &#123;\n            logger.error(&quot;创建s3Client时出错!&quot;);\n        &#125;\n    &#125;\n\n    /**\n     * 创建一个桶\n     */\n    public static void createBucket(String bucket) throws Exception &#123;\n        boolean found = s3Client.bucketExists(BucketExistsArgs.builder().bucket(bucket).build());\n        if (!found) &#123;\n            s3Client.makeBucket(MakeBucketArgs.builder().bucket(bucket).build());\n        &#125;\n    &#125;\n\n    /**\n     * 上传一个文件\n     */\n    public static void uploadFile(InputStream stream, String bucket, String objectName) throws Exception &#123;\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n    &#125;\n\n    /**\n     * 上传一个文件,并返回文件url\n     */\n    public static String uploadFileReturnUrl(InputStream stream, String bucket, String objectName) throws Exception &#123;\n        s3Client.putObject(PutObjectArgs.builder().bucket(bucket).object(objectName)\n                .stream(stream, -1, 10485760).build());\n        String url = s3Client.getPresignedObjectUrl(\n                GetPresignedObjectUrlArgs.builder()\n                        .method(Method.GET)\n                        .bucket(bucket)\n                        .object(objectName)\n                        .expiry(7, TimeUnit.DAYS)\n                        .build());\n        return StringUtils.split(url, &quot;?&quot;)[0];\n    &#125;\n\n    /**\n     * 列出所有的桶\n     */\n    public static List&lt;String&gt; listBuckets() throws Exception &#123;\n        List&lt;Bucket&gt; list = s3Client.listBuckets();\n        List&lt;String&gt; names = new ArrayList&lt;&gt;();\n        list.forEach(b -&gt; &#123;\n            names.add(b.name());\n        &#125;);\n        return names;\n    &#125;\n\n    /**\n     * 列出一个桶中的所有文件和目录\n     */\n    public static List&lt;S3FileInfo&gt; listFiles(String bucket) throws Exception &#123;\n        Iterable&lt;Result&lt;Item&gt;&gt; results = s3Client.listObjects(\n                ListObjectsArgs.builder().bucket(bucket).recursive(true).build());\n\n        List&lt;S3FileInfo&gt; infos = new ArrayList&lt;&gt;();\n        results.forEach(r -&gt; &#123;\n            S3FileInfo info = new S3FileInfo();\n            try &#123;\n                Item item = r.get();\n                info.setFileName(item.objectName());\n                info.setDirectory(item.isDir());\n                infos.add(info);\n            &#125; catch (Exception e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;);\n        return infos;\n    &#125;\n\n    /**\n     * 下载一个文件\n     */\n    public static InputStream download(String bucket, String objectName) throws Exception &#123;\n        InputStream stream = s3Client.getObject(\n                GetObjectArgs.builder().bucket(bucket).object(objectName).build());\n        return stream;\n    &#125;\n\n    /**\n     * 删除一个桶\n     */\n    public static void deleteBucket(String bucket) throws Exception &#123;\n        s3Client.removeBucket(RemoveBucketArgs.builder().bucket(bucket).build());\n    &#125;\n\n    /**\n     * 删除一个对象\n     */\n    public static void deleteObject(String bucket, String objectName) throws Exception &#123;\n        s3Client.removeObject(RemoveObjectArgs.builder().bucket(bucket).object(objectName).build());\n    &#125;\n\n\n    public static void main(String[] args) throws Exception &#123;\n        S3Utils.createBucket(SdkConstant.bucketName);\n        File file = new File(&quot;C:\\\\Users\\\\27049\\\\Desktop\\\\minio docker.txt&quot;);\n        InputStream inputStream = new FileInputStream(file);\n        System.out.println(S3Utils.uploadFileReturnUrl(inputStream, SdkConstant.bucketName, &quot;minio docker.txt&quot;));\n    &#125;\n&#125;\n\n</code></pre>\n<p>其中：S3FileInfo中只有两个字段，fileName和directory</p>\n<p>创建连接的参数为：</p>\n<p>SdkConstant.s3AccessKey 用户名</p>\n<p>SdkConstant.s3AccessSecret 密码</p>\n<p>SdkConstant.endpoint MinIO所在地址（ip:9000）</p>\n<p>存储位置：指定的data目录/桶名/…文件夹名…/文件名</p>\n<p>getPresignedObjectUrl()返回的url最大支持7天，若想永久使用，则配置</p>\n<p><a href=\"https://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&amp;utm_term=java+%E8%AE%BE%E7%BD%AE+minio%E6%B0%B8%E4%B9%85%E8%AE%BF%E9%97%AE%E9%93%BE%E6%8E%A5&amp;spm=1000.2123.3001.4430\">https://blog.csdn.net/instanceof_zjl/article/details/109601131?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-109601131.pc_agg_new_rank&amp;utm_term=java+设置+minio永久访问链接&amp;spm=1000.2123.3001.4430</a></p>\n<p><img src=\"/img/image-20220322221741703.png\" alt=\"image-20220322221741703\"></p>\n<p>3、AmazonS3客户端（DLC）</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt;\n    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n    &lt;optional&gt;false&lt;/optional&gt;\n    &lt;version&gt;1.11.257&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<pre><code class=\"language-java\">import com.amazonaws.auth.AWSStaticCredentialsProvider;\nimport com.amazonaws.auth.BasicAWSCredentials;\nimport com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration;\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Calendar;\nimport java.util.Date;\nimport java.util.List;\n\n\npublic class AWSS3Util &#123;\n\n    private static String accessKey = &quot;&quot;;\n    private static String secretKey = &quot;&quot;;\n    private static String endpointUrl = &quot;&quot;;\n\n\n    private volatile static AWSS3Util awsS3Util;\n    private volatile static AmazonS3 s3;\n    private static String region = &quot;default&quot;;\n\n    private AWSS3Util() &#123;\n\n    &#125;\n\n    /**\n     * 获取带签名认证文件的url\n     */\n    public URL getPresignedUrl(String bucket, String key) &#123;\n        URL url = s3.generatePresignedUrl(bucket, key, getDateAfter(1));\n        return url;\n    &#125;\n\n    /**\n     * 获取文件的url\n     */\n    public URL getUrl(String bucket, String key) &#123;\n        URL url = s3.getUrl(bucket, key);\n        return url;\n    &#125;\n\n\n    /**\n     * 获取指定bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List&lt;String&gt; getFileNames(String bucket) &#123;\n        List&lt;String&gt; fileNames = new ArrayList&lt;&gt;();\n        ObjectListing objectListing = s3.listObjects(bucket);\n        List&lt;S3ObjectSummary&gt; objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) &#123;\n            String name = s3ObjectSummary.getKey();\n            fileNames.add(name.substring(name.lastIndexOf(&quot;/&quot;) + 1));\n        &#125;\n        return fileNames;\n    &#125;\n    /**\n     * 获取指定路径前缀的bucket内所有文件名\n     *\n     * @param bucket\n     */\n    public List&lt;String&gt; getFiles(String bucket,String prefix) &#123;\n        List&lt;String&gt; files = new ArrayList&lt;&gt;();\n        ObjectListing objectListing = s3.listObjects(bucket,prefix);\n        List&lt;S3ObjectSummary&gt; objectSummaries = objectListing.getObjectSummaries();\n\n        for (S3ObjectSummary s3ObjectSummary : objectSummaries) &#123;\n            String name = s3ObjectSummary.getKey();\n            files.add(name.substring(name.lastIndexOf(&quot;/&quot;) + 1));\n        &#125;\n        return files;\n    &#125;\n\n    public void deleteObject(String bucket, String key) &#123;\n        s3.deleteObject(bucket, key);\n    &#125;\n\n    public static AWSS3Util getInstance() &#123;\n        if (awsS3Util == null) &#123;\n            synchronized (AWSS3Util.class) &#123;\n                if (awsS3Util == null) &#123;\n                    awsS3Util = new AWSS3Util();\n                    s3 = AmazonS3ClientBuilder.standard()\n                            .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))\n                            .withEndpointConfiguration(new EndpointConfiguration(endpointUrl, region)).withPathStyleAccessEnabled(true)\n                            .build();\n                &#125;\n            &#125;\n        &#125;\n        return awsS3Util;\n    &#125;\n\n    /**\n     * 得到几天后的时间\n     *\n     * @param day\n     * @return\n     */\n    private Date getDateAfter(int day) &#123;\n        Date date = new Date();\n        Calendar now = Calendar.getInstance();\n        now.setTime(date);\n        now.set(Calendar.DATE, now.get(Calendar.DATE) + day);\n        return now.getTime();\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"Nacos配置中心使用","author":"郑天祺","date":"2019-11-25T08:38:00.000Z","_content":"\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","source":"_posts/Nacos配置中心使用.md","raw":"title: Nacos配置中心使用\nauthor: 郑天祺\ntags:\n  - SpringCloud\n  - nacos-config\ncategories:\n  - spring\n  - ''\ndate: 2019-11-25 16:38:00\n---\n\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","slug":"Nacos配置中心使用","published":1,"updated":"2022-04-04T08:32:40.151Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzc003d7kt9cwys7jdg","content":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants &#123;\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n&#125;\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils &#123;\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static &#123;\n        try &#123;\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        &#125;\n    &#125;\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) &#123;\n        Properties properties = null;\n        try &#123;\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (Exception e) &#123;\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId, e);\n        &#125;\n        if (null == properties) &#123;\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId);\n        &#125;\n        return properties;\n    &#125;\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) &#123;\n        Properties properties = null;\n        try &#123;\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() &#123;\n                @Override\n                public void receiveConfigInfo(String configInfo) &#123;\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                &#125;\n\n                @Override\n                public Executor getExecutor() &#123;\n                    return null;\n                &#125;\n            &#125;);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        &#125; catch (IOException e) &#123;\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        &#125;\n        return properties;\n    &#125;\n\n&#125;\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    &#125;\n\n&#125;\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-greenwich.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants &#123;\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n&#125;\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils &#123;\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static &#123;\n        try &#123;\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        &#125;\n    &#125;\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) &#123;\n        Properties properties = null;\n        try &#123;\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (Exception e) &#123;\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId, e);\n        &#125;\n        if (null == properties) &#123;\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId);\n        &#125;\n        return properties;\n    &#125;\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) &#123;\n        Properties properties = null;\n        try &#123;\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() &#123;\n                @Override\n                public void receiveConfigInfo(String configInfo) &#123;\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                &#125;\n\n                @Override\n                public Executor getExecutor() &#123;\n                    return null;\n                &#125;\n            &#125;);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        &#125; catch (IOException e) &#123;\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        &#125;\n        return properties;\n    &#125;\n\n&#125;\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    &#125;\n\n&#125;\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-greenwich.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n"},{"title":"Redis数据结构与对象（二）-链表","author":"ztq","date":"2022-01-23T08:18:00.000Z","_content":"\n链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。\n\n作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以 Redis 构建了自己的链表实现。\n\n链表在 Redis 中的应用非常广泛，比如列表键的底层实现之一就是链表：当一个列表键包含了数量比较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。\n\n举个例子，以下展示的 `integers` 列表键包含了从 `1` 到 `1024` 共一千零二十四个整数：\n\n```java\nredis> LLEN integers\n(integer) 1024\n \nredis> LRANGE integers 0 10\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n5) \"5\"\n6) \"6\"\n7) \"7\"\n8) \"8\"\n9) \"9\"\n10) \"10\"\n11) \"11\"\n```\n\n`integers` 列表键的底层实现就是一个链表，链表中的每个节点都保存了一个整数值。\n\n除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer），本书后续的章节将陆续对这些链表应用进行介绍。\n\n本文接下来的内容将对 Redis 的链表实现进行介绍，并列出相应的链表和链表节点 API 。\n\n## 链表和链表节点的实现\n\n每个链表节点使用一个 `adlist.h/listNode` 结构来表示：\n\n```java\ntypedef struct listNode {\n    // 前置节点\n    struct listNode *prev;\n    // 后置节点\n    struct listNode *next;\n    // 节点的值\n    void *value;\n} listNode;\n```\n\n多个 `listNode` 可以通过 `prev` 和 `next` 指针组成双端链表，如图 3-1 所示。\n\n![img](/img/df56c6c619c3e1efb6f6140d56f67bcd.png)\n\n虽然仅仅使用多个 `listNode` 结构就可以组成链表，但使用 `adlist.h/list` 来持有链表的话，操作起来会更方便：\n\n```java\ntypedef struct list {\n    // 表头节点\n    listNode *head;\n    // 表尾节点\n    listNode *tail;\n    // 链表所包含的节点数量\n    unsigned long len;\n    // 节点值复制函数\n    void *(*dup)(void *ptr);\n    // 节点值释放函数\n    void (*free)(void *ptr);\n    // 节点值对比函数\n    int (*match)(void *ptr, void *key);\n} list;\n```\n\n`list` 结构为链表提供了表头指针 `head` 、表尾指针 `tail` ，以及链表长度计数器 `len` ，而 `dup` 、 `free` 和 `match` 成员则是用于实现多态链表所需的类型特定函数：\n\n- `dup` 函数用于复制链表节点所保存的值；\n- `free` 函数用于释放链表节点所保存的值；\n- `match` 函数则用于对比链表节点所保存的值和另一个输入值是否相等。\n\n图 3-2 是由一个 `list` 结构和三个 `listNode` 结构组成的链表：\n\n![img](/img/591a3d39db7558c3fd1db79821abec5e.png)\n\nRedis 的链表实现的特性可以总结如下：\n\n- 双端：链表节点带有 `prev` 和 `next` 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。\n- 无环：表头节点的 `prev` 指针和表尾节点的 `next` 指针都指向 `NULL` ，对链表的访问以 `NULL` 为终点。\n- 带表头指针和表尾指针：通过 `list` 结构的 `head` 指针和 `tail` 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。\n- 带链表长度计数器：程序使用 `list` 结构的 `len` 属性来对 `list` 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。\n- 多态：链表节点使用 `void*` 指针来保存节点值，并且可以通过 `list` 结构的 `dup` 、 `free` 、 `match` 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。\n\n## 链表和链表节点的 API\n\n表 3-1 列出了所有用于操作链表和链表节点的 API 。\n\n------\n\n表 3-1 链表和链表节点 API\n\n![image-20220123235957505](/img/image-20220123235957505.png)\n\n## 回顾\n\n- 链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。\n- 每个链表节点由一个 `listNode` 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。\n- 每个链表使用一个 `list` 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。\n- 因为链表表头节点的前置节点和表尾节点的后置节点都指向 `NULL` ，所以 Redis 的链表实现是无环链表。\n- 通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。\n","source":"_posts/Redis数据结构与对象（一）-链表.md","raw":"title: Redis数据结构与对象（二）-链表\nauthor: ztq\ntags:\n  - redis\ncategories:\n  - 数据库\ndate: 2022-01-23 16:18:00\n\n---\n\n链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。\n\n作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以 Redis 构建了自己的链表实现。\n\n链表在 Redis 中的应用非常广泛，比如列表键的底层实现之一就是链表：当一个列表键包含了数量比较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。\n\n举个例子，以下展示的 `integers` 列表键包含了从 `1` 到 `1024` 共一千零二十四个整数：\n\n```java\nredis> LLEN integers\n(integer) 1024\n \nredis> LRANGE integers 0 10\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n5) \"5\"\n6) \"6\"\n7) \"7\"\n8) \"8\"\n9) \"9\"\n10) \"10\"\n11) \"11\"\n```\n\n`integers` 列表键的底层实现就是一个链表，链表中的每个节点都保存了一个整数值。\n\n除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer），本书后续的章节将陆续对这些链表应用进行介绍。\n\n本文接下来的内容将对 Redis 的链表实现进行介绍，并列出相应的链表和链表节点 API 。\n\n## 链表和链表节点的实现\n\n每个链表节点使用一个 `adlist.h/listNode` 结构来表示：\n\n```java\ntypedef struct listNode {\n    // 前置节点\n    struct listNode *prev;\n    // 后置节点\n    struct listNode *next;\n    // 节点的值\n    void *value;\n} listNode;\n```\n\n多个 `listNode` 可以通过 `prev` 和 `next` 指针组成双端链表，如图 3-1 所示。\n\n![img](/img/df56c6c619c3e1efb6f6140d56f67bcd.png)\n\n虽然仅仅使用多个 `listNode` 结构就可以组成链表，但使用 `adlist.h/list` 来持有链表的话，操作起来会更方便：\n\n```java\ntypedef struct list {\n    // 表头节点\n    listNode *head;\n    // 表尾节点\n    listNode *tail;\n    // 链表所包含的节点数量\n    unsigned long len;\n    // 节点值复制函数\n    void *(*dup)(void *ptr);\n    // 节点值释放函数\n    void (*free)(void *ptr);\n    // 节点值对比函数\n    int (*match)(void *ptr, void *key);\n} list;\n```\n\n`list` 结构为链表提供了表头指针 `head` 、表尾指针 `tail` ，以及链表长度计数器 `len` ，而 `dup` 、 `free` 和 `match` 成员则是用于实现多态链表所需的类型特定函数：\n\n- `dup` 函数用于复制链表节点所保存的值；\n- `free` 函数用于释放链表节点所保存的值；\n- `match` 函数则用于对比链表节点所保存的值和另一个输入值是否相等。\n\n图 3-2 是由一个 `list` 结构和三个 `listNode` 结构组成的链表：\n\n![img](/img/591a3d39db7558c3fd1db79821abec5e.png)\n\nRedis 的链表实现的特性可以总结如下：\n\n- 双端：链表节点带有 `prev` 和 `next` 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。\n- 无环：表头节点的 `prev` 指针和表尾节点的 `next` 指针都指向 `NULL` ，对链表的访问以 `NULL` 为终点。\n- 带表头指针和表尾指针：通过 `list` 结构的 `head` 指针和 `tail` 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。\n- 带链表长度计数器：程序使用 `list` 结构的 `len` 属性来对 `list` 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。\n- 多态：链表节点使用 `void*` 指针来保存节点值，并且可以通过 `list` 结构的 `dup` 、 `free` 、 `match` 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。\n\n## 链表和链表节点的 API\n\n表 3-1 列出了所有用于操作链表和链表节点的 API 。\n\n------\n\n表 3-1 链表和链表节点 API\n\n![image-20220123235957505](/img/image-20220123235957505.png)\n\n## 回顾\n\n- 链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。\n- 每个链表节点由一个 `listNode` 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。\n- 每个链表使用一个 `list` 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。\n- 因为链表表头节点的前置节点和表尾节点的后置节点都指向 `NULL` ，所以 Redis 的链表实现是无环链表。\n- 通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。\n","slug":"Redis数据结构与对象（一）-链表","published":1,"updated":"2022-04-04T08:32:40.151Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzd003g7kt90ieue2gh","content":"<p>链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。</p>\n<p>作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以 Redis 构建了自己的链表实现。</p>\n<p>链表在 Redis 中的应用非常广泛，比如列表键的底层实现之一就是链表：当一个列表键包含了数量比较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。</p>\n<p>举个例子，以下展示的 <code>integers</code> 列表键包含了从 <code>1</code> 到 <code>1024</code> 共一千零二十四个整数：</p>\n<pre><code class=\"language-java\">redis&gt; LLEN integers\n(integer) 1024\n \nredis&gt; LRANGE integers 0 10\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n5) &quot;5&quot;\n6) &quot;6&quot;\n7) &quot;7&quot;\n8) &quot;8&quot;\n9) &quot;9&quot;\n10) &quot;10&quot;\n11) &quot;11&quot;\n</code></pre>\n<p><code>integers</code> 列表键的底层实现就是一个链表，链表中的每个节点都保存了一个整数值。</p>\n<p>除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer），本书后续的章节将陆续对这些链表应用进行介绍。</p>\n<p>本文接下来的内容将对 Redis 的链表实现进行介绍，并列出相应的链表和链表节点 API 。</p>\n<h2 id=\"链表和链表节点的实现\">链表和链表节点的实现</h2>\n<p>每个链表节点使用一个 <code>adlist.h/listNode</code> 结构来表示：</p>\n<pre><code class=\"language-java\">typedef struct listNode &#123;\n    // 前置节点\n    struct listNode *prev;\n    // 后置节点\n    struct listNode *next;\n    // 节点的值\n    void *value;\n&#125; listNode;\n</code></pre>\n<p>多个 <code>listNode</code> 可以通过 <code>prev</code> 和 <code>next</code> 指针组成双端链表，如图 3-1 所示。</p>\n<p><img src=\"/img/df56c6c619c3e1efb6f6140d56f67bcd.png\" alt=\"img\"></p>\n<p>虽然仅仅使用多个 <code>listNode</code> 结构就可以组成链表，但使用 <code>adlist.h/list</code> 来持有链表的话，操作起来会更方便：</p>\n<pre><code class=\"language-java\">typedef struct list &#123;\n    // 表头节点\n    listNode *head;\n    // 表尾节点\n    listNode *tail;\n    // 链表所包含的节点数量\n    unsigned long len;\n    // 节点值复制函数\n    void *(*dup)(void *ptr);\n    // 节点值释放函数\n    void (*free)(void *ptr);\n    // 节点值对比函数\n    int (*match)(void *ptr, void *key);\n&#125; list;\n</code></pre>\n<p><code>list</code> 结构为链表提供了表头指针 <code>head</code> 、表尾指针 <code>tail</code> ，以及链表长度计数器 <code>len</code> ，而 <code>dup</code> 、 <code>free</code> 和 <code>match</code> 成员则是用于实现多态链表所需的类型特定函数：</p>\n<ul>\n<li><code>dup</code> 函数用于复制链表节点所保存的值；</li>\n<li><code>free</code> 函数用于释放链表节点所保存的值；</li>\n<li><code>match</code> 函数则用于对比链表节点所保存的值和另一个输入值是否相等。</li>\n</ul>\n<p>图 3-2 是由一个 <code>list</code> 结构和三个 <code>listNode</code> 结构组成的链表：</p>\n<p><img src=\"/img/591a3d39db7558c3fd1db79821abec5e.png\" alt=\"img\"></p>\n<p>Redis 的链表实现的特性可以总结如下：</p>\n<ul>\n<li>双端：链表节点带有 <code>prev</code> 和 <code>next</code> 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。</li>\n<li>无环：表头节点的 <code>prev</code> 指针和表尾节点的 <code>next</code> 指针都指向 <code>NULL</code> ，对链表的访问以 <code>NULL</code> 为终点。</li>\n<li>带表头指针和表尾指针：通过 <code>list</code> 结构的 <code>head</code> 指针和 <code>tail</code> 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。</li>\n<li>带链表长度计数器：程序使用 <code>list</code> 结构的 <code>len</code> 属性来对 <code>list</code> 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。</li>\n<li>多态：链表节点使用 <code>void*</code> 指针来保存节点值，并且可以通过 <code>list</code> 结构的 <code>dup</code> 、 <code>free</code> 、 <code>match</code> 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。</li>\n</ul>\n<h2 id=\"链表和链表节点的-API\">链表和链表节点的 API</h2>\n<p>表 3-1 列出了所有用于操作链表和链表节点的 API 。</p>\n<hr>\n<p>表 3-1 链表和链表节点 API</p>\n<p><img src=\"/img/image-20220123235957505.png\" alt=\"image-20220123235957505\"></p>\n<h2 id=\"回顾\">回顾</h2>\n<ul>\n<li>链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。</li>\n<li>每个链表节点由一个 <code>listNode</code> 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。</li>\n<li>每个链表使用一个 <code>list</code> 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。</li>\n<li>因为链表表头节点的前置节点和表尾节点的后置节点都指向 <code>NULL</code> ，所以 Redis 的链表实现是无环链表。</li>\n<li>通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。</p>\n<p>作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以 Redis 构建了自己的链表实现。</p>\n<p>链表在 Redis 中的应用非常广泛，比如列表键的底层实现之一就是链表：当一个列表键包含了数量比较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。</p>\n<p>举个例子，以下展示的 <code>integers</code> 列表键包含了从 <code>1</code> 到 <code>1024</code> 共一千零二十四个整数：</p>\n<pre><code class=\"language-java\">redis&gt; LLEN integers\n(integer) 1024\n \nredis&gt; LRANGE integers 0 10\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n5) &quot;5&quot;\n6) &quot;6&quot;\n7) &quot;7&quot;\n8) &quot;8&quot;\n9) &quot;9&quot;\n10) &quot;10&quot;\n11) &quot;11&quot;\n</code></pre>\n<p><code>integers</code> 列表键的底层实现就是一个链表，链表中的每个节点都保存了一个整数值。</p>\n<p>除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer），本书后续的章节将陆续对这些链表应用进行介绍。</p>\n<p>本文接下来的内容将对 Redis 的链表实现进行介绍，并列出相应的链表和链表节点 API 。</p>\n<h2 id=\"链表和链表节点的实现\">链表和链表节点的实现</h2>\n<p>每个链表节点使用一个 <code>adlist.h/listNode</code> 结构来表示：</p>\n<pre><code class=\"language-java\">typedef struct listNode &#123;\n    // 前置节点\n    struct listNode *prev;\n    // 后置节点\n    struct listNode *next;\n    // 节点的值\n    void *value;\n&#125; listNode;\n</code></pre>\n<p>多个 <code>listNode</code> 可以通过 <code>prev</code> 和 <code>next</code> 指针组成双端链表，如图 3-1 所示。</p>\n<p><img src=\"/img/df56c6c619c3e1efb6f6140d56f67bcd.png\" alt=\"img\"></p>\n<p>虽然仅仅使用多个 <code>listNode</code> 结构就可以组成链表，但使用 <code>adlist.h/list</code> 来持有链表的话，操作起来会更方便：</p>\n<pre><code class=\"language-java\">typedef struct list &#123;\n    // 表头节点\n    listNode *head;\n    // 表尾节点\n    listNode *tail;\n    // 链表所包含的节点数量\n    unsigned long len;\n    // 节点值复制函数\n    void *(*dup)(void *ptr);\n    // 节点值释放函数\n    void (*free)(void *ptr);\n    // 节点值对比函数\n    int (*match)(void *ptr, void *key);\n&#125; list;\n</code></pre>\n<p><code>list</code> 结构为链表提供了表头指针 <code>head</code> 、表尾指针 <code>tail</code> ，以及链表长度计数器 <code>len</code> ，而 <code>dup</code> 、 <code>free</code> 和 <code>match</code> 成员则是用于实现多态链表所需的类型特定函数：</p>\n<ul>\n<li><code>dup</code> 函数用于复制链表节点所保存的值；</li>\n<li><code>free</code> 函数用于释放链表节点所保存的值；</li>\n<li><code>match</code> 函数则用于对比链表节点所保存的值和另一个输入值是否相等。</li>\n</ul>\n<p>图 3-2 是由一个 <code>list</code> 结构和三个 <code>listNode</code> 结构组成的链表：</p>\n<p><img src=\"/img/591a3d39db7558c3fd1db79821abec5e.png\" alt=\"img\"></p>\n<p>Redis 的链表实现的特性可以总结如下：</p>\n<ul>\n<li>双端：链表节点带有 <code>prev</code> 和 <code>next</code> 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。</li>\n<li>无环：表头节点的 <code>prev</code> 指针和表尾节点的 <code>next</code> 指针都指向 <code>NULL</code> ，对链表的访问以 <code>NULL</code> 为终点。</li>\n<li>带表头指针和表尾指针：通过 <code>list</code> 结构的 <code>head</code> 指针和 <code>tail</code> 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。</li>\n<li>带链表长度计数器：程序使用 <code>list</code> 结构的 <code>len</code> 属性来对 <code>list</code> 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。</li>\n<li>多态：链表节点使用 <code>void*</code> 指针来保存节点值，并且可以通过 <code>list</code> 结构的 <code>dup</code> 、 <code>free</code> 、 <code>match</code> 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。</li>\n</ul>\n<h2 id=\"链表和链表节点的-API\">链表和链表节点的 API</h2>\n<p>表 3-1 列出了所有用于操作链表和链表节点的 API 。</p>\n<hr>\n<p>表 3-1 链表和链表节点 API</p>\n<p><img src=\"/img/image-20220123235957505.png\" alt=\"image-20220123235957505\"></p>\n<h2 id=\"回顾\">回顾</h2>\n<ul>\n<li>链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。</li>\n<li>每个链表节点由一个 <code>listNode</code> 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。</li>\n<li>每个链表使用一个 <code>list</code> 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。</li>\n<li>因为链表表头节点的前置节点和表尾节点的后置节点都指向 <code>NULL</code> ，所以 Redis 的链表实现是无环链表。</li>\n<li>通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。</li>\n</ul>\n"},{"title":"Redis数据结构与对象（三）-字典","author":"ztq","date":"2022-01-25T07:58:00.000Z","_content":"\n## 字典的实现\n\nRedis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。\n\n接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现。\n\n## 哈希表\n\nRedis 字典所使用的哈希表由 `dict.h/dictht` 结构定义：\n\n```java\ntypedef struct dictht {\n    // 哈希表数组\n    dictEntry **table;\n    // 哈希表大小\n    unsigned long size;\n    // 哈希表大小掩码，用于计算索引值\n    // 总是等于 size - 1\n    unsigned long sizemask;\n    // 该哈希表已有节点的数量\n    unsigned long used;\n} dictht;\n```\n\n`table` 属性是一个数组，数组中的每个元素都是一个指向 `dict.h/dictEntry` 结构的指针，每个 `dictEntry` 结构保存着一个键值对。\n\n`size` 属性记录了哈希表的大小，也即是 `table` 数组的大小，而 `used` 属性则记录了哈希表目前已有节点（键值对）的数量。\n\n`sizemask` 属性的值总是等于 `size - 1` ，这个属性和哈希值一起决定一个键应该被放到 `table` 数组的哪个索引上面。\n\n图 4-1 展示了一个大小为 `4` 的空哈希表（没有包含任何键值对）。\n\n![字典的实现 - 图1](/img/d234cadc40e81da676a8b541d9cb6ab4.png)\n\n## 哈希表节点\n\n哈希表节点使用 `dictEntry` 结构表示，每个 `dictEntry` 结构都保存着一个键值对：\n\n```java\ntypedef struct dictEntry {\n    // 键\n    void *key;\n    // 值\n    union {\n        void *val;\n        uint64_t u64;\n        int64_t s64;\n    } v;\n    // 指向下个哈希表节点，形成链表\n    struct dictEntry *next;\n} dictEntry;\n```\n\n`key` 属性保存着键值对中的键，而 `v` 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 `uint64_t` 整数，又或者是一个 `int64_t` 整数。\n\n`next` 属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题。\n\n举个例子，图 4-2 就展示了如何通过 `next` 指针，将两个索引值相同的键 `k1` 和 `k0` 连接在一起。\n\n![字典的实现 - 图2](/img/d9710d39e895a6eb8210645c6e0b85e3.png)\n\n## 字典\n\nRedis 中的字典由 `dict.h/dict` 结构表示：\n\n```java\ntypedef struct dict {\n    // 类型特定函数\n    dictType *type;\n    // 私有数据\n    void *privdata;\n    // 哈希表\n    dictht ht[2];\n    // rehash 索引\n    // 当 rehash 不在进行时，值为 -1\n    int rehashidx; /* rehashing not in progress if rehashidx == -1 */\n} dict;\n```\n\n`type` 属性和 `privdata` 属性是针对不同类型的键值对，为创建多态字典而设置的：\n\n- `type` 属性是一个指向 `dictType` 结构的指针，每个 `dictType` 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。\n- 而 `privdata` 属性则保存了需要传给那些类型特定函数的可选参数。\n\n```java\ntypedef struct dictType {\n    // 计算哈希值的函数\n    unsigned int (*hashFunction)(const void *key);\n    // 复制键的函数\n    void *(*keyDup)(void *privdata, const void *key);\n    // 复制值的函数\n    void *(*valDup)(void *privdata, const void *obj);\n    // 对比键的函数\n    int (*keyCompare)(void *privdata, const void *key1, const void *key2);\n    // 销毁键的函数\n    void (*keyDestructor)(void *privdata, void *key);\n    // 销毁值的函数\n    void (*valDestructor)(void *privdata, void *obj);\n} dictType;\n```\n\n`ht` 属性是一个包含两个项的数组，数组中的每个项都是一个 `dictht` 哈希表，一般情况下，字典只使用 `ht[0]` 哈希表，`ht[1]` 哈希表只会在对 `ht[0]` 哈希表进行 rehash 时使用。\n\n除了 `ht[1]` 之外，另一个和 rehash 有关的属性就是 `rehashidx` ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 `-1` 。\n\n图 4-3 展示了一个普通状态下（没有进行 rehash）的字典：\n\n![字典的实现 - 图3](/img/eac62f3b0d93e90dd8f19789d013c1ec.png)\n\n## 哈希算法\n\n当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。\n\nRedis 计算哈希值和索引值的方法如下：\n\n```java\n# 使用字典设置的哈希函数，计算键 key 的哈希值\nhash = dict->type->hashFunction(key);\n \n# 使用哈希表的 sizemask 属性和哈希值，计算出索引值\n# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]\nindex = hash & dict->ht[x].sizemask;\n```\n\n![哈希算法 - 图1](/img/108cd6b414ab2dbb30126c0fb0700e23.png)\n\n举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 `k0` 和 `v0` 添加到字典里面，那么程序会先使用语句：\n\n```java\nhash = dict->type->hashFunction(k0);\n```\n\n计算键 `k0` 的哈希值。\n\n假设计算得出的哈希值为 `8` ，那么程序会继续使用语句：\n\n```java\nindex = hash & dict->ht[0].sizemask = 8 & 3 = 0;\n```\n\n计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。\n\n![哈希算法 - 图2](/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png)\n\n当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。\n\nMurmurHash 算法最初由 Austin Appleby 于 2008 年发明，这种算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。\n\nMurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：http://code.google.com/p/smhasher/ 。\n\n","source":"_posts/Redis数据结构与对象（三）-字典.md","raw":"title: Redis数据结构与对象（三）-字典\nauthor: ztq\ntags:\n\n  - redis\ncategories:\n  - 数据库\ndate: 2022-01-25 15:58:00\n\n---\n\n## 字典的实现\n\nRedis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。\n\n接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现。\n\n## 哈希表\n\nRedis 字典所使用的哈希表由 `dict.h/dictht` 结构定义：\n\n```java\ntypedef struct dictht {\n    // 哈希表数组\n    dictEntry **table;\n    // 哈希表大小\n    unsigned long size;\n    // 哈希表大小掩码，用于计算索引值\n    // 总是等于 size - 1\n    unsigned long sizemask;\n    // 该哈希表已有节点的数量\n    unsigned long used;\n} dictht;\n```\n\n`table` 属性是一个数组，数组中的每个元素都是一个指向 `dict.h/dictEntry` 结构的指针，每个 `dictEntry` 结构保存着一个键值对。\n\n`size` 属性记录了哈希表的大小，也即是 `table` 数组的大小，而 `used` 属性则记录了哈希表目前已有节点（键值对）的数量。\n\n`sizemask` 属性的值总是等于 `size - 1` ，这个属性和哈希值一起决定一个键应该被放到 `table` 数组的哪个索引上面。\n\n图 4-1 展示了一个大小为 `4` 的空哈希表（没有包含任何键值对）。\n\n![字典的实现 - 图1](/img/d234cadc40e81da676a8b541d9cb6ab4.png)\n\n## 哈希表节点\n\n哈希表节点使用 `dictEntry` 结构表示，每个 `dictEntry` 结构都保存着一个键值对：\n\n```java\ntypedef struct dictEntry {\n    // 键\n    void *key;\n    // 值\n    union {\n        void *val;\n        uint64_t u64;\n        int64_t s64;\n    } v;\n    // 指向下个哈希表节点，形成链表\n    struct dictEntry *next;\n} dictEntry;\n```\n\n`key` 属性保存着键值对中的键，而 `v` 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 `uint64_t` 整数，又或者是一个 `int64_t` 整数。\n\n`next` 属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题。\n\n举个例子，图 4-2 就展示了如何通过 `next` 指针，将两个索引值相同的键 `k1` 和 `k0` 连接在一起。\n\n![字典的实现 - 图2](/img/d9710d39e895a6eb8210645c6e0b85e3.png)\n\n## 字典\n\nRedis 中的字典由 `dict.h/dict` 结构表示：\n\n```java\ntypedef struct dict {\n    // 类型特定函数\n    dictType *type;\n    // 私有数据\n    void *privdata;\n    // 哈希表\n    dictht ht[2];\n    // rehash 索引\n    // 当 rehash 不在进行时，值为 -1\n    int rehashidx; /* rehashing not in progress if rehashidx == -1 */\n} dict;\n```\n\n`type` 属性和 `privdata` 属性是针对不同类型的键值对，为创建多态字典而设置的：\n\n- `type` 属性是一个指向 `dictType` 结构的指针，每个 `dictType` 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。\n- 而 `privdata` 属性则保存了需要传给那些类型特定函数的可选参数。\n\n```java\ntypedef struct dictType {\n    // 计算哈希值的函数\n    unsigned int (*hashFunction)(const void *key);\n    // 复制键的函数\n    void *(*keyDup)(void *privdata, const void *key);\n    // 复制值的函数\n    void *(*valDup)(void *privdata, const void *obj);\n    // 对比键的函数\n    int (*keyCompare)(void *privdata, const void *key1, const void *key2);\n    // 销毁键的函数\n    void (*keyDestructor)(void *privdata, void *key);\n    // 销毁值的函数\n    void (*valDestructor)(void *privdata, void *obj);\n} dictType;\n```\n\n`ht` 属性是一个包含两个项的数组，数组中的每个项都是一个 `dictht` 哈希表，一般情况下，字典只使用 `ht[0]` 哈希表，`ht[1]` 哈希表只会在对 `ht[0]` 哈希表进行 rehash 时使用。\n\n除了 `ht[1]` 之外，另一个和 rehash 有关的属性就是 `rehashidx` ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 `-1` 。\n\n图 4-3 展示了一个普通状态下（没有进行 rehash）的字典：\n\n![字典的实现 - 图3](/img/eac62f3b0d93e90dd8f19789d013c1ec.png)\n\n## 哈希算法\n\n当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。\n\nRedis 计算哈希值和索引值的方法如下：\n\n```java\n# 使用字典设置的哈希函数，计算键 key 的哈希值\nhash = dict->type->hashFunction(key);\n \n# 使用哈希表的 sizemask 属性和哈希值，计算出索引值\n# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]\nindex = hash & dict->ht[x].sizemask;\n```\n\n![哈希算法 - 图1](/img/108cd6b414ab2dbb30126c0fb0700e23.png)\n\n举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 `k0` 和 `v0` 添加到字典里面，那么程序会先使用语句：\n\n```java\nhash = dict->type->hashFunction(k0);\n```\n\n计算键 `k0` 的哈希值。\n\n假设计算得出的哈希值为 `8` ，那么程序会继续使用语句：\n\n```java\nindex = hash & dict->ht[0].sizemask = 8 & 3 = 0;\n```\n\n计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。\n\n![哈希算法 - 图2](/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png)\n\n当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。\n\nMurmurHash 算法最初由 Austin Appleby 于 2008 年发明，这种算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。\n\nMurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：http://code.google.com/p/smhasher/ 。\n\n","slug":"Redis数据结构与对象（三）-字典","published":1,"updated":"2022-04-04T08:32:40.151Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnze003l7kt96hgq4rae","content":"<h2 id=\"字典的实现\">字典的实现</h2>\n<p>Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。</p>\n<p>接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现。</p>\n<h2 id=\"哈希表\">哈希表</h2>\n<p>Redis 字典所使用的哈希表由 <code>dict.h/dictht</code> 结构定义：</p>\n<pre><code class=\"language-java\">typedef struct dictht &#123;\n    // 哈希表数组\n    dictEntry **table;\n    // 哈希表大小\n    unsigned long size;\n    // 哈希表大小掩码，用于计算索引值\n    // 总是等于 size - 1\n    unsigned long sizemask;\n    // 该哈希表已有节点的数量\n    unsigned long used;\n&#125; dictht;\n</code></pre>\n<p><code>table</code> 属性是一个数组，数组中的每个元素都是一个指向 <code>dict.h/dictEntry</code> 结构的指针，每个 <code>dictEntry</code> 结构保存着一个键值对。</p>\n<p><code>size</code> 属性记录了哈希表的大小，也即是 <code>table</code> 数组的大小，而 <code>used</code> 属性则记录了哈希表目前已有节点（键值对）的数量。</p>\n<p><code>sizemask</code> 属性的值总是等于 <code>size - 1</code> ，这个属性和哈希值一起决定一个键应该被放到 <code>table</code> 数组的哪个索引上面。</p>\n<p>图 4-1 展示了一个大小为 <code>4</code> 的空哈希表（没有包含任何键值对）。</p>\n<p><img src=\"/img/d234cadc40e81da676a8b541d9cb6ab4.png\" alt=\"字典的实现 - 图1\"></p>\n<h2 id=\"哈希表节点\">哈希表节点</h2>\n<p>哈希表节点使用 <code>dictEntry</code> 结构表示，每个 <code>dictEntry</code> 结构都保存着一个键值对：</p>\n<pre><code class=\"language-java\">typedef struct dictEntry &#123;\n    // 键\n    void *key;\n    // 值\n    union &#123;\n        void *val;\n        uint64_t u64;\n        int64_t s64;\n    &#125; v;\n    // 指向下个哈希表节点，形成链表\n    struct dictEntry *next;\n&#125; dictEntry;\n</code></pre>\n<p><code>key</code> 属性保存着键值对中的键，而 <code>v</code> 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 <code>uint64_t</code> 整数，又或者是一个 <code>int64_t</code> 整数。</p>\n<p><code>next</code> 属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题。</p>\n<p>举个例子，图 4-2 就展示了如何通过 <code>next</code> 指针，将两个索引值相同的键 <code>k1</code> 和 <code>k0</code> 连接在一起。</p>\n<p><img src=\"/img/d9710d39e895a6eb8210645c6e0b85e3.png\" alt=\"字典的实现 - 图2\"></p>\n<h2 id=\"字典\">字典</h2>\n<p>Redis 中的字典由 <code>dict.h/dict</code> 结构表示：</p>\n<pre><code class=\"language-java\">typedef struct dict &#123;\n    // 类型特定函数\n    dictType *type;\n    // 私有数据\n    void *privdata;\n    // 哈希表\n    dictht ht[2];\n    // rehash 索引\n    // 当 rehash 不在进行时，值为 -1\n    int rehashidx; /* rehashing not in progress if rehashidx == -1 */\n&#125; dict;\n</code></pre>\n<p><code>type</code> 属性和 <code>privdata</code> 属性是针对不同类型的键值对，为创建多态字典而设置的：</p>\n<ul>\n<li><code>type</code> 属性是一个指向 <code>dictType</code> 结构的指针，每个 <code>dictType</code> 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。</li>\n<li>而 <code>privdata</code> 属性则保存了需要传给那些类型特定函数的可选参数。</li>\n</ul>\n<pre><code class=\"language-java\">typedef struct dictType &#123;\n    // 计算哈希值的函数\n    unsigned int (*hashFunction)(const void *key);\n    // 复制键的函数\n    void *(*keyDup)(void *privdata, const void *key);\n    // 复制值的函数\n    void *(*valDup)(void *privdata, const void *obj);\n    // 对比键的函数\n    int (*keyCompare)(void *privdata, const void *key1, const void *key2);\n    // 销毁键的函数\n    void (*keyDestructor)(void *privdata, void *key);\n    // 销毁值的函数\n    void (*valDestructor)(void *privdata, void *obj);\n&#125; dictType;\n</code></pre>\n<p><code>ht</code> 属性是一个包含两个项的数组，数组中的每个项都是一个 <code>dictht</code> 哈希表，一般情况下，字典只使用 <code>ht[0]</code> 哈希表，<code>ht[1]</code> 哈希表只会在对 <code>ht[0]</code> 哈希表进行 rehash 时使用。</p>\n<p>除了 <code>ht[1]</code> 之外，另一个和 rehash 有关的属性就是 <code>rehashidx</code> ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 <code>-1</code> 。</p>\n<p>图 4-3 展示了一个普通状态下（没有进行 rehash）的字典：</p>\n<p><img src=\"/img/eac62f3b0d93e90dd8f19789d013c1ec.png\" alt=\"字典的实现 - 图3\"></p>\n<h2 id=\"哈希算法\">哈希算法</h2>\n<p>当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。</p>\n<p>Redis 计算哈希值和索引值的方法如下：</p>\n<pre><code class=\"language-java\"># 使用字典设置的哈希函数，计算键 key 的哈希值\nhash = dict-&gt;type-&gt;hashFunction(key);\n \n# 使用哈希表的 sizemask 属性和哈希值，计算出索引值\n# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]\nindex = hash &amp; dict-&gt;ht[x].sizemask;\n</code></pre>\n<p><img src=\"/img/108cd6b414ab2dbb30126c0fb0700e23.png\" alt=\"哈希算法 - 图1\"></p>\n<p>举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 <code>k0</code> 和 <code>v0</code> 添加到字典里面，那么程序会先使用语句：</p>\n<pre><code class=\"language-java\">hash = dict-&gt;type-&gt;hashFunction(k0);\n</code></pre>\n<p>计算键 <code>k0</code> 的哈希值。</p>\n<p>假设计算得出的哈希值为 <code>8</code> ，那么程序会继续使用语句：</p>\n<pre><code class=\"language-java\">index = hash &amp; dict-&gt;ht[0].sizemask = 8 &amp; 3 = 0;\n</code></pre>\n<p>计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。</p>\n<p><img src=\"/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png\" alt=\"哈希算法 - 图2\"></p>\n<p>当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。</p>\n<p>MurmurHash 算法最初由 Austin Appleby 于 2008 年发明，这种算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。</p>\n<p>MurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：<a href=\"http://code.google.com/p/smhasher/\">http://code.google.com/p/smhasher/</a> 。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"字典的实现\">字典的实现</h2>\n<p>Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。</p>\n<p>接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现。</p>\n<h2 id=\"哈希表\">哈希表</h2>\n<p>Redis 字典所使用的哈希表由 <code>dict.h/dictht</code> 结构定义：</p>\n<pre><code class=\"language-java\">typedef struct dictht &#123;\n    // 哈希表数组\n    dictEntry **table;\n    // 哈希表大小\n    unsigned long size;\n    // 哈希表大小掩码，用于计算索引值\n    // 总是等于 size - 1\n    unsigned long sizemask;\n    // 该哈希表已有节点的数量\n    unsigned long used;\n&#125; dictht;\n</code></pre>\n<p><code>table</code> 属性是一个数组，数组中的每个元素都是一个指向 <code>dict.h/dictEntry</code> 结构的指针，每个 <code>dictEntry</code> 结构保存着一个键值对。</p>\n<p><code>size</code> 属性记录了哈希表的大小，也即是 <code>table</code> 数组的大小，而 <code>used</code> 属性则记录了哈希表目前已有节点（键值对）的数量。</p>\n<p><code>sizemask</code> 属性的值总是等于 <code>size - 1</code> ，这个属性和哈希值一起决定一个键应该被放到 <code>table</code> 数组的哪个索引上面。</p>\n<p>图 4-1 展示了一个大小为 <code>4</code> 的空哈希表（没有包含任何键值对）。</p>\n<p><img src=\"/img/d234cadc40e81da676a8b541d9cb6ab4.png\" alt=\"字典的实现 - 图1\"></p>\n<h2 id=\"哈希表节点\">哈希表节点</h2>\n<p>哈希表节点使用 <code>dictEntry</code> 结构表示，每个 <code>dictEntry</code> 结构都保存着一个键值对：</p>\n<pre><code class=\"language-java\">typedef struct dictEntry &#123;\n    // 键\n    void *key;\n    // 值\n    union &#123;\n        void *val;\n        uint64_t u64;\n        int64_t s64;\n    &#125; v;\n    // 指向下个哈希表节点，形成链表\n    struct dictEntry *next;\n&#125; dictEntry;\n</code></pre>\n<p><code>key</code> 属性保存着键值对中的键，而 <code>v</code> 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 <code>uint64_t</code> 整数，又或者是一个 <code>int64_t</code> 整数。</p>\n<p><code>next</code> 属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题。</p>\n<p>举个例子，图 4-2 就展示了如何通过 <code>next</code> 指针，将两个索引值相同的键 <code>k1</code> 和 <code>k0</code> 连接在一起。</p>\n<p><img src=\"/img/d9710d39e895a6eb8210645c6e0b85e3.png\" alt=\"字典的实现 - 图2\"></p>\n<h2 id=\"字典\">字典</h2>\n<p>Redis 中的字典由 <code>dict.h/dict</code> 结构表示：</p>\n<pre><code class=\"language-java\">typedef struct dict &#123;\n    // 类型特定函数\n    dictType *type;\n    // 私有数据\n    void *privdata;\n    // 哈希表\n    dictht ht[2];\n    // rehash 索引\n    // 当 rehash 不在进行时，值为 -1\n    int rehashidx; /* rehashing not in progress if rehashidx == -1 */\n&#125; dict;\n</code></pre>\n<p><code>type</code> 属性和 <code>privdata</code> 属性是针对不同类型的键值对，为创建多态字典而设置的：</p>\n<ul>\n<li><code>type</code> 属性是一个指向 <code>dictType</code> 结构的指针，每个 <code>dictType</code> 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。</li>\n<li>而 <code>privdata</code> 属性则保存了需要传给那些类型特定函数的可选参数。</li>\n</ul>\n<pre><code class=\"language-java\">typedef struct dictType &#123;\n    // 计算哈希值的函数\n    unsigned int (*hashFunction)(const void *key);\n    // 复制键的函数\n    void *(*keyDup)(void *privdata, const void *key);\n    // 复制值的函数\n    void *(*valDup)(void *privdata, const void *obj);\n    // 对比键的函数\n    int (*keyCompare)(void *privdata, const void *key1, const void *key2);\n    // 销毁键的函数\n    void (*keyDestructor)(void *privdata, void *key);\n    // 销毁值的函数\n    void (*valDestructor)(void *privdata, void *obj);\n&#125; dictType;\n</code></pre>\n<p><code>ht</code> 属性是一个包含两个项的数组，数组中的每个项都是一个 <code>dictht</code> 哈希表，一般情况下，字典只使用 <code>ht[0]</code> 哈希表，<code>ht[1]</code> 哈希表只会在对 <code>ht[0]</code> 哈希表进行 rehash 时使用。</p>\n<p>除了 <code>ht[1]</code> 之外，另一个和 rehash 有关的属性就是 <code>rehashidx</code> ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 <code>-1</code> 。</p>\n<p>图 4-3 展示了一个普通状态下（没有进行 rehash）的字典：</p>\n<p><img src=\"/img/eac62f3b0d93e90dd8f19789d013c1ec.png\" alt=\"字典的实现 - 图3\"></p>\n<h2 id=\"哈希算法\">哈希算法</h2>\n<p>当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。</p>\n<p>Redis 计算哈希值和索引值的方法如下：</p>\n<pre><code class=\"language-java\"># 使用字典设置的哈希函数，计算键 key 的哈希值\nhash = dict-&gt;type-&gt;hashFunction(key);\n \n# 使用哈希表的 sizemask 属性和哈希值，计算出索引值\n# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]\nindex = hash &amp; dict-&gt;ht[x].sizemask;\n</code></pre>\n<p><img src=\"/img/108cd6b414ab2dbb30126c0fb0700e23.png\" alt=\"哈希算法 - 图1\"></p>\n<p>举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 <code>k0</code> 和 <code>v0</code> 添加到字典里面，那么程序会先使用语句：</p>\n<pre><code class=\"language-java\">hash = dict-&gt;type-&gt;hashFunction(k0);\n</code></pre>\n<p>计算键 <code>k0</code> 的哈希值。</p>\n<p>假设计算得出的哈希值为 <code>8</code> ，那么程序会继续使用语句：</p>\n<pre><code class=\"language-java\">index = hash &amp; dict-&gt;ht[0].sizemask = 8 &amp; 3 = 0;\n</code></pre>\n<p>计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。</p>\n<p><img src=\"/img/b126a2a46dcd9ec3380c1b5bd4992bb3.png\" alt=\"哈希算法 - 图2\"></p>\n<p>当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。</p>\n<p>MurmurHash 算法最初由 Austin Appleby 于 2008 年发明，这种算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。</p>\n<p>MurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：<a href=\"http://code.google.com/p/smhasher/\">http://code.google.com/p/smhasher/</a> 。</p>\n"},{"title":"SimpleDateFormat引发的线程安全问题","author":"郑天祺","date":"2019-10-12T10:42:00.000Z","_content":"\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","source":"_posts/SimpleDateFormat引发的线程安全问题.md","raw":"title: SimpleDateFormat引发的线程安全问题\nauthor: 郑天祺\ntags:\n  - 并发\n  - 线程安全\ncategories:\n  - java基础\ndate: 2019-10-12 18:42:00\n\n---\n\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","slug":"SimpleDateFormat引发的线程安全问题","published":1,"updated":"2022-04-04T08:32:40.152Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzf003n7kt95ti7bewu","content":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans &#123;\n\n    public static void main(String[] args) &#123;\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try &#123;\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        &#125; catch (ParseException e) &#123;\n            System.out.println(&quot;时间转换出错，出错信息为 =&#123;&#125;&quot; + e);\n        &#125;\n\n    &#125;\n&#125;\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n&#125;\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：<a href=\"https://blog.csdn.net/lululove19870526/article/details/83684568\">https://blog.csdn.net/lululove19870526/article/details/83684568</a></p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\t<a href=\"https://www.jianshu.com/p/3c5d7f09dfbd\">https://www.jianshu.com/p/3c5d7f09dfbd</a></p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans &#123;\n\n    public static void main(String[] args) &#123;\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try &#123;\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        &#125; catch (ParseException e) &#123;\n            System.out.println(&quot;时间转换出错，出错信息为 =&#123;&#125;&quot; + e);\n        &#125;\n\n    &#125;\n&#125;\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n&#125;\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：<a href=\"https://blog.csdn.net/lululove19870526/article/details/83684568\">https://blog.csdn.net/lululove19870526/article/details/83684568</a></p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\t<a href=\"https://www.jianshu.com/p/3c5d7f09dfbd\">https://www.jianshu.com/p/3c5d7f09dfbd</a></p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n"},{"title":"Spark相关概述","author":"郑天祺","date":"2019-12-18T05:41:00.000Z","_content":"\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","source":"_posts/Spark相关概述.md","raw":"title: Spark相关概述\nauthor: 郑天祺\ntags:\n\n  - Spark\ncategories:\n  - 大数据\ndate: 2019-12-18 13:41:00\n\n---\n\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","slug":"Spark相关概述","published":1,"updated":"2022-04-04T08:32:40.152Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzh003r7kt9a9h7g7ae","content":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n"},{"title":"Spring Bean作用域与生命周期","author":"郑天祺","date":"2019-11-14T08:52:00.000Z","_content":"\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","source":"_posts/Spring-Bean生命周期.md","raw":"title: Spring Bean作用域与生命周期\nauthor: 郑天祺\ntags:\n\n  - spring\ncategories:\n  - java基础\ndate: 2019-11-14 16:52:00\n\n---\n\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","slug":"Spring-Bean生命周期","published":1,"updated":"2022-04-04T08:32:40.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzi003u7kt9gsww4z1x","content":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2 id=\"1、使用-Profile\">1、使用@Profile</h2>\n<h4 id=\"1）假设存在dev-spring-boot-和-test-spring-boot两个数据库，使用注解-Profile定义两个Bean\">1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4 id=\"2）在-Java-启动项目中，我们只需要如下配置就能启动Profile机制：\">2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4 id=\"3）按照-springboot-的规则\">3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2 id=\"2、使用-Spring-EL\">2、使用 Spring EL</h2>\n<h4 id=\"1）读取属性文件的值，如：\">1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// $&#123;......&#125; 代表占位符\n@Value(&quot;$&#123;database.driverName&#125;&quot;)   \nString driver\n</code></pre>\n<h4 id=\"2）记录一个Bean初始化事件，如：\">2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #&#123;......&#125; 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#&#123;T(System).currentTimeMillis()&#125;&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#&#123;‘使用 Spring EL 赋值字符串’&#125;&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#&#123;9.3E3&#125;&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#&#123;beanName.str&#125;&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2 id=\"1、使用-Profile\">1、使用@Profile</h2>\n<h4 id=\"1）假设存在dev-spring-boot-和-test-spring-boot两个数据库，使用注解-Profile定义两个Bean\">1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4 id=\"2）在-Java-启动项目中，我们只需要如下配置就能启动Profile机制：\">2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4 id=\"3）按照-springboot-的规则\">3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2 id=\"2、使用-Spring-EL\">2、使用 Spring EL</h2>\n<h4 id=\"1）读取属性文件的值，如：\">1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// $&#123;......&#125; 代表占位符\n@Value(&quot;$&#123;database.driverName&#125;&quot;)   \nString driver\n</code></pre>\n<h4 id=\"2）记录一个Bean初始化事件，如：\">2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #&#123;......&#125; 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#&#123;T(System).currentTimeMillis()&#125;&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#&#123;‘使用 Spring EL 赋值字符串’&#125;&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#&#123;9.3E3&#125;&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#&#123;beanName.str&#125;&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n"},{"title":"SpringCloud-Alibaba整合Nacos服务注册发现","author":"郑天祺","date":"2019-12-03T07:18:00.000Z","_content":"\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","source":"_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","raw":"title: SpringCloud-Alibaba整合Nacos服务注册发现\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2019-12-03 15:18:00\n---\n\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","slug":"SpringCloud-Alibaba整合Nacos服务注册发现","published":1,"updated":"2022-04-04T08:32:40.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzk003z7kt97i4j4hv4","content":"<h1>一、服务注册</h1>\n<h2 id=\"1、引入依赖\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置application-yml\">2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2 id=\"3、springboot启动类\">3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、确认注册成功\">4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2 id=\"1、引入依赖-2\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置文件配置\">2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2 id=\"3、开启服务发现、负载均衡、熔断器功能\">3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.sy&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.sy&quot;&#125;)\npublic class NacosDiscoveryExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"4、创建服务代理类\">4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：<br>\n<a href=\"http://1.name\">1.name</a>：服务提供者注册在服务注册中心的名称；<br>\n2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；<br>\n3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n&#125;\n</code></pre>\n<h2 id=\"5、创建Hystrix的断路器工厂类\">5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; &#123;\n    @Override\n    public ConsumerService create(Throwable cause) &#123;\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() &#123;\n            @Override\n            public String hello() &#123;\n                return &quot;请求失败&quot;;\n            &#125;\n\n            @Override\n            public String hello(String string) &#123;\n                return &quot;请求失败. string=&quot; + string;\n            &#125;\n        &#125;;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"6、通用代理类的实例进行服务调用，与本地调用无异。如下：\">6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController &#123;\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return consumerService.hello(string);\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、服务注册</h1>\n<h2 id=\"1、引入依赖\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置application-yml\">2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2 id=\"3、springboot启动类\">3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、确认注册成功\">4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2 id=\"1、引入依赖-2\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置文件配置\">2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2 id=\"3、开启服务发现、负载均衡、熔断器功能\">3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.sy&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.sy&quot;&#125;)\npublic class NacosDiscoveryExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"4、创建服务代理类\">4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：<br>\n<a href=\"http://1.name\">1.name</a>：服务提供者注册在服务注册中心的名称；<br>\n2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；<br>\n3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n&#125;\n</code></pre>\n<h2 id=\"5、创建Hystrix的断路器工厂类\">5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; &#123;\n    @Override\n    public ConsumerService create(Throwable cause) &#123;\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() &#123;\n            @Override\n            public String hello() &#123;\n                return &quot;请求失败&quot;;\n            &#125;\n\n            @Override\n            public String hello(String string) &#123;\n                return &quot;请求失败. string=&quot; + string;\n            &#125;\n        &#125;;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"6、通用代理类的实例进行服务调用，与本地调用无异。如下：\">6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController &#123;\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return consumerService.hello(string);\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"SpringCloud Hystrix参数配置","author":"郑天祺","date":"2020-12-14T03:33:00.000Z","_content":"\nHystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。\n\n# 1、方法一：注解参数注入\n\n```java\n    @RequestMapping(value = \"/helloHystrixA/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = \"testFallback\", // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = {// 针对单个方法的配置\n            @HystrixProperty(name = \"circuitBreaker.enabled\", value = \"true\"), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"50\"), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"10\"), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = \"circuitBreaker.sleepWindowInMilliseconds\", value = \"10000\"),// 开启熔断器后过10秒再尝试访问\n        })\n    public String helloHystirxA(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n\n```\n\n# 2、方法二：配置文件\n\n分两步，首先在代码里配置commandKey：\n\n```java\n@RequestMapping(value = \"/helloHystrixB/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(commandKey = \"testCommand\", // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = \"testFallback\"// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n    }\n\n```\n\n然后在application.yml里配置 commandKey = \"testCommand\" 对应的配置项：\n\n```java\nhystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n```\n\n注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。","source":"_posts/SpringCloud-Hystrix参数配置.md","raw":"title: SpringCloud Hystrix参数配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 11:33:00\n---\n\nHystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。\n\n# 1、方法一：注解参数注入\n\n```java\n    @RequestMapping(value = \"/helloHystrixA/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = \"testFallback\", // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = {// 针对单个方法的配置\n            @HystrixProperty(name = \"circuitBreaker.enabled\", value = \"true\"), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"50\"), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"10\"), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = \"circuitBreaker.sleepWindowInMilliseconds\", value = \"10000\"),// 开启熔断器后过10秒再尝试访问\n        })\n    public String helloHystirxA(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n\n```\n\n# 2、方法二：配置文件\n\n分两步，首先在代码里配置commandKey：\n\n```java\n@RequestMapping(value = \"/helloHystrixB/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(commandKey = \"testCommand\", // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = \"testFallback\"// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n    }\n\n```\n\n然后在application.yml里配置 commandKey = \"testCommand\" 对应的配置项：\n\n```java\nhystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n```\n\n注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。","slug":"SpringCloud-Hystrix参数配置","published":1,"updated":"2022-04-04T08:32:40.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzl00427kt937lj9sst","content":"<p>Hystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。</p>\n<h1>1、方法一：注解参数注入</h1>\n<pre><code class=\"language-java\">    @RequestMapping(value = &quot;/helloHystrixA/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = &quot;testFallback&quot;, // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = &#123;// 针对单个方法的配置\n            @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;50&quot;), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;),// 开启熔断器后过10秒再尝试访问\n        &#125;)\n    public String helloHystirxA(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n\n</code></pre>\n<h1>2、方法二：配置文件</h1>\n<p>分两步，首先在代码里配置commandKey：</p>\n<pre><code class=\"language-java\">@RequestMapping(value = &quot;/helloHystrixB/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(commandKey = &quot;testCommand&quot;, // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = &quot;testFallback&quot;// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n    &#125;\n\n</code></pre>\n<p>然后在application.yml里配置 commandKey = “testCommand” 对应的配置项：</p>\n<pre><code class=\"language-java\">hystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n</code></pre>\n<p>注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。</p>\n<h1>1、方法一：注解参数注入</h1>\n<pre><code class=\"language-java\">    @RequestMapping(value = &quot;/helloHystrixA/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = &quot;testFallback&quot;, // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = &#123;// 针对单个方法的配置\n            @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;50&quot;), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;),// 开启熔断器后过10秒再尝试访问\n        &#125;)\n    public String helloHystirxA(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n\n</code></pre>\n<h1>2、方法二：配置文件</h1>\n<p>分两步，首先在代码里配置commandKey：</p>\n<pre><code class=\"language-java\">@RequestMapping(value = &quot;/helloHystrixB/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(commandKey = &quot;testCommand&quot;, // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = &quot;testFallback&quot;// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n    &#125;\n\n</code></pre>\n<p>然后在application.yml里配置 commandKey = “testCommand” 对应的配置项：</p>\n<pre><code class=\"language-java\">hystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n</code></pre>\n<p>注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。</p>\n"},{"title":"SpringCloud Ribbon参数配置","author":"郑天祺","date":"2020-12-14T03:33:00.000Z","_content":"\n# Ribbon策略类型\n\n![image-20201214123508379](/img/image-20201214123508379.png)\n\nRibbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。\n\n# 方法一：创建配置类\n\n```java\n @Configuration  \npublic class MyRibbonConfig {\n    @Bean\n    public IRule ribbonRule() {\n        //随机策略\n        return new RandomRule();\n    }\n}\n然后在启动类上加注解：\n@RibbonClient(name = \"nacos.provider.demo\", configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n```\n\n# 方法二：配置文件\n\n无须任何配置类和代码，只需要在在application.yml中添加配置：\n\n```java\n#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n```\n\n","source":"_posts/SpringCloud-Ribbon参数配置.md","raw":"title: SpringCloud Ribbon参数配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 11:33:00\n---\n\n# Ribbon策略类型\n\n![image-20201214123508379](/img/image-20201214123508379.png)\n\nRibbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。\n\n# 方法一：创建配置类\n\n```java\n @Configuration  \npublic class MyRibbonConfig {\n    @Bean\n    public IRule ribbonRule() {\n        //随机策略\n        return new RandomRule();\n    }\n}\n然后在启动类上加注解：\n@RibbonClient(name = \"nacos.provider.demo\", configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n```\n\n# 方法二：配置文件\n\n无须任何配置类和代码，只需要在在application.yml中添加配置：\n\n```java\n#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n```\n\n","slug":"SpringCloud-Ribbon参数配置","published":1,"updated":"2022-04-04T08:32:40.154Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzm00467kt95dnz1p3m","content":"<h1>Ribbon策略类型</h1>\n<p><img src=\"/img/image-20201214123508379.png\" alt=\"image-20201214123508379\"></p>\n<p>Ribbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。</p>\n<h1>方法一：创建配置类</h1>\n<pre><code class=\"language-java\"> @Configuration  \npublic class MyRibbonConfig &#123;\n    @Bean\n    public IRule ribbonRule() &#123;\n        //随机策略\n        return new RandomRule();\n    &#125;\n&#125;\n然后在启动类上加注解：\n@RibbonClient(name = &quot;nacos.provider.demo&quot;, configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n</code></pre>\n<h1>方法二：配置文件</h1>\n<p>无须任何配置类和代码，只需要在在application.yml中添加配置：</p>\n<pre><code class=\"language-java\">#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>Ribbon策略类型</h1>\n<p><img src=\"/img/image-20201214123508379.png\" alt=\"image-20201214123508379\"></p>\n<p>Ribbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。</p>\n<h1>方法一：创建配置类</h1>\n<pre><code class=\"language-java\"> @Configuration  \npublic class MyRibbonConfig &#123;\n    @Bean\n    public IRule ribbonRule() &#123;\n        //随机策略\n        return new RandomRule();\n    &#125;\n&#125;\n然后在启动类上加注解：\n@RibbonClient(name = &quot;nacos.provider.demo&quot;, configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n</code></pre>\n<h1>方法二：配置文件</h1>\n<p>无须任何配置类和代码，只需要在在application.yml中添加配置：</p>\n<pre><code class=\"language-java\">#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n</code></pre>\n"},{"title":"SpringCloud client配置","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config -->\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n```\n\n# 2、配置文件\n\n创建配置文件  bootstrap.properties ，并添加基础配置信息：\n\n```java\nbootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n```\n\n# 3、加载配置\n\n使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。 \n\n示例代码\n\n```java\n@RestController\n@RefreshScope\npublic class HelloController {\n \n    @Value(\"${user.code:默认值}\")\n    private String userCode;\n \n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello, this is a nacos-config-demo. userCode=\" + userCode;\n    }\n}\n```\n\n","source":"_posts/SpringCloud-client配置.md","raw":"title: SpringCloud client配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:32:00\n---\n\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config -->\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n```\n\n# 2、配置文件\n\n创建配置文件  bootstrap.properties ，并添加基础配置信息：\n\n```java\nbootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n```\n\n# 3、加载配置\n\n使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。 \n\n示例代码\n\n```java\n@RestController\n@RefreshScope\npublic class HelloController {\n \n    @Value(\"${user.code:默认值}\")\n    private String userCode;\n \n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello, this is a nacos-config-demo. userCode=\" + userCode;\n    }\n}\n```\n\n","slug":"SpringCloud-client配置","published":1,"updated":"2022-04-04T08:32:40.154Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzn00497kt91734f6s6","content":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config --&gt;\n&lt;dependency&gt;\n\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置文件</h1>\n<p>创建配置文件  bootstrap.properties ，并添加基础配置信息：</p>\n<pre><code class=\"language-java\">bootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n</code></pre>\n<h1>3、加载配置</h1>\n<p>使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。</p>\n<p>示例代码</p>\n<pre><code class=\"language-java\">@RestController\n@RefreshScope\npublic class HelloController &#123;\n \n    @Value(&quot;$&#123;user.code:默认值&#125;&quot;)\n    private String userCode;\n \n    @RequestMapping(&quot;/hello&quot;)\n    public String hello() &#123;\n        return &quot;Hello, this is a nacos-config-demo. userCode=&quot; + userCode;\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config --&gt;\n&lt;dependency&gt;\n\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置文件</h1>\n<p>创建配置文件  bootstrap.properties ，并添加基础配置信息：</p>\n<pre><code class=\"language-java\">bootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n</code></pre>\n<h1>3、加载配置</h1>\n<p>使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。</p>\n<p>示例代码</p>\n<pre><code class=\"language-java\">@RestController\n@RefreshScope\npublic class HelloController &#123;\n \n    @Value(&quot;$&#123;user.code:默认值&#125;&quot;)\n    private String userCode;\n \n    @RequestMapping(&quot;/hello&quot;)\n    public String hello() &#123;\n        return &quot;Hello, this is a nacos-config-demo. userCode=&quot; + userCode;\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"SpringCloud使用Feign+Ribbon+Hystrix","author":"郑天祺","date":"2020-12-14T04:39:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、增加FeignClient\n\n```java\n@FeignClient(name = \"discovery-provider\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n\n    @LoadBalanced\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 4、修改默认的负载均衡规则\n\n```java\n@Configuration\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    }\n}\n```\n\n# 5、增加对应方法的Hystrix配置\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<RemoteClient> {\n\n    @Override\n    public RemoteClient create(Throwable throwable) {\n       return (name)-> \"请求失败. name=\" + name;\n    }\n}\n```\n\n# 6、使用效果如下\n\n## 1）.启动两个服务端，端口分别为8080，8090\n\n![image-20201214125402694](/img/image-20201214125402694.png)\n\n## 2）.启动服务消费端，多次调用对应的服务\n\n![image-20201214125453643](/img/image-20201214125453643.png)\n\n![image-20201214125504552](/img/image-20201214125504552.png)\n\n由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。\n\n## 3）. 关掉两个服务端实例\n\n![image-20201214125529023](/img/image-20201214125529023.png)\n\n再次访问时返回了对应fallback中的返回值。","source":"_posts/SpringCloud使用Feign-Ribbon-Hystrix.md","raw":"title: SpringCloud使用Feign+Ribbon+Hystrix\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 12:39:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、增加FeignClient\n\n```java\n@FeignClient(name = \"discovery-provider\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n\n    @LoadBalanced\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 4、修改默认的负载均衡规则\n\n```java\n@Configuration\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    }\n}\n```\n\n# 5、增加对应方法的Hystrix配置\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<RemoteClient> {\n\n    @Override\n    public RemoteClient create(Throwable throwable) {\n       return (name)-> \"请求失败. name=\" + name;\n    }\n}\n```\n\n# 6、使用效果如下\n\n## 1）.启动两个服务端，端口分别为8080，8090\n\n![image-20201214125402694](/img/image-20201214125402694.png)\n\n## 2）.启动服务消费端，多次调用对应的服务\n\n![image-20201214125453643](/img/image-20201214125453643.png)\n\n![image-20201214125504552](/img/image-20201214125504552.png)\n\n由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。\n\n## 3）. 关掉两个服务端实例\n\n![image-20201214125529023](/img/image-20201214125529023.png)\n\n再次访问时返回了对应fallback中的返回值。","slug":"SpringCloud使用Feign-Ribbon-Hystrix","published":1,"updated":"2022-04-04T08:32:40.154Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzn004d7kt9abfj6lxb","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、增加FeignClient</h1>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;discovery-provider&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>4、修改默认的负载均衡规则</h1>\n<pre><code class=\"language-java\">@Configuration\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>5、增加对应方法的Hystrix配置</h1>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;RemoteClient&gt; &#123;\n\n    @Override\n    public RemoteClient create(Throwable throwable) &#123;\n       return (name)-&gt; &quot;请求失败. name=&quot; + name;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、使用效果如下</h1>\n<h2 id=\"1）-启动两个服务端，端口分别为8080，8090\">1）.启动两个服务端，端口分别为8080，8090</h2>\n<p><img src=\"/img/image-20201214125402694.png\" alt=\"image-20201214125402694\"></p>\n<h2 id=\"2）-启动服务消费端，多次调用对应的服务\">2）.启动服务消费端，多次调用对应的服务</h2>\n<p><img src=\"/img/image-20201214125453643.png\" alt=\"image-20201214125453643\"></p>\n<p><img src=\"/img/image-20201214125504552.png\" alt=\"image-20201214125504552\"></p>\n<p>由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。</p>\n<h2 id=\"3）-关掉两个服务端实例\">3）. 关掉两个服务端实例</h2>\n<p><img src=\"/img/image-20201214125529023.png\" alt=\"image-20201214125529023\"></p>\n<p>再次访问时返回了对应fallback中的返回值。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、增加FeignClient</h1>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;discovery-provider&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>4、修改默认的负载均衡规则</h1>\n<pre><code class=\"language-java\">@Configuration\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>5、增加对应方法的Hystrix配置</h1>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;RemoteClient&gt; &#123;\n\n    @Override\n    public RemoteClient create(Throwable throwable) &#123;\n       return (name)-&gt; &quot;请求失败. name=&quot; + name;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、使用效果如下</h1>\n<h2 id=\"1）-启动两个服务端，端口分别为8080，8090\">1）.启动两个服务端，端口分别为8080，8090</h2>\n<p><img src=\"/img/image-20201214125402694.png\" alt=\"image-20201214125402694\"></p>\n<h2 id=\"2）-启动服务消费端，多次调用对应的服务\">2）.启动服务消费端，多次调用对应的服务</h2>\n<p><img src=\"/img/image-20201214125453643.png\" alt=\"image-20201214125453643\"></p>\n<p><img src=\"/img/image-20201214125504552.png\" alt=\"image-20201214125504552\"></p>\n<p>由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。</p>\n<h2 id=\"3）-关掉两个服务端实例\">3）. 关掉两个服务端实例</h2>\n<p><img src=\"/img/image-20201214125529023.png\" alt=\"image-20201214125529023\"></p>\n<p>再次访问时返回了对应fallback中的返回值。</p>\n"},{"title":"SpringCloud使用RestTemplate","author":"郑天祺","date":"2020-12-14T04:37:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、配置\n\n```java\nspring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n```\n\n3、使用RestTemplate进行服务调用\n\n```java\n@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = \"/hello/{name}\", method = RequestMethod.GET)\npublic String echo(@PathVariable String name) {\n    try {\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(\"server-provider\", true);\n        String sendUrl = \"http://\" + instance.getIp() + \":\" + instance.getPort() + \"/echo/\" + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    } catch (NacosException e) {\n        e.printStackTrace();\n    }\n    return null;\n}\n\n```\n\n# 4、调用\n\n调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法\n\n![image-20201214124300675](/img/image-20201214124300675.png)","source":"_posts/SpringCloud使用RestTemplate.md","raw":"title: SpringCloud使用RestTemplate\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:37:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、配置\n\n```java\nspring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n```\n\n3、使用RestTemplate进行服务调用\n\n```java\n@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = \"/hello/{name}\", method = RequestMethod.GET)\npublic String echo(@PathVariable String name) {\n    try {\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(\"server-provider\", true);\n        String sendUrl = \"http://\" + instance.getIp() + \":\" + instance.getPort() + \"/echo/\" + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    } catch (NacosException e) {\n        e.printStackTrace();\n    }\n    return null;\n}\n\n```\n\n# 4、调用\n\n调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法\n\n![image-20201214124300675](/img/image-20201214124300675.png)","slug":"SpringCloud使用RestTemplate","published":1,"updated":"2022-04-04T08:32:40.155Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzo004g7kt9bajo3xsz","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置</h1>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n</code></pre>\n<p>3、使用RestTemplate进行服务调用</p>\n<pre><code class=\"language-java\">@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = &quot;/hello/&#123;name&#125;&quot;, method = RequestMethod.GET)\npublic String echo(@PathVariable String name) &#123;\n    try &#123;\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(&quot;server-provider&quot;, true);\n        String sendUrl = &quot;http://&quot; + instance.getIp() + &quot;:&quot; + instance.getPort() + &quot;/echo/&quot; + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    &#125; catch (NacosException e) &#123;\n        e.printStackTrace();\n    &#125;\n    return null;\n&#125;\n\n</code></pre>\n<h1>4、调用</h1>\n<p>调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法</p>\n<p><img src=\"/img/image-20201214124300675.png\" alt=\"image-20201214124300675\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置</h1>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n</code></pre>\n<p>3、使用RestTemplate进行服务调用</p>\n<pre><code class=\"language-java\">@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = &quot;/hello/&#123;name&#125;&quot;, method = RequestMethod.GET)\npublic String echo(@PathVariable String name) &#123;\n    try &#123;\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(&quot;server-provider&quot;, true);\n        String sendUrl = &quot;http://&quot; + instance.getIp() + &quot;:&quot; + instance.getPort() + &quot;/echo/&quot; + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    &#125; catch (NacosException e) &#123;\n        e.printStackTrace();\n    &#125;\n    return null;\n&#125;\n\n</code></pre>\n<h1>4、调用</h1>\n<p>调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法</p>\n<p><img src=\"/img/image-20201214124300675.png\" alt=\"image-20201214124300675\"></p>\n"},{"title":"SpringCloud使用Feign+Ribbon","author":"郑天祺","date":"2020-12-14T04:39:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、远程调用\n\n基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡\n\n```java\n/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate(){\n    return new RestTemplate();\n}\n```\n\n# 4、定义Feign接口\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 5、可选的负载均衡策略\n\n```java\n//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n        return new BestAvailableRule();\n    }\n}\n```\n\n# 6、主类增加相关注解\n\n```java\n/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = {@RibbonClient(name = \"discovery-provider\",configuration = AppointRibbonMetric.class)})\n@ComponentScan(excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = {AvoidScan.class})})\npublic class ConsumerRibbonApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 7、测试调用\n\n多次点击调用成功，并路由到不同端口的实例上。\n\n![image-20201214125020584](/img/image-20201214125020584.png)\n\n![image-20201214125032862](/img/image-20201214125032862.png)","source":"_posts/SpringCloud使用Feign-Ribbon.md","raw":"title: SpringCloud使用Feign+Ribbon\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:39:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、远程调用\n\n基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡\n\n```java\n/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate(){\n    return new RestTemplate();\n}\n```\n\n# 4、定义Feign接口\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 5、可选的负载均衡策略\n\n```java\n//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n        return new BestAvailableRule();\n    }\n}\n```\n\n# 6、主类增加相关注解\n\n```java\n/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = {@RibbonClient(name = \"discovery-provider\",configuration = AppointRibbonMetric.class)})\n@ComponentScan(excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = {AvoidScan.class})})\npublic class ConsumerRibbonApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 7、测试调用\n\n多次点击调用成功，并路由到不同端口的实例上。\n\n![image-20201214125020584](/img/image-20201214125020584.png)\n\n![image-20201214125032862](/img/image-20201214125032862.png)","slug":"SpringCloud使用Feign-Ribbon","published":1,"updated":"2022-04-04T08:32:40.155Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzp004k7kt92utz6fb2","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、远程调用</h1>\n<p>基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡</p>\n<pre><code class=\"language-java\">/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate()&#123;\n    return new RestTemplate();\n&#125;\n</code></pre>\n<h1>4、定义Feign接口</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>5、可选的负载均衡策略</h1>\n<pre><code class=\"language-java\">//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n        return new BestAvailableRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>6、主类增加相关注解</h1>\n<pre><code class=\"language-java\">/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = &#123;@RibbonClient(name = &quot;discovery-provider&quot;,configuration = AppointRibbonMetric.class)&#125;)\n@ComponentScan(excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = &#123;AvoidScan.class&#125;)&#125;)\npublic class ConsumerRibbonApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>7、测试调用</h1>\n<p>多次点击调用成功，并路由到不同端口的实例上。</p>\n<p><img src=\"/img/image-20201214125020584.png\" alt=\"image-20201214125020584\"></p>\n<p><img src=\"/img/image-20201214125032862.png\" alt=\"image-20201214125032862\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、远程调用</h1>\n<p>基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡</p>\n<pre><code class=\"language-java\">/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate()&#123;\n    return new RestTemplate();\n&#125;\n</code></pre>\n<h1>4、定义Feign接口</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>5、可选的负载均衡策略</h1>\n<pre><code class=\"language-java\">//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n        return new BestAvailableRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>6、主类增加相关注解</h1>\n<pre><code class=\"language-java\">/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = &#123;@RibbonClient(name = &quot;discovery-provider&quot;,configuration = AppointRibbonMetric.class)&#125;)\n@ComponentScan(excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = &#123;AvoidScan.class&#125;)&#125;)\npublic class ConsumerRibbonApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>7、测试调用</h1>\n<p>多次点击调用成功，并路由到不同端口的实例上。</p>\n<p><img src=\"/img/image-20201214125020584.png\" alt=\"image-20201214125020584\"></p>\n<p><img src=\"/img/image-20201214125032862.png\" alt=\"image-20201214125032862\"></p>\n"},{"title":"SpringCloud使用Feign","author":"郑天祺","date":"2020-12-14T04:38:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、添加配置\n\n此处主要为nacos服务端地址配置\n\n```java\nspring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n```\n\n# 3、主类添加相关注解\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、定义FeignClient,用与服务调用\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n\n}\n```\n\n# 5.注入4中的FeignClient\n\n调用其hello方法 即可调用远程服务。\n\n![image-20201214124603810](/img/image-20201214124603810.png)","source":"_posts/SpringCloud使用Feign.md","raw":"title: SpringCloud使用Feign\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:38:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、添加配置\n\n此处主要为nacos服务端地址配置\n\n```java\nspring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n```\n\n# 3、主类添加相关注解\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、定义FeignClient,用与服务调用\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n\n}\n```\n\n# 5.注入4中的FeignClient\n\n调用其hello方法 即可调用远程服务。\n\n![image-20201214124603810](/img/image-20201214124603810.png)","slug":"SpringCloud使用Feign","published":1,"updated":"2022-04-04T08:32:40.155Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzq004n7kt93kp80967","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置</h1>\n<p>此处主要为nacos服务端地址配置</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n</code></pre>\n<h1>3、主类添加相关注解</h1>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、定义FeignClient,用与服务调用</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n\n&#125;\n</code></pre>\n<h1>5.注入4中的FeignClient</h1>\n<p>调用其hello方法 即可调用远程服务。</p>\n<p><img src=\"/img/image-20201214124603810.png\" alt=\"image-20201214124603810\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置</h1>\n<p>此处主要为nacos服务端地址配置</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n</code></pre>\n<h1>3、主类添加相关注解</h1>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、定义FeignClient,用与服务调用</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n\n&#125;\n</code></pre>\n<h1>5.注入4中的FeignClient</h1>\n<p>调用其hello方法 即可调用远程服务。</p>\n<p><img src=\"/img/image-20201214124603810.png\" alt=\"image-20201214124603810\"></p>\n"},{"title":"SpringCloud健康检查","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n# 服务健康检查：\n\n基于Spring Cloud体系，可以使用spring cloud  actuator组件\n\n# 1、POM依赖\n\n```java\n<dependency>\n     <groupId>org.springframework.boot</groupId>\n     <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n# 2、Application.yml配置\n\n```java\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n```\n\n# 3、访问路径： \n\nhttp://服务地址/actuator/health\n\n\n\n# 4、响应报文内容\n\n```java\n{\n    \"status\": \"UP\", \n    \"details\": {\n        \"diskSpace\": {\n            \"status\": \"UP\", \n            \"details\": {\n                \"total\": 499963174912, \n                \"free\": 200715714560, \n                \"threshold\": 10485760\n            }\n        }\n    }\n}\n```\n\n其中status状态含义如下\n\n![image-20201214114013294](/img/image-20201214114013294.png)\n\n\n\n# 5、服务自检主动通知\n\n```java\n{\n\t\"status\": \"当前状态（以整数形式字符串表示）\"，\n\t\"msg\": \"说明信息\"，\n}\n```\n\nstatus状态值描述：\n\n0：服务正常\n\n1：处理能力紧张，需要扩容\n\n2：服务内部错误 ，需要重启或版本回滚\n\n3：服务获取依赖资源失败，需要人工干预","source":"_posts/SpringCloud健康检查.md","raw":"title: SpringCloud健康检查\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n# 服务健康检查：\n\n基于Spring Cloud体系，可以使用spring cloud  actuator组件\n\n# 1、POM依赖\n\n```java\n<dependency>\n     <groupId>org.springframework.boot</groupId>\n     <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n# 2、Application.yml配置\n\n```java\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n```\n\n# 3、访问路径： \n\nhttp://服务地址/actuator/health\n\n\n\n# 4、响应报文内容\n\n```java\n{\n    \"status\": \"UP\", \n    \"details\": {\n        \"diskSpace\": {\n            \"status\": \"UP\", \n            \"details\": {\n                \"total\": 499963174912, \n                \"free\": 200715714560, \n                \"threshold\": 10485760\n            }\n        }\n    }\n}\n```\n\n其中status状态含义如下\n\n![image-20201214114013294](/img/image-20201214114013294.png)\n\n\n\n# 5、服务自检主动通知\n\n```java\n{\n\t\"status\": \"当前状态（以整数形式字符串表示）\"，\n\t\"msg\": \"说明信息\"，\n}\n```\n\nstatus状态值描述：\n\n0：服务正常\n\n1：处理能力紧张，需要扩容\n\n2：服务内部错误 ，需要重启或版本回滚\n\n3：服务获取依赖资源失败，需要人工干预","slug":"SpringCloud健康检查","published":1,"updated":"2022-04-04T08:32:40.156Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzr004r7kt90d304k0l","content":"<h1>服务健康检查：</h1>\n<p>基于Spring Cloud体系，可以使用spring cloud  actuator组件</p>\n<h1>1、POM依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n     &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、Application.yml配置</h1>\n<pre><code class=\"language-java\">management:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n</code></pre>\n<h1>3、访问路径：</h1>\n<p><a href=\"http://xn--zfry9hnb732h/actuator/health\">http://服务地址/actuator/health</a></p>\n<h1>4、响应报文内容</h1>\n<pre><code class=\"language-java\">&#123;\n    &quot;status&quot;: &quot;UP&quot;, \n    &quot;details&quot;: &#123;\n        &quot;diskSpace&quot;: &#123;\n            &quot;status&quot;: &quot;UP&quot;, \n            &quot;details&quot;: &#123;\n                &quot;total&quot;: 499963174912, \n                &quot;free&quot;: 200715714560, \n                &quot;threshold&quot;: 10485760\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<p>其中status状态含义如下</p>\n<p><img src=\"/img/image-20201214114013294.png\" alt=\"image-20201214114013294\"></p>\n<h1>5、服务自检主动通知</h1>\n<pre><code class=\"language-java\">&#123;\n\t&quot;status&quot;: &quot;当前状态（以整数形式字符串表示）&quot;，\n\t&quot;msg&quot;: &quot;说明信息&quot;，\n&#125;\n</code></pre>\n<p>status状态值描述：</p>\n<p>0：服务正常</p>\n<p>1：处理能力紧张，需要扩容</p>\n<p>2：服务内部错误 ，需要重启或版本回滚</p>\n<p>3：服务获取依赖资源失败，需要人工干预</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>服务健康检查：</h1>\n<p>基于Spring Cloud体系，可以使用spring cloud  actuator组件</p>\n<h1>1、POM依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n     &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、Application.yml配置</h1>\n<pre><code class=\"language-java\">management:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n</code></pre>\n<h1>3、访问路径：</h1>\n<p><a href=\"http://xn--zfry9hnb732h/actuator/health\">http://服务地址/actuator/health</a></p>\n<h1>4、响应报文内容</h1>\n<pre><code class=\"language-java\">&#123;\n    &quot;status&quot;: &quot;UP&quot;, \n    &quot;details&quot;: &#123;\n        &quot;diskSpace&quot;: &#123;\n            &quot;status&quot;: &quot;UP&quot;, \n            &quot;details&quot;: &#123;\n                &quot;total&quot;: 499963174912, \n                &quot;free&quot;: 200715714560, \n                &quot;threshold&quot;: 10485760\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<p>其中status状态含义如下</p>\n<p><img src=\"/img/image-20201214114013294.png\" alt=\"image-20201214114013294\"></p>\n<h1>5、服务自检主动通知</h1>\n<pre><code class=\"language-java\">&#123;\n\t&quot;status&quot;: &quot;当前状态（以整数形式字符串表示）&quot;，\n\t&quot;msg&quot;: &quot;说明信息&quot;，\n&#125;\n</code></pre>\n<p>status状态值描述：</p>\n<p>0：服务正常</p>\n<p>1：处理能力紧张，需要扩容</p>\n<p>2：服务内部错误 ，需要重启或版本回滚</p>\n<p>3：服务获取依赖资源失败，需要人工干预</p>\n"},{"title":"SpringCloud异常配置","date":"2020-12-14T03:34:00.000Z","_content":"\n\n\n\n1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。\n 说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。\n正例:if (obj != null) {...}\n反例:try { obj.method(); } catch (NullPointerException e) {...}\n\n2.【强制】异常不要用来做流程控制，条件控制。\n 说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。\n\n3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n 说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。\n\n4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n\n5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。\n\n6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。\n\n7.【强制】不要在 finally 块中使用 return。\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n\n8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。\n\n10. 【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。\n      反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。\n2)  数据库的查询结果可能为null。\n3)  集合里的元素即使isNotEmpty，取出的数据元素也可能为null。\n4)  远程调用返回对象时，一律要求进行空指针判断，防止NPE。\n5)  对于Session中获取的数据，建议NPE检查，避免空指针。\n6)  级联调用obj.getA().getB().getC();一连串调用，易产生NPE。\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。\n\n\n11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。\n\n12. 【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。\n\n13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 \n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {...}","source":"_posts/SpringCloud异常配置.md","raw":"title: SpringCloud异常配置\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:34:00\n\n---\n\n\n\n\n1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。\n 说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。\n正例:if (obj != null) {...}\n反例:try { obj.method(); } catch (NullPointerException e) {...}\n\n2.【强制】异常不要用来做流程控制，条件控制。\n 说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。\n\n3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n 说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。\n\n4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n\n5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。\n\n6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。\n\n7.【强制】不要在 finally 块中使用 return。\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n\n8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。\n\n10. 【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。\n      反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。\n2)  数据库的查询结果可能为null。\n3)  集合里的元素即使isNotEmpty，取出的数据元素也可能为null。\n4)  远程调用返回对象时，一律要求进行空指针判断，防止NPE。\n5)  对于Session中获取的数据，建议NPE检查，避免空指针。\n6)  级联调用obj.getA().getB().getC();一连串调用，易产生NPE。\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。\n\n\n11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。\n\n12. 【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。\n\n13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 \n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {...}","slug":"SpringCloud异常配置","published":1,"updated":"2022-04-04T08:32:40.156Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzs004u7kt9c8xx3frj","content":"<p>1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。<br>\n说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。<br>\n正例:if (obj != null) {…}<br>\n反例:try { obj.method(); } catch (NullPointerException e) {…}</p>\n<p>2.【强制】异常不要用来做流程控制，条件控制。<br>\n说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</p>\n<p>3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<p>说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。</p>\n<p>4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</p>\n<p>5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。</p>\n<p>6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。</p>\n<p>7.【强制】不要在 finally 块中使用 return。<br>\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</p>\n<p>8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<p>9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。<br>\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。</p>\n<ol start=\"10\">\n<li>【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:<br>\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。<br>\n反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。</li>\n</ol>\n<ol start=\"2\">\n<li>数据库的查询结果可能为null。</li>\n<li>集合里的元素即使isNotEmpty，取出的数据元素也可能为null。</li>\n<li>远程调用返回对象时，一律要求进行空指针判断，防止NPE。</li>\n<li>对于Session中获取的数据，建议NPE检查，避免空指针。</li>\n<li>级联调用obj.getA().getB().getC();一连串调用，易产生NPE。<br>\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。</li>\n</ol>\n<p>11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。</p>\n<ol start=\"12\">\n<li>【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。<br>\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:<br>\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。</li>\n</ol>\n<p>13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。<br>\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。<br>\n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {…}</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。<br>\n说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。<br>\n正例:if (obj != null) {…}<br>\n反例:try { obj.method(); } catch (NullPointerException e) {…}</p>\n<p>2.【强制】异常不要用来做流程控制，条件控制。<br>\n说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</p>\n<p>3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<p>说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。</p>\n<p>4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</p>\n<p>5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。</p>\n<p>6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。</p>\n<p>7.【强制】不要在 finally 块中使用 return。<br>\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</p>\n<p>8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<p>9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。<br>\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。</p>\n<ol start=\"10\">\n<li>【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:<br>\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。<br>\n反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。</li>\n</ol>\n<ol start=\"2\">\n<li>数据库的查询结果可能为null。</li>\n<li>集合里的元素即使isNotEmpty，取出的数据元素也可能为null。</li>\n<li>远程调用返回对象时，一律要求进行空指针判断，防止NPE。</li>\n<li>对于Session中获取的数据，建议NPE检查，避免空指针。</li>\n<li>级联调用obj.getA().getB().getC();一连串调用，易产生NPE。<br>\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。</li>\n</ol>\n<p>11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。</p>\n<ol start=\"12\">\n<li>【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。<br>\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:<br>\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。</li>\n</ol>\n<p>13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。<br>\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。<br>\n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {…}</p>\n"},{"title":"SpringCloud服务构建","author":"郑天祺","date":"2020-12-14T03:30:00.000Z","_content":"\n# 1、添加依赖\n\n建议使用官方提供的在线地址进行工程的初始化创建：https://start.spring.io\n\n添加Spring Cloud和Spring Cloud Alibaba依赖管理：\n\t\n\n```java\n<properties>\n\t\t<java.version>1.8</java.version>\n\t\t<spring-cloud.version>Greenwich.SR3</spring-cloud.version>\n\t\t<spring-cloud-alibaba.version>0.9.0.RELEASE</spring-cloud-alibaba.version>\n</properties>\n\t<dependencyManagement>\n\t\t<dependencies>\n\t\t\t<!--Spring Cloud -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t\t<!--Spring Cloud Alibaba -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-alibaba-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud-alibaba.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t</dependencies>\n\t</dependencyManagement>\n```\n\n注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。\n\n# 2、application.yml配置\n\n注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。\n\n```java\n# 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #<负责人姓名>\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n```\n\n","source":"_posts/SpringCloud服务构建.md","raw":"title: SpringCloud服务构建\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:30:00\n---\n\n# 1、添加依赖\n\n建议使用官方提供的在线地址进行工程的初始化创建：https://start.spring.io\n\n添加Spring Cloud和Spring Cloud Alibaba依赖管理：\n\t\n\n```java\n<properties>\n\t\t<java.version>1.8</java.version>\n\t\t<spring-cloud.version>Greenwich.SR3</spring-cloud.version>\n\t\t<spring-cloud-alibaba.version>0.9.0.RELEASE</spring-cloud-alibaba.version>\n</properties>\n\t<dependencyManagement>\n\t\t<dependencies>\n\t\t\t<!--Spring Cloud -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t\t<!--Spring Cloud Alibaba -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-alibaba-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud-alibaba.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t</dependencies>\n\t</dependencyManagement>\n```\n\n注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。\n\n# 2、application.yml配置\n\n注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。\n\n```java\n# 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #<负责人姓名>\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n```\n\n","slug":"SpringCloud服务构建","published":1,"updated":"2022-04-04T08:32:40.156Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzs004x7kt96idget6k","content":"<h1>1、添加依赖</h1>\n<p>建议使用官方提供的在线地址进行工程的初始化创建：<a href=\"https://start.spring.io\">https://start.spring.io</a></p>\n<p>添加Spring Cloud和Spring Cloud Alibaba依赖管理：</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;spring-cloud.version&gt;Greenwich.SR3&lt;/spring-cloud.version&gt;\n\t\t&lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n&lt;/properties&gt;\n\t&lt;dependencyManagement&gt;\n\t\t&lt;dependencies&gt;\n\t\t\t&lt;!--Spring Cloud --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t\t&lt;!--Spring Cloud Alibaba --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t&lt;/dependencies&gt;\n\t&lt;/dependencyManagement&gt;\n</code></pre>\n<p>注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。</p>\n<h1>2、application.yml配置</h1>\n<p>注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。</p>\n<pre><code class=\"language-java\"># 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #&lt;负责人姓名&gt;\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、添加依赖</h1>\n<p>建议使用官方提供的在线地址进行工程的初始化创建：<a href=\"https://start.spring.io\">https://start.spring.io</a></p>\n<p>添加Spring Cloud和Spring Cloud Alibaba依赖管理：</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;spring-cloud.version&gt;Greenwich.SR3&lt;/spring-cloud.version&gt;\n\t\t&lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n&lt;/properties&gt;\n\t&lt;dependencyManagement&gt;\n\t\t&lt;dependencies&gt;\n\t\t\t&lt;!--Spring Cloud --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t\t&lt;!--Spring Cloud Alibaba --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t&lt;/dependencies&gt;\n\t&lt;/dependencyManagement&gt;\n</code></pre>\n<p>注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。</p>\n<h1>2、application.yml配置</h1>\n<p>注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。</p>\n<pre><code class=\"language-java\"># 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #&lt;负责人姓名&gt;\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n</code></pre>\n"},{"title":"SpringCloud服务注册","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery -->\n\t\t<dependency>\t\n            <groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n\n```\n\n# 2、配置文件添加注册服务器地址\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n```\n\n# 3、开启服务发现功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：\n\n```\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n\n}\n```\n\n# 4、配置服务接口\n\n```java\n@RestController\npublic class EchoController {\n\n    @RequestMapping(value = \"/echo/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return \"Hello Nacos Discovery \" + string;\n    }\n\n}\n```\n\n# 5、确认服务注册结果\n\n运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。\n\n![image-20201214122255720](/img/image-20201214122255720.png)","source":"_posts/SpringCloud服务注册.md","raw":"title: SpringCloud服务注册\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:32:00\n---\n\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery -->\n\t\t<dependency>\t\n            <groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n\n```\n\n# 2、配置文件添加注册服务器地址\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n```\n\n# 3、开启服务发现功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：\n\n```\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n\n}\n```\n\n# 4、配置服务接口\n\n```java\n@RestController\npublic class EchoController {\n\n    @RequestMapping(value = \"/echo/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return \"Hello Nacos Discovery \" + string;\n    }\n\n}\n```\n\n# 5、确认服务注册结果\n\n运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。\n\n![image-20201214122255720](/img/image-20201214122255720.png)","slug":"SpringCloud服务注册","published":1,"updated":"2022-04-04T08:32:40.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzt00517kt977pu01l8","content":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery --&gt;\n\t\t&lt;dependency&gt;\t\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\n</code></pre>\n<h1>2、配置文件添加注册服务器地址</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code>#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n</code></pre>\n<h1>3、开启服务发现功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：</p>\n<pre><code>@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n\n&#125;\n</code></pre>\n<h1>4、配置服务接口</h1>\n<pre><code class=\"language-java\">@RestController\npublic class EchoController &#123;\n\n    @RequestMapping(value = &quot;/echo/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return &quot;Hello Nacos Discovery &quot; + string;\n    &#125;\n\n&#125;\n</code></pre>\n<h1>5、确认服务注册结果</h1>\n<p>运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。</p>\n<p><img src=\"/img/image-20201214122255720.png\" alt=\"image-20201214122255720\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery --&gt;\n\t\t&lt;dependency&gt;\t\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\n</code></pre>\n<h1>2、配置文件添加注册服务器地址</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code>#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n</code></pre>\n<h1>3、开启服务发现功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：</p>\n<pre><code>@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n\n&#125;\n</code></pre>\n<h1>4、配置服务接口</h1>\n<pre><code class=\"language-java\">@RestController\npublic class EchoController &#123;\n\n    @RequestMapping(value = &quot;/echo/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return &quot;Hello Nacos Discovery &quot; + string;\n    &#125;\n\n&#125;\n</code></pre>\n<h1>5、确认服务注册结果</h1>\n<p>运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。</p>\n<p><img src=\"/img/image-20201214122255720.png\" alt=\"image-20201214122255720\"></p>\n"},{"title":"SpringCloud服务消费","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）\n\n# 1、pom.xml添加starter依赖\n\n```java\n\t\t<!-- Nacos服务发现 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n        <!-- 声明式调用 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n\t\t<!-- 负载均衡 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n\t\t</dependency>\n\t\t<!-- 熔断器 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n\t\t</dependency>\n```\n\n# 2、添加配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n```\n\n# 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.example\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.example\"})\npublic class NacosFeignDemoApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n1.\tname：服务提供者注册在服务注册中心的名称；\n2.\tfallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n3.\tfallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@FeignClient(name = \"nacos-provider-demo\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n\n```\n\n","source":"_posts/SpringCloud服务消费.md","raw":"title: SpringCloud服务消费\nauthor: 郑天祺\ntags:\n\n  - SpringCloud\ncategories: []\ndate: 2020-12-14 11:32:00\n\n---\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）\n\n# 1、pom.xml添加starter依赖\n\n```java\n\t\t<!-- Nacos服务发现 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n        <!-- 声明式调用 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n\t\t<!-- 负载均衡 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n\t\t</dependency>\n\t\t<!-- 熔断器 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n\t\t</dependency>\n```\n\n# 2、添加配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n```\n\n# 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.example\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.example\"})\npublic class NacosFeignDemoApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n1.\tname：服务提供者注册在服务注册中心的名称；\n2.\tfallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n3.\tfallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@FeignClient(name = \"nacos-provider-demo\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n\n```\n\n","slug":"SpringCloud服务消费","published":1,"updated":"2022-04-04T08:32:40.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzu00547kt9gbs61pd8","content":"<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）</p>\n<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">\t\t&lt;!-- Nacos服务发现 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n        &lt;!-- 声明式调用 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\t\t&lt;!-- 负载均衡 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\t\t&lt;!-- 熔断器 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置文件配置</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n</code></pre>\n<h1>3、开启服务发现、负载均衡、熔断器功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.example&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.example&quot;&#125;)\npublic class NacosFeignDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、创建服务代理类</h1>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：</p>\n<ol>\n<li>name：服务提供者注册在服务注册中心的名称；</li>\n<li>fallback：使用者提供的断路器实现，必须是当前代理类的实现类；</li>\n<li>fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。<br>\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</li>\n</ol>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;nacos-provider-demo&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）</p>\n<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">\t\t&lt;!-- Nacos服务发现 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n        &lt;!-- 声明式调用 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\t\t&lt;!-- 负载均衡 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\t\t&lt;!-- 熔断器 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置文件配置</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n</code></pre>\n<h1>3、开启服务发现、负载均衡、熔断器功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.example&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.example&quot;&#125;)\npublic class NacosFeignDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、创建服务代理类</h1>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：</p>\n<ol>\n<li>name：服务提供者注册在服务注册中心的名称；</li>\n<li>fallback：使用者提供的断路器实现，必须是当前代理类的实现类；</li>\n<li>fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。<br>\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</li>\n</ol>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;nacos-provider-demo&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n\n</code></pre>\n"},{"title":"SpringCloud管理配置页面","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n# 1、配置中心管理服务\n\n1、配置 ID：Data ID\n\n![image-20201214121332594](/img/image-20201214121332594.png)\n\n```java\n在 Nacos Spring Cloud 中，dataId 的完整格式如下：${prefix}-${spring.profile.active}.${file-extension}\n```\n\n```java\n${prefix} 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n${file-extension} 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n${spring.profile.active} 为为当前环境对应的 profile，如为空，则变为${prefix}-${spring.profile.active}.${file-extension}形式。\n```\n\n建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。\n全部字符小写。只允许英文字符和 4 种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过 256 字节。\n\n\n\n2、配置分组：group\n\n一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过128字节。\n\n![image-20201214121613146](/img/image-20201214121613146.png)\n\n3、配置格式\n\n可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。\n\n![image-20201214121645728](/img/image-20201214121645728.png)\n\n4、配置内容\n\n配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。\n\n![image-20201214121721100](/img/image-20201214121721100.png)","source":"_posts/SpringCloud管理配置页面.md","raw":"title: SpringCloud管理配置页面\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n# 1、配置中心管理服务\n\n1、配置 ID：Data ID\n\n![image-20201214121332594](/img/image-20201214121332594.png)\n\n```java\n在 Nacos Spring Cloud 中，dataId 的完整格式如下：${prefix}-${spring.profile.active}.${file-extension}\n```\n\n```java\n${prefix} 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n${file-extension} 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n${spring.profile.active} 为为当前环境对应的 profile，如为空，则变为${prefix}-${spring.profile.active}.${file-extension}形式。\n```\n\n建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。\n全部字符小写。只允许英文字符和 4 种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过 256 字节。\n\n\n\n2、配置分组：group\n\n一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过128字节。\n\n![image-20201214121613146](/img/image-20201214121613146.png)\n\n3、配置格式\n\n可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。\n\n![image-20201214121645728](/img/image-20201214121645728.png)\n\n4、配置内容\n\n配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。\n\n![image-20201214121721100](/img/image-20201214121721100.png)","slug":"SpringCloud管理配置页面","published":1,"updated":"2022-04-04T08:32:40.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzv00587kt96f6y15qc","content":"<h1>1、配置中心管理服务</h1>\n<p>1、配置 ID：Data ID</p>\n<p><img src=\"/img/image-20201214121332594.png\" alt=\"image-20201214121332594\"></p>\n<pre><code class=\"language-java\">在 Nacos Spring Cloud 中，dataId 的完整格式如下：$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;\n</code></pre>\n<pre><code class=\"language-java\">$&#123;prefix&#125; 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n$&#123;file-extension&#125; 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n$&#123;spring.profile.active&#125; 为为当前环境对应的 profile，如为空，则变为$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;形式。\n</code></pre>\n<p>建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。<br>\n全部字符小写。只允许英文字符和 4 种特殊字符（“.”、“:”、“-”、“_”），不超过 256 字节。</p>\n<p>2、配置分组：group</p>\n<p>一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（“.”、“:”、“-”、“_”），不超过128字节。</p>\n<p><img src=\"/img/image-20201214121613146.png\" alt=\"image-20201214121613146\"></p>\n<p>3、配置格式</p>\n<p>可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。</p>\n<p><img src=\"/img/image-20201214121645728.png\" alt=\"image-20201214121645728\"></p>\n<p>4、配置内容</p>\n<p>配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。</p>\n<p><img src=\"/img/image-20201214121721100.png\" alt=\"image-20201214121721100\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、配置中心管理服务</h1>\n<p>1、配置 ID：Data ID</p>\n<p><img src=\"/img/image-20201214121332594.png\" alt=\"image-20201214121332594\"></p>\n<pre><code class=\"language-java\">在 Nacos Spring Cloud 中，dataId 的完整格式如下：$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;\n</code></pre>\n<pre><code class=\"language-java\">$&#123;prefix&#125; 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n$&#123;file-extension&#125; 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n$&#123;spring.profile.active&#125; 为为当前环境对应的 profile，如为空，则变为$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;形式。\n</code></pre>\n<p>建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。<br>\n全部字符小写。只允许英文字符和 4 种特殊字符（“.”、“:”、“-”、“_”），不超过 256 字节。</p>\n<p>2、配置分组：group</p>\n<p>一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（“.”、“:”、“-”、“_”），不超过128字节。</p>\n<p><img src=\"/img/image-20201214121613146.png\" alt=\"image-20201214121613146\"></p>\n<p>3、配置格式</p>\n<p>可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。</p>\n<p><img src=\"/img/image-20201214121645728.png\" alt=\"image-20201214121645728\"></p>\n<p>4、配置内容</p>\n<p>配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。</p>\n<p><img src=\"/img/image-20201214121721100.png\" alt=\"image-20201214121721100\"></p>\n"},{"title":"SpringCloud运维接口","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n注意：默认端点 path 前面有一级 /actuator ，例如：http://服务地址/actuator/info\n\n```java\nEndpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/{name}\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/{name}\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n```\n\n","source":"_posts/SpringCloud运维接口.md","raw":"title: SpringCloud运维接口\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n注意：默认端点 path 前面有一级 /actuator ，例如：http://服务地址/actuator/info\n\n```java\nEndpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/{name}\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/{name}\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n```\n\n","slug":"SpringCloud运维接口","published":1,"updated":"2022-04-04T08:32:40.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzw005b7kt9a9um35yf","content":"<p>注意：默认端点 path 前面有一级 /actuator ，例如：<a href=\"http://xn--zfry9hnb732h/actuator/info\">http://服务地址/actuator/info</a></p>\n<pre><code class=\"language-java\">Endpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/&#123;name&#125;\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/&#123;name&#125;\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>注意：默认端点 path 前面有一级 /actuator ，例如：<a href=\"http://xn--zfry9hnb732h/actuator/info\">http://服务地址/actuator/info</a></p>\n<pre><code class=\"language-java\">Endpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/&#123;name&#125;\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/&#123;name&#125;\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n</code></pre>\n"},{"title":"TCP IP四层网络模型","author":"郑天祺","date":"2019-08-30T07:12:00.000Z","_content":"\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","source":"_posts/TCP-IP四层网络模型.md","raw":"title: TCP IP四层网络模型\nauthor: 郑天祺\ntags:\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:12:00\n\n---\n\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","slug":"TCP-IP四层网络模型","published":1,"updated":"2022-04-04T08:32:40.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzx005f7kt98cjuggo2","content":"<h2 id=\"1、用户发送请求\">1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"2、服务器接收请求\">2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"3、网络连接模型\">3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"4、使用协议进行通讯\">4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：<br>\n​\t1.流套接字（stream socket）-TCP<br>\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt=\"\"></p>\n<h2 id=\"5、Socket通讯模型\">5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、用户发送请求\">1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"2、服务器接收请求\">2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"3、网络连接模型\">3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"4、使用协议进行通讯\">4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：<br>\n​\t1.流套接字（stream socket）-TCP<br>\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt=\"\"></p>\n<h2 id=\"5、Socket通讯模型\">5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt=\"\"></p>\n"},{"title":"TCP与UDP的区别","author":"郑天祺","date":"2020-07-21T00:00:00.000Z","_content":"\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","source":"_posts/TCP与UDP的区别.md","raw":"title: TCP与UDP的区别\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\n  - UDP\ncategories:\n  - 网络\ndate: 2020-07-21 08:00:00\n\n---\n\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","slug":"TCP与UDP的区别","published":1,"updated":"2022-04-04T08:32:40.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzy005i7kt9cpfv0rra","content":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2 id=\"（1）面向无连接\">（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了<br>\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2 id=\"（2）单播、多播、广播\">（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2 id=\"（3）不可靠性\">（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2 id=\"（4）头部开销小、传输高效\">（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口<br>\n​\t\t整个数据报文的长度<br>\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误<br>\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2 id=\"（1）TCP连接\">（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2 id=\"（2）特点\">（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：<br>\nTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：<br>\nTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2 id=\"4、对比\">4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为<br>\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2 id=\"（1）面向无连接\">（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了<br>\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2 id=\"（2）单播、多播、广播\">（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2 id=\"（3）不可靠性\">（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2 id=\"（4）头部开销小、传输高效\">（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口<br>\n​\t\t整个数据报文的长度<br>\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误<br>\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2 id=\"（1）TCP连接\">（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2 id=\"（2）特点\">（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：<br>\nTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：<br>\nTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2 id=\"4、对比\">4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为<br>\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n"},{"title":"TCP握手、挥手协议","author":"郑天祺","date":"2019-08-30T07:58:00.000Z","_content":"\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","source":"_posts/TCP握手、挥手协议.md","raw":"title: TCP握手、挥手协议\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:58:00\n\n---\n\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","slug":"TCP握手、挥手协议","published":1,"updated":"2022-04-04T08:32:40.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzy005m7kt92m7i2pjh","content":"<h2 id=\"1、TCP三次握手协议（打开连接）\">1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt=\"\"></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<h2 id=\"2、TCP四次挥手协议（关闭连接）\">2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt=\"\"></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3 id=\"（2）四次挥手\">（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、TCP三次握手协议（打开连接）\">1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt=\"\"></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<h2 id=\"2、TCP四次挥手协议（关闭连接）\">2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt=\"\"></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3 id=\"（2）四次挥手\">（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n"},{"title":"ThreadLocal","author":"郑天祺","date":"2020-03-28T01:59:00.000Z","_content":"\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","source":"_posts/ThreadLocal.md","raw":"title: ThreadLocal\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-03-28 09:59:00\n\n---\n\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","slug":"ThreadLocal","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcnnzz005o7kt9haoig9h5","content":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取<br>\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest &#123;\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) &#123;\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal2&quot;).start();\n    &#125;\n&#125;\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： <a href=\"http://www.jiangxinlingdu.com/\">http://www.jiangxinlingdu.com/</a> 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n","site":{"data":{}},"excerpt":"","more":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取<br>\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest &#123;\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) &#123;\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal2&quot;).start();\n    &#125;\n&#125;\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： <a href=\"http://www.jiangxinlingdu.com/\">http://www.jiangxinlingdu.com/</a> 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n"},{"title":"Tomcat性能优化整理","author":"郑天祺","date":"2020-11-17T04:34:00.000Z","_content":"\n# 1、JVM参数调优\n\n```java\n-Xms<size>\n```\n\n表示JVM初始化堆的大小\n\n```java\n-Xmx<size>\n```\n\n表示JVM堆的最大值\n\n​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。\n\n​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。\n\n# 2、禁用DNS查询\n\n​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。\n\n​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。\n\n​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。\n\n# 3、调整线程数\n\n​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。\n\n​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。\n\n​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。\n\n​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。\n\n​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。\n\n​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。\n\n​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。\n\n在Tomcat5对这些参数进行了调整，请看下面属性:\n\n​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。\n\n​\t\tminSpareThreadsTomcat初始化时创建的线程数。\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。\n\n​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。\n\n# 4、加大Tomcat内存\n\n​\t\t首先检查程序有没有限入死循环\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。\n\n​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。\n\n跟据网上的答案大致有这两种解决方法:\n\n（1）设置环境变量\n\t\t解决方法:手动设置Heap size\n\t\t修改TOMCAT_HOME/bin/catalina.sh\n\n​\t\tset JAVA_OPTS=-Xms32m-Xmx512m\n\n​\t\t可以根据自己机器的内存进行更改。\n\n（2）java-Xms32m-Xmx800m className\n\t\t就是在执行JAVA类文件时加上这个参数，其中className\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。\n\n​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse->run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。\n\n（3）java.lang.OutOfMemoryError: PermGen space\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。\n解决方法:手动设置MaxPermSize大小修\n\n（4）java.lang.OutOfMemoryError:Java heap space Heap size设置\n\t\tJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。\n\n​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。\n解决方法:手动设置Heap size\n修改 TOMCAT_HOME/bin/catalina.sh\n在“echo\"Using CATALINA_BASE:$CATALINA_BASE\"\"上面加入以下行:JAVA_OPTS=\"-server-Xms800m-Xmx800m-XX:MaxNewSize=256m\"\n\n\n\n\n\n\n\n5、","source":"_posts/Tomcat性能优化整理.md","raw":"title: Tomcat性能优化整理\nauthor: 郑天祺\ntags:\n\n  - tomcat\ncategories:\n  - 面试\ndate: 2020-11-17 12:34:00\n\n---\n\n# 1、JVM参数调优\n\n```java\n-Xms<size>\n```\n\n表示JVM初始化堆的大小\n\n```java\n-Xmx<size>\n```\n\n表示JVM堆的最大值\n\n​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。\n\n​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。\n\n# 2、禁用DNS查询\n\n​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。\n\n​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。\n\n​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。\n\n# 3、调整线程数\n\n​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。\n\n​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。\n\n​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。\n\n​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。\n\n​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。\n\n​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。\n\n​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。\n\n在Tomcat5对这些参数进行了调整，请看下面属性:\n\n​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。\n\n​\t\tminSpareThreadsTomcat初始化时创建的线程数。\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。\n\n​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。\n\n# 4、加大Tomcat内存\n\n​\t\t首先检查程序有没有限入死循环\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。\n\n​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。\n\n跟据网上的答案大致有这两种解决方法:\n\n（1）设置环境变量\n\t\t解决方法:手动设置Heap size\n\t\t修改TOMCAT_HOME/bin/catalina.sh\n\n​\t\tset JAVA_OPTS=-Xms32m-Xmx512m\n\n​\t\t可以根据自己机器的内存进行更改。\n\n（2）java-Xms32m-Xmx800m className\n\t\t就是在执行JAVA类文件时加上这个参数，其中className\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。\n\n​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse->run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。\n\n（3）java.lang.OutOfMemoryError: PermGen space\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。\n解决方法:手动设置MaxPermSize大小修\n\n（4）java.lang.OutOfMemoryError:Java heap space Heap size设置\n\t\tJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。\n\n​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。\n解决方法:手动设置Heap size\n修改 TOMCAT_HOME/bin/catalina.sh\n在“echo\"Using CATALINA_BASE:$CATALINA_BASE\"\"上面加入以下行:JAVA_OPTS=\"-server-Xms800m-Xmx800m-XX:MaxNewSize=256m\"\n\n\n\n\n\n\n\n5、","slug":"Tomcat性能优化整理","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno00005t7kt9anh540n9","content":"<h1>1、JVM参数调优</h1>\n<pre><code class=\"language-java\">-Xms&lt;size&gt;\n</code></pre>\n<p>表示JVM初始化堆的大小</p>\n<pre><code class=\"language-java\">-Xmx&lt;size&gt;\n</code></pre>\n<p>表示JVM堆的最大值</p>\n<p>​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。</p>\n<p>​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。</p>\n<h1>2、禁用DNS查询</h1>\n<p>​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。</p>\n<p>​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。</p>\n<p>​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。</p>\n<h1>3、调整线程数</h1>\n<p>​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。</p>\n<p>​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。</p>\n<p>​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。</p>\n<p>​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。</p>\n<p>​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。</p>\n<p>​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。</p>\n<p>​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。</p>\n<p>在Tomcat5对这些参数进行了调整，请看下面属性:</p>\n<p>​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。<br>\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。<br>\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。</p>\n<p>​\t\tminSpareThreadsTomcat初始化时创建的线程数。<br>\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。</p>\n<p>​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。</p>\n<h1>4、加大Tomcat内存</h1>\n<p>​\t\t首先检查程序有没有限入死循环<br>\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。</p>\n<p>​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。</p>\n<p>跟据网上的答案大致有这两种解决方法:</p>\n<p>（1）设置环境变量<br>\n解决方法:手动设置Heap size<br>\n修改TOMCAT_HOME/bin/catalina.sh</p>\n<p>​\t\tset JAVA_OPTS=-Xms32m-Xmx512m</p>\n<p>​\t\t可以根据自己机器的内存进行更改。</p>\n<p>（2）java-Xms32m-Xmx800m className<br>\n就是在执行JAVA类文件时加上这个参数，其中className<br>\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。</p>\n<p>​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse-&gt;run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。</p>\n<p>（3）java.lang.OutOfMemoryError: PermGen space<br>\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。<br>\n解决方法:手动设置MaxPermSize大小修</p>\n<p>（4）java.lang.OutOfMemoryError:Java heap space Heap size设置<br>\nJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。</p>\n<p>​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。<br>\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。<br>\n解决方法:手动设置Heap size<br>\n修改 TOMCAT_HOME/bin/catalina.sh<br>\n在“echo&quot;Using CATALINA_BASE:$CATALINA_BASE&quot;“上面加入以下行:JAVA_OPTS=”-server-Xms800m-Xmx800m-XX:MaxNewSize=256m&quot;</p>\n<p>5、</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、JVM参数调优</h1>\n<pre><code class=\"language-java\">-Xms&lt;size&gt;\n</code></pre>\n<p>表示JVM初始化堆的大小</p>\n<pre><code class=\"language-java\">-Xmx&lt;size&gt;\n</code></pre>\n<p>表示JVM堆的最大值</p>\n<p>​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。</p>\n<p>​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。</p>\n<h1>2、禁用DNS查询</h1>\n<p>​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。</p>\n<p>​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。</p>\n<p>​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。</p>\n<h1>3、调整线程数</h1>\n<p>​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。</p>\n<p>​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。</p>\n<p>​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。</p>\n<p>​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。</p>\n<p>​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。</p>\n<p>​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。</p>\n<p>​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。</p>\n<p>在Tomcat5对这些参数进行了调整，请看下面属性:</p>\n<p>​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。<br>\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。<br>\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。</p>\n<p>​\t\tminSpareThreadsTomcat初始化时创建的线程数。<br>\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。</p>\n<p>​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。</p>\n<h1>4、加大Tomcat内存</h1>\n<p>​\t\t首先检查程序有没有限入死循环<br>\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。</p>\n<p>​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。</p>\n<p>跟据网上的答案大致有这两种解决方法:</p>\n<p>（1）设置环境变量<br>\n解决方法:手动设置Heap size<br>\n修改TOMCAT_HOME/bin/catalina.sh</p>\n<p>​\t\tset JAVA_OPTS=-Xms32m-Xmx512m</p>\n<p>​\t\t可以根据自己机器的内存进行更改。</p>\n<p>（2）java-Xms32m-Xmx800m className<br>\n就是在执行JAVA类文件时加上这个参数，其中className<br>\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。</p>\n<p>​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse-&gt;run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。</p>\n<p>（3）java.lang.OutOfMemoryError: PermGen space<br>\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。<br>\n解决方法:手动设置MaxPermSize大小修</p>\n<p>（4）java.lang.OutOfMemoryError:Java heap space Heap size设置<br>\nJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。</p>\n<p>​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。<br>\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。<br>\n解决方法:手动设置Heap size<br>\n修改 TOMCAT_HOME/bin/catalina.sh<br>\n在“echo&quot;Using CATALINA_BASE:$CATALINA_BASE&quot;“上面加入以下行:JAVA_OPTS=”-server-Xms800m-Xmx800m-XX:MaxNewSize=256m&quot;</p>\n<p>5、</p>\n"},{"title":"Yarn概述","author":"郑天祺","date":"2019-12-18T01:12:00.000Z","_content":"\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","source":"_posts/Yarn概述.md","raw":"title: Yarn概述\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 09:12:00\n\n---\n\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","slug":"Yarn概述","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno02005w7kt95m2ca9ms","content":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2 id=\"（1）Resource-Manager\">（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2 id=\"（2）Application-Master\">（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2 id=\"（3）Node-Manager\">（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2 id=\"（4）Resource-Request-和-Container\">（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2 id=\"（1）Resource-Manager\">（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2 id=\"（2）Application-Master\">（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2 id=\"（3）Node-Manager\">（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2 id=\"（4）Resource-Request-和-Container\">（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n"},{"title":"WordCount简析","author":"郑天祺","date":"2019-12-18T03:57:00.000Z","_content":"\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","source":"_posts/WordCount简析.md","raw":"title: WordCount简析\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 11:57:00\n\n---\n\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","slug":"WordCount简析","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0300617kt9a21a76xa","content":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount &#123;\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException &#123;\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) &#123;\n                word.set(itr.nextToken());\n                context.write(word, one);\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException &#123;\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) &#123;\n                sum += val.get();\n            &#125;\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        &#125;\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) &#123;\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        &#125;\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        &#125;\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    &#125;\n&#125;\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount &#123;\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException &#123;\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) &#123;\n                word.set(itr.nextToken());\n                context.write(word, one);\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException &#123;\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) &#123;\n                sum += val.get();\n            &#125;\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        &#125;\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) &#123;\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        &#125;\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        &#125;\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    &#125;\n&#125;\n\n</code></pre>\n"},{"title":"docker本机打镜像","author":"ztq","date":"2021-07-18T11:18:00.000Z","_content":"\n# 1、安装docker\n\n## （1）安装Gcc\n\n```java\nyum -y install gcc\nyum -y install gcc-c++\n```\n\n## （2）卸载旧版本\n\n```java\nyum -y remove docker docker-common docker-selinux docker-engine\n\nyum remove\ndocker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-engine\n```\n\n## （3）设置stable镜像仓库\n\n```java\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n\n## （4）更新yum软件包索引\n\n```java\nyum makecache (centos 8)\nyum makecache fast(centos 7)\n```\n\n## （5）安装Docker CE\n\n```java\nyum -y install docker-ce docker-ce-cli containerd.io\n```\n\n\n\n## （6）启动docker\n\n```java\n启动：\nsystemctl start docker\n设置开机自启动：\nsystemctl start docker\nsystemctl enable docker\n```\n\n```\n测试：\ndocker version\ndocker run hello-world\ndocker images\n```\n\n```java\n[root@localhost ~]# docker version\nClient: Docker Engine - Community\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.13.15\n Git commit:        f0df350\n Built:             Wed Jun  2 11:56:24 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:54:48 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0-rc95\n  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n## 2、开启2375端口，提供外部访问docker\n\n在docker配置文件\n\n```java\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \n```\n\n之后加上\n\n```java\n-H tcp://0.0.0.0:2375\n```\n\n具体操作为\n\n```java\nvim /usr/lib/systemd/system/docker.service\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375\n```\n\n```\nsystemctl restart docker\n```\n\n然后linux防火墙开启2375端口\n\n# 3、使用idea连接docker服务\n\nidea安装docker服务\n\nidea -----> setting -----> Plugins -----> Marketplace -----> docker install\n\n![image-20210718193618402](/img/image-20210718193618402.png)\n\nidea -----> setting -----> Build, Execution, Deployment - Docker -----> TCP socket -----> Engine API URL 键入 tcp://192.168.2.195:2375\n\n![image-20210718193314796](/img/image-20210718193314796.png)\n\n# 4、配置\n\npom和Dockerfile在同一目录下：\n\npom中添加：\n\n```java\n<!--docke maven编译插件-->\n            <plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>dockerfile-maven-plugin</artifactId>\n                <version>1.4.9</version>\n                <configuration>\n                    <repository>${docker.image.prefix}/${project.artifactId}</repository>\n                    <buildArgs>\n                        <JAR_FILE>target/${project.build.finalName}.jar</JAR_FILE>\n                    </buildArgs>\n                </configuration>\n            </plugin>\n```\n\nDockerfile：\n\n```java\n# docker中的镜像\nFROM openjdk:8-jdk-alpine\nMAINTAINER zhengtianqi <270490096@qq.com>\nVOLUME /tmp\nADD target/springcloud-ztq.jar app.jar\nEXPOSE 8080\nENTRYPOINT [\"java\",\"-jar\",\"app.jar\"]\n```\n\n执行的命令：\n\n```java\nwindows进行打包并上传镜像：\nDOCKER_HOST=tcp://92.168.2.195:2375 mvn clean package dockerfile:build\nlinux查看镜像并运行：\n\n[root@localhost ~]# docker images\nREPOSITORY                      TAG            IMAGE ID       CREATED       SIZE\nspringboot-ztq/springboot-ztq   latest         825d7f3967cf   3 hours ago   221MB\nopenjdk                         8-jdk-alpine   a3562aa0b991   2 years ago   105MB\n\n[root@localhost ~]# docker run -d -p 8080:8080 -i 825d7f3967cf\n```\n\n","source":"_posts/docker20-10-7本机开启2375配置.md","raw":"title: docker本机打镜像\nauthor: ztq\ntags:\n\n  - docker\ncategories:\n  - CICD\ndate: 2021-07-18 19:18:00\n---\n\n# 1、安装docker\n\n## （1）安装Gcc\n\n```java\nyum -y install gcc\nyum -y install gcc-c++\n```\n\n## （2）卸载旧版本\n\n```java\nyum -y remove docker docker-common docker-selinux docker-engine\n\nyum remove\ndocker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-engine\n```\n\n## （3）设置stable镜像仓库\n\n```java\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n\n## （4）更新yum软件包索引\n\n```java\nyum makecache (centos 8)\nyum makecache fast(centos 7)\n```\n\n## （5）安装Docker CE\n\n```java\nyum -y install docker-ce docker-ce-cli containerd.io\n```\n\n\n\n## （6）启动docker\n\n```java\n启动：\nsystemctl start docker\n设置开机自启动：\nsystemctl start docker\nsystemctl enable docker\n```\n\n```\n测试：\ndocker version\ndocker run hello-world\ndocker images\n```\n\n```java\n[root@localhost ~]# docker version\nClient: Docker Engine - Community\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.13.15\n Git commit:        f0df350\n Built:             Wed Jun  2 11:56:24 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:54:48 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0-rc95\n  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n## 2、开启2375端口，提供外部访问docker\n\n在docker配置文件\n\n```java\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \n```\n\n之后加上\n\n```java\n-H tcp://0.0.0.0:2375\n```\n\n具体操作为\n\n```java\nvim /usr/lib/systemd/system/docker.service\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375\n```\n\n```\nsystemctl restart docker\n```\n\n然后linux防火墙开启2375端口\n\n# 3、使用idea连接docker服务\n\nidea安装docker服务\n\nidea -----> setting -----> Plugins -----> Marketplace -----> docker install\n\n![image-20210718193618402](/img/image-20210718193618402.png)\n\nidea -----> setting -----> Build, Execution, Deployment - Docker -----> TCP socket -----> Engine API URL 键入 tcp://192.168.2.195:2375\n\n![image-20210718193314796](/img/image-20210718193314796.png)\n\n# 4、配置\n\npom和Dockerfile在同一目录下：\n\npom中添加：\n\n```java\n<!--docke maven编译插件-->\n            <plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>dockerfile-maven-plugin</artifactId>\n                <version>1.4.9</version>\n                <configuration>\n                    <repository>${docker.image.prefix}/${project.artifactId}</repository>\n                    <buildArgs>\n                        <JAR_FILE>target/${project.build.finalName}.jar</JAR_FILE>\n                    </buildArgs>\n                </configuration>\n            </plugin>\n```\n\nDockerfile：\n\n```java\n# docker中的镜像\nFROM openjdk:8-jdk-alpine\nMAINTAINER zhengtianqi <270490096@qq.com>\nVOLUME /tmp\nADD target/springcloud-ztq.jar app.jar\nEXPOSE 8080\nENTRYPOINT [\"java\",\"-jar\",\"app.jar\"]\n```\n\n执行的命令：\n\n```java\nwindows进行打包并上传镜像：\nDOCKER_HOST=tcp://92.168.2.195:2375 mvn clean package dockerfile:build\nlinux查看镜像并运行：\n\n[root@localhost ~]# docker images\nREPOSITORY                      TAG            IMAGE ID       CREATED       SIZE\nspringboot-ztq/springboot-ztq   latest         825d7f3967cf   3 hours ago   221MB\nopenjdk                         8-jdk-alpine   a3562aa0b991   2 years ago   105MB\n\n[root@localhost ~]# docker run -d -p 8080:8080 -i 825d7f3967cf\n```\n\n","slug":"docker20-10-7本机开启2375配置","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0400647kt95x5048oj","content":"<h1>1、安装docker</h1>\n<h2 id=\"（1）安装Gcc\">（1）安装Gcc</h2>\n<pre><code class=\"language-java\">yum -y install gcc\nyum -y install gcc-c++\n</code></pre>\n<h2 id=\"（2）卸载旧版本\">（2）卸载旧版本</h2>\n<pre><code class=\"language-java\">yum -y remove docker docker-common docker-selinux docker-engine\n\nyum remove\ndocker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-engine\n</code></pre>\n<h2 id=\"（3）设置stable镜像仓库\">（3）设置stable镜像仓库</h2>\n<pre><code class=\"language-java\">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<h2 id=\"（4）更新yum软件包索引\">（4）更新yum软件包索引</h2>\n<pre><code class=\"language-java\">yum makecache (centos 8)\nyum makecache fast(centos 7)\n</code></pre>\n<h2 id=\"（5）安装Docker-CE\">（5）安装Docker CE</h2>\n<pre><code class=\"language-java\">yum -y install docker-ce docker-ce-cli containerd.io\n</code></pre>\n<h2 id=\"（6）启动docker\">（6）启动docker</h2>\n<pre><code class=\"language-java\">启动：\nsystemctl start docker\n设置开机自启动：\nsystemctl start docker\nsystemctl enable docker\n</code></pre>\n<pre><code>测试：\ndocker version\ndocker run hello-world\ndocker images\n</code></pre>\n<pre><code class=\"language-java\">[root@localhost ~]# docker version\nClient: Docker Engine - Community\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.13.15\n Git commit:        f0df350\n Built:             Wed Jun  2 11:56:24 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:54:48 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0-rc95\n  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre>\n<h2 id=\"2、开启2375端口，提供外部访问docker\">2、开启2375端口，提供外部访问docker</h2>\n<p>在docker配置文件</p>\n<pre><code class=\"language-java\">ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \n</code></pre>\n<p>之后加上</p>\n<pre><code class=\"language-java\">-H tcp://0.0.0.0:2375\n</code></pre>\n<p>具体操作为</p>\n<pre><code class=\"language-java\">vim /usr/lib/systemd/system/docker.service\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375\n</code></pre>\n<pre><code>systemctl restart docker\n</code></pre>\n<p>然后linux防火墙开启2375端口</p>\n<h1>3、使用idea连接docker服务</h1>\n<p>idea安装docker服务</p>\n<p>idea -----&gt; setting -----&gt; Plugins -----&gt; Marketplace -----&gt; docker install</p>\n<p><img src=\"/img/image-20210718193618402.png\" alt=\"image-20210718193618402\"></p>\n<p>idea -----&gt; setting -----&gt; Build, Execution, Deployment - Docker -----&gt; TCP socket -----&gt; Engine API URL 键入 tcp://192.168.2.195:2375</p>\n<p><img src=\"/img/image-20210718193314796.png\" alt=\"image-20210718193314796\"></p>\n<h1>4、配置</h1>\n<p>pom和Dockerfile在同一目录下：</p>\n<p>pom中添加：</p>\n<pre><code class=\"language-java\">&lt;!--docke maven编译插件--&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;com.spotify&lt;/groupId&gt;\n                &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt;\n                &lt;version&gt;1.4.9&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;repository&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/repository&gt;\n                    &lt;buildArgs&gt;\n                        &lt;JAR_FILE&gt;target/$&#123;project.build.finalName&#125;.jar&lt;/JAR_FILE&gt;\n                    &lt;/buildArgs&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n</code></pre>\n<p>Dockerfile：</p>\n<pre><code class=\"language-java\"># docker中的镜像\nFROM openjdk:8-jdk-alpine\nMAINTAINER zhengtianqi &lt;270490096@qq.com&gt;\nVOLUME /tmp\nADD target/springcloud-ztq.jar app.jar\nEXPOSE 8080\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n</code></pre>\n<p>执行的命令：</p>\n<pre><code class=\"language-java\">windows进行打包并上传镜像：\nDOCKER_HOST=tcp://92.168.2.195:2375 mvn clean package dockerfile:build\nlinux查看镜像并运行：\n\n[root@localhost ~]# docker images\nREPOSITORY                      TAG            IMAGE ID       CREATED       SIZE\nspringboot-ztq/springboot-ztq   latest         825d7f3967cf   3 hours ago   221MB\nopenjdk                         8-jdk-alpine   a3562aa0b991   2 years ago   105MB\n\n[root@localhost ~]# docker run -d -p 8080:8080 -i 825d7f3967cf\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、安装docker</h1>\n<h2 id=\"（1）安装Gcc\">（1）安装Gcc</h2>\n<pre><code class=\"language-java\">yum -y install gcc\nyum -y install gcc-c++\n</code></pre>\n<h2 id=\"（2）卸载旧版本\">（2）卸载旧版本</h2>\n<pre><code class=\"language-java\">yum -y remove docker docker-common docker-selinux docker-engine\n\nyum remove\ndocker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-engine\n</code></pre>\n<h2 id=\"（3）设置stable镜像仓库\">（3）设置stable镜像仓库</h2>\n<pre><code class=\"language-java\">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<h2 id=\"（4）更新yum软件包索引\">（4）更新yum软件包索引</h2>\n<pre><code class=\"language-java\">yum makecache (centos 8)\nyum makecache fast(centos 7)\n</code></pre>\n<h2 id=\"（5）安装Docker-CE\">（5）安装Docker CE</h2>\n<pre><code class=\"language-java\">yum -y install docker-ce docker-ce-cli containerd.io\n</code></pre>\n<h2 id=\"（6）启动docker\">（6）启动docker</h2>\n<pre><code class=\"language-java\">启动：\nsystemctl start docker\n设置开机自启动：\nsystemctl start docker\nsystemctl enable docker\n</code></pre>\n<pre><code>测试：\ndocker version\ndocker run hello-world\ndocker images\n</code></pre>\n<pre><code class=\"language-java\">[root@localhost ~]# docker version\nClient: Docker Engine - Community\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.13.15\n Git commit:        f0df350\n Built:             Wed Jun  2 11:56:24 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:54:48 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0-rc95\n  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre>\n<h2 id=\"2、开启2375端口，提供外部访问docker\">2、开启2375端口，提供外部访问docker</h2>\n<p>在docker配置文件</p>\n<pre><code class=\"language-java\">ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \n</code></pre>\n<p>之后加上</p>\n<pre><code class=\"language-java\">-H tcp://0.0.0.0:2375\n</code></pre>\n<p>具体操作为</p>\n<pre><code class=\"language-java\">vim /usr/lib/systemd/system/docker.service\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375\n</code></pre>\n<pre><code>systemctl restart docker\n</code></pre>\n<p>然后linux防火墙开启2375端口</p>\n<h1>3、使用idea连接docker服务</h1>\n<p>idea安装docker服务</p>\n<p>idea -----&gt; setting -----&gt; Plugins -----&gt; Marketplace -----&gt; docker install</p>\n<p><img src=\"/img/image-20210718193618402.png\" alt=\"image-20210718193618402\"></p>\n<p>idea -----&gt; setting -----&gt; Build, Execution, Deployment - Docker -----&gt; TCP socket -----&gt; Engine API URL 键入 tcp://192.168.2.195:2375</p>\n<p><img src=\"/img/image-20210718193314796.png\" alt=\"image-20210718193314796\"></p>\n<h1>4、配置</h1>\n<p>pom和Dockerfile在同一目录下：</p>\n<p>pom中添加：</p>\n<pre><code class=\"language-java\">&lt;!--docke maven编译插件--&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;com.spotify&lt;/groupId&gt;\n                &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt;\n                &lt;version&gt;1.4.9&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;repository&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/repository&gt;\n                    &lt;buildArgs&gt;\n                        &lt;JAR_FILE&gt;target/$&#123;project.build.finalName&#125;.jar&lt;/JAR_FILE&gt;\n                    &lt;/buildArgs&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n</code></pre>\n<p>Dockerfile：</p>\n<pre><code class=\"language-java\"># docker中的镜像\nFROM openjdk:8-jdk-alpine\nMAINTAINER zhengtianqi &lt;270490096@qq.com&gt;\nVOLUME /tmp\nADD target/springcloud-ztq.jar app.jar\nEXPOSE 8080\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n</code></pre>\n<p>执行的命令：</p>\n<pre><code class=\"language-java\">windows进行打包并上传镜像：\nDOCKER_HOST=tcp://92.168.2.195:2375 mvn clean package dockerfile:build\nlinux查看镜像并运行：\n\n[root@localhost ~]# docker images\nREPOSITORY                      TAG            IMAGE ID       CREATED       SIZE\nspringboot-ztq/springboot-ztq   latest         825d7f3967cf   3 hours ago   221MB\nopenjdk                         8-jdk-alpine   a3562aa0b991   2 years ago   105MB\n\n[root@localhost ~]# docker run -d -p 8080:8080 -i 825d7f3967cf\n</code></pre>\n"},{"title":"docker镜像部署到k8s集群","author":"ztq","date":"2022-04-11T13:20:00.000Z","_content":"\n# 一、搭建docker私有仓库\n\n本文使用阿里云私有仓库（个人免费版），阿里云--->容器镜像服务\n\n![image-20220411212506831](/img/image-20220411212506831.png)\n\n# 二、docker打包\n\n1、程序目录\n\n```java\nll\n    \n-rw-r--r-- 1 root root      872 4月  11 22:03 dockerfile\n-rw-r--r-- 1 root root     8678 4月  11 21:53 jdk-8u251-linux-x64.tar.gz\n-rw-r--r-- 1 root root 75306958 4月  11 21:47 ruoyi.jar\n```\n\n![image-20220411220547876](/img/image-20220411220512656.png)\n\n2、编写dockerfile并打包\n\n```java\nFROM uquote/jdk8-tomee1.7.3-plume\n## 编写人\nMAINTAINER zhengtianqi\n# 在docker容器构建时拷贝程序\nADD ruoyi.jar ruoyi.jar\n# 容器暴露的端口号，需要与jar包在容器中运行使用端口号一致\nEXPOSE 80\n# 容器启动之后执行的命令， java -jar ROOT.jar\nENTRYPOINT [\"java\",\"-jar\",\"ruoyi.jar\"]\n```\n\n```java\n# 打包\ndocker build -t test:latest .\n# 查看\ndocker images\n```\n\n3、登录阿里云镜像仓库\n\n```java\ndocker login --username=z130****66717 registry.cn-beijing.aliyuncs.com\n```\n\n4、将镜像推送到阿里云镜像仓库Registry\n\n```java\ndocker tag [ImageId] registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\ndocker push registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\n```\n\n5、查看阿里云镜像仓库Registry中的镜像\n\n![image-20220411221525635](/img/image-20220411221525635.png)\n\n6、拉取镜像\n\n```java\ndocker pull registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n```\n\n# 三、k8s\n\n1、创建yaml文件\n\n```java\nkubectl create deployment [pod名称] --image=[docker私有仓库地址/镜像:版本号] -o yaml --dry-run=client > [yaml文件名称].yaml\n示例：\nkubectl create deployment demo --port=80 --replicas=3 --image=registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest -o yaml --dry-run=client > demo.yaml\n```\n\n2、查看yaml文件\n\n```java\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: demo\n  name: demo\nspec:\n  replicas: 3\n  ports:\n  - port: 80\n    targetPort: 80  \n  selector:\n    matchLabels:\n      app: demo\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: demo\n    spec:\n      containers:\n      - image: registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n        name: demo\n        resources: {}\nstatus: {}\n```\n\n3、通过yaml创建服务\n\n```java\nkubectl create -f demo.yaml\n```\n\n![image-20220411222247494](/img/image-20220411222247494.png)\n\n4、查看创建状态\n\n```java\nkubectl get pod\n```\n\n5、创建日志查看\n\n```java\nkubectl describe pod\n```\n\n6、杀死当前pod\n\n```\nkubectl delete deployment <deployment名> -n <namespace>\n```\n\n7、pod升级\n\n[Kubernetes--k8s---滚动更新--零停机不停服发布服务](https://blog.csdn.net/zzq900503/article/details/101221899)\n\n[Kubernetes滚动更新（无中断平滑发布） (idcsec.com)](http://idcsec.com/2019/03/05/kubernetes滚动更新（无中断平滑发布）/)\n\n# 附：yaml文件详解\n\n```java\n# yaml格式的pod定义文件完整内容：\napiVersion: v1           #必选，版本号，例如v1\nkind: Pod                #必选，Pod\nmetadata:                #必选，元数据\n  name: string           #必选，Pod名称\n  namespace: string      #必选，Pod所属的命名空间\n  labels:                #自定义标签\n    - name: string       #自定义标签名字\n  annotations:           #自定义注释列表\n    - name: string\nspec:                    #必选，Pod中容器的详细定义\n  containers:            #必选，Pod中容器列表\n  - name: string         #必选，容器名称\n    image: string        #必选，容器的镜像名称\n    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像\n    command: [string]                  #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]                     #容器的启动命令参数列表\n    workingDir: string                 #容器的工作目录\n    volumeMounts:                      #挂载到容器内部的存储卷配置\n    - name: string                     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string               #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean               #是否为只读模式\n    ports:                           #需要暴露的端口库号列表\n    - name: string                   #端口号名称\n      containerPort: int             #容器需要监听的端口号\n      hostPort: int                  #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string               #端口协议，支持TCP和UDP，默认TCP\n    env:                             #容器运行前需设置的环境变量列表\n    - name: string                   #环境变量名称\n      value: string                  #环境变量的值\n    resources:                       #资源限制和请求的设置\n      limits:                        #资源限制的设置\n        cpu: string                  #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string               #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests:                       #资源请求的设置\n        cpu: string                   #Cpu请求，容器启动的初始可用数量\n        memory: string                #内存请求，容器启动的初始可用数量\n    livenessProbe:                    #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可\n      exec:                           #对Pod容器内检查方式设置为exec方式\n        command: [string]             #exec方式需要制定的命令或脚本\n      httpGet:                        #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:                       #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0            #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0             #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged:false\n    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod\n    nodeSelector: obeject          #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定\n    imagePullSecrets:              #Pull镜像时使用的secret名称，以key：secretkey格式指定\n    - name: string\n    hostNetwork:false     #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n    volumes:                    #在该pod上定义共享存储卷列表\n    - name: string              #共享存储卷名称 （volumes类型有很多种）\n      emptyDir: {}              #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n      hostPath: string         #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n        path: string           #Pod所在宿主机的目录，将被用于同期中mount的目录\n      secret:                  #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部\n        scretname: string  \n        items:     \n        - key: string\n          path: string\n      configMap:             #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n        name: string\n        items:\n        - key: string\n          path: string\n```\n\n","source":"_posts/docker镜像部署到k8s集群.md","raw":"title: docker镜像部署到k8s集群\nauthor: ztq\ntags:\n\n  - k8s\n  - docker\ncategories:\n  - CICD\ndate: 2022-04-11 21:20:00\n\n---\n\n# 一、搭建docker私有仓库\n\n本文使用阿里云私有仓库（个人免费版），阿里云--->容器镜像服务\n\n![image-20220411212506831](/img/image-20220411212506831.png)\n\n# 二、docker打包\n\n1、程序目录\n\n```java\nll\n    \n-rw-r--r-- 1 root root      872 4月  11 22:03 dockerfile\n-rw-r--r-- 1 root root     8678 4月  11 21:53 jdk-8u251-linux-x64.tar.gz\n-rw-r--r-- 1 root root 75306958 4月  11 21:47 ruoyi.jar\n```\n\n![image-20220411220547876](/img/image-20220411220512656.png)\n\n2、编写dockerfile并打包\n\n```java\nFROM uquote/jdk8-tomee1.7.3-plume\n## 编写人\nMAINTAINER zhengtianqi\n# 在docker容器构建时拷贝程序\nADD ruoyi.jar ruoyi.jar\n# 容器暴露的端口号，需要与jar包在容器中运行使用端口号一致\nEXPOSE 80\n# 容器启动之后执行的命令， java -jar ROOT.jar\nENTRYPOINT [\"java\",\"-jar\",\"ruoyi.jar\"]\n```\n\n```java\n# 打包\ndocker build -t test:latest .\n# 查看\ndocker images\n```\n\n3、登录阿里云镜像仓库\n\n```java\ndocker login --username=z130****66717 registry.cn-beijing.aliyuncs.com\n```\n\n4、将镜像推送到阿里云镜像仓库Registry\n\n```java\ndocker tag [ImageId] registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\ndocker push registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\n```\n\n5、查看阿里云镜像仓库Registry中的镜像\n\n![image-20220411221525635](/img/image-20220411221525635.png)\n\n6、拉取镜像\n\n```java\ndocker pull registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n```\n\n# 三、k8s\n\n1、创建yaml文件\n\n```java\nkubectl create deployment [pod名称] --image=[docker私有仓库地址/镜像:版本号] -o yaml --dry-run=client > [yaml文件名称].yaml\n示例：\nkubectl create deployment demo --port=80 --replicas=3 --image=registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest -o yaml --dry-run=client > demo.yaml\n```\n\n2、查看yaml文件\n\n```java\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: demo\n  name: demo\nspec:\n  replicas: 3\n  ports:\n  - port: 80\n    targetPort: 80  \n  selector:\n    matchLabels:\n      app: demo\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: demo\n    spec:\n      containers:\n      - image: registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n        name: demo\n        resources: {}\nstatus: {}\n```\n\n3、通过yaml创建服务\n\n```java\nkubectl create -f demo.yaml\n```\n\n![image-20220411222247494](/img/image-20220411222247494.png)\n\n4、查看创建状态\n\n```java\nkubectl get pod\n```\n\n5、创建日志查看\n\n```java\nkubectl describe pod\n```\n\n6、杀死当前pod\n\n```\nkubectl delete deployment <deployment名> -n <namespace>\n```\n\n7、pod升级\n\n[Kubernetes--k8s---滚动更新--零停机不停服发布服务](https://blog.csdn.net/zzq900503/article/details/101221899)\n\n[Kubernetes滚动更新（无中断平滑发布） (idcsec.com)](http://idcsec.com/2019/03/05/kubernetes滚动更新（无中断平滑发布）/)\n\n# 附：yaml文件详解\n\n```java\n# yaml格式的pod定义文件完整内容：\napiVersion: v1           #必选，版本号，例如v1\nkind: Pod                #必选，Pod\nmetadata:                #必选，元数据\n  name: string           #必选，Pod名称\n  namespace: string      #必选，Pod所属的命名空间\n  labels:                #自定义标签\n    - name: string       #自定义标签名字\n  annotations:           #自定义注释列表\n    - name: string\nspec:                    #必选，Pod中容器的详细定义\n  containers:            #必选，Pod中容器列表\n  - name: string         #必选，容器名称\n    image: string        #必选，容器的镜像名称\n    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像\n    command: [string]                  #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]                     #容器的启动命令参数列表\n    workingDir: string                 #容器的工作目录\n    volumeMounts:                      #挂载到容器内部的存储卷配置\n    - name: string                     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string               #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean               #是否为只读模式\n    ports:                           #需要暴露的端口库号列表\n    - name: string                   #端口号名称\n      containerPort: int             #容器需要监听的端口号\n      hostPort: int                  #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string               #端口协议，支持TCP和UDP，默认TCP\n    env:                             #容器运行前需设置的环境变量列表\n    - name: string                   #环境变量名称\n      value: string                  #环境变量的值\n    resources:                       #资源限制和请求的设置\n      limits:                        #资源限制的设置\n        cpu: string                  #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string               #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests:                       #资源请求的设置\n        cpu: string                   #Cpu请求，容器启动的初始可用数量\n        memory: string                #内存请求，容器启动的初始可用数量\n    livenessProbe:                    #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可\n      exec:                           #对Pod容器内检查方式设置为exec方式\n        command: [string]             #exec方式需要制定的命令或脚本\n      httpGet:                        #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:                       #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0            #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0             #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged:false\n    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod\n    nodeSelector: obeject          #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定\n    imagePullSecrets:              #Pull镜像时使用的secret名称，以key：secretkey格式指定\n    - name: string\n    hostNetwork:false     #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n    volumes:                    #在该pod上定义共享存储卷列表\n    - name: string              #共享存储卷名称 （volumes类型有很多种）\n      emptyDir: {}              #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n      hostPath: string         #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n        path: string           #Pod所在宿主机的目录，将被用于同期中mount的目录\n      secret:                  #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部\n        scretname: string  \n        items:     \n        - key: string\n          path: string\n      configMap:             #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n        name: string\n        items:\n        - key: string\n          path: string\n```\n\n","slug":"docker镜像部署到k8s集群","published":1,"updated":"2022-04-11T15:03:22.723Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0500697kt98i116rz8","content":"<h1>一、搭建docker私有仓库</h1>\n<p>本文使用阿里云私有仓库（个人免费版），阿里云—&gt;容器镜像服务</p>\n<p><img src=\"/img/image-20220411212506831.png\" alt=\"image-20220411212506831\"></p>\n<h1>二、docker打包</h1>\n<p>1、程序目录</p>\n<pre><code class=\"language-java\">ll\n    \n-rw-r--r-- 1 root root      872 4月  11 22:03 dockerfile\n-rw-r--r-- 1 root root     8678 4月  11 21:53 jdk-8u251-linux-x64.tar.gz\n-rw-r--r-- 1 root root 75306958 4月  11 21:47 ruoyi.jar\n</code></pre>\n<p><img src=\"/img/image-20220411220512656.png\" alt=\"image-20220411220547876\"></p>\n<p>2、编写dockerfile并打包</p>\n<pre><code class=\"language-java\">FROM uquote/jdk8-tomee1.7.3-plume\n## 编写人\nMAINTAINER zhengtianqi\n# 在docker容器构建时拷贝程序\nADD ruoyi.jar ruoyi.jar\n# 容器暴露的端口号，需要与jar包在容器中运行使用端口号一致\nEXPOSE 80\n# 容器启动之后执行的命令， java -jar ROOT.jar\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;ruoyi.jar&quot;]\n</code></pre>\n<pre><code class=\"language-java\"># 打包\ndocker build -t test:latest .\n# 查看\ndocker images\n</code></pre>\n<p>3、登录阿里云镜像仓库</p>\n<pre><code class=\"language-java\">docker login --username=z130****66717 registry.cn-beijing.aliyuncs.com\n</code></pre>\n<p>4、将镜像推送到阿里云镜像仓库Registry</p>\n<pre><code class=\"language-java\">docker tag [ImageId] registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\ndocker push registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\n</code></pre>\n<p>5、查看阿里云镜像仓库Registry中的镜像</p>\n<p><img src=\"/img/image-20220411221525635.png\" alt=\"image-20220411221525635\"></p>\n<p>6、拉取镜像</p>\n<pre><code class=\"language-java\">docker pull registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n</code></pre>\n<h1>三、k8s</h1>\n<p>1、创建yaml文件</p>\n<pre><code class=\"language-java\">kubectl create deployment [pod名称] --image=[docker私有仓库地址/镜像:版本号] -o yaml --dry-run=client &gt; [yaml文件名称].yaml\n示例：\nkubectl create deployment demo --port=80 --replicas=3 --image=registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest -o yaml --dry-run=client &gt; demo.yaml\n</code></pre>\n<p>2、查看yaml文件</p>\n<pre><code class=\"language-java\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: demo\n  name: demo\nspec:\n  replicas: 3\n  ports:\n  - port: 80\n    targetPort: 80  \n  selector:\n    matchLabels:\n      app: demo\n  strategy: &#123;&#125;\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: demo\n    spec:\n      containers:\n      - image: registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n        name: demo\n        resources: &#123;&#125;\nstatus: &#123;&#125;\n</code></pre>\n<p>3、通过yaml创建服务</p>\n<pre><code class=\"language-java\">kubectl create -f demo.yaml\n</code></pre>\n<p><img src=\"/img/image-20220411222247494.png\" alt=\"image-20220411222247494\"></p>\n<p>4、查看创建状态</p>\n<pre><code class=\"language-java\">kubectl get pod\n</code></pre>\n<p>5、创建日志查看</p>\n<pre><code class=\"language-java\">kubectl describe pod\n</code></pre>\n<p>6、杀死当前pod</p>\n<pre><code>kubectl delete deployment &lt;deployment名&gt; -n &lt;namespace&gt;\n</code></pre>\n<p>7、pod升级</p>\n<p><a href=\"https://blog.csdn.net/zzq900503/article/details/101221899\">Kubernetes–k8s—滚动更新–零停机不停服发布服务</a></p>\n<p><a href=\"http://idcsec.com/2019/03/05/kubernetes%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0%EF%BC%88%E6%97%A0%E4%B8%AD%E6%96%AD%E5%B9%B3%E6%BB%91%E5%8F%91%E5%B8%83%EF%BC%89/\">Kubernetes滚动更新（无中断平滑发布） (idcsec.com)</a></p>\n<h1>附：yaml文件详解</h1>\n<pre><code class=\"language-java\"># yaml格式的pod定义文件完整内容：\napiVersion: v1           #必选，版本号，例如v1\nkind: Pod                #必选，Pod\nmetadata:                #必选，元数据\n  name: string           #必选，Pod名称\n  namespace: string      #必选，Pod所属的命名空间\n  labels:                #自定义标签\n    - name: string       #自定义标签名字\n  annotations:           #自定义注释列表\n    - name: string\nspec:                    #必选，Pod中容器的详细定义\n  containers:            #必选，Pod中容器列表\n  - name: string         #必选，容器名称\n    image: string        #必选，容器的镜像名称\n    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像\n    command: [string]                  #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]                     #容器的启动命令参数列表\n    workingDir: string                 #容器的工作目录\n    volumeMounts:                      #挂载到容器内部的存储卷配置\n    - name: string                     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string               #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean               #是否为只读模式\n    ports:                           #需要暴露的端口库号列表\n    - name: string                   #端口号名称\n      containerPort: int             #容器需要监听的端口号\n      hostPort: int                  #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string               #端口协议，支持TCP和UDP，默认TCP\n    env:                             #容器运行前需设置的环境变量列表\n    - name: string                   #环境变量名称\n      value: string                  #环境变量的值\n    resources:                       #资源限制和请求的设置\n      limits:                        #资源限制的设置\n        cpu: string                  #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string               #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests:                       #资源请求的设置\n        cpu: string                   #Cpu请求，容器启动的初始可用数量\n        memory: string                #内存请求，容器启动的初始可用数量\n    livenessProbe:                    #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可\n      exec:                           #对Pod容器内检查方式设置为exec方式\n        command: [string]             #exec方式需要制定的命令或脚本\n      httpGet:                        #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:                       #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0            #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0             #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged:false\n    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod\n    nodeSelector: obeject          #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定\n    imagePullSecrets:              #Pull镜像时使用的secret名称，以key：secretkey格式指定\n    - name: string\n    hostNetwork:false     #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n    volumes:                    #在该pod上定义共享存储卷列表\n    - name: string              #共享存储卷名称 （volumes类型有很多种）\n      emptyDir: &#123;&#125;              #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n      hostPath: string         #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n        path: string           #Pod所在宿主机的目录，将被用于同期中mount的目录\n      secret:                  #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部\n        scretname: string  \n        items:     \n        - key: string\n          path: string\n      configMap:             #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n        name: string\n        items:\n        - key: string\n          path: string\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、搭建docker私有仓库</h1>\n<p>本文使用阿里云私有仓库（个人免费版），阿里云—&gt;容器镜像服务</p>\n<p><img src=\"/img/image-20220411212506831.png\" alt=\"image-20220411212506831\"></p>\n<h1>二、docker打包</h1>\n<p>1、程序目录</p>\n<pre><code class=\"language-java\">ll\n    \n-rw-r--r-- 1 root root      872 4月  11 22:03 dockerfile\n-rw-r--r-- 1 root root     8678 4月  11 21:53 jdk-8u251-linux-x64.tar.gz\n-rw-r--r-- 1 root root 75306958 4月  11 21:47 ruoyi.jar\n</code></pre>\n<p><img src=\"/img/image-20220411220512656.png\" alt=\"image-20220411220547876\"></p>\n<p>2、编写dockerfile并打包</p>\n<pre><code class=\"language-java\">FROM uquote/jdk8-tomee1.7.3-plume\n## 编写人\nMAINTAINER zhengtianqi\n# 在docker容器构建时拷贝程序\nADD ruoyi.jar ruoyi.jar\n# 容器暴露的端口号，需要与jar包在容器中运行使用端口号一致\nEXPOSE 80\n# 容器启动之后执行的命令， java -jar ROOT.jar\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;ruoyi.jar&quot;]\n</code></pre>\n<pre><code class=\"language-java\"># 打包\ndocker build -t test:latest .\n# 查看\ndocker images\n</code></pre>\n<p>3、登录阿里云镜像仓库</p>\n<pre><code class=\"language-java\">docker login --username=z130****66717 registry.cn-beijing.aliyuncs.com\n</code></pre>\n<p>4、将镜像推送到阿里云镜像仓库Registry</p>\n<pre><code class=\"language-java\">docker tag [ImageId] registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\ndocker push registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:[镜像版本号]\n</code></pre>\n<p>5、查看阿里云镜像仓库Registry中的镜像</p>\n<p><img src=\"/img/image-20220411221525635.png\" alt=\"image-20220411221525635\"></p>\n<p>6、拉取镜像</p>\n<pre><code class=\"language-java\">docker pull registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n</code></pre>\n<h1>三、k8s</h1>\n<p>1、创建yaml文件</p>\n<pre><code class=\"language-java\">kubectl create deployment [pod名称] --image=[docker私有仓库地址/镜像:版本号] -o yaml --dry-run=client &gt; [yaml文件名称].yaml\n示例：\nkubectl create deployment demo --port=80 --replicas=3 --image=registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest -o yaml --dry-run=client &gt; demo.yaml\n</code></pre>\n<p>2、查看yaml文件</p>\n<pre><code class=\"language-java\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: demo\n  name: demo\nspec:\n  replicas: 3\n  ports:\n  - port: 80\n    targetPort: 80  \n  selector:\n    matchLabels:\n      app: demo\n  strategy: &#123;&#125;\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: demo\n    spec:\n      containers:\n      - image: registry.cn-beijing.aliyuncs.com/zhengtianqi/demo:latest\n        name: demo\n        resources: &#123;&#125;\nstatus: &#123;&#125;\n</code></pre>\n<p>3、通过yaml创建服务</p>\n<pre><code class=\"language-java\">kubectl create -f demo.yaml\n</code></pre>\n<p><img src=\"/img/image-20220411222247494.png\" alt=\"image-20220411222247494\"></p>\n<p>4、查看创建状态</p>\n<pre><code class=\"language-java\">kubectl get pod\n</code></pre>\n<p>5、创建日志查看</p>\n<pre><code class=\"language-java\">kubectl describe pod\n</code></pre>\n<p>6、杀死当前pod</p>\n<pre><code>kubectl delete deployment &lt;deployment名&gt; -n &lt;namespace&gt;\n</code></pre>\n<p>7、pod升级</p>\n<p><a href=\"https://blog.csdn.net/zzq900503/article/details/101221899\">Kubernetes–k8s—滚动更新–零停机不停服发布服务</a></p>\n<p><a href=\"http://idcsec.com/2019/03/05/kubernetes%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0%EF%BC%88%E6%97%A0%E4%B8%AD%E6%96%AD%E5%B9%B3%E6%BB%91%E5%8F%91%E5%B8%83%EF%BC%89/\">Kubernetes滚动更新（无中断平滑发布） (idcsec.com)</a></p>\n<h1>附：yaml文件详解</h1>\n<pre><code class=\"language-java\"># yaml格式的pod定义文件完整内容：\napiVersion: v1           #必选，版本号，例如v1\nkind: Pod                #必选，Pod\nmetadata:                #必选，元数据\n  name: string           #必选，Pod名称\n  namespace: string      #必选，Pod所属的命名空间\n  labels:                #自定义标签\n    - name: string       #自定义标签名字\n  annotations:           #自定义注释列表\n    - name: string\nspec:                    #必选，Pod中容器的详细定义\n  containers:            #必选，Pod中容器列表\n  - name: string         #必选，容器名称\n    image: string        #必选，容器的镜像名称\n    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像\n    command: [string]                  #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]                     #容器的启动命令参数列表\n    workingDir: string                 #容器的工作目录\n    volumeMounts:                      #挂载到容器内部的存储卷配置\n    - name: string                     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string               #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean               #是否为只读模式\n    ports:                           #需要暴露的端口库号列表\n    - name: string                   #端口号名称\n      containerPort: int             #容器需要监听的端口号\n      hostPort: int                  #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string               #端口协议，支持TCP和UDP，默认TCP\n    env:                             #容器运行前需设置的环境变量列表\n    - name: string                   #环境变量名称\n      value: string                  #环境变量的值\n    resources:                       #资源限制和请求的设置\n      limits:                        #资源限制的设置\n        cpu: string                  #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string               #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests:                       #资源请求的设置\n        cpu: string                   #Cpu请求，容器启动的初始可用数量\n        memory: string                #内存请求，容器启动的初始可用数量\n    livenessProbe:                    #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可\n      exec:                           #对Pod容器内检查方式设置为exec方式\n        command: [string]             #exec方式需要制定的命令或脚本\n      httpGet:                        #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:                       #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0            #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0             #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged:false\n    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod\n    nodeSelector: obeject          #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定\n    imagePullSecrets:              #Pull镜像时使用的secret名称，以key：secretkey格式指定\n    - name: string\n    hostNetwork:false     #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n    volumes:                    #在该pod上定义共享存储卷列表\n    - name: string              #共享存储卷名称 （volumes类型有很多种）\n      emptyDir: &#123;&#125;              #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n      hostPath: string         #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n        path: string           #Pod所在宿主机的目录，将被用于同期中mount的目录\n      secret:                  #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部\n        scretname: string  \n        items:     \n        - key: string\n          path: string\n      configMap:             #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n        name: string\n        items:\n        - key: string\n          path: string\n</code></pre>\n"},{"title":"java8新特性","author":"郑天祺","date":"2019-11-14T06:18:00.000Z","_content":"​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","source":"_posts/java8新特性.md","raw":"title: java8新特性\nauthor: 郑天祺\ntags:\n\n  - JDK1.8新特性\ncategories:\n  - java基础\ndate: 2019-11-14 14:18:00\n---\n​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","slug":"java8新特性","published":1,"updated":"2022-04-04T08:32:40.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno05006c7kt9fp83fnpl","content":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest &#123;\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() &#123;\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    &#125;\n\n    @Test\n    public void forEach() &#123;\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void map() &#123;\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    &#125;\n\n    @Test\n    public void sorted() &#123;\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void parallel() &#123;\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    &#125;\n\n    @Test\n    public void join() &#123;\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    &#125;\n\n    @Test\n    public void statistics()&#123;\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    &#125;\n\n&#125;\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;&#123;\n    T get();\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car &#123;\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) &#123;\n        return supplier.get();\n    &#125;\n\n    public static void collide(final Car car) &#123;\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    &#125;\n\n    public void follow(final Car another) &#123;\n        System.out.println(&quot;Following the&quot; + another.toString());\n    &#125;\n\n    public void repair() &#123;\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest &#123;\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() &#123;\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    &#125;\n\n    @Test\n    public void quoteExample()&#123;\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface &#123;\n    default void print()&#123;\n        System.out.println(&quot;默认方法&quot;);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface&#123;\n\n    @Test\n    public void test()&#123;\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    &#125;\n&#125;\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest &#123;\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() &#123;\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    &#125;\n\n    @Test\n    public void testStructDateTime() &#123;\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    &#125;\n\n    @Test\n    public void testZonedDateTime() &#123;\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest &#123;\n\n    @Test\n    public void testOptional() &#123;\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    &#125;\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) &#123;\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    &#125;\n&#125;\n\n</code></pre>\n<p>参考文献：<a href=\"https://www.runoob.com/java/java8-new-features.html\">https://www.runoob.com/java/java8-new-features.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest &#123;\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() &#123;\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    &#125;\n\n    @Test\n    public void forEach() &#123;\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void map() &#123;\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    &#125;\n\n    @Test\n    public void sorted() &#123;\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void parallel() &#123;\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    &#125;\n\n    @Test\n    public void join() &#123;\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    &#125;\n\n    @Test\n    public void statistics()&#123;\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    &#125;\n\n&#125;\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;&#123;\n    T get();\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car &#123;\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) &#123;\n        return supplier.get();\n    &#125;\n\n    public static void collide(final Car car) &#123;\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    &#125;\n\n    public void follow(final Car another) &#123;\n        System.out.println(&quot;Following the&quot; + another.toString());\n    &#125;\n\n    public void repair() &#123;\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest &#123;\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() &#123;\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    &#125;\n\n    @Test\n    public void quoteExample()&#123;\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface &#123;\n    default void print()&#123;\n        System.out.println(&quot;默认方法&quot;);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface&#123;\n\n    @Test\n    public void test()&#123;\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    &#125;\n&#125;\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest &#123;\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() &#123;\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    &#125;\n\n    @Test\n    public void testStructDateTime() &#123;\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    &#125;\n\n    @Test\n    public void testZonedDateTime() &#123;\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest &#123;\n\n    @Test\n    public void testOptional() &#123;\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    &#125;\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) &#123;\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    &#125;\n&#125;\n\n</code></pre>\n<p>参考文献：<a href=\"https://www.runoob.com/java/java8-new-features.html\">https://www.runoob.com/java/java8-new-features.html</a></p>\n"},{"title":"java中的Queue队列","author":"郑天祺","date":"2020-12-14T05:04:00.000Z","_content":"\n# 1、介绍\n\n​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。\n\n# 2、Queue的实现：\n\n 一个是以ConcurrentLinkedQueue为代表的高性能队列； \n 一个是以BlockingQueue接口为代表的阻塞队列； \n\n## （1）没有实现的阻塞接口队列\n\n​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue\n\n- PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。 \n- PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位\n\n- ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，\n- ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。\n\n## （2）实现阻塞接口的队列\n\njava.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。\n五个队列所提供的各有不同：\n\n　　* ArrayBlockingQueue ：一个由数组支持的有界队列。\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。\n\n![image-20201214130757812](/img/image-20201214130757812.png)\n\n# 3、ConcurrentLinkedQueue\n\n```java\n/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo {\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) {\n        q.offer(\"张三\");\n        q.offer(\"李四\");\n        q.offer(\"王五\");\n        q.offer(\"赵六\");\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    }\n}\n\n```\n\n# 4、BlockingQueue\n\n 定义： \n\t\t阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是： \n\t\t1、在队列为空时，获取元素的线程会等待队列变为非空。 \n\t\t2、当队列满时，存储元素的线程会等待队列可用。 \n阻塞队列是线程安全的。 \n用途： \n\t\t阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。\n\n​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n\n## 1）ArrayBlockingQueue\n\n```java\n/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(\"张三\");\n        array.add(\"李四\");\n        array.add(\"王五\");\n        try {\n            // 添加阻塞队列\n            boolean a = array.offer(\"赵六\", 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n## 2）LinkedBlockingQueue\n\n```java\n/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(\"张三\");\n        lbq.add(\"李四\");\n        lbq.add(\"李四\");\n        // 运行结果：3\n        System.out.println(lbq.size());\n    }\n}\n```\n\n## 3）PriorityBlockingQueue\n\n```java\n/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * <p>\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * <p>\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo {\n    public static void main(String[] args) {\n        PriorityBlockingQueue<Task> q = new PriorityBlockingQueue<>();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(\"id为2\"); t2.setName(\"id为3\"); t3.setName(\"id为1\");\n        q.add(t1); q.add(t2); q.add(t3);\n        try {\n            System.out.println(\"容器：\" + q);\n            System.out.println(q.take().getId());\n            System.out.println(\"容器：\" + q);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    public static class Task implements Comparable<Task> {\n        private int id;\n        private String name;\n        public int getId() {\n            return id;\n        }\n        public void setId(int id) {\n            this.id = id;\n        }\n        public String getName() {\n            return name;\n        }\n        public void setName(String name) {\n            this.name = name;\n        }\n        @Override\n        public int compareTo(Task task) {\n            return this.id > task.id ? 1 : (this.id < task.id ? -1 : 0);\n        }\n        @Override\n        public String toString() {\n            return this.id + \",\" + this.name;\n        }\n    }\n}\n\n```\n\n## 4）SynchronousQueue\n\nhttps://blog.51cto.com/14220760/2416470?source=dra","source":"_posts/java中的Queue队列.md","raw":"title: java中的Queue队列\nauthor: 郑天祺\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2020-12-14 13:04:00\n\n---\n\n# 1、介绍\n\n​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。\n\n# 2、Queue的实现：\n\n 一个是以ConcurrentLinkedQueue为代表的高性能队列； \n 一个是以BlockingQueue接口为代表的阻塞队列； \n\n## （1）没有实现的阻塞接口队列\n\n​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue\n\n- PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。 \n- PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位\n\n- ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，\n- ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。\n\n## （2）实现阻塞接口的队列\n\njava.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。\n五个队列所提供的各有不同：\n\n　　* ArrayBlockingQueue ：一个由数组支持的有界队列。\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。\n\n![image-20201214130757812](/img/image-20201214130757812.png)\n\n# 3、ConcurrentLinkedQueue\n\n```java\n/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo {\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) {\n        q.offer(\"张三\");\n        q.offer(\"李四\");\n        q.offer(\"王五\");\n        q.offer(\"赵六\");\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    }\n}\n\n```\n\n# 4、BlockingQueue\n\n 定义： \n\t\t阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是： \n\t\t1、在队列为空时，获取元素的线程会等待队列变为非空。 \n\t\t2、当队列满时，存储元素的线程会等待队列可用。 \n阻塞队列是线程安全的。 \n用途： \n\t\t阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。\n\n​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n\n## 1）ArrayBlockingQueue\n\n```java\n/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(\"张三\");\n        array.add(\"李四\");\n        array.add(\"王五\");\n        try {\n            // 添加阻塞队列\n            boolean a = array.offer(\"赵六\", 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n## 2）LinkedBlockingQueue\n\n```java\n/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(\"张三\");\n        lbq.add(\"李四\");\n        lbq.add(\"李四\");\n        // 运行结果：3\n        System.out.println(lbq.size());\n    }\n}\n```\n\n## 3）PriorityBlockingQueue\n\n```java\n/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * <p>\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * <p>\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo {\n    public static void main(String[] args) {\n        PriorityBlockingQueue<Task> q = new PriorityBlockingQueue<>();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(\"id为2\"); t2.setName(\"id为3\"); t3.setName(\"id为1\");\n        q.add(t1); q.add(t2); q.add(t3);\n        try {\n            System.out.println(\"容器：\" + q);\n            System.out.println(q.take().getId());\n            System.out.println(\"容器：\" + q);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    public static class Task implements Comparable<Task> {\n        private int id;\n        private String name;\n        public int getId() {\n            return id;\n        }\n        public void setId(int id) {\n            this.id = id;\n        }\n        public String getName() {\n            return name;\n        }\n        public void setName(String name) {\n            this.name = name;\n        }\n        @Override\n        public int compareTo(Task task) {\n            return this.id > task.id ? 1 : (this.id < task.id ? -1 : 0);\n        }\n        @Override\n        public String toString() {\n            return this.id + \",\" + this.name;\n        }\n    }\n}\n\n```\n\n## 4）SynchronousQueue\n\nhttps://blog.51cto.com/14220760/2416470?source=dra","slug":"java中的Queue队列","published":1,"updated":"2022-04-04T08:32:40.160Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno08006g7kt9dmev1y2s","content":"<h1>1、介绍</h1>\n<p>​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构<br>\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。</p>\n<h1>2、Queue的实现：</h1>\n<p>一个是以ConcurrentLinkedQueue为代表的高性能队列；<br>\n一个是以BlockingQueue接口为代表的阻塞队列；</p>\n<h2 id=\"（1）没有实现的阻塞接口队列\">（1）没有实现的阻塞接口队列</h2>\n<p>​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口<br>\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue</p>\n<ul>\n<li>\n<p>PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。</p>\n</li>\n<li>\n<p>PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。</p>\n</li>\n</ul>\n<h2 id=\"（2）实现阻塞接口的队列\">（2）实现阻塞接口的队列</h2>\n<p>java.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。<br>\n五个队列所提供的各有不同：</p>\n<p>* ArrayBlockingQueue ：一个由数组支持的有界队列。<br>\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。<br>\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。<br>\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。<br>\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。</p>\n<p><img src=\"/img/image-20201214130757812.png\" alt=\"image-20201214130757812\"></p>\n<h1>3、ConcurrentLinkedQueue</h1>\n<pre><code class=\"language-java\">/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo &#123;\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) &#123;\n        q.offer(&quot;张三&quot;);\n        q.offer(&quot;李四&quot;);\n        q.offer(&quot;王五&quot;);\n        q.offer(&quot;赵六&quot;);\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、BlockingQueue</h1>\n<p>定义：<br>\n阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<br>\n1、在队列为空时，获取元素的线程会等待队列变为非空。<br>\n2、当队列满时，存储元素的线程会等待队列可用。<br>\n阻塞队列是线程安全的。<br>\n用途：<br>\n阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。</p>\n<p>​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>\n<h2 id=\"1）ArrayBlockingQueue\">1）ArrayBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(&quot;张三&quot;);\n        array.add(&quot;李四&quot;);\n        array.add(&quot;王五&quot;);\n        try &#123;\n            // 添加阻塞队列\n            boolean a = array.offer(&quot;赵六&quot;, 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"2）LinkedBlockingQueue\">2）LinkedBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(&quot;张三&quot;);\n        lbq.add(&quot;李四&quot;);\n        lbq.add(&quot;李四&quot;);\n        // 运行结果：3\n        System.out.println(lbq.size());\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"3）PriorityBlockingQueue\">3）PriorityBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * &lt;p&gt;\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * &lt;p&gt;\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        PriorityBlockingQueue&lt;Task&gt; q = new PriorityBlockingQueue&lt;&gt;();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(&quot;id为2&quot;); t2.setName(&quot;id为3&quot;); t3.setName(&quot;id为1&quot;);\n        q.add(t1); q.add(t2); q.add(t3);\n        try &#123;\n            System.out.println(&quot;容器：&quot; + q);\n            System.out.println(q.take().getId());\n            System.out.println(&quot;容器：&quot; + q);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n    public static class Task implements Comparable&lt;Task&gt; &#123;\n        private int id;\n        private String name;\n        public int getId() &#123;\n            return id;\n        &#125;\n        public void setId(int id) &#123;\n            this.id = id;\n        &#125;\n        public String getName() &#123;\n            return name;\n        &#125;\n        public void setName(String name) &#123;\n            this.name = name;\n        &#125;\n        @Override\n        public int compareTo(Task task) &#123;\n            return this.id &gt; task.id ? 1 : (this.id &lt; task.id ? -1 : 0);\n        &#125;\n        @Override\n        public String toString() &#123;\n            return this.id + &quot;,&quot; + this.name;\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4）SynchronousQueue\">4）SynchronousQueue</h2>\n<p><a href=\"https://blog.51cto.com/14220760/2416470?source=dra\">https://blog.51cto.com/14220760/2416470?source=dra</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构<br>\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。</p>\n<h1>2、Queue的实现：</h1>\n<p>一个是以ConcurrentLinkedQueue为代表的高性能队列；<br>\n一个是以BlockingQueue接口为代表的阻塞队列；</p>\n<h2 id=\"（1）没有实现的阻塞接口队列\">（1）没有实现的阻塞接口队列</h2>\n<p>​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口<br>\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue</p>\n<ul>\n<li>\n<p>PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。</p>\n</li>\n<li>\n<p>PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。</p>\n</li>\n</ul>\n<h2 id=\"（2）实现阻塞接口的队列\">（2）实现阻塞接口的队列</h2>\n<p>java.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。<br>\n五个队列所提供的各有不同：</p>\n<p>* ArrayBlockingQueue ：一个由数组支持的有界队列。<br>\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。<br>\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。<br>\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。<br>\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。</p>\n<p><img src=\"/img/image-20201214130757812.png\" alt=\"image-20201214130757812\"></p>\n<h1>3、ConcurrentLinkedQueue</h1>\n<pre><code class=\"language-java\">/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo &#123;\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) &#123;\n        q.offer(&quot;张三&quot;);\n        q.offer(&quot;李四&quot;);\n        q.offer(&quot;王五&quot;);\n        q.offer(&quot;赵六&quot;);\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、BlockingQueue</h1>\n<p>定义：<br>\n阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<br>\n1、在队列为空时，获取元素的线程会等待队列变为非空。<br>\n2、当队列满时，存储元素的线程会等待队列可用。<br>\n阻塞队列是线程安全的。<br>\n用途：<br>\n阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。</p>\n<p>​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>\n<h2 id=\"1）ArrayBlockingQueue\">1）ArrayBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(&quot;张三&quot;);\n        array.add(&quot;李四&quot;);\n        array.add(&quot;王五&quot;);\n        try &#123;\n            // 添加阻塞队列\n            boolean a = array.offer(&quot;赵六&quot;, 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"2）LinkedBlockingQueue\">2）LinkedBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(&quot;张三&quot;);\n        lbq.add(&quot;李四&quot;);\n        lbq.add(&quot;李四&quot;);\n        // 运行结果：3\n        System.out.println(lbq.size());\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"3）PriorityBlockingQueue\">3）PriorityBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * &lt;p&gt;\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * &lt;p&gt;\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        PriorityBlockingQueue&lt;Task&gt; q = new PriorityBlockingQueue&lt;&gt;();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(&quot;id为2&quot;); t2.setName(&quot;id为3&quot;); t3.setName(&quot;id为1&quot;);\n        q.add(t1); q.add(t2); q.add(t3);\n        try &#123;\n            System.out.println(&quot;容器：&quot; + q);\n            System.out.println(q.take().getId());\n            System.out.println(&quot;容器：&quot; + q);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n    public static class Task implements Comparable&lt;Task&gt; &#123;\n        private int id;\n        private String name;\n        public int getId() &#123;\n            return id;\n        &#125;\n        public void setId(int id) &#123;\n            this.id = id;\n        &#125;\n        public String getName() &#123;\n            return name;\n        &#125;\n        public void setName(String name) &#123;\n            this.name = name;\n        &#125;\n        @Override\n        public int compareTo(Task task) &#123;\n            return this.id &gt; task.id ? 1 : (this.id &lt; task.id ? -1 : 0);\n        &#125;\n        @Override\n        public String toString() &#123;\n            return this.id + &quot;,&quot; + this.name;\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4）SynchronousQueue\">4）SynchronousQueue</h2>\n<p><a href=\"https://blog.51cto.com/14220760/2416470?source=dra\">https://blog.51cto.com/14220760/2416470?source=dra</a></p>\n"},{"title":"k8s集群搭建","author":"ztq","date":"2022-04-04T13:42:00.000Z","_content":"\n## 1. 安装要求\n\n在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n\n- 一台或多台机器，操作系统 CentOS7.x-86_x64\n- 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多\n- 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点\n- 禁止swap分区\n- docker和k8s保持官网一致的版本，否则有报错\n\n## 2. 准备环境\n\n| 角色   | IP            |\n| ------ | ------------- |\n| master | 192.168.2.145 |\n| node1  | 192.168.2.146 |\n| node2  | 192.168.2.147 |\n\n```java\n# 关闭防火墙\nsystemctl stop firewalld # 临时\nsystemctl disable firewalld # 永久\n\n# 关闭selinux\nsetenforce 0  # 临时\nsed -i 's/enforcing/disabled/' /etc/selinux/config  # 永久\n\n# 关闭swap\nswapoff -a  # 临时\nsed -ri 's/.*swap.*/#&/' /etc/fstab    # 永久\n\n# 根据规划设置主机名\nhostnamectl set-hostname <hostname>\n\n# 在master添加hosts\ncat >> /etc/hosts << EOF\n192.168.2.145 k8smaster\n192.168.2.146 k8snode1\n192.168.2.147 k8snode2\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\ncat > /etc/sysctl.d/k8s.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system  # 生效\n\n# 时间同步\nyum install ntpdate -y\nntpdate time.windows.com\n```\n\n## 3. 所有节点安装Docker/kubeadm/kubelet\n\nKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n\n### 3.1 安装Docker\n\n```java\n$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n$ systemctl enable docker && systemctl start docker\n$ docker --version\nDocker version 18.06.1-ce, build e68fc7a\n```\n\n```java\n$ cat > /etc/docker/daemon.json << EOF\n{\n  \"registry-mirrors\": [\"https://b9pmyelo.mirror.aliyuncs.com\"]\n}\nEOF\n```\n\n### 3.2 添加阿里云YUM软件源\n\n```java\n$ cat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n### 3.3 安装kubeadm，kubelet和kubectl\n\n由于版本更新频繁，这里指定版本号部署：\n\n```java\n$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n$ systemctl enable kubelet\n```\n\n## 4. 部署Kubernetes Master\n\n在192.168.2.145（Master）执行。\n\n```java\n$ kubeadm init \\\n  --apiserver-advertise-address=192.168.2.145 \\\n  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\\n  --kubernetes-version v1.23.5 \\\n  --service-cidr=10.96.0.0/12 \\\n  --pod-network-cidr=10.244.0.0/16\n```\n\n由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。\n\n使用kubectl工具：\n\n从节点没有，从主节点拷过去\n\n[The connection to the server localhost:8080 was refused - did you specify the right host or port?](https://blog.csdn.net/M82_A1/article/details/99671934)）\n\n```java\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ kubectl get nodes\n```\n\n![image-20220404214709099](/img/image-20220404214709099.png)\n\n## 5. 加入Kubernetes Node\n\n在192.168.2.146/13（Node）执行。\n\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：（这条命令是Master执行init时生成的）\n\n![image-20220404214514222](/img/image-20220404214514222.png)\n\n```java\n$ kubeadm join 192.168.2.145:6443 --token tps61f.37r065fhvmvgmxk6 \\\n        --discovery-token-ca-cert-hash sha256:ccff7f62596ebe95207ec22d3d32fa9183a0200d7cfc50b6a10cfa1626593a2e\n```\n\n默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n\n```java\nkubeadm token create --print-join-command\n```\n\n## 6. 部署CNI网络插件\n\n```java\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。\n\n可以参考[kube-flannel.yml(已修改镜像下载数据源)](https://blog.csdn.net/weixin_43298522/article/details/109769013)\n\n```java\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\nNAME                          READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-amd64-2pc95   1/1     Running   0          72s\n```\n\n## 7. 测试kubernetes集群\n\n在Kubernetes集群中创建一个pod，验证是否正常运行：\n\n```java\n$ kubectl create deployment nginx --image=nginx\n$ kubectl expose deployment nginx --port=80 --type=NodePort\n$ kubectl get pod,svc\n```\n\n访问地址：http://NodeIP:Port  \n","source":"_posts/k8s集群搭建.md","raw":"title: k8s集群搭建\nauthor: ztq\ntags:\n\n  - k8s\ncategories:\n  - CICD\ndate: 2022-04-04 21:42:00\n\n---\n\n## 1. 安装要求\n\n在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n\n- 一台或多台机器，操作系统 CentOS7.x-86_x64\n- 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多\n- 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点\n- 禁止swap分区\n- docker和k8s保持官网一致的版本，否则有报错\n\n## 2. 准备环境\n\n| 角色   | IP            |\n| ------ | ------------- |\n| master | 192.168.2.145 |\n| node1  | 192.168.2.146 |\n| node2  | 192.168.2.147 |\n\n```java\n# 关闭防火墙\nsystemctl stop firewalld # 临时\nsystemctl disable firewalld # 永久\n\n# 关闭selinux\nsetenforce 0  # 临时\nsed -i 's/enforcing/disabled/' /etc/selinux/config  # 永久\n\n# 关闭swap\nswapoff -a  # 临时\nsed -ri 's/.*swap.*/#&/' /etc/fstab    # 永久\n\n# 根据规划设置主机名\nhostnamectl set-hostname <hostname>\n\n# 在master添加hosts\ncat >> /etc/hosts << EOF\n192.168.2.145 k8smaster\n192.168.2.146 k8snode1\n192.168.2.147 k8snode2\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\ncat > /etc/sysctl.d/k8s.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system  # 生效\n\n# 时间同步\nyum install ntpdate -y\nntpdate time.windows.com\n```\n\n## 3. 所有节点安装Docker/kubeadm/kubelet\n\nKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n\n### 3.1 安装Docker\n\n```java\n$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n$ systemctl enable docker && systemctl start docker\n$ docker --version\nDocker version 18.06.1-ce, build e68fc7a\n```\n\n```java\n$ cat > /etc/docker/daemon.json << EOF\n{\n  \"registry-mirrors\": [\"https://b9pmyelo.mirror.aliyuncs.com\"]\n}\nEOF\n```\n\n### 3.2 添加阿里云YUM软件源\n\n```java\n$ cat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n### 3.3 安装kubeadm，kubelet和kubectl\n\n由于版本更新频繁，这里指定版本号部署：\n\n```java\n$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n$ systemctl enable kubelet\n```\n\n## 4. 部署Kubernetes Master\n\n在192.168.2.145（Master）执行。\n\n```java\n$ kubeadm init \\\n  --apiserver-advertise-address=192.168.2.145 \\\n  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\\n  --kubernetes-version v1.23.5 \\\n  --service-cidr=10.96.0.0/12 \\\n  --pod-network-cidr=10.244.0.0/16\n```\n\n由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。\n\n使用kubectl工具：\n\n从节点没有，从主节点拷过去\n\n[The connection to the server localhost:8080 was refused - did you specify the right host or port?](https://blog.csdn.net/M82_A1/article/details/99671934)）\n\n```java\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ kubectl get nodes\n```\n\n![image-20220404214709099](/img/image-20220404214709099.png)\n\n## 5. 加入Kubernetes Node\n\n在192.168.2.146/13（Node）执行。\n\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：（这条命令是Master执行init时生成的）\n\n![image-20220404214514222](/img/image-20220404214514222.png)\n\n```java\n$ kubeadm join 192.168.2.145:6443 --token tps61f.37r065fhvmvgmxk6 \\\n        --discovery-token-ca-cert-hash sha256:ccff7f62596ebe95207ec22d3d32fa9183a0200d7cfc50b6a10cfa1626593a2e\n```\n\n默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n\n```java\nkubeadm token create --print-join-command\n```\n\n## 6. 部署CNI网络插件\n\n```java\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。\n\n可以参考[kube-flannel.yml(已修改镜像下载数据源)](https://blog.csdn.net/weixin_43298522/article/details/109769013)\n\n```java\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\nNAME                          READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-amd64-2pc95   1/1     Running   0          72s\n```\n\n## 7. 测试kubernetes集群\n\n在Kubernetes集群中创建一个pod，验证是否正常运行：\n\n```java\n$ kubectl create deployment nginx --image=nginx\n$ kubectl expose deployment nginx --port=80 --type=NodePort\n$ kubectl get pod,svc\n```\n\n访问地址：http://NodeIP:Port  \n","slug":"k8s集群搭建","published":1,"updated":"2022-04-04T13:51:08.877Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno08006j7kt93fkb81tl","content":"<h2 id=\"1-安装要求\">1. 安装要求</h2>\n<p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p>\n<ul>\n<li>一台或多台机器，操作系统 CentOS7.x-86_x64</li>\n<li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li>\n<li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li>\n<li>禁止swap分区</li>\n<li>docker和k8s保持官网一致的版本，否则有报错</li>\n</ul>\n<h2 id=\"2-准备环境\">2. 准备环境</h2>\n<table>\n<thead>\n<tr>\n<th>角色</th>\n<th>IP</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>master</td>\n<td>192.168.2.145</td>\n</tr>\n<tr>\n<td>node1</td>\n<td>192.168.2.146</td>\n</tr>\n<tr>\n<td>node2</td>\n<td>192.168.2.147</td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-java\"># 关闭防火墙\nsystemctl stop firewalld # 临时\nsystemctl disable firewalld # 永久\n\n# 关闭selinux\nsetenforce 0  # 临时\nsed -i 's/enforcing/disabled/' /etc/selinux/config  # 永久\n\n# 关闭swap\nswapoff -a  # 临时\nsed -ri 's/.*swap.*/#&amp;/' /etc/fstab    # 永久\n\n# 根据规划设置主机名\nhostnamectl set-hostname &lt;hostname&gt;\n\n# 在master添加hosts\ncat &gt;&gt; /etc/hosts &lt;&lt; EOF\n192.168.2.145 k8smaster\n192.168.2.146 k8snode1\n192.168.2.147 k8snode2\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\ncat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system  # 生效\n\n# 时间同步\nyum install ntpdate -y\nntpdate time.windows.com\n</code></pre>\n<h2 id=\"3-所有节点安装Docker-kubeadm-kubelet\">3. 所有节点安装Docker/kubeadm/kubelet</h2>\n<p>Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。</p>\n<h3 id=\"3-1-安装Docker\">3.1 安装Docker</h3>\n<pre><code class=\"language-java\">$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n$ systemctl enable docker &amp;&amp; systemctl start docker\n$ docker --version\nDocker version 18.06.1-ce, build e68fc7a\n</code></pre>\n<pre><code class=\"language-java\">$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF\n&#123;\n  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]\n&#125;\nEOF\n</code></pre>\n<h3 id=\"3-2-添加阿里云YUM软件源\">3.2 添加阿里云YUM软件源</h3>\n<pre><code class=\"language-java\">$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n</code></pre>\n<h3 id=\"3-3-安装kubeadm，kubelet和kubectl\">3.3 安装kubeadm，kubelet和kubectl</h3>\n<p>由于版本更新频繁，这里指定版本号部署：</p>\n<pre><code class=\"language-java\">$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n$ systemctl enable kubelet\n</code></pre>\n<h2 id=\"4-部署Kubernetes-Master\">4. 部署Kubernetes Master</h2>\n<p>在192.168.2.145（Master）执行。</p>\n<pre><code class=\"language-java\">$ kubeadm init \\\n  --apiserver-advertise-address=192.168.2.145 \\\n  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\\n  --kubernetes-version v1.23.5 \\\n  --service-cidr=10.96.0.0/12 \\\n  --pod-network-cidr=10.244.0.0/16\n</code></pre>\n<p>由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。</p>\n<p>使用kubectl工具：</p>\n<p>从节点没有，从主节点拷过去</p>\n<p><a href=\"https://blog.csdn.net/M82_A1/article/details/99671934\">The connection to the server localhost:8080 was refused - did you specify the right host or port?</a>）</p>\n<pre><code class=\"language-java\">mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ kubectl get nodes\n</code></pre>\n<p><img src=\"/img/image-20220404214709099.png\" alt=\"image-20220404214709099\"></p>\n<h2 id=\"5-加入Kubernetes-Node\">5. 加入Kubernetes Node</h2>\n<p>在192.168.2.146/13（Node）执行。</p>\n<p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：（这条命令是Master执行init时生成的）</p>\n<p><img src=\"/img/image-20220404214514222.png\" alt=\"image-20220404214514222\"></p>\n<pre><code class=\"language-java\">$ kubeadm join 192.168.2.145:6443 --token tps61f.37r065fhvmvgmxk6 \\\n        --discovery-token-ca-cert-hash sha256:ccff7f62596ebe95207ec22d3d32fa9183a0200d7cfc50b6a10cfa1626593a2e\n</code></pre>\n<p>默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：</p>\n<pre><code class=\"language-java\">kubeadm token create --print-join-command\n</code></pre>\n<h2 id=\"6-部署CNI网络插件\">6. 部署CNI网络插件</h2>\n<pre><code class=\"language-java\">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。</p>\n<p>可以参考<a href=\"https://blog.csdn.net/weixin_43298522/article/details/109769013\">kube-flannel.yml(已修改镜像下载数据源)</a></p>\n<pre><code class=\"language-java\">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\nNAME                          READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-amd64-2pc95   1/1     Running   0          72s\n</code></pre>\n<h2 id=\"7-测试kubernetes集群\">7. 测试kubernetes集群</h2>\n<p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p>\n<pre><code class=\"language-java\">$ kubectl create deployment nginx --image=nginx\n$ kubectl expose deployment nginx --port=80 --type=NodePort\n$ kubectl get pod,svc\n</code></pre>\n<p>访问地址：<a href=\"http://NodeIP\">http://NodeIP</a>:Port</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-安装要求\">1. 安装要求</h2>\n<p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p>\n<ul>\n<li>一台或多台机器，操作系统 CentOS7.x-86_x64</li>\n<li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li>\n<li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li>\n<li>禁止swap分区</li>\n<li>docker和k8s保持官网一致的版本，否则有报错</li>\n</ul>\n<h2 id=\"2-准备环境\">2. 准备环境</h2>\n<table>\n<thead>\n<tr>\n<th>角色</th>\n<th>IP</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>master</td>\n<td>192.168.2.145</td>\n</tr>\n<tr>\n<td>node1</td>\n<td>192.168.2.146</td>\n</tr>\n<tr>\n<td>node2</td>\n<td>192.168.2.147</td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-java\"># 关闭防火墙\nsystemctl stop firewalld # 临时\nsystemctl disable firewalld # 永久\n\n# 关闭selinux\nsetenforce 0  # 临时\nsed -i 's/enforcing/disabled/' /etc/selinux/config  # 永久\n\n# 关闭swap\nswapoff -a  # 临时\nsed -ri 's/.*swap.*/#&amp;/' /etc/fstab    # 永久\n\n# 根据规划设置主机名\nhostnamectl set-hostname &lt;hostname&gt;\n\n# 在master添加hosts\ncat &gt;&gt; /etc/hosts &lt;&lt; EOF\n192.168.2.145 k8smaster\n192.168.2.146 k8snode1\n192.168.2.147 k8snode2\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\ncat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system  # 生效\n\n# 时间同步\nyum install ntpdate -y\nntpdate time.windows.com\n</code></pre>\n<h2 id=\"3-所有节点安装Docker-kubeadm-kubelet\">3. 所有节点安装Docker/kubeadm/kubelet</h2>\n<p>Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。</p>\n<h3 id=\"3-1-安装Docker\">3.1 安装Docker</h3>\n<pre><code class=\"language-java\">$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n$ systemctl enable docker &amp;&amp; systemctl start docker\n$ docker --version\nDocker version 18.06.1-ce, build e68fc7a\n</code></pre>\n<pre><code class=\"language-java\">$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF\n&#123;\n  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]\n&#125;\nEOF\n</code></pre>\n<h3 id=\"3-2-添加阿里云YUM软件源\">3.2 添加阿里云YUM软件源</h3>\n<pre><code class=\"language-java\">$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n</code></pre>\n<h3 id=\"3-3-安装kubeadm，kubelet和kubectl\">3.3 安装kubeadm，kubelet和kubectl</h3>\n<p>由于版本更新频繁，这里指定版本号部署：</p>\n<pre><code class=\"language-java\">$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n$ systemctl enable kubelet\n</code></pre>\n<h2 id=\"4-部署Kubernetes-Master\">4. 部署Kubernetes Master</h2>\n<p>在192.168.2.145（Master）执行。</p>\n<pre><code class=\"language-java\">$ kubeadm init \\\n  --apiserver-advertise-address=192.168.2.145 \\\n  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\\n  --kubernetes-version v1.23.5 \\\n  --service-cidr=10.96.0.0/12 \\\n  --pod-network-cidr=10.244.0.0/16\n</code></pre>\n<p>由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。</p>\n<p>使用kubectl工具：</p>\n<p>从节点没有，从主节点拷过去</p>\n<p><a href=\"https://blog.csdn.net/M82_A1/article/details/99671934\">The connection to the server localhost:8080 was refused - did you specify the right host or port?</a>）</p>\n<pre><code class=\"language-java\">mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ kubectl get nodes\n</code></pre>\n<p><img src=\"/img/image-20220404214709099.png\" alt=\"image-20220404214709099\"></p>\n<h2 id=\"5-加入Kubernetes-Node\">5. 加入Kubernetes Node</h2>\n<p>在192.168.2.146/13（Node）执行。</p>\n<p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：（这条命令是Master执行init时生成的）</p>\n<p><img src=\"/img/image-20220404214514222.png\" alt=\"image-20220404214514222\"></p>\n<pre><code class=\"language-java\">$ kubeadm join 192.168.2.145:6443 --token tps61f.37r065fhvmvgmxk6 \\\n        --discovery-token-ca-cert-hash sha256:ccff7f62596ebe95207ec22d3d32fa9183a0200d7cfc50b6a10cfa1626593a2e\n</code></pre>\n<p>默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：</p>\n<pre><code class=\"language-java\">kubeadm token create --print-join-command\n</code></pre>\n<h2 id=\"6-部署CNI网络插件\">6. 部署CNI网络插件</h2>\n<pre><code class=\"language-java\">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。</p>\n<p>可以参考<a href=\"https://blog.csdn.net/weixin_43298522/article/details/109769013\">kube-flannel.yml(已修改镜像下载数据源)</a></p>\n<pre><code class=\"language-java\">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\nNAME                          READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-amd64-2pc95   1/1     Running   0          72s\n</code></pre>\n<h2 id=\"7-测试kubernetes集群\">7. 测试kubernetes集群</h2>\n<p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p>\n<pre><code class=\"language-java\">$ kubectl create deployment nginx --image=nginx\n$ kubectl expose deployment nginx --port=80 --type=NodePort\n$ kubectl get pod,svc\n</code></pre>\n<p>访问地址：<a href=\"http://NodeIP\">http://NodeIP</a>:Port</p>\n"},{"title":"k8s构建ELK日志平台","author":"ztq","date":"2021-04-13T06:40:00.000Z","_content":"\n\n\n# k8s构建ELK日志平台\n\n## Pod中附加专用日志收集的容器\n\n# 一、概述\n\n目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；\n\n日志收集方案设计图\n\n![img](/img/70db7f87.jpg)\n\n# 二、优缺点\n\n每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。\n\n![img](/img/743e6cec.jpg)\n\n优点：低耦合。\n\n缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。\n\n \n\n# 三、部署ELK日志平台\n\nELK官网：\n\nhttps://www.elastic.co/cn/\n\n配置yum源参考：\n\nhttps://www.elastic.co/guide/en/logstash/current/installing-logstash.html\n\n## 3.1 安装JDK\n\n```java\n$ yum install -y java-1.8.0-openjdk \n```\n\n\n\n## 3.2 配置yum源\n\n```java\n[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n```\n\n\n\n## 3.3 安装ELK\n\n```java\n[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n```\n\n## 3.4 ELK相关配置\n\n这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。\n\n### 配置ES：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n```\n\n注意，ES需要优化一些内核参数：\n\n```java\n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n```\n\n### 运行：\n\n```java\n [root@qixiao1v zhengtianqi]# sysctl –p\n```\n\n生效配置。\n\n### 配置kibana：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: \"10.16.13.52\" \nserver.name: \"kibana\" \nelasticsearch.hosts: [\"http://10.16.13.52:9200\"] \nkibana.index: \".kibana\" \ni18n.locale: \"zh-CN\" \n```\n\n## 3.5 启动ES和Kibana\n\n```java\n[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n```\n\n访问kibana：[http://10.16.13.52:5601](http://10.16.13.52:5601/)\n\n \n\n# 四、采集k8s应用日志部署\n\n采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。\n\n## 4.1 部署Filebeat日志收集客户端\n\n### 4.1.1 编写Filebeat配置文件\n\n采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。\n\n```java\n[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          \"-c\", \"/etc/filebeat.yml\",\n          \"-e\",\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n```\n\n\n\n### 4.1.2 上传Filebeat配置文件\n\n```\n[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 <none> 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n```\n\n\n\n### 4.1.3 上传Filebeat配置文件是否成功\n\n检测配置的日志目录是否有挂载到Pod中：\n\n```java\n[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n```\n\n4.1.3过程也可以进入k8s管理页面 -> 命名空间选择 -> 配置与存储修改配置文件\n\n\n\n### 4.1.4 创建/修改pod，更新项目配置文件\n\n```java\n[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n```\n\n\n\n4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -> 工作负载 -> 部署 -> qixiao –> 右侧三个点 –> 查看 /编辑YAML -> 复制出来修改 -> 修改完点击更新\n\n## 4.2 配置Logstash接收日志\n\n### 4.2.1 配置logstash配置文件\n\n```java\n[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput {\n  beats {\n     port => 5044\n  }\n}\n\nfilter {\n}\n\noutput {\n    if [type] == \"module\" {\n        elasticsearch {\n            hosts => [\"http://127.0.0.1:9200\"]\n            index => \"k8s-log-%{+YYYY.MM.dd}\"\n        }\n    }\n    stdout { codec=> rubydebug }\n}\n```\n\n上述配置文件中的[type] == \"pipeline\"为filebeat-configmap.yaml中的fields:\n\ntype:\n\n### 4.2.2 启动/重启logstash\n\n```java\n[root@]# systemctl start logstash\n```\n\n注意：每次修改需要重启logstash\n\n```java\n[root@]# systemctl restart logstash\n```\n\n \n\n### 4.2.3 logstash部署是否成功\n\n```java\n# 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n```\n\n \n\n# 五、展示ELK日志\n\n## 5.1 配置Kibana展示日志\n\n左侧导航栏 -> 点击Management –> 点击Stack Management –> Kibana 索引模式 -> 创建索引模式 ）-> 时间字段@timestamp -> 创建成功\n\n![img](/img/1337b059.png)\n\n \n\n\n\n## 5.2 查看kibana日志\n\n左侧导航栏 -> Kibana -> Discover\n\n![img](/img/fb523dd6.png)\n\n![img](/img/e18c9709.jpg)\n\n \n\n \n\n## 5.3 绘制kibana图表\n\n左侧导航栏 -> Kibana -> dashboards –> 创建 仪表板 -> 新建\n\n \n\n以TSVB为例：\n\n面板选择 -> 索引模式-> 时间字段@timestamp\n\n![img](/img/480c47d9.png)\n\n![img](/img/8fb0cd0c.png)\n\n![img](/img/ea179a09.png)\n\n![img](/img/5a824e2f.png)\n\n参考文档：https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html","source":"_posts/k8s构建ELK日志平台.md","raw":"title: k8s构建ELK日志平台\nauthor: ztq\ntags:\n  - k8s\n  - elk\ncategories:\n  - CICD\n  - ''\ndate: 2021-04-13 14:40:00\n---\n\n\n\n# k8s构建ELK日志平台\n\n## Pod中附加专用日志收集的容器\n\n# 一、概述\n\n目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；\n\n日志收集方案设计图\n\n![img](/img/70db7f87.jpg)\n\n# 二、优缺点\n\n每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。\n\n![img](/img/743e6cec.jpg)\n\n优点：低耦合。\n\n缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。\n\n \n\n# 三、部署ELK日志平台\n\nELK官网：\n\nhttps://www.elastic.co/cn/\n\n配置yum源参考：\n\nhttps://www.elastic.co/guide/en/logstash/current/installing-logstash.html\n\n## 3.1 安装JDK\n\n```java\n$ yum install -y java-1.8.0-openjdk \n```\n\n\n\n## 3.2 配置yum源\n\n```java\n[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n```\n\n\n\n## 3.3 安装ELK\n\n```java\n[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n```\n\n## 3.4 ELK相关配置\n\n这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。\n\n### 配置ES：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n```\n\n注意，ES需要优化一些内核参数：\n\n```java\n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n```\n\n### 运行：\n\n```java\n [root@qixiao1v zhengtianqi]# sysctl –p\n```\n\n生效配置。\n\n### 配置kibana：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: \"10.16.13.52\" \nserver.name: \"kibana\" \nelasticsearch.hosts: [\"http://10.16.13.52:9200\"] \nkibana.index: \".kibana\" \ni18n.locale: \"zh-CN\" \n```\n\n## 3.5 启动ES和Kibana\n\n```java\n[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n```\n\n访问kibana：[http://10.16.13.52:5601](http://10.16.13.52:5601/)\n\n \n\n# 四、采集k8s应用日志部署\n\n采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。\n\n## 4.1 部署Filebeat日志收集客户端\n\n### 4.1.1 编写Filebeat配置文件\n\n采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。\n\n```java\n[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          \"-c\", \"/etc/filebeat.yml\",\n          \"-e\",\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n```\n\n\n\n### 4.1.2 上传Filebeat配置文件\n\n```\n[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 <none> 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n```\n\n\n\n### 4.1.3 上传Filebeat配置文件是否成功\n\n检测配置的日志目录是否有挂载到Pod中：\n\n```java\n[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n```\n\n4.1.3过程也可以进入k8s管理页面 -> 命名空间选择 -> 配置与存储修改配置文件\n\n\n\n### 4.1.4 创建/修改pod，更新项目配置文件\n\n```java\n[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n```\n\n\n\n4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -> 工作负载 -> 部署 -> qixiao –> 右侧三个点 –> 查看 /编辑YAML -> 复制出来修改 -> 修改完点击更新\n\n## 4.2 配置Logstash接收日志\n\n### 4.2.1 配置logstash配置文件\n\n```java\n[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput {\n  beats {\n     port => 5044\n  }\n}\n\nfilter {\n}\n\noutput {\n    if [type] == \"module\" {\n        elasticsearch {\n            hosts => [\"http://127.0.0.1:9200\"]\n            index => \"k8s-log-%{+YYYY.MM.dd}\"\n        }\n    }\n    stdout { codec=> rubydebug }\n}\n```\n\n上述配置文件中的[type] == \"pipeline\"为filebeat-configmap.yaml中的fields:\n\ntype:\n\n### 4.2.2 启动/重启logstash\n\n```java\n[root@]# systemctl start logstash\n```\n\n注意：每次修改需要重启logstash\n\n```java\n[root@]# systemctl restart logstash\n```\n\n \n\n### 4.2.3 logstash部署是否成功\n\n```java\n# 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n```\n\n \n\n# 五、展示ELK日志\n\n## 5.1 配置Kibana展示日志\n\n左侧导航栏 -> 点击Management –> 点击Stack Management –> Kibana 索引模式 -> 创建索引模式 ）-> 时间字段@timestamp -> 创建成功\n\n![img](/img/1337b059.png)\n\n \n\n\n\n## 5.2 查看kibana日志\n\n左侧导航栏 -> Kibana -> Discover\n\n![img](/img/fb523dd6.png)\n\n![img](/img/e18c9709.jpg)\n\n \n\n \n\n## 5.3 绘制kibana图表\n\n左侧导航栏 -> Kibana -> dashboards –> 创建 仪表板 -> 新建\n\n \n\n以TSVB为例：\n\n面板选择 -> 索引模式-> 时间字段@timestamp\n\n![img](/img/480c47d9.png)\n\n![img](/img/8fb0cd0c.png)\n\n![img](/img/ea179a09.png)\n\n![img](/img/5a824e2f.png)\n\n参考文档：https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html","slug":"k8s构建ELK日志平台","published":1,"updated":"2022-04-04T08:32:40.160Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno09006n7kt9c7z0bt3n","content":"<h1>k8s构建ELK日志平台</h1>\n<h2 id=\"Pod中附加专用日志收集的容器\">Pod中附加专用日志收集的容器</h2>\n<h1>一、概述</h1>\n<p>目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；</p>\n<p>日志收集方案设计图</p>\n<p><img src=\"/img/70db7f87.jpg\" alt=\"img\"></p>\n<h1>二、优缺点</h1>\n<p>每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。</p>\n<p><img src=\"/img/743e6cec.jpg\" alt=\"img\"></p>\n<p>优点：低耦合。</p>\n<p>缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。</p>\n<h1>三、部署ELK日志平台</h1>\n<p>ELK官网：</p>\n<p><a href=\"https://www.elastic.co/cn/\">https://www.elastic.co/cn/</a></p>\n<p>配置yum源参考：</p>\n<p><a href=\"https://www.elastic.co/guide/en/logstash/current/installing-logstash.html\">https://www.elastic.co/guide/en/logstash/current/installing-logstash.html</a></p>\n<h2 id=\"3-1-安装JDK\">3.1 安装JDK</h2>\n<pre><code class=\"language-java\">$ yum install -y java-1.8.0-openjdk \n</code></pre>\n<h2 id=\"3-2-配置yum源\">3.2 配置yum源</h2>\n<pre><code class=\"language-java\">[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n</code></pre>\n<h2 id=\"3-3-安装ELK\">3.3 安装ELK</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n</code></pre>\n<h2 id=\"3-4-ELK相关配置\">3.4 ELK相关配置</h2>\n<p>这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。</p>\n<h3 id=\"配置ES：\">配置ES：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n</code></pre>\n<p>注意，ES需要优化一些内核参数：</p>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n</code></pre>\n<h3 id=\"运行：\">运行：</h3>\n<pre><code class=\"language-java\"> [root@qixiao1v zhengtianqi]# sysctl –p\n</code></pre>\n<p>生效配置。</p>\n<h3 id=\"配置kibana：\">配置kibana：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: &quot;10.16.13.52&quot; \nserver.name: &quot;kibana&quot; \nelasticsearch.hosts: [&quot;http://10.16.13.52:9200&quot;] \nkibana.index: &quot;.kibana&quot; \ni18n.locale: &quot;zh-CN&quot; \n</code></pre>\n<h2 id=\"3-5-启动ES和Kibana\">3.5 启动ES和Kibana</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n</code></pre>\n<p>访问kibana：<a href=\"http://10.16.13.52:5601/\">http://10.16.13.52:5601</a></p>\n<h1>四、采集k8s应用日志部署</h1>\n<p>采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。</p>\n<h2 id=\"4-1-部署Filebeat日志收集客户端\">4.1 部署Filebeat日志收集客户端</h2>\n<h3 id=\"4-1-1-编写Filebeat配置文件\">4.1.1 编写Filebeat配置文件</h3>\n<p>采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,\n          &quot;-e&quot;,\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n</code></pre>\n<h3 id=\"4-1-2-上传Filebeat配置文件\">4.1.2 上传Filebeat配置文件</h3>\n<pre><code>[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 &lt;none&gt; 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n</code></pre>\n<h3 id=\"4-1-3-上传Filebeat配置文件是否成功\">4.1.3 上传Filebeat配置文件是否成功</h3>\n<p>检测配置的日志目录是否有挂载到Pod中：</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n</code></pre>\n<p>4.1.3过程也可以进入k8s管理页面 -&gt; 命名空间选择 -&gt; 配置与存储修改配置文件</p>\n<h3 id=\"4-1-4-创建-修改pod，更新项目配置文件\">4.1.4 创建/修改pod，更新项目配置文件</h3>\n<pre><code class=\"language-java\">[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n</code></pre>\n<p>4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -&gt; 工作负载 -&gt; 部署 -&gt; qixiao –&gt; 右侧三个点 –&gt; 查看 /编辑YAML -&gt; 复制出来修改 -&gt; 修改完点击更新</p>\n<h2 id=\"4-2-配置Logstash接收日志\">4.2 配置Logstash接收日志</h2>\n<h3 id=\"4-2-1-配置logstash配置文件\">4.2.1 配置logstash配置文件</h3>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput &#123;\n  beats &#123;\n     port =&gt; 5044\n  &#125;\n&#125;\n\nfilter &#123;\n&#125;\n\noutput &#123;\n    if [type] == &quot;module&quot; &#123;\n        elasticsearch &#123;\n            hosts =&gt; [&quot;http://127.0.0.1:9200&quot;]\n            index =&gt; &quot;k8s-log-%&#123;+YYYY.MM.dd&#125;&quot;\n        &#125;\n    &#125;\n    stdout &#123; codec=&gt; rubydebug &#125;\n&#125;\n</code></pre>\n<p>上述配置文件中的[type] == &quot;pipeline&quot;为filebeat-configmap.yaml中的fields:</p>\n<p>type:</p>\n<h3 id=\"4-2-2-启动-重启logstash\">4.2.2 启动/重启logstash</h3>\n<pre><code class=\"language-java\">[root@]# systemctl start logstash\n</code></pre>\n<p>注意：每次修改需要重启logstash</p>\n<pre><code class=\"language-java\">[root@]# systemctl restart logstash\n</code></pre>\n<h3 id=\"4-2-3-logstash部署是否成功\">4.2.3 logstash部署是否成功</h3>\n<pre><code class=\"language-java\"># 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n</code></pre>\n<h1>五、展示ELK日志</h1>\n<h2 id=\"5-1-配置Kibana展示日志\">5.1 配置Kibana展示日志</h2>\n<p>左侧导航栏 -&gt; 点击Management –&gt; 点击Stack Management –&gt; Kibana 索引模式 -&gt; 创建索引模式 ）-&gt; 时间字段@timestamp -&gt; 创建成功</p>\n<p><img src=\"/img/1337b059.png\" alt=\"img\"></p>\n<h2 id=\"5-2-查看kibana日志\">5.2 查看kibana日志</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; Discover</p>\n<p><img src=\"/img/fb523dd6.png\" alt=\"img\"></p>\n<p><img src=\"/img/e18c9709.jpg\" alt=\"img\"></p>\n<h2 id=\"5-3-绘制kibana图表\">5.3 绘制kibana图表</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; dashboards –&gt; 创建 仪表板 -&gt; 新建</p>\n<p>以TSVB为例：</p>\n<p>面板选择 -&gt; 索引模式-&gt; 时间字段@timestamp</p>\n<p><img src=\"/img/480c47d9.png\" alt=\"img\"></p>\n<p><img src=\"/img/8fb0cd0c.png\" alt=\"img\"></p>\n<p><img src=\"/img/ea179a09.png\" alt=\"img\"></p>\n<p><img src=\"/img/5a824e2f.png\" alt=\"img\"></p>\n<p>参考文档：<a href=\"https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html\">https://nicksors.cc/2019/07/11/kubernetes系列之《k8s构建ELK日志平台》.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>k8s构建ELK日志平台</h1>\n<h2 id=\"Pod中附加专用日志收集的容器\">Pod中附加专用日志收集的容器</h2>\n<h1>一、概述</h1>\n<p>目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；</p>\n<p>日志收集方案设计图</p>\n<p><img src=\"/img/70db7f87.jpg\" alt=\"img\"></p>\n<h1>二、优缺点</h1>\n<p>每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。</p>\n<p><img src=\"/img/743e6cec.jpg\" alt=\"img\"></p>\n<p>优点：低耦合。</p>\n<p>缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。</p>\n<h1>三、部署ELK日志平台</h1>\n<p>ELK官网：</p>\n<p><a href=\"https://www.elastic.co/cn/\">https://www.elastic.co/cn/</a></p>\n<p>配置yum源参考：</p>\n<p><a href=\"https://www.elastic.co/guide/en/logstash/current/installing-logstash.html\">https://www.elastic.co/guide/en/logstash/current/installing-logstash.html</a></p>\n<h2 id=\"3-1-安装JDK\">3.1 安装JDK</h2>\n<pre><code class=\"language-java\">$ yum install -y java-1.8.0-openjdk \n</code></pre>\n<h2 id=\"3-2-配置yum源\">3.2 配置yum源</h2>\n<pre><code class=\"language-java\">[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n</code></pre>\n<h2 id=\"3-3-安装ELK\">3.3 安装ELK</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n</code></pre>\n<h2 id=\"3-4-ELK相关配置\">3.4 ELK相关配置</h2>\n<p>这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。</p>\n<h3 id=\"配置ES：\">配置ES：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n</code></pre>\n<p>注意，ES需要优化一些内核参数：</p>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n</code></pre>\n<h3 id=\"运行：\">运行：</h3>\n<pre><code class=\"language-java\"> [root@qixiao1v zhengtianqi]# sysctl –p\n</code></pre>\n<p>生效配置。</p>\n<h3 id=\"配置kibana：\">配置kibana：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: &quot;10.16.13.52&quot; \nserver.name: &quot;kibana&quot; \nelasticsearch.hosts: [&quot;http://10.16.13.52:9200&quot;] \nkibana.index: &quot;.kibana&quot; \ni18n.locale: &quot;zh-CN&quot; \n</code></pre>\n<h2 id=\"3-5-启动ES和Kibana\">3.5 启动ES和Kibana</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n</code></pre>\n<p>访问kibana：<a href=\"http://10.16.13.52:5601/\">http://10.16.13.52:5601</a></p>\n<h1>四、采集k8s应用日志部署</h1>\n<p>采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。</p>\n<h2 id=\"4-1-部署Filebeat日志收集客户端\">4.1 部署Filebeat日志收集客户端</h2>\n<h3 id=\"4-1-1-编写Filebeat配置文件\">4.1.1 编写Filebeat配置文件</h3>\n<p>采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,\n          &quot;-e&quot;,\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n</code></pre>\n<h3 id=\"4-1-2-上传Filebeat配置文件\">4.1.2 上传Filebeat配置文件</h3>\n<pre><code>[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 &lt;none&gt; 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n</code></pre>\n<h3 id=\"4-1-3-上传Filebeat配置文件是否成功\">4.1.3 上传Filebeat配置文件是否成功</h3>\n<p>检测配置的日志目录是否有挂载到Pod中：</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n</code></pre>\n<p>4.1.3过程也可以进入k8s管理页面 -&gt; 命名空间选择 -&gt; 配置与存储修改配置文件</p>\n<h3 id=\"4-1-4-创建-修改pod，更新项目配置文件\">4.1.4 创建/修改pod，更新项目配置文件</h3>\n<pre><code class=\"language-java\">[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n</code></pre>\n<p>4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -&gt; 工作负载 -&gt; 部署 -&gt; qixiao –&gt; 右侧三个点 –&gt; 查看 /编辑YAML -&gt; 复制出来修改 -&gt; 修改完点击更新</p>\n<h2 id=\"4-2-配置Logstash接收日志\">4.2 配置Logstash接收日志</h2>\n<h3 id=\"4-2-1-配置logstash配置文件\">4.2.1 配置logstash配置文件</h3>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput &#123;\n  beats &#123;\n     port =&gt; 5044\n  &#125;\n&#125;\n\nfilter &#123;\n&#125;\n\noutput &#123;\n    if [type] == &quot;module&quot; &#123;\n        elasticsearch &#123;\n            hosts =&gt; [&quot;http://127.0.0.1:9200&quot;]\n            index =&gt; &quot;k8s-log-%&#123;+YYYY.MM.dd&#125;&quot;\n        &#125;\n    &#125;\n    stdout &#123; codec=&gt; rubydebug &#125;\n&#125;\n</code></pre>\n<p>上述配置文件中的[type] == &quot;pipeline&quot;为filebeat-configmap.yaml中的fields:</p>\n<p>type:</p>\n<h3 id=\"4-2-2-启动-重启logstash\">4.2.2 启动/重启logstash</h3>\n<pre><code class=\"language-java\">[root@]# systemctl start logstash\n</code></pre>\n<p>注意：每次修改需要重启logstash</p>\n<pre><code class=\"language-java\">[root@]# systemctl restart logstash\n</code></pre>\n<h3 id=\"4-2-3-logstash部署是否成功\">4.2.3 logstash部署是否成功</h3>\n<pre><code class=\"language-java\"># 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n</code></pre>\n<h1>五、展示ELK日志</h1>\n<h2 id=\"5-1-配置Kibana展示日志\">5.1 配置Kibana展示日志</h2>\n<p>左侧导航栏 -&gt; 点击Management –&gt; 点击Stack Management –&gt; Kibana 索引模式 -&gt; 创建索引模式 ）-&gt; 时间字段@timestamp -&gt; 创建成功</p>\n<p><img src=\"/img/1337b059.png\" alt=\"img\"></p>\n<h2 id=\"5-2-查看kibana日志\">5.2 查看kibana日志</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; Discover</p>\n<p><img src=\"/img/fb523dd6.png\" alt=\"img\"></p>\n<p><img src=\"/img/e18c9709.jpg\" alt=\"img\"></p>\n<h2 id=\"5-3-绘制kibana图表\">5.3 绘制kibana图表</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; dashboards –&gt; 创建 仪表板 -&gt; 新建</p>\n<p>以TSVB为例：</p>\n<p>面板选择 -&gt; 索引模式-&gt; 时间字段@timestamp</p>\n<p><img src=\"/img/480c47d9.png\" alt=\"img\"></p>\n<p><img src=\"/img/8fb0cd0c.png\" alt=\"img\"></p>\n<p><img src=\"/img/ea179a09.png\" alt=\"img\"></p>\n<p><img src=\"/img/5a824e2f.png\" alt=\"img\"></p>\n<p>参考文档：<a href=\"https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html\">https://nicksors.cc/2019/07/11/kubernetes系列之《k8s构建ELK日志平台》.html</a></p>\n"},{"title":"maven梳理","author":"郑天祺","date":"2019-08-28T09:01:00.000Z","_content":"# Maven使用\n\n## maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n## 1、Maven的简介  \n\n### 1.1 构建（build）\n\n除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。\n\n### 1.2 Maven的用途\n\n自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。\n\n## 2、Maven的安装和配置 \n\n### 2.1 Maven怎么升级：\n\n解压新的maven到一个目录，只需更新系统变量指向它。\n\n### 2.2 Maven目录介绍：\n\n（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）\n\n1. Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。\n\n2. Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。\n\n3. Conf目录：settings.xml可以在机器上定义全局的行为。\n\n4. Lib类库\n\n默认情况下：\n\n大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml\n\n### 2.3 设置HTTP代理\n\n有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。\n\n### 2.4 设置MAVEN_OPTS环境变量\n\n目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。\n\n###    2.5 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​ Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t2.6 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n\n## 3、使用入门\n\n### 3.1 编写pom.xml\n\nMaven的核心是pom.xml\n\n![1618244002479](/img/1618244002479.png)\n\n第三方工具可以快速构建pom.xml的头\n\nProject是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。\n\n最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。\n\nGroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。\n\nArtifactId定义了当前Maven项目在组中唯一的ID，子模块\n\nVersion指定了版本\n\n### 3.2编写主代码，项目打包过程\n\n此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install\n\n#### 3.2.1使用maven编译项目\n\n当我们编写一个main调用sayHello()打印helloworld字符串时。\n\n该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。\n\n当编码完毕，使用Maven进行编译，\n\n在项目根目录下运行命令mvn clean compile，\n\nmvn clean compile运行步骤：\n\n![1618244073306](/img/1618244073306.png)\n\n（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；\n\n（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；\n\n（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录\n\n####  3.2.2 使用maven编译测试类\n\n编写完测试用例运行命令mvn clean test\n\n需要maven-compiler-plugin插件\n\n![1618244110010](/img/1618244110010.png)\n\nmvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少  \n\n![1618244137853](/img/1618244137853.png)\n\n#### 3.2.3 使用maven将项目打包和运行\n\nmvn clean package进行打包，可以看到target下生成jar，\n\n它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。\n\n![1618244166891](/img/1618244166891.png)\n\n#### 3.2.4 使用maven运行带main方法的类\n\n目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置\n\n​         maven-shade-plug \n\n![1618244193984](/img/1618244193984.png)\n\n#### 3.2.5 将项目打包放到本地maven仓库\n\nmvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址\n\n![1618244251013](/img/1618244251013.png)\n\n## 4、坐标和依赖\n\n### 4.1 坐标详解\n\nMaven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier\n\n![1618244289269](/img/1618244289269.png)\n\n#### 4.1.1 groupId定义到项目\n\n与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应\n\n#### 4.1.2 artifactId定义到项目其中的一个模块\n\n​    为了方便区分，便于寻找实际构件，用nexus作为前缀\n\n#### 4.1.3 version为版本\n\n#### 4.1.4 packaging为该项目的打包方式，默认jar（可选）\n\n#### 4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）\n\n## 5、依赖\n\n### 5.1 依赖范围\n\n例：Junit依赖的测试范围test， 测试范围用元素scope表示\n\nMaven在编译项目主代码时候使用一套classpath\n\n在编译和执行测试的时候回使用另一套classpath\n\n依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：\n\n  Compile：编译依赖范围，三种classpath都有效(默认)\n\n  Test：测试依赖范围\n\n  Provided：已提供依赖范围（编译和测试）\n\n  Runtime：运行时依赖规范 \n\n  System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）\n\n![1618244361109](/img/1618244361109.png)\n\n### 5.2 传递性依赖\n\n有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。\n\nA依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期\n\n![1618244387386](/img/1618244387386.png)\n\n### 5.3 排除依赖\n\n#### 5.3.1引入相同版本的依赖\n\n像framework定义version\n\n```java\n<properties>\n\t <springframework.version>5.2.2</springframework.version>\n</properties>\n```\n\n#### 5.3.2 依赖优化\n\nmvn dependency:list 查看当前项目已解析的依赖\n\nmvn dependency:tree 查看当前项目的依赖树\n\nmvn dependency:analyze 帮助分析当前项目的依赖\n\n## 6、仓库\n\n### 6.1概念\n\n任何一个依赖，插件或者项目构建的输出，都可以称为构件\n\n### 6.2 仓库的布局\n\n仓库布局的源码，是基于简单的文件系统\n\n### 6.3 仓库的分类  \n\n仓库的配置，中央仓库、远程仓库\n\n分类：\n\n![1618244497271](/img/1618244497271.png)\n\n### 6.4 仓库的配置\n\n#### 6.4.1设置仓库\n\n（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。\n\n![1618244522945](/img/1618244522945.png)\n\n（2）配置maven 更新频率和检查文件策略\n\n![1618244544994](/img/1618244544994.png)\n\n（3）Maven的认证\n\n![1618244566899](/img/1618244566899.png)\n\n#### 6.4.2 上传构件到私有仓库\n\n（1）首先配置好distributionManagement配置\n\ndistributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。\n\n![1618244592782](/img/1618244592782.png)\n\n（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。\n\n#### 6.4.3 Maven版本号机制  \n\n![1618244623527](/img/1618244623527.png)\n\n版本号(version number)是版本的标识号。\n\n1.版本命名规范\n\n软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号\n\n2.软件版本阶段说明\n\n3.版本号修改规则\n\n（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。\n\n（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。\n\n（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。\n\n（4）日期版本号：发版当天的日期，需要包括年份。如：20160617 ","source":"_posts/maven梳理.md","raw":"title: maven梳理\ntags:\n\n  - maven\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-28 17:01:00\n---\n# Maven使用\n\n## maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n## 1、Maven的简介  \n\n### 1.1 构建（build）\n\n除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。\n\n### 1.2 Maven的用途\n\n自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。\n\n## 2、Maven的安装和配置 \n\n### 2.1 Maven怎么升级：\n\n解压新的maven到一个目录，只需更新系统变量指向它。\n\n### 2.2 Maven目录介绍：\n\n（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）\n\n1. Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。\n\n2. Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。\n\n3. Conf目录：settings.xml可以在机器上定义全局的行为。\n\n4. Lib类库\n\n默认情况下：\n\n大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml\n\n### 2.3 设置HTTP代理\n\n有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。\n\n### 2.4 设置MAVEN_OPTS环境变量\n\n目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。\n\n###    2.5 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​ Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t2.6 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n\n## 3、使用入门\n\n### 3.1 编写pom.xml\n\nMaven的核心是pom.xml\n\n![1618244002479](/img/1618244002479.png)\n\n第三方工具可以快速构建pom.xml的头\n\nProject是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。\n\n最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。\n\nGroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。\n\nArtifactId定义了当前Maven项目在组中唯一的ID，子模块\n\nVersion指定了版本\n\n### 3.2编写主代码，项目打包过程\n\n此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install\n\n#### 3.2.1使用maven编译项目\n\n当我们编写一个main调用sayHello()打印helloworld字符串时。\n\n该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。\n\n当编码完毕，使用Maven进行编译，\n\n在项目根目录下运行命令mvn clean compile，\n\nmvn clean compile运行步骤：\n\n![1618244073306](/img/1618244073306.png)\n\n（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；\n\n（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；\n\n（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录\n\n####  3.2.2 使用maven编译测试类\n\n编写完测试用例运行命令mvn clean test\n\n需要maven-compiler-plugin插件\n\n![1618244110010](/img/1618244110010.png)\n\nmvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少  \n\n![1618244137853](/img/1618244137853.png)\n\n#### 3.2.3 使用maven将项目打包和运行\n\nmvn clean package进行打包，可以看到target下生成jar，\n\n它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。\n\n![1618244166891](/img/1618244166891.png)\n\n#### 3.2.4 使用maven运行带main方法的类\n\n目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置\n\n​         maven-shade-plug \n\n![1618244193984](/img/1618244193984.png)\n\n#### 3.2.5 将项目打包放到本地maven仓库\n\nmvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址\n\n![1618244251013](/img/1618244251013.png)\n\n## 4、坐标和依赖\n\n### 4.1 坐标详解\n\nMaven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier\n\n![1618244289269](/img/1618244289269.png)\n\n#### 4.1.1 groupId定义到项目\n\n与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应\n\n#### 4.1.2 artifactId定义到项目其中的一个模块\n\n​    为了方便区分，便于寻找实际构件，用nexus作为前缀\n\n#### 4.1.3 version为版本\n\n#### 4.1.4 packaging为该项目的打包方式，默认jar（可选）\n\n#### 4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）\n\n## 5、依赖\n\n### 5.1 依赖范围\n\n例：Junit依赖的测试范围test， 测试范围用元素scope表示\n\nMaven在编译项目主代码时候使用一套classpath\n\n在编译和执行测试的时候回使用另一套classpath\n\n依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：\n\n  Compile：编译依赖范围，三种classpath都有效(默认)\n\n  Test：测试依赖范围\n\n  Provided：已提供依赖范围（编译和测试）\n\n  Runtime：运行时依赖规范 \n\n  System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）\n\n![1618244361109](/img/1618244361109.png)\n\n### 5.2 传递性依赖\n\n有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。\n\nA依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期\n\n![1618244387386](/img/1618244387386.png)\n\n### 5.3 排除依赖\n\n#### 5.3.1引入相同版本的依赖\n\n像framework定义version\n\n```java\n<properties>\n\t <springframework.version>5.2.2</springframework.version>\n</properties>\n```\n\n#### 5.3.2 依赖优化\n\nmvn dependency:list 查看当前项目已解析的依赖\n\nmvn dependency:tree 查看当前项目的依赖树\n\nmvn dependency:analyze 帮助分析当前项目的依赖\n\n## 6、仓库\n\n### 6.1概念\n\n任何一个依赖，插件或者项目构建的输出，都可以称为构件\n\n### 6.2 仓库的布局\n\n仓库布局的源码，是基于简单的文件系统\n\n### 6.3 仓库的分类  \n\n仓库的配置，中央仓库、远程仓库\n\n分类：\n\n![1618244497271](/img/1618244497271.png)\n\n### 6.4 仓库的配置\n\n#### 6.4.1设置仓库\n\n（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。\n\n![1618244522945](/img/1618244522945.png)\n\n（2）配置maven 更新频率和检查文件策略\n\n![1618244544994](/img/1618244544994.png)\n\n（3）Maven的认证\n\n![1618244566899](/img/1618244566899.png)\n\n#### 6.4.2 上传构件到私有仓库\n\n（1）首先配置好distributionManagement配置\n\ndistributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。\n\n![1618244592782](/img/1618244592782.png)\n\n（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。\n\n#### 6.4.3 Maven版本号机制  \n\n![1618244623527](/img/1618244623527.png)\n\n版本号(version number)是版本的标识号。\n\n1.版本命名规范\n\n软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号\n\n2.软件版本阶段说明\n\n3.版本号修改规则\n\n（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。\n\n（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。\n\n（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。\n\n（4）日期版本号：发版当天的日期，需要包括年份。如：20160617 ","slug":"maven梳理","published":1,"updated":"2022-04-04T08:32:40.161Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0a006q7kt91fxf6ja1","content":"<h1>Maven使用</h1>\n<h2 id=\"maven的命令：\">maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n<h2 id=\"1、Maven的简介\">1、Maven的简介</h2>\n<h3 id=\"1-1-构建（build）\">1.1 构建（build）</h3>\n<p>除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。</p>\n<h3 id=\"1-2-Maven的用途\">1.2 Maven的用途</h3>\n<p>自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。<br>\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。</p>\n<h2 id=\"2、Maven的安装和配置\">2、Maven的安装和配置</h2>\n<h3 id=\"2-1-Maven怎么升级：\">2.1 Maven怎么升级：</h3>\n<p>解压新的maven到一个目录，只需更新系统变量指向它。</p>\n<h3 id=\"2-2-Maven目录介绍：\">2.2 Maven目录介绍：</h3>\n<p>（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）</p>\n<ol>\n<li>\n<p>Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。</p>\n</li>\n<li>\n<p>Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。</p>\n</li>\n<li>\n<p>Conf目录：settings.xml可以在机器上定义全局的行为。</p>\n</li>\n<li>\n<p>Lib类库</p>\n</li>\n</ol>\n<p>默认情况下：</p>\n<p>大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml</p>\n<h3 id=\"2-3-设置HTTP代理\">2.3 设置HTTP代理</h3>\n<p>有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。</p>\n<h3 id=\"2-4-设置MAVEN-OPTS环境变量\">2.4 设置MAVEN_OPTS环境变量</h3>\n<p>目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。</p>\n<h3 id=\"2-5-参数设置：\">2.5 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）<br>\n​ Windows：如下图<br>\n​\t\t<img src=\"/img/maven配置.png\"><br>\n​</p>\n<h3 id=\"2-6-用户配置：\">2.6 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2 id=\"3、使用入门\">3、使用入门</h2>\n<h3 id=\"3-1-编写pom-xml\">3.1 编写pom.xml</h3>\n<p>Maven的核心是pom.xml</p>\n<p><img src=\"/img/1618244002479.png\" alt=\"1618244002479\"></p>\n<p>第三方工具可以快速构建pom.xml的头</p>\n<p>Project是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。</p>\n<p>最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。</p>\n<p>GroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。</p>\n<p>ArtifactId定义了当前Maven项目在组中唯一的ID，子模块</p>\n<p>Version指定了版本</p>\n<h3 id=\"3-2编写主代码，项目打包过程\">3.2编写主代码，项目打包过程</h3>\n<p>此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install</p>\n<h4 id=\"3-2-1使用maven编译项目\">3.2.1使用maven编译项目</h4>\n<p>当我们编写一个main调用sayHello()打印helloworld字符串时。</p>\n<p>该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。</p>\n<p>当编码完毕，使用Maven进行编译，</p>\n<p>在项目根目录下运行命令mvn clean compile，</p>\n<p>mvn clean compile运行步骤：</p>\n<p><img src=\"/img/1618244073306.png\" alt=\"1618244073306\"></p>\n<p>（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；</p>\n<p>（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；</p>\n<p>（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录</p>\n<h4 id=\"3-2-2-使用maven编译测试类\">3.2.2 使用maven编译测试类</h4>\n<p>编写完测试用例运行命令mvn clean test</p>\n<p>需要maven-compiler-plugin插件</p>\n<p><img src=\"/img/1618244110010.png\" alt=\"1618244110010\"></p>\n<p>mvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少</p>\n<p><img src=\"/img/1618244137853.png\" alt=\"1618244137853\"></p>\n<h4 id=\"3-2-3-使用maven将项目打包和运行\">3.2.3 使用maven将项目打包和运行</h4>\n<p>mvn clean package进行打包，可以看到target下生成jar，</p>\n<p>它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。</p>\n<p><img src=\"/img/1618244166891.png\" alt=\"1618244166891\"></p>\n<h4 id=\"3-2-4-使用maven运行带main方法的类\">3.2.4 使用maven运行带main方法的类</h4>\n<p>目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置</p>\n<p>​         maven-shade-plug</p>\n<p><img src=\"/img/1618244193984.png\" alt=\"1618244193984\"></p>\n<h4 id=\"3-2-5-将项目打包放到本地maven仓库\">3.2.5 将项目打包放到本地maven仓库</h4>\n<p>mvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址</p>\n<p><img src=\"/img/1618244251013.png\" alt=\"1618244251013\"></p>\n<h2 id=\"4、坐标和依赖\">4、坐标和依赖</h2>\n<h3 id=\"4-1-坐标详解\">4.1 坐标详解</h3>\n<p>Maven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier</p>\n<p><img src=\"/img/1618244289269.png\" alt=\"1618244289269\"></p>\n<h4 id=\"4-1-1-groupId定义到项目\">4.1.1 groupId定义到项目</h4>\n<p>与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应</p>\n<h4 id=\"4-1-2-artifactId定义到项目其中的一个模块\">4.1.2 artifactId定义到项目其中的一个模块</h4>\n<p>​    为了方便区分，便于寻找实际构件，用nexus作为前缀</p>\n<h4 id=\"4-1-3-version为版本\">4.1.3 version为版本</h4>\n<h4 id=\"4-1-4-packaging为该项目的打包方式，默认jar（可选）\">4.1.4 packaging为该项目的打包方式，默认jar（可选）</h4>\n<h4 id=\"4-1-5-classifier为（可选，不能直接定义，由附加插件帮助生成）\">4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）</h4>\n<h2 id=\"5、依赖\">5、依赖</h2>\n<h3 id=\"5-1-依赖范围\">5.1 依赖范围</h3>\n<p>例：Junit依赖的测试范围test， 测试范围用元素scope表示</p>\n<p>Maven在编译项目主代码时候使用一套classpath</p>\n<p>在编译和执行测试的时候回使用另一套classpath</p>\n<p>依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：</p>\n<p>Compile：编译依赖范围，三种classpath都有效(默认)</p>\n<p>Test：测试依赖范围</p>\n<p>Provided：已提供依赖范围（编译和测试）</p>\n<p>Runtime：运行时依赖规范</p>\n<p>System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）</p>\n<p><img src=\"/img/1618244361109.png\" alt=\"1618244361109\"></p>\n<h3 id=\"5-2-传递性依赖\">5.2 传递性依赖</h3>\n<p>有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。</p>\n<p>A依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期</p>\n<p><img src=\"/img/1618244387386.png\" alt=\"1618244387386\"></p>\n<h3 id=\"5-3-排除依赖\">5.3 排除依赖</h3>\n<h4 id=\"5-3-1引入相同版本的依赖\">5.3.1引入相同版本的依赖</h4>\n<p>像framework定义version</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t &lt;springframework.version&gt;5.2.2&lt;/springframework.version&gt;\n&lt;/properties&gt;\n</code></pre>\n<h4 id=\"5-3-2-依赖优化\">5.3.2 依赖优化</h4>\n<p>mvn dependency:list 查看当前项目已解析的依赖</p>\n<p>mvn dependency:tree 查看当前项目的依赖树</p>\n<p>mvn dependency:analyze 帮助分析当前项目的依赖</p>\n<h2 id=\"6、仓库\">6、仓库</h2>\n<h3 id=\"6-1概念\">6.1概念</h3>\n<p>任何一个依赖，插件或者项目构建的输出，都可以称为构件</p>\n<h3 id=\"6-2-仓库的布局\">6.2 仓库的布局</h3>\n<p>仓库布局的源码，是基于简单的文件系统</p>\n<h3 id=\"6-3-仓库的分类\">6.3 仓库的分类</h3>\n<p>仓库的配置，中央仓库、远程仓库</p>\n<p>分类：</p>\n<p><img src=\"/img/1618244497271.png\" alt=\"1618244497271\"></p>\n<h3 id=\"6-4-仓库的配置\">6.4 仓库的配置</h3>\n<h4 id=\"6-4-1设置仓库\">6.4.1设置仓库</h4>\n<p>（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。</p>\n<p><img src=\"/img/1618244522945.png\" alt=\"1618244522945\"></p>\n<p>（2）配置maven 更新频率和检查文件策略</p>\n<p><img src=\"/img/1618244544994.png\" alt=\"1618244544994\"></p>\n<p>（3）Maven的认证</p>\n<p><img src=\"/img/1618244566899.png\" alt=\"1618244566899\"></p>\n<h4 id=\"6-4-2-上传构件到私有仓库\">6.4.2 上传构件到私有仓库</h4>\n<p>（1）首先配置好distributionManagement配置</p>\n<p>distributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。</p>\n<p><img src=\"/img/1618244592782.png\" alt=\"1618244592782\"></p>\n<p>（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。</p>\n<h4 id=\"6-4-3-Maven版本号机制\">6.4.3 Maven版本号机制</h4>\n<p><img src=\"/img/1618244623527.png\" alt=\"1618244623527\"></p>\n<p>版本号(version number)是版本的标识号。</p>\n<p>1.版本命名规范</p>\n<p>软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号</p>\n<p>2.软件版本阶段说明</p>\n<p>3.版本号修改规则</p>\n<p>（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。</p>\n<p>（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。</p>\n<p>（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。</p>\n<p>（4）日期版本号：发版当天的日期，需要包括年份。如：20160617</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Maven使用</h1>\n<h2 id=\"maven的命令：\">maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n<h2 id=\"1、Maven的简介\">1、Maven的简介</h2>\n<h3 id=\"1-1-构建（build）\">1.1 构建（build）</h3>\n<p>除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。</p>\n<h3 id=\"1-2-Maven的用途\">1.2 Maven的用途</h3>\n<p>自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。<br>\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。</p>\n<h2 id=\"2、Maven的安装和配置\">2、Maven的安装和配置</h2>\n<h3 id=\"2-1-Maven怎么升级：\">2.1 Maven怎么升级：</h3>\n<p>解压新的maven到一个目录，只需更新系统变量指向它。</p>\n<h3 id=\"2-2-Maven目录介绍：\">2.2 Maven目录介绍：</h3>\n<p>（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）</p>\n<ol>\n<li>\n<p>Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。</p>\n</li>\n<li>\n<p>Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。</p>\n</li>\n<li>\n<p>Conf目录：settings.xml可以在机器上定义全局的行为。</p>\n</li>\n<li>\n<p>Lib类库</p>\n</li>\n</ol>\n<p>默认情况下：</p>\n<p>大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml</p>\n<h3 id=\"2-3-设置HTTP代理\">2.3 设置HTTP代理</h3>\n<p>有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。</p>\n<h3 id=\"2-4-设置MAVEN-OPTS环境变量\">2.4 设置MAVEN_OPTS环境变量</h3>\n<p>目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。</p>\n<h3 id=\"2-5-参数设置：\">2.5 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）<br>\n​ Windows：如下图<br>\n​\t\t<img src=\"/img/maven配置.png\"><br>\n​</p>\n<h3 id=\"2-6-用户配置：\">2.6 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2 id=\"3、使用入门\">3、使用入门</h2>\n<h3 id=\"3-1-编写pom-xml\">3.1 编写pom.xml</h3>\n<p>Maven的核心是pom.xml</p>\n<p><img src=\"/img/1618244002479.png\" alt=\"1618244002479\"></p>\n<p>第三方工具可以快速构建pom.xml的头</p>\n<p>Project是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。</p>\n<p>最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。</p>\n<p>GroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。</p>\n<p>ArtifactId定义了当前Maven项目在组中唯一的ID，子模块</p>\n<p>Version指定了版本</p>\n<h3 id=\"3-2编写主代码，项目打包过程\">3.2编写主代码，项目打包过程</h3>\n<p>此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install</p>\n<h4 id=\"3-2-1使用maven编译项目\">3.2.1使用maven编译项目</h4>\n<p>当我们编写一个main调用sayHello()打印helloworld字符串时。</p>\n<p>该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。</p>\n<p>当编码完毕，使用Maven进行编译，</p>\n<p>在项目根目录下运行命令mvn clean compile，</p>\n<p>mvn clean compile运行步骤：</p>\n<p><img src=\"/img/1618244073306.png\" alt=\"1618244073306\"></p>\n<p>（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；</p>\n<p>（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；</p>\n<p>（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录</p>\n<h4 id=\"3-2-2-使用maven编译测试类\">3.2.2 使用maven编译测试类</h4>\n<p>编写完测试用例运行命令mvn clean test</p>\n<p>需要maven-compiler-plugin插件</p>\n<p><img src=\"/img/1618244110010.png\" alt=\"1618244110010\"></p>\n<p>mvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少</p>\n<p><img src=\"/img/1618244137853.png\" alt=\"1618244137853\"></p>\n<h4 id=\"3-2-3-使用maven将项目打包和运行\">3.2.3 使用maven将项目打包和运行</h4>\n<p>mvn clean package进行打包，可以看到target下生成jar，</p>\n<p>它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。</p>\n<p><img src=\"/img/1618244166891.png\" alt=\"1618244166891\"></p>\n<h4 id=\"3-2-4-使用maven运行带main方法的类\">3.2.4 使用maven运行带main方法的类</h4>\n<p>目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置</p>\n<p>​         maven-shade-plug</p>\n<p><img src=\"/img/1618244193984.png\" alt=\"1618244193984\"></p>\n<h4 id=\"3-2-5-将项目打包放到本地maven仓库\">3.2.5 将项目打包放到本地maven仓库</h4>\n<p>mvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址</p>\n<p><img src=\"/img/1618244251013.png\" alt=\"1618244251013\"></p>\n<h2 id=\"4、坐标和依赖\">4、坐标和依赖</h2>\n<h3 id=\"4-1-坐标详解\">4.1 坐标详解</h3>\n<p>Maven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier</p>\n<p><img src=\"/img/1618244289269.png\" alt=\"1618244289269\"></p>\n<h4 id=\"4-1-1-groupId定义到项目\">4.1.1 groupId定义到项目</h4>\n<p>与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应</p>\n<h4 id=\"4-1-2-artifactId定义到项目其中的一个模块\">4.1.2 artifactId定义到项目其中的一个模块</h4>\n<p>​    为了方便区分，便于寻找实际构件，用nexus作为前缀</p>\n<h4 id=\"4-1-3-version为版本\">4.1.3 version为版本</h4>\n<h4 id=\"4-1-4-packaging为该项目的打包方式，默认jar（可选）\">4.1.4 packaging为该项目的打包方式，默认jar（可选）</h4>\n<h4 id=\"4-1-5-classifier为（可选，不能直接定义，由附加插件帮助生成）\">4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）</h4>\n<h2 id=\"5、依赖\">5、依赖</h2>\n<h3 id=\"5-1-依赖范围\">5.1 依赖范围</h3>\n<p>例：Junit依赖的测试范围test， 测试范围用元素scope表示</p>\n<p>Maven在编译项目主代码时候使用一套classpath</p>\n<p>在编译和执行测试的时候回使用另一套classpath</p>\n<p>依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：</p>\n<p>Compile：编译依赖范围，三种classpath都有效(默认)</p>\n<p>Test：测试依赖范围</p>\n<p>Provided：已提供依赖范围（编译和测试）</p>\n<p>Runtime：运行时依赖规范</p>\n<p>System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）</p>\n<p><img src=\"/img/1618244361109.png\" alt=\"1618244361109\"></p>\n<h3 id=\"5-2-传递性依赖\">5.2 传递性依赖</h3>\n<p>有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。</p>\n<p>A依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期</p>\n<p><img src=\"/img/1618244387386.png\" alt=\"1618244387386\"></p>\n<h3 id=\"5-3-排除依赖\">5.3 排除依赖</h3>\n<h4 id=\"5-3-1引入相同版本的依赖\">5.3.1引入相同版本的依赖</h4>\n<p>像framework定义version</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t &lt;springframework.version&gt;5.2.2&lt;/springframework.version&gt;\n&lt;/properties&gt;\n</code></pre>\n<h4 id=\"5-3-2-依赖优化\">5.3.2 依赖优化</h4>\n<p>mvn dependency:list 查看当前项目已解析的依赖</p>\n<p>mvn dependency:tree 查看当前项目的依赖树</p>\n<p>mvn dependency:analyze 帮助分析当前项目的依赖</p>\n<h2 id=\"6、仓库\">6、仓库</h2>\n<h3 id=\"6-1概念\">6.1概念</h3>\n<p>任何一个依赖，插件或者项目构建的输出，都可以称为构件</p>\n<h3 id=\"6-2-仓库的布局\">6.2 仓库的布局</h3>\n<p>仓库布局的源码，是基于简单的文件系统</p>\n<h3 id=\"6-3-仓库的分类\">6.3 仓库的分类</h3>\n<p>仓库的配置，中央仓库、远程仓库</p>\n<p>分类：</p>\n<p><img src=\"/img/1618244497271.png\" alt=\"1618244497271\"></p>\n<h3 id=\"6-4-仓库的配置\">6.4 仓库的配置</h3>\n<h4 id=\"6-4-1设置仓库\">6.4.1设置仓库</h4>\n<p>（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。</p>\n<p><img src=\"/img/1618244522945.png\" alt=\"1618244522945\"></p>\n<p>（2）配置maven 更新频率和检查文件策略</p>\n<p><img src=\"/img/1618244544994.png\" alt=\"1618244544994\"></p>\n<p>（3）Maven的认证</p>\n<p><img src=\"/img/1618244566899.png\" alt=\"1618244566899\"></p>\n<h4 id=\"6-4-2-上传构件到私有仓库\">6.4.2 上传构件到私有仓库</h4>\n<p>（1）首先配置好distributionManagement配置</p>\n<p>distributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。</p>\n<p><img src=\"/img/1618244592782.png\" alt=\"1618244592782\"></p>\n<p>（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。</p>\n<h4 id=\"6-4-3-Maven版本号机制\">6.4.3 Maven版本号机制</h4>\n<p><img src=\"/img/1618244623527.png\" alt=\"1618244623527\"></p>\n<p>版本号(version number)是版本的标识号。</p>\n<p>1.版本命名规范</p>\n<p>软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号</p>\n<p>2.软件版本阶段说明</p>\n<p>3.版本号修改规则</p>\n<p>（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。</p>\n<p>（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。</p>\n<p>（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。</p>\n<p>（4）日期版本号：发版当天的日期，需要包括年份。如：20160617</p>\n"},{"title":"String、 StringBuilder、StringBuffer区别","author":"郑天祺","date":"2020-07-21T07:43:00.000Z","_content":"\n# 1、介绍\n\n## （1）运行速度\n\n​\t\tStringBuilder > StringBuffer > String\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。\n\n​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。\n\n![image-20200721154908228](/img/StringUpdate.png)\n\n​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。\n\n## （2）线程安全\n\n　　StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的\n\n## （3）继承关系\n\n![image-20200721155019047](/img/StringStringBuilderStringBuffer.png)\n\n# 2、对比\n\n![image-20200721154514018](/img/StringBuilder.png)\n\n\n\n参考：https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303","source":"_posts/l-String、-StringBuilder、StringBuffer区别.md","raw":"title: String、 StringBuilder、StringBuffer区别\nauthor: 郑天祺\ntags:\n  - String\n  - StringBuilder\n  - StringBuffer\ncategories:\n  - java基础\ndate: 2020-07-21 15:43:00\n\n---\n\n# 1、介绍\n\n## （1）运行速度\n\n​\t\tStringBuilder > StringBuffer > String\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。\n\n​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。\n\n![image-20200721154908228](/img/StringUpdate.png)\n\n​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。\n\n## （2）线程安全\n\n　　StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的\n\n## （3）继承关系\n\n![image-20200721155019047](/img/StringStringBuilderStringBuffer.png)\n\n# 2、对比\n\n![image-20200721154514018](/img/StringBuilder.png)\n\n\n\n参考：https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303","slug":"l-String、-StringBuilder、StringBuffer区别","published":1,"updated":"2022-04-04T08:32:40.160Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0b006t7kt9gzbu7c38","content":"<h1>1、介绍</h1>\n<h2 id=\"（1）运行速度\">（1）运行速度</h2>\n<p>​\t\tStringBuilder &gt; StringBuffer &gt; String<br>\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。</p>\n<p>​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。</p>\n<p><img src=\"/img/StringUpdate.png\" alt=\"image-20200721154908228\"></p>\n<p>​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。</p>\n<h2 id=\"（2）线程安全\">（2）线程安全</h2>\n<p>StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的</p>\n<h2 id=\"（3）继承关系\">（3）继承关系</h2>\n<p><img src=\"/img/StringStringBuilderStringBuffer.png\" alt=\"image-20200721155019047\"></p>\n<h1>2、对比</h1>\n<p><img src=\"/img/StringBuilder.png\" alt=\"image-20200721154514018\"></p>\n<p>参考：<a href=\"https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303\">https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<h2 id=\"（1）运行速度\">（1）运行速度</h2>\n<p>​\t\tStringBuilder &gt; StringBuffer &gt; String<br>\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。</p>\n<p>​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。</p>\n<p><img src=\"/img/StringUpdate.png\" alt=\"image-20200721154908228\"></p>\n<p>​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。</p>\n<h2 id=\"（2）线程安全\">（2）线程安全</h2>\n<p>StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的</p>\n<h2 id=\"（3）继承关系\">（3）继承关系</h2>\n<p><img src=\"/img/StringStringBuilderStringBuffer.png\" alt=\"image-20200721155019047\"></p>\n<h1>2、对比</h1>\n<p><img src=\"/img/StringBuilder.png\" alt=\"image-20200721154514018\"></p>\n<p>参考：<a href=\"https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303\">https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303</a></p>\n"},{"title":"mysql事务","author":"郑天祺","date":"2020-07-21T01:42:00.000Z","_content":"\n# 1、介绍\n\n​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：\n\n​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。\n\n```java\n原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n```\n\n## 举例：\n\n​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：\n\n```java\n1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n```\n\n原子性：\n\t\t保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。\n\n一致性\n\t\t在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。\n\n隔离性\n\t\t在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。\n\n持久性\n\t\t一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）\n\n## 原子性与隔离行\n\n​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。\n\n因此，事务的原子性与一致性缺一不可。\n\n借鉴于：http://www.hollischuang.com/archives/898\n\n# 2、事务的隔离级别\n\n## （1）read uncommited\n\n​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。\n\n## （2）read commited\n\n​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。\n\n## （3）repeatable read\n\n​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。\n\n## （4）serializable\n\n​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读\n\n## （5）脏读、不可重复读、幻象\n\n​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）\n\n# 3、JAVA中的事务管理器\n\n​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：\n\n![image-20200721113313947](/img/transaction.png)","source":"_posts/mysql事务.md","raw":"title: mysql事务\nauthor: 郑天祺\ntags:\n\n  - mysql\ncategories:\n  - 数据库\ndate: 2020-07-21 09:42:00\n\n---\n\n# 1、介绍\n\n​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：\n\n​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。\n\n```java\n原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n```\n\n## 举例：\n\n​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：\n\n```java\n1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n```\n\n原子性：\n\t\t保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。\n\n一致性\n\t\t在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。\n\n隔离性\n\t\t在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。\n\n持久性\n\t\t一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）\n\n## 原子性与隔离行\n\n​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。\n\n因此，事务的原子性与一致性缺一不可。\n\n借鉴于：http://www.hollischuang.com/archives/898\n\n# 2、事务的隔离级别\n\n## （1）read uncommited\n\n​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。\n\n## （2）read commited\n\n​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。\n\n## （3）repeatable read\n\n​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。\n\n## （4）serializable\n\n​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读\n\n## （5）脏读、不可重复读、幻象\n\n​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）\n\n# 3、JAVA中的事务管理器\n\n​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：\n\n![image-20200721113313947](/img/transaction.png)","slug":"mysql事务","published":1,"updated":"2022-04-04T08:32:40.161Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0b006w7kt9ctvdddnt","content":"<h1>1、介绍</h1>\n<p>​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：</p>\n<p>​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。<br>\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。</p>\n<p>​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。</p>\n<pre><code class=\"language-java\">原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n</code></pre>\n<h2 id=\"举例：\">举例：</h2>\n<p>​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：</p>\n<pre><code class=\"language-java\">1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n</code></pre>\n<p>原子性：<br>\n保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。</p>\n<p>一致性<br>\n在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。</p>\n<p>隔离性<br>\n在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。<br>\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。</p>\n<p>持久性<br>\n一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）</p>\n<h2 id=\"原子性与隔离行\">原子性与隔离行</h2>\n<p>​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。<br>\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。</p>\n<p>因此，事务的原子性与一致性缺一不可。</p>\n<p>借鉴于：<a href=\"http://www.hollischuang.com/archives/898\">http://www.hollischuang.com/archives/898</a></p>\n<h1>2、事务的隔离级别</h1>\n<h2 id=\"（1）read-uncommited\">（1）read uncommited</h2>\n<p>​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。</p>\n<h2 id=\"（2）read-commited\">（2）read commited</h2>\n<p>​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。</p>\n<h2 id=\"（3）repeatable-read\">（3）repeatable read</h2>\n<p>​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。</p>\n<h2 id=\"（4）serializable\">（4）serializable</h2>\n<p>​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读</p>\n<h2 id=\"（5）脏读、不可重复读、幻象\">（5）脏读、不可重复读、幻象</h2>\n<p>​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。<br>\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。<br>\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）</p>\n<h1>3、JAVA中的事务管理器</h1>\n<p>​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：</p>\n<p><img src=\"/img/transaction.png\" alt=\"image-20200721113313947\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：</p>\n<p>​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。<br>\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。</p>\n<p>​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。</p>\n<pre><code class=\"language-java\">原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n</code></pre>\n<h2 id=\"举例：\">举例：</h2>\n<p>​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：</p>\n<pre><code class=\"language-java\">1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n</code></pre>\n<p>原子性：<br>\n保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。</p>\n<p>一致性<br>\n在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。</p>\n<p>隔离性<br>\n在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。<br>\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。</p>\n<p>持久性<br>\n一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）</p>\n<h2 id=\"原子性与隔离行\">原子性与隔离行</h2>\n<p>​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。<br>\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。</p>\n<p>因此，事务的原子性与一致性缺一不可。</p>\n<p>借鉴于：<a href=\"http://www.hollischuang.com/archives/898\">http://www.hollischuang.com/archives/898</a></p>\n<h1>2、事务的隔离级别</h1>\n<h2 id=\"（1）read-uncommited\">（1）read uncommited</h2>\n<p>​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。</p>\n<h2 id=\"（2）read-commited\">（2）read commited</h2>\n<p>​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。</p>\n<h2 id=\"（3）repeatable-read\">（3）repeatable read</h2>\n<p>​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。</p>\n<h2 id=\"（4）serializable\">（4）serializable</h2>\n<p>​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读</p>\n<h2 id=\"（5）脏读、不可重复读、幻象\">（5）脏读、不可重复读、幻象</h2>\n<p>​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。<br>\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。<br>\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）</p>\n<h1>3、JAVA中的事务管理器</h1>\n<p>​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：</p>\n<p><img src=\"/img/transaction.png\" alt=\"image-20200721113313947\"></p>\n"},{"title":"mysql排序","author":"郑天祺","date":"2019-11-20T12:35:00.000Z","_content":"\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","source":"_posts/mysql排序.md","raw":"title: mysql排序\ntags:\n\n  - mysql\ncategories:\n  - 数据库\nauthor: 郑天祺\ndate: 2019-11-20 20:35:00\n---\n\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","slug":"mysql排序","published":1,"updated":"2022-04-04T08:32:40.161Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0c006z7kt988alfu9s","content":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n"},{"title":"mysql突然变慢排查","author":"ztq","date":"2021-10-16T03:02:00.000Z","_content":"\n# 一、查看当前Mysql所有的进程\n\n```java\nshow full processlist;\n```\n\n# 二、查看Mysql的最大缓存\n\n```java\nshow global variables like \"global max_allowed_packet\";\n```\n\n# 三、查看当前正在进行的事务\n\n```java\nselect * from information_schema.INNODB_TRX;\n```\n\n# 四、查看当前Mysql的连接数\n\n```java\nshow status like 'thread%';\n```\n\n# 五、查看慢sql的开启状态和日志\n\n```java\nshow variables like 'slow_query%';\n```\n\n![image-20211014175135307](/img/image-20211014175135307.png)\n\n显示已开启慢sql的日志：去服务器上查询慢sql\n\n```java\ntail -n 100 /dev/vdc1/home/mysql/3306/log/slow.log\n```\n\n# 六、查看mysql错误日志\n\n```java\n#1.查找错误日志位置\ngrep log_error /etc/my.cnf\n> log_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\n#2.打开错误日志进行查看\ntail -n 100 /dev/vdc1/home/mysql/3306/log/error.log\n```\n\n\n\n# 附录\n\nmysql5.7.34-log 简单的配置文件注释\n\n```java\n[root@i-jp6npxh6 ~]# vi /etc/my.cnf\nport = 3306                                    # MySQL监听端口\nbasedir = /usr/local/mysql                      # MySQL安装根目录\ndatadir = /dev/vdc1/home/mysql/3306/data                      # MySQL数据文件所在位置\ntmpdir  = /dev/vdc1/home/mysql/3306/tmp                                  # 临时目录，比如load data infile会用到\nsocket = /dev/vdc1/home/mysql/3306/tmp/mysql.sock        # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件\npid-file = /dev/vdc1/home/mysql/3306/log/mysql.pid      # pid文件所在目录\nskip_name_resolve = 1                          # 只能用IP地址检查客户端的登录，不用主机名\ncharacter-set-server = utf8mb4                  # 数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）\ntransaction_isolation = READ-COMMITTED          # 事务隔离级别，默认为可重复读，MySQL默认可重复读级别\ncollation-server = utf8mb4_general_ci          # 数据库字符集对应一些排序等规则，注意要和character-set-server对应\ninit_connect='SET NAMES utf8mb4'                # 设置client连接mysql时的字符集,防止乱码\nlower_case_table_names = 1                      # 是否对sql语句大小写敏感，1表示不敏感\nmax_connections = 400                          # 最大连接数\nmax_connect_errors = 1000                      # 最大错误连接数\nexplicit_defaults_for_timestamp = true          # TIMESTAMP如果没有显示声明NOT NULL，允许NULL值\nmax_allowed_packet = 128M                      # SQL数据包发送的大小，如果有BLOB对象建议修改成1G\ninteractive_timeout = 1800                      # MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭\nwait_timeout = 1800                            # MySQL默认的wait_timeout值为8个小时, interactive_timeout参数需要同时配置才能生效\ntmp_table_size = 16M                            # 内部内存临时表的最大值 ，设置成128M；比如大数据量的group by ,order by时可能用到临时表；超过了这个值将写入磁盘，系统IO压力增大\nmax_heap_table_size = 128M                      # 定义了用户可以创建的内存表(memory table)的大小\nquery_cache_size = 0                            # 禁用mysql的缓存查询结果集功能；后期根据业务情况测试决定是否开启；大部分情况下关闭下面两项\nquery_cache_type = 0\n\n# 用户进程分配到的内存设置，每个session将会分配参数设置的内存大小\nread_buffer_size = 2M                          # MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。\nread_rnd_buffer_size = 8M                      # MySQL的随机读缓冲区大小\nsort_buffer_size = 8M                          # MySQL执行排序使用的缓冲大小\nbinlog_cache_size = 1M                          # 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。默认binlog_cache_size大小32K\n\nback_log = 130                                  # 在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中；官方建议back_log = 50 + (max_connections / 5),封顶数为900\n\n# 日志设置\nlog_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\nslow_query_log = 1                              # 慢查询sql日志设置\nlong_query_time = 1                            # 慢查询时间；超过1秒则为慢查询\nslow_query_log_file = /dev/vdc1/home/mysql/3306/log/slow.log                  # 慢查询日志文件\nlog_queries_not_using_indexes = 1              # 检查未使用到索引的sql\nlog_throttle_queries_not_using_indexes = 5      # 用来表示每分钟允许记录到slow log的且未使用索引的SQL语句次数。该值默认为0，表示没有限制\nmin_examined_row_limit = 100                    # 检索的行数必须达到此值才可被记为慢查询，查询检查返回少于该参数指定行的SQL不被记录到慢查询日志\nexpire_logs_days = 5                            # MySQL binlog日志文件保存的过期时间，过期后自动删除\n\n# 主从复制设置\n# log-bin = mysql-bin                            # 开启mysql binlog功能\n# binlog_format = ROW                            # binlog记录内容的方式，记录被操作的每一行\n# binlog_row_image = minimal                      # 对于binlog_format = ROW模式时，减少记录日志的内容，只记录受影响的列\n\n# Innodb设置\ninnodb_file_per_table=1\ninnodb_open_files = 500                        # 限制Innodb能打开的表的数据，如果库里的表特别多的情况，请增加这个。这个值默认是300\ninnodb_buffer_pool_size = 64M                  # InnoDB使用一个缓冲池来保存索引和原始数据，一般设置物理存储的60% ~ 70%；这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少\ninnodb_log_buffer_size = 2M                    # 此参数确定写日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL开发人员建议设置为1－8M之间\ninnodb_flush_method = O_DIRECT                  # O_DIRECT减少操作系统级别VFS的缓存和Innodb本身的buffer缓存之间的冲突\ninnodb_write_io_threads = 4                    # CPU多核处理能力设置，根据读，写比例进行调整\ninnodb_read_io_threads = 4\ninnodb_lock_wait_timeout = 120                  # InnoDB事务在被回滚之前可以等待一个锁定的超时秒数。InnoDB在它自己的锁定表中自动检测事务死锁并且回滚事务。InnoDB用LOCK TABLES语句注意到锁定设置。默认值是50秒\ninnodb_log_file_size = 32M                      # 此参数确定数据日志文件的大小，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间\n```\n\n\n\n","source":"_posts/mysql突然变慢排查.md","raw":"title: mysql突然变慢排查\nauthor: ztq\ntags:\n  - mysql\ncategories:\n  - '数据库'\ndate: 2021-10-16 11:02:00\n\n---\n\n# 一、查看当前Mysql所有的进程\n\n```java\nshow full processlist;\n```\n\n# 二、查看Mysql的最大缓存\n\n```java\nshow global variables like \"global max_allowed_packet\";\n```\n\n# 三、查看当前正在进行的事务\n\n```java\nselect * from information_schema.INNODB_TRX;\n```\n\n# 四、查看当前Mysql的连接数\n\n```java\nshow status like 'thread%';\n```\n\n# 五、查看慢sql的开启状态和日志\n\n```java\nshow variables like 'slow_query%';\n```\n\n![image-20211014175135307](/img/image-20211014175135307.png)\n\n显示已开启慢sql的日志：去服务器上查询慢sql\n\n```java\ntail -n 100 /dev/vdc1/home/mysql/3306/log/slow.log\n```\n\n# 六、查看mysql错误日志\n\n```java\n#1.查找错误日志位置\ngrep log_error /etc/my.cnf\n> log_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\n#2.打开错误日志进行查看\ntail -n 100 /dev/vdc1/home/mysql/3306/log/error.log\n```\n\n\n\n# 附录\n\nmysql5.7.34-log 简单的配置文件注释\n\n```java\n[root@i-jp6npxh6 ~]# vi /etc/my.cnf\nport = 3306                                    # MySQL监听端口\nbasedir = /usr/local/mysql                      # MySQL安装根目录\ndatadir = /dev/vdc1/home/mysql/3306/data                      # MySQL数据文件所在位置\ntmpdir  = /dev/vdc1/home/mysql/3306/tmp                                  # 临时目录，比如load data infile会用到\nsocket = /dev/vdc1/home/mysql/3306/tmp/mysql.sock        # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件\npid-file = /dev/vdc1/home/mysql/3306/log/mysql.pid      # pid文件所在目录\nskip_name_resolve = 1                          # 只能用IP地址检查客户端的登录，不用主机名\ncharacter-set-server = utf8mb4                  # 数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）\ntransaction_isolation = READ-COMMITTED          # 事务隔离级别，默认为可重复读，MySQL默认可重复读级别\ncollation-server = utf8mb4_general_ci          # 数据库字符集对应一些排序等规则，注意要和character-set-server对应\ninit_connect='SET NAMES utf8mb4'                # 设置client连接mysql时的字符集,防止乱码\nlower_case_table_names = 1                      # 是否对sql语句大小写敏感，1表示不敏感\nmax_connections = 400                          # 最大连接数\nmax_connect_errors = 1000                      # 最大错误连接数\nexplicit_defaults_for_timestamp = true          # TIMESTAMP如果没有显示声明NOT NULL，允许NULL值\nmax_allowed_packet = 128M                      # SQL数据包发送的大小，如果有BLOB对象建议修改成1G\ninteractive_timeout = 1800                      # MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭\nwait_timeout = 1800                            # MySQL默认的wait_timeout值为8个小时, interactive_timeout参数需要同时配置才能生效\ntmp_table_size = 16M                            # 内部内存临时表的最大值 ，设置成128M；比如大数据量的group by ,order by时可能用到临时表；超过了这个值将写入磁盘，系统IO压力增大\nmax_heap_table_size = 128M                      # 定义了用户可以创建的内存表(memory table)的大小\nquery_cache_size = 0                            # 禁用mysql的缓存查询结果集功能；后期根据业务情况测试决定是否开启；大部分情况下关闭下面两项\nquery_cache_type = 0\n\n# 用户进程分配到的内存设置，每个session将会分配参数设置的内存大小\nread_buffer_size = 2M                          # MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。\nread_rnd_buffer_size = 8M                      # MySQL的随机读缓冲区大小\nsort_buffer_size = 8M                          # MySQL执行排序使用的缓冲大小\nbinlog_cache_size = 1M                          # 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。默认binlog_cache_size大小32K\n\nback_log = 130                                  # 在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中；官方建议back_log = 50 + (max_connections / 5),封顶数为900\n\n# 日志设置\nlog_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\nslow_query_log = 1                              # 慢查询sql日志设置\nlong_query_time = 1                            # 慢查询时间；超过1秒则为慢查询\nslow_query_log_file = /dev/vdc1/home/mysql/3306/log/slow.log                  # 慢查询日志文件\nlog_queries_not_using_indexes = 1              # 检查未使用到索引的sql\nlog_throttle_queries_not_using_indexes = 5      # 用来表示每分钟允许记录到slow log的且未使用索引的SQL语句次数。该值默认为0，表示没有限制\nmin_examined_row_limit = 100                    # 检索的行数必须达到此值才可被记为慢查询，查询检查返回少于该参数指定行的SQL不被记录到慢查询日志\nexpire_logs_days = 5                            # MySQL binlog日志文件保存的过期时间，过期后自动删除\n\n# 主从复制设置\n# log-bin = mysql-bin                            # 开启mysql binlog功能\n# binlog_format = ROW                            # binlog记录内容的方式，记录被操作的每一行\n# binlog_row_image = minimal                      # 对于binlog_format = ROW模式时，减少记录日志的内容，只记录受影响的列\n\n# Innodb设置\ninnodb_file_per_table=1\ninnodb_open_files = 500                        # 限制Innodb能打开的表的数据，如果库里的表特别多的情况，请增加这个。这个值默认是300\ninnodb_buffer_pool_size = 64M                  # InnoDB使用一个缓冲池来保存索引和原始数据，一般设置物理存储的60% ~ 70%；这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少\ninnodb_log_buffer_size = 2M                    # 此参数确定写日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL开发人员建议设置为1－8M之间\ninnodb_flush_method = O_DIRECT                  # O_DIRECT减少操作系统级别VFS的缓存和Innodb本身的buffer缓存之间的冲突\ninnodb_write_io_threads = 4                    # CPU多核处理能力设置，根据读，写比例进行调整\ninnodb_read_io_threads = 4\ninnodb_lock_wait_timeout = 120                  # InnoDB事务在被回滚之前可以等待一个锁定的超时秒数。InnoDB在它自己的锁定表中自动检测事务死锁并且回滚事务。InnoDB用LOCK TABLES语句注意到锁定设置。默认值是50秒\ninnodb_log_file_size = 32M                      # 此参数确定数据日志文件的大小，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间\n```\n\n\n\n","slug":"mysql突然变慢排查","published":1,"updated":"2022-04-04T08:32:40.161Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0d00727kt97iho2wrx","content":"<h1>一、查看当前Mysql所有的进程</h1>\n<pre><code class=\"language-java\">show full processlist;\n</code></pre>\n<h1>二、查看Mysql的最大缓存</h1>\n<pre><code class=\"language-java\">show global variables like &quot;global max_allowed_packet&quot;;\n</code></pre>\n<h1>三、查看当前正在进行的事务</h1>\n<pre><code class=\"language-java\">select * from information_schema.INNODB_TRX;\n</code></pre>\n<h1>四、查看当前Mysql的连接数</h1>\n<pre><code class=\"language-java\">show status like 'thread%';\n</code></pre>\n<h1>五、查看慢sql的开启状态和日志</h1>\n<pre><code class=\"language-java\">show variables like 'slow_query%';\n</code></pre>\n<p><img src=\"/img/image-20211014175135307.png\" alt=\"image-20211014175135307\"></p>\n<p>显示已开启慢sql的日志：去服务器上查询慢sql</p>\n<pre><code class=\"language-java\">tail -n 100 /dev/vdc1/home/mysql/3306/log/slow.log\n</code></pre>\n<h1>六、查看mysql错误日志</h1>\n<pre><code class=\"language-java\">#1.查找错误日志位置\ngrep log_error /etc/my.cnf\n&gt; log_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\n#2.打开错误日志进行查看\ntail -n 100 /dev/vdc1/home/mysql/3306/log/error.log\n</code></pre>\n<h1>附录</h1>\n<p>mysql5.7.34-log 简单的配置文件注释</p>\n<pre><code class=\"language-java\">[root@i-jp6npxh6 ~]# vi /etc/my.cnf\nport = 3306                                    # MySQL监听端口\nbasedir = /usr/local/mysql                      # MySQL安装根目录\ndatadir = /dev/vdc1/home/mysql/3306/data                      # MySQL数据文件所在位置\ntmpdir  = /dev/vdc1/home/mysql/3306/tmp                                  # 临时目录，比如load data infile会用到\nsocket = /dev/vdc1/home/mysql/3306/tmp/mysql.sock        # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件\npid-file = /dev/vdc1/home/mysql/3306/log/mysql.pid      # pid文件所在目录\nskip_name_resolve = 1                          # 只能用IP地址检查客户端的登录，不用主机名\ncharacter-set-server = utf8mb4                  # 数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）\ntransaction_isolation = READ-COMMITTED          # 事务隔离级别，默认为可重复读，MySQL默认可重复读级别\ncollation-server = utf8mb4_general_ci          # 数据库字符集对应一些排序等规则，注意要和character-set-server对应\ninit_connect='SET NAMES utf8mb4'                # 设置client连接mysql时的字符集,防止乱码\nlower_case_table_names = 1                      # 是否对sql语句大小写敏感，1表示不敏感\nmax_connections = 400                          # 最大连接数\nmax_connect_errors = 1000                      # 最大错误连接数\nexplicit_defaults_for_timestamp = true          # TIMESTAMP如果没有显示声明NOT NULL，允许NULL值\nmax_allowed_packet = 128M                      # SQL数据包发送的大小，如果有BLOB对象建议修改成1G\ninteractive_timeout = 1800                      # MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭\nwait_timeout = 1800                            # MySQL默认的wait_timeout值为8个小时, interactive_timeout参数需要同时配置才能生效\ntmp_table_size = 16M                            # 内部内存临时表的最大值 ，设置成128M；比如大数据量的group by ,order by时可能用到临时表；超过了这个值将写入磁盘，系统IO压力增大\nmax_heap_table_size = 128M                      # 定义了用户可以创建的内存表(memory table)的大小\nquery_cache_size = 0                            # 禁用mysql的缓存查询结果集功能；后期根据业务情况测试决定是否开启；大部分情况下关闭下面两项\nquery_cache_type = 0\n\n# 用户进程分配到的内存设置，每个session将会分配参数设置的内存大小\nread_buffer_size = 2M                          # MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。\nread_rnd_buffer_size = 8M                      # MySQL的随机读缓冲区大小\nsort_buffer_size = 8M                          # MySQL执行排序使用的缓冲大小\nbinlog_cache_size = 1M                          # 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。默认binlog_cache_size大小32K\n\nback_log = 130                                  # 在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中；官方建议back_log = 50 + (max_connections / 5),封顶数为900\n\n# 日志设置\nlog_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\nslow_query_log = 1                              # 慢查询sql日志设置\nlong_query_time = 1                            # 慢查询时间；超过1秒则为慢查询\nslow_query_log_file = /dev/vdc1/home/mysql/3306/log/slow.log                  # 慢查询日志文件\nlog_queries_not_using_indexes = 1              # 检查未使用到索引的sql\nlog_throttle_queries_not_using_indexes = 5      # 用来表示每分钟允许记录到slow log的且未使用索引的SQL语句次数。该值默认为0，表示没有限制\nmin_examined_row_limit = 100                    # 检索的行数必须达到此值才可被记为慢查询，查询检查返回少于该参数指定行的SQL不被记录到慢查询日志\nexpire_logs_days = 5                            # MySQL binlog日志文件保存的过期时间，过期后自动删除\n\n# 主从复制设置\n# log-bin = mysql-bin                            # 开启mysql binlog功能\n# binlog_format = ROW                            # binlog记录内容的方式，记录被操作的每一行\n# binlog_row_image = minimal                      # 对于binlog_format = ROW模式时，减少记录日志的内容，只记录受影响的列\n\n# Innodb设置\ninnodb_file_per_table=1\ninnodb_open_files = 500                        # 限制Innodb能打开的表的数据，如果库里的表特别多的情况，请增加这个。这个值默认是300\ninnodb_buffer_pool_size = 64M                  # InnoDB使用一个缓冲池来保存索引和原始数据，一般设置物理存储的60% ~ 70%；这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少\ninnodb_log_buffer_size = 2M                    # 此参数确定写日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL开发人员建议设置为1－8M之间\ninnodb_flush_method = O_DIRECT                  # O_DIRECT减少操作系统级别VFS的缓存和Innodb本身的buffer缓存之间的冲突\ninnodb_write_io_threads = 4                    # CPU多核处理能力设置，根据读，写比例进行调整\ninnodb_read_io_threads = 4\ninnodb_lock_wait_timeout = 120                  # InnoDB事务在被回滚之前可以等待一个锁定的超时秒数。InnoDB在它自己的锁定表中自动检测事务死锁并且回滚事务。InnoDB用LOCK TABLES语句注意到锁定设置。默认值是50秒\ninnodb_log_file_size = 32M                      # 此参数确定数据日志文件的大小，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、查看当前Mysql所有的进程</h1>\n<pre><code class=\"language-java\">show full processlist;\n</code></pre>\n<h1>二、查看Mysql的最大缓存</h1>\n<pre><code class=\"language-java\">show global variables like &quot;global max_allowed_packet&quot;;\n</code></pre>\n<h1>三、查看当前正在进行的事务</h1>\n<pre><code class=\"language-java\">select * from information_schema.INNODB_TRX;\n</code></pre>\n<h1>四、查看当前Mysql的连接数</h1>\n<pre><code class=\"language-java\">show status like 'thread%';\n</code></pre>\n<h1>五、查看慢sql的开启状态和日志</h1>\n<pre><code class=\"language-java\">show variables like 'slow_query%';\n</code></pre>\n<p><img src=\"/img/image-20211014175135307.png\" alt=\"image-20211014175135307\"></p>\n<p>显示已开启慢sql的日志：去服务器上查询慢sql</p>\n<pre><code class=\"language-java\">tail -n 100 /dev/vdc1/home/mysql/3306/log/slow.log\n</code></pre>\n<h1>六、查看mysql错误日志</h1>\n<pre><code class=\"language-java\">#1.查找错误日志位置\ngrep log_error /etc/my.cnf\n&gt; log_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\n#2.打开错误日志进行查看\ntail -n 100 /dev/vdc1/home/mysql/3306/log/error.log\n</code></pre>\n<h1>附录</h1>\n<p>mysql5.7.34-log 简单的配置文件注释</p>\n<pre><code class=\"language-java\">[root@i-jp6npxh6 ~]# vi /etc/my.cnf\nport = 3306                                    # MySQL监听端口\nbasedir = /usr/local/mysql                      # MySQL安装根目录\ndatadir = /dev/vdc1/home/mysql/3306/data                      # MySQL数据文件所在位置\ntmpdir  = /dev/vdc1/home/mysql/3306/tmp                                  # 临时目录，比如load data infile会用到\nsocket = /dev/vdc1/home/mysql/3306/tmp/mysql.sock        # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件\npid-file = /dev/vdc1/home/mysql/3306/log/mysql.pid      # pid文件所在目录\nskip_name_resolve = 1                          # 只能用IP地址检查客户端的登录，不用主机名\ncharacter-set-server = utf8mb4                  # 数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）\ntransaction_isolation = READ-COMMITTED          # 事务隔离级别，默认为可重复读，MySQL默认可重复读级别\ncollation-server = utf8mb4_general_ci          # 数据库字符集对应一些排序等规则，注意要和character-set-server对应\ninit_connect='SET NAMES utf8mb4'                # 设置client连接mysql时的字符集,防止乱码\nlower_case_table_names = 1                      # 是否对sql语句大小写敏感，1表示不敏感\nmax_connections = 400                          # 最大连接数\nmax_connect_errors = 1000                      # 最大错误连接数\nexplicit_defaults_for_timestamp = true          # TIMESTAMP如果没有显示声明NOT NULL，允许NULL值\nmax_allowed_packet = 128M                      # SQL数据包发送的大小，如果有BLOB对象建议修改成1G\ninteractive_timeout = 1800                      # MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭\nwait_timeout = 1800                            # MySQL默认的wait_timeout值为8个小时, interactive_timeout参数需要同时配置才能生效\ntmp_table_size = 16M                            # 内部内存临时表的最大值 ，设置成128M；比如大数据量的group by ,order by时可能用到临时表；超过了这个值将写入磁盘，系统IO压力增大\nmax_heap_table_size = 128M                      # 定义了用户可以创建的内存表(memory table)的大小\nquery_cache_size = 0                            # 禁用mysql的缓存查询结果集功能；后期根据业务情况测试决定是否开启；大部分情况下关闭下面两项\nquery_cache_type = 0\n\n# 用户进程分配到的内存设置，每个session将会分配参数设置的内存大小\nread_buffer_size = 2M                          # MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。\nread_rnd_buffer_size = 8M                      # MySQL的随机读缓冲区大小\nsort_buffer_size = 8M                          # MySQL执行排序使用的缓冲大小\nbinlog_cache_size = 1M                          # 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。默认binlog_cache_size大小32K\n\nback_log = 130                                  # 在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中；官方建议back_log = 50 + (max_connections / 5),封顶数为900\n\n# 日志设置\nlog_error = /dev/vdc1/home/mysql/3306/log/error.log                          # 数据库错误日志文件\nslow_query_log = 1                              # 慢查询sql日志设置\nlong_query_time = 1                            # 慢查询时间；超过1秒则为慢查询\nslow_query_log_file = /dev/vdc1/home/mysql/3306/log/slow.log                  # 慢查询日志文件\nlog_queries_not_using_indexes = 1              # 检查未使用到索引的sql\nlog_throttle_queries_not_using_indexes = 5      # 用来表示每分钟允许记录到slow log的且未使用索引的SQL语句次数。该值默认为0，表示没有限制\nmin_examined_row_limit = 100                    # 检索的行数必须达到此值才可被记为慢查询，查询检查返回少于该参数指定行的SQL不被记录到慢查询日志\nexpire_logs_days = 5                            # MySQL binlog日志文件保存的过期时间，过期后自动删除\n\n# 主从复制设置\n# log-bin = mysql-bin                            # 开启mysql binlog功能\n# binlog_format = ROW                            # binlog记录内容的方式，记录被操作的每一行\n# binlog_row_image = minimal                      # 对于binlog_format = ROW模式时，减少记录日志的内容，只记录受影响的列\n\n# Innodb设置\ninnodb_file_per_table=1\ninnodb_open_files = 500                        # 限制Innodb能打开的表的数据，如果库里的表特别多的情况，请增加这个。这个值默认是300\ninnodb_buffer_pool_size = 64M                  # InnoDB使用一个缓冲池来保存索引和原始数据，一般设置物理存储的60% ~ 70%；这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少\ninnodb_log_buffer_size = 2M                    # 此参数确定写日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL开发人员建议设置为1－8M之间\ninnodb_flush_method = O_DIRECT                  # O_DIRECT减少操作系统级别VFS的缓存和Innodb本身的buffer缓存之间的冲突\ninnodb_write_io_threads = 4                    # CPU多核处理能力设置，根据读，写比例进行调整\ninnodb_read_io_threads = 4\ninnodb_lock_wait_timeout = 120                  # InnoDB事务在被回滚之前可以等待一个锁定的超时秒数。InnoDB在它自己的锁定表中自动检测事务死锁并且回滚事务。InnoDB用LOCK TABLES语句注意到锁定设置。默认值是50秒\ninnodb_log_file_size = 32M                      # 此参数确定数据日志文件的大小，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间\n</code></pre>\n"},{"title":"mysql简单主从搭建过程","author":"ztq","date":"2022-04-11T15:32:00.000Z","_content":"\n# 一、mysql集群介绍\n\n[MySql集群简介-告诉你为什么要用集群部署 - 掘金 (juejin.cn)](https://juejin.cn/post/6985187754493607966)\n\n[MySQL高可用集群方案 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/102798762)\n\n[MYSQL集群简介及对比 - 掘金 (juejin.cn)](https://juejin.cn/post/7027910561459503141)\n\n[MySQL八大集群架构的优点和缺点总结](https://www.yisu.com/zixun/527196.html)\n\n# 二、MySQL安装（一主一从）\n\n一台作为主服务器，一台作为从服务器，主服务器进行写操作，从服务器进行读操作。\n\n所有机器\n\n## step1、删除自带mariadb\n\n```java\nrpm -qa | grep mariadb\nrpm -e --nodeps mariadb-libs-5.5.60-1.el7_5.x86_64\n```\n\n## step2、安装mysql\n\n```java\ncd /data/environment\ntar -xvf mysql-5.7.37-1.el7.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.37-1.el7.x86_64.rpm\n```\n\n## step3、启动并创建用户\n\n```java\nsystemctl start mysqld\nsystemctl enable mysqld\n\ncat /var/log/mysqld.log | grep password\nmysql -u root -p\nuse mysql;\nalter user 'root'@'localhost' identified by 'zheng&test123';\nselect host from user where user='root';\nupdate user set host = '%' where user ='root';\nflush privileges;\nexit;\n```\n\n## step4、my.cnf配置\n\n\n\n```java\nvim /etc/my.cnf\n主配置\nserver-id=1\nlog-bin=mysql-bin\nlog-slave-updates=1\nbinlog-do-db=repl  #需要同步的数据库,如果没有本行表示同步所有的数据库\nbinlog-ignore-db=mysql  #被忽略的数据\n\n从配置\nserver-id=2\nlog-bin= mysql-bin\nrelay-log= mysql-relay-bin\nread-only=1 #设置为只读\nlog-slave-updates=1 #实现级联复制需要，即把relay-log中的同步到bin-log中\nreplicate-do-db=repl #要同步的数据库,不写本行表示同步所有数据库\n    \n```\n\n## step5、slave启动\n\n```java\nmysql -u root -p\n\nCHANGE MASTER TO\nMASTER_HOST='192.168.2.145',\nMASTER_USER='master',\nMASTER_PASSWORD='zheng&test123',\nMASTER_PORT=3306,\nMASTER_LOG_FILE='mysql-bin.000001',\nMASTER_LOG_POS=120;\n```\n\nstep6、启动是否成功\n\n```java\nmysql -u root -p\n\nshow slave status \\G;\n```\n","source":"_posts/mysql简单主从搭建过程.md","raw":"title: mysql简单主从搭建过程\nauthor: ztq\ntags:\n\n  - mysql\ncategories:\n  - 数据库\ndate: 2022-04-11 23:32:00\n\n---\n\n# 一、mysql集群介绍\n\n[MySql集群简介-告诉你为什么要用集群部署 - 掘金 (juejin.cn)](https://juejin.cn/post/6985187754493607966)\n\n[MySQL高可用集群方案 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/102798762)\n\n[MYSQL集群简介及对比 - 掘金 (juejin.cn)](https://juejin.cn/post/7027910561459503141)\n\n[MySQL八大集群架构的优点和缺点总结](https://www.yisu.com/zixun/527196.html)\n\n# 二、MySQL安装（一主一从）\n\n一台作为主服务器，一台作为从服务器，主服务器进行写操作，从服务器进行读操作。\n\n所有机器\n\n## step1、删除自带mariadb\n\n```java\nrpm -qa | grep mariadb\nrpm -e --nodeps mariadb-libs-5.5.60-1.el7_5.x86_64\n```\n\n## step2、安装mysql\n\n```java\ncd /data/environment\ntar -xvf mysql-5.7.37-1.el7.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.37-1.el7.x86_64.rpm\n```\n\n## step3、启动并创建用户\n\n```java\nsystemctl start mysqld\nsystemctl enable mysqld\n\ncat /var/log/mysqld.log | grep password\nmysql -u root -p\nuse mysql;\nalter user 'root'@'localhost' identified by 'zheng&test123';\nselect host from user where user='root';\nupdate user set host = '%' where user ='root';\nflush privileges;\nexit;\n```\n\n## step4、my.cnf配置\n\n\n\n```java\nvim /etc/my.cnf\n主配置\nserver-id=1\nlog-bin=mysql-bin\nlog-slave-updates=1\nbinlog-do-db=repl  #需要同步的数据库,如果没有本行表示同步所有的数据库\nbinlog-ignore-db=mysql  #被忽略的数据\n\n从配置\nserver-id=2\nlog-bin= mysql-bin\nrelay-log= mysql-relay-bin\nread-only=1 #设置为只读\nlog-slave-updates=1 #实现级联复制需要，即把relay-log中的同步到bin-log中\nreplicate-do-db=repl #要同步的数据库,不写本行表示同步所有数据库\n    \n```\n\n## step5、slave启动\n\n```java\nmysql -u root -p\n\nCHANGE MASTER TO\nMASTER_HOST='192.168.2.145',\nMASTER_USER='master',\nMASTER_PASSWORD='zheng&test123',\nMASTER_PORT=3306,\nMASTER_LOG_FILE='mysql-bin.000001',\nMASTER_LOG_POS=120;\n```\n\nstep6、启动是否成功\n\n```java\nmysql -u root -p\n\nshow slave status \\G;\n```\n","slug":"mysql简单主从搭建过程","published":1,"updated":"2022-04-11T23:32:05.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0d00747kt934xscje4","content":"<h1>一、mysql集群介绍</h1>\n<p><a href=\"https://juejin.cn/post/6985187754493607966\">MySql集群简介-告诉你为什么要用集群部署 - 掘金 (juejin.cn)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/102798762\">MySQL高可用集群方案 - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://juejin.cn/post/7027910561459503141\">MYSQL集群简介及对比 - 掘金 (juejin.cn)</a></p>\n<p><a href=\"https://www.yisu.com/zixun/527196.html\">MySQL八大集群架构的优点和缺点总结</a></p>\n<h1>二、MySQL安装（一主一从）</h1>\n<p>一台作为主服务器，一台作为从服务器，主服务器进行写操作，从服务器进行读操作。</p>\n<p>所有机器</p>\n<h2 id=\"step1、删除自带mariadb\">step1、删除自带mariadb</h2>\n<pre><code class=\"language-java\">rpm -qa | grep mariadb\nrpm -e --nodeps mariadb-libs-5.5.60-1.el7_5.x86_64\n</code></pre>\n<h2 id=\"step2、安装mysql\">step2、安装mysql</h2>\n<pre><code class=\"language-java\">cd /data/environment\ntar -xvf mysql-5.7.37-1.el7.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.37-1.el7.x86_64.rpm\n</code></pre>\n<h2 id=\"step3、启动并创建用户\">step3、启动并创建用户</h2>\n<pre><code class=\"language-java\">systemctl start mysqld\nsystemctl enable mysqld\n\ncat /var/log/mysqld.log | grep password\nmysql -u root -p\nuse mysql;\nalter user 'root'@'localhost' identified by 'zheng&amp;test123';\nselect host from user where user='root';\nupdate user set host = '%' where user ='root';\nflush privileges;\nexit;\n</code></pre>\n<h2 id=\"step4、my-cnf配置\">step4、my.cnf配置</h2>\n<pre><code class=\"language-java\">vim /etc/my.cnf\n主配置\nserver-id=1\nlog-bin=mysql-bin\nlog-slave-updates=1\nbinlog-do-db=repl  #需要同步的数据库,如果没有本行表示同步所有的数据库\nbinlog-ignore-db=mysql  #被忽略的数据\n\n从配置\nserver-id=2\nlog-bin= mysql-bin\nrelay-log= mysql-relay-bin\nread-only=1 #设置为只读\nlog-slave-updates=1 #实现级联复制需要，即把relay-log中的同步到bin-log中\nreplicate-do-db=repl #要同步的数据库,不写本行表示同步所有数据库\n    \n</code></pre>\n<h2 id=\"step5、slave启动\">step5、slave启动</h2>\n<pre><code class=\"language-java\">mysql -u root -p\n\nCHANGE MASTER TO\nMASTER_HOST='192.168.2.145',\nMASTER_USER='master',\nMASTER_PASSWORD='zheng&amp;test123',\nMASTER_PORT=3306,\nMASTER_LOG_FILE='mysql-bin.000001',\nMASTER_LOG_POS=120;\n</code></pre>\n<p>step6、启动是否成功</p>\n<pre><code class=\"language-java\">mysql -u root -p\n\nshow slave status \\G;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、mysql集群介绍</h1>\n<p><a href=\"https://juejin.cn/post/6985187754493607966\">MySql集群简介-告诉你为什么要用集群部署 - 掘金 (juejin.cn)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/102798762\">MySQL高可用集群方案 - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://juejin.cn/post/7027910561459503141\">MYSQL集群简介及对比 - 掘金 (juejin.cn)</a></p>\n<p><a href=\"https://www.yisu.com/zixun/527196.html\">MySQL八大集群架构的优点和缺点总结</a></p>\n<h1>二、MySQL安装（一主一从）</h1>\n<p>一台作为主服务器，一台作为从服务器，主服务器进行写操作，从服务器进行读操作。</p>\n<p>所有机器</p>\n<h2 id=\"step1、删除自带mariadb\">step1、删除自带mariadb</h2>\n<pre><code class=\"language-java\">rpm -qa | grep mariadb\nrpm -e --nodeps mariadb-libs-5.5.60-1.el7_5.x86_64\n</code></pre>\n<h2 id=\"step2、安装mysql\">step2、安装mysql</h2>\n<pre><code class=\"language-java\">cd /data/environment\ntar -xvf mysql-5.7.37-1.el7.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.37-1.el7.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.37-1.el7.x86_64.rpm\n</code></pre>\n<h2 id=\"step3、启动并创建用户\">step3、启动并创建用户</h2>\n<pre><code class=\"language-java\">systemctl start mysqld\nsystemctl enable mysqld\n\ncat /var/log/mysqld.log | grep password\nmysql -u root -p\nuse mysql;\nalter user 'root'@'localhost' identified by 'zheng&amp;test123';\nselect host from user where user='root';\nupdate user set host = '%' where user ='root';\nflush privileges;\nexit;\n</code></pre>\n<h2 id=\"step4、my-cnf配置\">step4、my.cnf配置</h2>\n<pre><code class=\"language-java\">vim /etc/my.cnf\n主配置\nserver-id=1\nlog-bin=mysql-bin\nlog-slave-updates=1\nbinlog-do-db=repl  #需要同步的数据库,如果没有本行表示同步所有的数据库\nbinlog-ignore-db=mysql  #被忽略的数据\n\n从配置\nserver-id=2\nlog-bin= mysql-bin\nrelay-log= mysql-relay-bin\nread-only=1 #设置为只读\nlog-slave-updates=1 #实现级联复制需要，即把relay-log中的同步到bin-log中\nreplicate-do-db=repl #要同步的数据库,不写本行表示同步所有数据库\n    \n</code></pre>\n<h2 id=\"step5、slave启动\">step5、slave启动</h2>\n<pre><code class=\"language-java\">mysql -u root -p\n\nCHANGE MASTER TO\nMASTER_HOST='192.168.2.145',\nMASTER_USER='master',\nMASTER_PASSWORD='zheng&amp;test123',\nMASTER_PORT=3306,\nMASTER_LOG_FILE='mysql-bin.000001',\nMASTER_LOG_POS=120;\n</code></pre>\n<p>step6、启动是否成功</p>\n<pre><code class=\"language-java\">mysql -u root -p\n\nshow slave status \\G;\n</code></pre>\n"},{"title":"mysql表设计及优化","author":"郑天祺","date":"2019-08-31T07:28:00.000Z","_content":"\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","source":"_posts/mysql表设计及优化.md","raw":"title: mysql表设计及优化\nauthor: 郑天祺\ntags:\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 15:28:00\n\n---\n\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","slug":"mysql表设计及优化","published":1,"updated":"2022-04-04T08:32:40.162Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0e00787kt9399h6c3x","content":"<h2 id=\"一、一些建议\">一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select…like ‘%**%’ 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？<a href=\"https://blog.csdn.net/n88Lpo/article/details/78099114\">https://blog.csdn.net/n88Lpo/article/details/78099114</a></p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2 id=\"二、算是面试题吧\">二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2 id=\"三、数据结构\">三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、一些建议\">一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select…like ‘%**%’ 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？<a href=\"https://blog.csdn.net/n88Lpo/article/details/78099114\">https://blog.csdn.net/n88Lpo/article/details/78099114</a></p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2 id=\"二、算是面试题吧\">二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2 id=\"三、数据结构\">三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n"},{"title":"互斥锁","author":"郑天祺","date":"2019-08-31T05:13:00.000Z","_content":"\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","source":"_posts/互斥锁.md","raw":"title: 互斥锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:13:00\n\n---\n\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","slug":"互斥锁","published":1,"updated":"2022-04-04T08:32:40.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0f007a7kt98nxhcvbu","content":"<h2 id=\"1、关于“互斥”和“同步”的概念\">1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了<br>\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2 id=\"2、JAVA中synchronized和Lock是互斥锁\">2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象<br>\n修饰在实例方法上，锁对象是当前实例对象<br>\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2 id=\"3、synchronized的用法\">3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo &#123;\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1()&#123;\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     &#125;\n\n    public synchronized void s2() &#123;\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     &#125;\n\n    public void s3() &#123;\n         synchronized (LOCK) &#123;\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         &#125;\n     &#125;\n\n    public void s4() &#123;\n         synchronized (SynchronizedDemo.class) &#123;\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         &#125;\n     &#125;\n\n&#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、关于“互斥”和“同步”的概念\">1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了<br>\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2 id=\"2、JAVA中synchronized和Lock是互斥锁\">2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象<br>\n修饰在实例方法上，锁对象是当前实例对象<br>\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2 id=\"3、synchronized的用法\">3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo &#123;\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1()&#123;\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     &#125;\n\n    public synchronized void s2() &#123;\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     &#125;\n\n    public void s3() &#123;\n         synchronized (LOCK) &#123;\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         &#125;\n     &#125;\n\n    public void s4() &#123;\n         synchronized (SynchronizedDemo.class) &#123;\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         &#125;\n     &#125;\n\n&#125;\n\n \n</code></pre>\n"},{"title":"伪共享","author":"郑天祺","date":"2020-12-14T05:34:00.000Z","_content":"\n# 1、CPU缓存介绍\n\n​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。\n\n​\t\t图具有3级缓存的处理器\n\n![image-20201214133601256](/img/image-20201214133601256.png)\n\n![image-20201214133612299](/img/image-20201214133612299.png)\n\n图片来自： https://lwn.net/Articles/252125/\n\n​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。\n\n​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，\n\n​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。\n\n# 2、L1 L2 L3\n\nL1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。\n\n# 3、缓存命中或错过和延迟\n\n数据从RAM流到L3缓存，然后是L2，最后是L1。\n\n当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。\n\n然后它继续在L2中找到它，然后在L3中找到它。\n\n如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。\n\n# 4、缓存行（Cache Line）\n\n缓存，是由缓存行组成的。一般一行缓存行有64字节\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。\n\n# 5、伪共享的发生\n\n![image-20201214133830361](/img/image-20201214133830361.png)\n\n## 产生原因：\n\n数据X、Y、Z被加载到同一Cache Line中，\n线程A在Core1修改X，线程B在Core2上修改Y\n\n根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。\n\n## 解决方法：\n\n此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。\n\n图片来自：https://blog.csdn.net/qq_27680317/article/details/78486220\n\nM 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)\n\n# 6、伪共享的实例\n\n## 6.1、伪共享的产生\n\n​\t\t假如业务场景中，上述的类满足以下几个特点：\n\n​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。\n\n​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。\n\n![image-20201214133935704](/img/image-20201214133935704.png)\n\n​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。\n\n![image-20201214133955605](/img/image-20201214133955605.png)\n\n## 6.2、解决方法\n\n### （1）缓存行的填充\n\n![image-20201214134132982](/img/image-20201214134132982.png)\n\n​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。\n\n​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数\n\n​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求\n\n## 6.2、解决方法\n\n### （2）Contended注解方式\n\n![image-20201214134208507](/img/image-20201214134208507.png)\n\n​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能 \n\n采取上述措施图示：\n\n![image-20201214134235050](/img/image-20201214134235050.png)\n\n更多实例：ConcurrentHashMap、Thread 、Disruptor","source":"_posts/伪共享.md","raw":"title: 伪共享\nauthor: 郑天祺\ntags:\n  - 伪共享\ncategories:\n  - 操作系统\ndate: 2020-12-14 13:34:00\n\n---\n\n# 1、CPU缓存介绍\n\n​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。\n\n​\t\t图具有3级缓存的处理器\n\n![image-20201214133601256](/img/image-20201214133601256.png)\n\n![image-20201214133612299](/img/image-20201214133612299.png)\n\n图片来自： https://lwn.net/Articles/252125/\n\n​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。\n\n​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，\n\n​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。\n\n# 2、L1 L2 L3\n\nL1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。\n\n# 3、缓存命中或错过和延迟\n\n数据从RAM流到L3缓存，然后是L2，最后是L1。\n\n当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。\n\n然后它继续在L2中找到它，然后在L3中找到它。\n\n如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。\n\n# 4、缓存行（Cache Line）\n\n缓存，是由缓存行组成的。一般一行缓存行有64字节\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。\n\n# 5、伪共享的发生\n\n![image-20201214133830361](/img/image-20201214133830361.png)\n\n## 产生原因：\n\n数据X、Y、Z被加载到同一Cache Line中，\n线程A在Core1修改X，线程B在Core2上修改Y\n\n根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。\n\n## 解决方法：\n\n此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。\n\n图片来自：https://blog.csdn.net/qq_27680317/article/details/78486220\n\nM 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)\n\n# 6、伪共享的实例\n\n## 6.1、伪共享的产生\n\n​\t\t假如业务场景中，上述的类满足以下几个特点：\n\n​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。\n\n​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。\n\n![image-20201214133935704](/img/image-20201214133935704.png)\n\n​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。\n\n![image-20201214133955605](/img/image-20201214133955605.png)\n\n## 6.2、解决方法\n\n### （1）缓存行的填充\n\n![image-20201214134132982](/img/image-20201214134132982.png)\n\n​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。\n\n​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数\n\n​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求\n\n## 6.2、解决方法\n\n### （2）Contended注解方式\n\n![image-20201214134208507](/img/image-20201214134208507.png)\n\n​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能 \n\n采取上述措施图示：\n\n![image-20201214134235050](/img/image-20201214134235050.png)\n\n更多实例：ConcurrentHashMap、Thread 、Disruptor","slug":"伪共享","published":1,"updated":"2022-04-04T08:32:40.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0g007e7kt9e52j50jx","content":"<h1>1、CPU缓存介绍</h1>\n<p>​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。</p>\n<p>​\t\t图具有3级缓存的处理器</p>\n<p><img src=\"/img/image-20201214133601256.png\" alt=\"image-20201214133601256\"></p>\n<p><img src=\"/img/image-20201214133612299.png\" alt=\"image-20201214133612299\"></p>\n<p>图片来自： <a href=\"https://lwn.net/Articles/252125/\">https://lwn.net/Articles/252125/</a></p>\n<p>​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。</p>\n<p>​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，</p>\n<p>​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。</p>\n<h1>2、L1 L2 L3</h1>\n<p>L1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。<br>\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。<br>\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。</p>\n<h1>3、缓存命中或错过和延迟</h1>\n<p>数据从RAM流到L3缓存，然后是L2，最后是L1。</p>\n<p>当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。</p>\n<p>然后它继续在L2中找到它，然后在L3中找到它。</p>\n<p>如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。</p>\n<h1>4、缓存行（Cache Line）</h1>\n<p>缓存，是由缓存行组成的。一般一行缓存行有64字节<br>\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。</p>\n<h1>5、伪共享的发生</h1>\n<p><img src=\"/img/image-20201214133830361.png\" alt=\"image-20201214133830361\"></p>\n<h2 id=\"产生原因：\">产生原因：</h2>\n<p>数据X、Y、Z被加载到同一Cache Line中，<br>\n线程A在Core1修改X，线程B在Core2上修改Y</p>\n<p>根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；<br>\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。</p>\n<h2 id=\"解决方法：\">解决方法：</h2>\n<p>此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。</p>\n<p>图片来自：<a href=\"https://blog.csdn.net/qq_27680317/article/details/78486220\">https://blog.csdn.net/qq_27680317/article/details/78486220</a></p>\n<p>M 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)</p>\n<h1>6、伪共享的实例</h1>\n<h2 id=\"6-1、伪共享的产生\">6.1、伪共享的产生</h2>\n<p>​\t\t假如业务场景中，上述的类满足以下几个特点：</p>\n<p>​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。</p>\n<p>​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。</p>\n<p><img src=\"/img/image-20201214133935704.png\" alt=\"image-20201214133935704\"></p>\n<p>​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。</p>\n<p><img src=\"/img/image-20201214133955605.png\" alt=\"image-20201214133955605\"></p>\n<h2 id=\"6-2、解决方法\">6.2、解决方法</h2>\n<h3 id=\"（1）缓存行的填充\">（1）缓存行的填充</h3>\n<p><img src=\"/img/image-20201214134132982.png\" alt=\"image-20201214134132982\"></p>\n<p>​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。</p>\n<p>​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数</p>\n<p>​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求</p>\n<h2 id=\"6-2、解决方法-2\">6.2、解决方法</h2>\n<h3 id=\"（2）Contended注解方式\">（2）Contended注解方式</h3>\n<p><img src=\"/img/image-20201214134208507.png\" alt=\"image-20201214134208507\"></p>\n<p>​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能</p>\n<p>采取上述措施图示：</p>\n<p><img src=\"/img/image-20201214134235050.png\" alt=\"image-20201214134235050\"></p>\n<p>更多实例：ConcurrentHashMap、Thread 、Disruptor</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、CPU缓存介绍</h1>\n<p>​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。</p>\n<p>​\t\t图具有3级缓存的处理器</p>\n<p><img src=\"/img/image-20201214133601256.png\" alt=\"image-20201214133601256\"></p>\n<p><img src=\"/img/image-20201214133612299.png\" alt=\"image-20201214133612299\"></p>\n<p>图片来自： <a href=\"https://lwn.net/Articles/252125/\">https://lwn.net/Articles/252125/</a></p>\n<p>​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。</p>\n<p>​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，</p>\n<p>​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。</p>\n<h1>2、L1 L2 L3</h1>\n<p>L1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。<br>\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。<br>\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。</p>\n<h1>3、缓存命中或错过和延迟</h1>\n<p>数据从RAM流到L3缓存，然后是L2，最后是L1。</p>\n<p>当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。</p>\n<p>然后它继续在L2中找到它，然后在L3中找到它。</p>\n<p>如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。</p>\n<h1>4、缓存行（Cache Line）</h1>\n<p>缓存，是由缓存行组成的。一般一行缓存行有64字节<br>\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。</p>\n<h1>5、伪共享的发生</h1>\n<p><img src=\"/img/image-20201214133830361.png\" alt=\"image-20201214133830361\"></p>\n<h2 id=\"产生原因：\">产生原因：</h2>\n<p>数据X、Y、Z被加载到同一Cache Line中，<br>\n线程A在Core1修改X，线程B在Core2上修改Y</p>\n<p>根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；<br>\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。</p>\n<h2 id=\"解决方法：\">解决方法：</h2>\n<p>此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。</p>\n<p>图片来自：<a href=\"https://blog.csdn.net/qq_27680317/article/details/78486220\">https://blog.csdn.net/qq_27680317/article/details/78486220</a></p>\n<p>M 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)</p>\n<h1>6、伪共享的实例</h1>\n<h2 id=\"6-1、伪共享的产生\">6.1、伪共享的产生</h2>\n<p>​\t\t假如业务场景中，上述的类满足以下几个特点：</p>\n<p>​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。</p>\n<p>​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。</p>\n<p><img src=\"/img/image-20201214133935704.png\" alt=\"image-20201214133935704\"></p>\n<p>​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。</p>\n<p><img src=\"/img/image-20201214133955605.png\" alt=\"image-20201214133955605\"></p>\n<h2 id=\"6-2、解决方法\">6.2、解决方法</h2>\n<h3 id=\"（1）缓存行的填充\">（1）缓存行的填充</h3>\n<p><img src=\"/img/image-20201214134132982.png\" alt=\"image-20201214134132982\"></p>\n<p>​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。</p>\n<p>​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数</p>\n<p>​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求</p>\n<h2 id=\"6-2、解决方法-2\">6.2、解决方法</h2>\n<h3 id=\"（2）Contended注解方式\">（2）Contended注解方式</h3>\n<p><img src=\"/img/image-20201214134208507.png\" alt=\"image-20201214134208507\"></p>\n<p>​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能</p>\n<p>采取上述措施图示：</p>\n<p><img src=\"/img/image-20201214134235050.png\" alt=\"image-20201214134235050\"></p>\n<p>更多实例：ConcurrentHashMap、Thread 、Disruptor</p>\n"},{"title":"信息系统项目管理师-信息化","author":"ztq","date":"2021-04-15T11:34:00.000Z","_content":"\n![image-20210415193556837](/img/image-20210415193556837.png)","source":"_posts/信息系统项目管理师-信息化-1.md","raw":"title: 信息系统项目管理师-信息化\nauthor: ztq\ntags:\n  - 信息系统项目管理师\ncategories:\n  - 软件管理\ndate: 2021-04-15 19:34:00\n\n---\n\n![image-20210415193556837](/img/image-20210415193556837.png)","slug":"信息系统项目管理师-信息化-1","published":1,"updated":"2022-04-04T08:32:40.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0g007g7kt9c1stcdtf","content":"<p><img src=\"/img/image-20210415193556837.png\" alt=\"image-20210415193556837\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/img/image-20210415193556837.png\" alt=\"image-20210415193556837\"></p>\n"},{"title":"偏向锁","author":"郑天祺","date":"2019-08-31T05:22:00.000Z","_content":"\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","source":"_posts/偏向锁.md","raw":"title: 偏向锁\nauthor: 郑天祺\ntags:\n\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:22:00\n\n---\n\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","slug":"偏向锁","published":1,"updated":"2022-04-04T08:32:40.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0h007k7kt9e4cj7jjb","content":"<h2 id=\"0、从偏向锁到重量锁\">0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test()&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2 id=\"1、先了解一下对象在JVM内存中的布局，如下图\">1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2 id=\"2、锁的创建\">2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.<br>\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.<br>\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。<br>\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"0、从偏向锁到重量锁\">0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test()&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2 id=\"1、先了解一下对象在JVM内存中的布局，如下图\">1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2 id=\"2、锁的创建\">2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.<br>\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.<br>\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。<br>\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n"},{"title":"公平锁、非公平锁","author":"郑天祺","date":"2019-08-31T05:21:00.000Z","_content":"\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","source":"_posts/公平锁、非公平锁.md","raw":"title: 公平锁、非公平锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:21:00\n\n---\n\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","slug":"公平锁、非公平锁","published":1,"updated":"2022-04-04T08:32:40.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0i007n7kt9e7b7fzzn","content":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。<br>\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： <a href=\"https://www.cnblogs.com/little-fly/p/10365109.html\">https://www.cnblogs.com/little-fly/p/10365109.html</a></p>\n<p>​        <a href=\"https://www.jianshu.com/p/06340f8feb05\">https://www.jianshu.com/p/06340f8feb05</a></p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：<a href=\"https://www.jianshu.com/p/c7d17b5c6be3\">https://www.jianshu.com/p/c7d17b5c6be3</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。<br>\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： <a href=\"https://www.cnblogs.com/little-fly/p/10365109.html\">https://www.cnblogs.com/little-fly/p/10365109.html</a></p>\n<p>​        <a href=\"https://www.jianshu.com/p/06340f8feb05\">https://www.jianshu.com/p/06340f8feb05</a></p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：<a href=\"https://www.jianshu.com/p/c7d17b5c6be3\">https://www.jianshu.com/p/c7d17b5c6be3</a></p>\n"},{"title":"分布式CAP概念","author":"郑天祺","date":"2020-12-14T05:47:00.000Z","_content":"\n​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；\n\n## CAP原理概述\n\n​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个\n\n![image-20201214134902337](/img/image-20201214134902337.png)\n\n## Consistency 一致性\n\n​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。\n\n## 三种一致性策略\n\n​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。\n\n## Availability 可用性\n\n​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：\n\n![image-20201214135053884](/img/image-20201214135053884.png)\n\n​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)*365*24*60 = 5.256 min，这是一个极高的要求。\n\n​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。\n\n## Partition Tolerance分区容错性\n\n​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。\n\n## CA without P\n\n​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。\n\n## CP without A\n\n​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？\n\n## CPwithoutA示例说明\n\n ZooKeeper是个CP（一致性+分区容错性）\n\t\t即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。\n\n## HBase是强一致性系统\n\nHbase具有以下特点\n•\t每个值只出现在一个REGION\n•\t同一时间一个Region只分配给一个Region服务器\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。\n•\tput操作要么成功，要么完全失败。\n\n​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。\n\n​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。\n\n## CPwithoutA示例说明\n\n## AP wihtout C\n\n​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。\n\n## APwithoutC示例说明\n\n​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n\n## APwithoutC示例说明\n\n​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。","source":"_posts/分布式CAP概念.md","raw":"title: 分布式CAP概念\nauthor: 郑天祺\ntags:\n\n  - CAP\ncategories:\n  - 分布式\ndate: 2020-12-14 13:47:00\n\n---\n\n​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；\n\n## CAP原理概述\n\n​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个\n\n![image-20201214134902337](/img/image-20201214134902337.png)\n\n## Consistency 一致性\n\n​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。\n\n## 三种一致性策略\n\n​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。\n\n## Availability 可用性\n\n​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：\n\n![image-20201214135053884](/img/image-20201214135053884.png)\n\n​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)*365*24*60 = 5.256 min，这是一个极高的要求。\n\n​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。\n\n## Partition Tolerance分区容错性\n\n​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。\n\n## CA without P\n\n​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。\n\n## CP without A\n\n​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？\n\n## CPwithoutA示例说明\n\n ZooKeeper是个CP（一致性+分区容错性）\n\t\t即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。\n\n## HBase是强一致性系统\n\nHbase具有以下特点\n•\t每个值只出现在一个REGION\n•\t同一时间一个Region只分配给一个Region服务器\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。\n•\tput操作要么成功，要么完全失败。\n\n​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。\n\n​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。\n\n## CPwithoutA示例说明\n\n## AP wihtout C\n\n​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。\n\n## APwithoutC示例说明\n\n​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n\n## APwithoutC示例说明\n\n​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。","slug":"分布式CAP概念","published":1,"updated":"2022-04-04T08:32:40.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0j007q7kt9h79r7oqa","content":"<p>​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；</p>\n<h2 id=\"CAP原理概述\">CAP原理概述</h2>\n<p>​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个</p>\n<p><img src=\"/img/image-20201214134902337.png\" alt=\"image-20201214134902337\"></p>\n<h2 id=\"Consistency-一致性\">Consistency 一致性</h2>\n<p>​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。<br>\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。<br>\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。</p>\n<h2 id=\"三种一致性策略\">三种一致性策略</h2>\n<p>​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。<br>\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。<br>\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。<br>\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。</p>\n<h2 id=\"Availability-可用性\">Availability 可用性</h2>\n<p>​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。<br>\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：</p>\n<p><img src=\"/img/image-20201214135053884.png\" alt=\"image-20201214135053884\"></p>\n<p>​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)<em>365</em>24*60 = 5.256 min，这是一个极高的要求。</p>\n<p>​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。</p>\n<h2 id=\"Partition-Tolerance分区容错性\">Partition Tolerance分区容错性</h2>\n<p>​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。<br>\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。<br>\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。</p>\n<h2 id=\"CA-without-P\">CA without P</h2>\n<p>​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。<br>\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。<br>\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：<br>\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。<br>\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。<br>\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。</p>\n<h2 id=\"CP-without-A\">CP without A</h2>\n<p>​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。<br>\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。<br>\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p>\n<h2 id=\"CPwithoutA示例说明\">CPwithoutA示例说明</h2>\n<p>ZooKeeper是个CP（一致性+分区容错性）<br>\n即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。</p>\n<h2 id=\"HBase是强一致性系统\">HBase是强一致性系统</h2>\n<p>Hbase具有以下特点<br>\n•\t每个值只出现在一个REGION<br>\n•\t同一时间一个Region只分配给一个Region服务器<br>\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。<br>\n•\tput操作要么成功，要么完全失败。</p>\n<p>​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。</p>\n<p>​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。</p>\n<h2 id=\"CPwithoutA示例说明-2\">CPwithoutA示例说明</h2>\n<h2 id=\"AP-wihtout-C\">AP wihtout C</h2>\n<p>​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。<br>\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p>\n<h2 id=\"APwithoutC示例说明\">APwithoutC示例说明</h2>\n<p>​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。<br>\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。<br>\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。<br>\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p>\n<h2 id=\"APwithoutC示例说明-2\">APwithoutC示例说明</h2>\n<p>​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。<br>\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。<br>\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；</p>\n<h2 id=\"CAP原理概述\">CAP原理概述</h2>\n<p>​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个</p>\n<p><img src=\"/img/image-20201214134902337.png\" alt=\"image-20201214134902337\"></p>\n<h2 id=\"Consistency-一致性\">Consistency 一致性</h2>\n<p>​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。<br>\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。<br>\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。</p>\n<h2 id=\"三种一致性策略\">三种一致性策略</h2>\n<p>​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。<br>\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。<br>\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。<br>\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。</p>\n<h2 id=\"Availability-可用性\">Availability 可用性</h2>\n<p>​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。<br>\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：</p>\n<p><img src=\"/img/image-20201214135053884.png\" alt=\"image-20201214135053884\"></p>\n<p>​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)<em>365</em>24*60 = 5.256 min，这是一个极高的要求。</p>\n<p>​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。</p>\n<h2 id=\"Partition-Tolerance分区容错性\">Partition Tolerance分区容错性</h2>\n<p>​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。<br>\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。<br>\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。</p>\n<h2 id=\"CA-without-P\">CA without P</h2>\n<p>​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。<br>\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。<br>\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：<br>\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。<br>\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。<br>\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。</p>\n<h2 id=\"CP-without-A\">CP without A</h2>\n<p>​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。<br>\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。<br>\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p>\n<h2 id=\"CPwithoutA示例说明\">CPwithoutA示例说明</h2>\n<p>ZooKeeper是个CP（一致性+分区容错性）<br>\n即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。</p>\n<h2 id=\"HBase是强一致性系统\">HBase是强一致性系统</h2>\n<p>Hbase具有以下特点<br>\n•\t每个值只出现在一个REGION<br>\n•\t同一时间一个Region只分配给一个Region服务器<br>\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。<br>\n•\tput操作要么成功，要么完全失败。</p>\n<p>​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。</p>\n<p>​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。</p>\n<h2 id=\"CPwithoutA示例说明-2\">CPwithoutA示例说明</h2>\n<h2 id=\"AP-wihtout-C\">AP wihtout C</h2>\n<p>​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。<br>\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p>\n<h2 id=\"APwithoutC示例说明\">APwithoutC示例说明</h2>\n<p>​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。<br>\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。<br>\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。<br>\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p>\n<h2 id=\"APwithoutC示例说明-2\">APwithoutC示例说明</h2>\n<p>​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。<br>\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。<br>\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。</p>\n"},{"title":"分布式全局唯一ID生成策略","author":"郑天祺","date":"2019-10-09T04:00:00.000Z","_content":"\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","source":"_posts/分布式全局唯一ID生成策略.md","raw":"title: 分布式全局唯一ID生成策略\nauthor: 郑天祺\ntags:\n\n  - 分布式\ncategories:\n  - 分布式\ndate: 2019-10-09 12:00:00\n\n---\n\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","slug":"分布式全局唯一ID生成策略","published":1,"updated":"2022-04-04T08:32:40.167Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0k007s7kt97a0rfmab","content":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2 id=\"1、雪花算法-snowflake\">1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3 id=\"（1）优点：\"><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3 id=\"（2）缺点：\"><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2 id=\"2、UUID\">2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3 id=\"1-优点：\">(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3 id=\"2-缺点：\">(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2 id=\"3、利用数据库\">3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/#description\">https://docs.mongodb.com/manual/reference/method/ObjectId/#description</a><br>\nLeaf——美团点评分布式ID生成系统: <a href=\"https://tech.meituan.com/2017/04/21/mt-leaf.html\">https://tech.meituan.com/2017/04/21/mt-leaf.html</a><br>\n分布式ID生成 - 雪花算法: <a href=\"https://blog.csdn.net/u012488504/article/details/82194495\">https://blog.csdn.net/u012488504/article/details/82194495</a><br>\n梅丽莎病毒: <a href=\"https://baike.baidu.com/item/%E6%A2%85%E4%B8%BD%E8%8E%8E%E7%97%85%E6%AF%92/9739231\">https://baike.baidu.com/item/梅丽莎病毒/9739231</a><br>\nmysql中InnoDB表为什么要建议用自增列做主键: <a href=\"https://www.cnblogs.com/moyand/p/9013663.html\">https://www.cnblogs.com/moyand/p/9013663.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2 id=\"1、雪花算法-snowflake\">1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3 id=\"（1）优点：\"><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3 id=\"（2）缺点：\"><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2 id=\"2、UUID\">2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3 id=\"1-优点：\">(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3 id=\"2-缺点：\">(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2 id=\"3、利用数据库\">3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/#description\">https://docs.mongodb.com/manual/reference/method/ObjectId/#description</a><br>\nLeaf——美团点评分布式ID生成系统: <a href=\"https://tech.meituan.com/2017/04/21/mt-leaf.html\">https://tech.meituan.com/2017/04/21/mt-leaf.html</a><br>\n分布式ID生成 - 雪花算法: <a href=\"https://blog.csdn.net/u012488504/article/details/82194495\">https://blog.csdn.net/u012488504/article/details/82194495</a><br>\n梅丽莎病毒: <a href=\"https://baike.baidu.com/item/%E6%A2%85%E4%B8%BD%E8%8E%8E%E7%97%85%E6%AF%92/9739231\">https://baike.baidu.com/item/梅丽莎病毒/9739231</a><br>\nmysql中InnoDB表为什么要建议用自增列做主键: <a href=\"https://www.cnblogs.com/moyand/p/9013663.html\">https://www.cnblogs.com/moyand/p/9013663.html</a></p>\n"},{"title":"初识redis（1）-数据结构","author":"ztq","date":"2021-07-31T04:49:00.000Z","_content":"\n# 1、Redis数据结构介绍\n\n（1）STRING    \n\n​\t\tvalue：可以是字符串、整数或者浮点数      \n\n​\t\toperate：对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数执行自增（increment）或者自减(decrement)操作\n\n（2）LIST\n\n​\t\tvalue：一个链表，链表上的每个节点都包含了—个字符串\n\n​\t\toperate：从链表的两端推入或者弹出元素;根据偏移量对链表进行修剪(trim)；读取单个或者多个元素;根据值查找或者移除元素\n\n（3）SET\n\n​\t\tvalue：包含字符串的无序收集器( unordered collection)，并且被包含的每个字符串都是独一无二、各不相同的\n\n​\t\toperate：添加、获取、移除单个元素；检查一个元素是否存在于集合中;计算交集、并集、差集；从集合里面随机获取元素\n\n（4）HASH\n\n​\t\tvalue：包含键值对的无序散列表\n\n​\t\toperate：添加、获取、移除单个键值对;获取所有键值对\n\n（5）ZSET（有序集合)\n\n​\t\tvalue：字符串成员( member)与浮点数分值( score）之间的有序映射，元素的排列顺序由分值的大小决定\n\n​\t\toperate：添加、获取、删除单个元素；根据分值范围（range)或者成员来获取元素\n\n# 2、常用命令\n\n## （1）STRING字符串命令\n\n- SET key-name value\t\t设置值\n- GET key-name\t\t获得值\n- DEL key-name\t\t删除值\n\n例子：\n\n```java\n127.0.0.1:6379> set hello world\nOK\n127.0.0.1:6379> get hello\n\"world\"\n127.0.0.1:6379> del hello\n(integer) 1\n127.0.0.1:6379> get hello\n(nil)\n```\n\n- INCR key-name\t\t将键存储的值加上1\n- DECR key-name\t\t将键存储的值减去1\n- INCRBY key-name amount\t\t将键存储的值加上整数amount\n- DECRBY key-name amount\t\t将键存储的值减去整数amount\n- INCRBYFLOAT key-name amount\t\t将键存储的值加上浮点数 amount，这个命令在Redis 2.6或以上的版本可用\n\n例子：\n\n```\n127.0.0.1:6379> set name 1\nOK\n127.0.0.1:6379> incr name\n(integer) 2\n127.0.0.1:6379> get name\n\"2\"\n127.0.0.1:6379> decr name\n(integer) 1\n127.0.0.1:6379> get name\n\"1\"\n127.0.0.1:6379> incrby name 2\n(integer) 3\n127.0.0.1:6379> get name\n\"3\"\n```\n\n\n\n## （2）LIST列表命令\n\n- RPUSH key-name value [value ...]\t\t将一个或多个值推入列表的右端\n- LPUSH key-name value [value ...]\t\t将一个或多个值推入列表的左端\n- RPOP key-name\t\t移除并返回列表最右端的元素\n- LPOP key-name\t\t移除并返回列表最左端的元素\n- LINDEX key-name offset\t\t返回列表中偏移量为offset的元素\n- LRANGE key-name start end\t\t返回列表从start偏移量到end偏移量范围内的所有元素,其中偏移量为start和偏移量为end 的元素也会包含在被返回的元素之内\n- LTRIM key-name start end\t\t对列表进行修剪，只保留从start偏移量到end偏移量范围内的元素，其中偏移量为start和偏移量为end的元素也会被保留\n\n例子：\n\n```java\n127.0.0.1:6379> rpush myList 1 2 3 4\n(integer) 4\n127.0.0.1:6379> lrange myList 0 4\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n127.0.0.1:6379> lrange myList 0 -1\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n127.0.0.1:6379> lpush myList -1 0\n(integer) 6\n127.0.0.1:6379> lrange myList 0 -1\n1) \"0\"\n2) \"-1\"\n3) \"1\"\n4) \"2\"\n5) \"3\"\n6) \"4\"\n```\n\n- BLPOP key-name [ key-name ...] timeout\t\t从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现\n- BRPOP key-name [key-name ...] timeout\t\t从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现\n- RPOPLPUSH source-key dest-key\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素\n- BRPOPLPUSH source-key dest-key timeout\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素；如果source-key为空，那么在timeout秒之内阻塞并等待可弹出的元素出现\n\n```java\n127.0.0.1:6379> blpop myList 2\n1) \"myList\"\n2) \"0\"\n127.0.0.1:6379> blpop myList 2000\n1) \"myList\"\n2) \"-1\"\n127.0.0.1:6379> blpop myList 6000\n1) \"myList\"\n2) \"1\"\n127.0.0.1:6379> brpop myList 2\n1) \"myList\"\n2) \"4\"\n```\n\n# （3）SET集合命令\n\n- SADD key-name item [item ...]\t\t将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量\n- SREM key-name item [item ...]\t\t从集合里面移除一个或多个元素，并返回被移除元素的数量\n- SISMEMBER key-name item\t\t检查元素item是否存在于集合key-name 里\n- SCARD key-name\t\t返回集合包含的元素的数量\n- SMEMBERS key-name\t\t返回集合包含的所有元素\n- SRANDMEMBER key-name [count]\t\t从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复:当count为负数时，命令返回的随机元素可能会出现重复\n- SPOP key-name\t\t随机地移除集合中的一个元素，并返回被移除的元素\n- SMOVE source-key dest-key item\t\t如果集合source-key包含元素item，那么从集合source-key里面移除元素item，并将元素item添加到集合dest-key中;如果item被成功移除,那么命令返回1，否则返回0\n\n## 用于组合和处理多个集合的Redis命令\n\n- SDIFF key-name [key-name ...]\t\t返回那些存在于第一个集合、但不存在于其他集合中的元素（数学上的差集运算)\n- SDIFFSTORE dest-key key-name [key-name ...]\t\t将那些存在于第一个集合但并不存在于其他集合中的元素（数学上的差集运算）存储到dest-key键里面\n- SINTER key-name [key-name ...]\t\t返回那些同时存在于所有集合中的元素（数学上的交集运算)\n- SINTERSTORE dest-key key-name [key-name ...]\t\t将那些同时存在于所有集合的元素（数学上的交集运算)存储到dest-key键里面\n- SUNION key-name [key-name ...]\t\t返回那些至少存在于一个集合中的元素（数学上的并集计算)\n- SUNIONSTORE dest-key key-name [key-name ...]\t\t将那些至少存在于一个集合中的元素（数学上的并集计算)存储到dest-key键里面\n\n# （4）HASH散列命令\n\nHMGET key-name key [key ...]\t\t从散列里面获取一个或多个键的值\n\nHMSET key-name key value [key value ...]\t\t为散列里面的一个或多个键设置值\n\nHDEL key-name key [key ...]\t\t删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量\n\nHLEN key-name\t\t返回散列包含的键值对数量\n\n## 散列的更高级特性\n\nHEXISTS key-name key\t\t检查给定键是否存在于散列中HKEYS key-name———获取散列包含的所有键\n\nHKEYS key-name\t\t获取散列包含的所有键\n\nHVALS key-name\t\t获取散列包含的所有值\n\nHGETALL key-name\t\t获取散列包含的所有键值对\n\nHINCRBY key-name key increment\t\t将键key存储的值加上整数increment\n\nHINCRBYFLOAT key-name key increment\t\t将键key存储的值加上浮点数increment\n\n# （5）ZSET有序集合命令\n\nZADD key-name score member [score member ...]\t\t将带有给定分值的成员添加到有序集合里面\n\nZREM key-name member [member ...]\t\t从有序集合里面移除给定的成员，并返回被移除成员的数量\n\nZCARD key-name\t\t返回有序集合包含的成员数量\n\nZINCRBY key-name increment member\t\t将member成员的分值加上increment\n\nZCOUNT key-name min max——返回分值介于min和max之间的成员数量\n\nZRANK key-name member\t\t返回成员member在有序集合中的排名\n\nZSCORE key-name member\t\t返回成员member的分值\n\nZRANGE key-name start stop [WITHSCORES]\t\t返回有序集合中排名介于start和 stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回\n\n## 有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令\n\nZREVRANK key-name member\t\t返回有序集合里成员member的排名，成员按照分值从大到小排列\n\nZREVRANGE key-name start stop [WITHSCORES]\t\t返回有序集合给定排名范围内的成员，成员按照分值从大到小排列\n\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]\t\t返回有序集合中，分值介于min和max之间的所有成员\n\nZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]\t\t获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回它们\n\nZREMRANGEBYRANK key-name start stop\t\t移除有序集合中排名介于start和stop之间的所有成员\n\nZREMRANGEBYSCORE key-name min max\t\t移除有序集合中分值介于min和 max之间的所有成员\n\nZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight[ weight ...]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的交集运算\n\nZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight[ weight ...]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的并集运算\n","source":"_posts/初识redis（1）.md","raw":"title: 初识redis（1）-数据结构\nauthor: ztq\ntags:\n  - redis\ncategories:\n  - 数据库\ndate: 2021-07-31 12:49:00\n---\n\n# 1、Redis数据结构介绍\n\n（1）STRING    \n\n​\t\tvalue：可以是字符串、整数或者浮点数      \n\n​\t\toperate：对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数执行自增（increment）或者自减(decrement)操作\n\n（2）LIST\n\n​\t\tvalue：一个链表，链表上的每个节点都包含了—个字符串\n\n​\t\toperate：从链表的两端推入或者弹出元素;根据偏移量对链表进行修剪(trim)；读取单个或者多个元素;根据值查找或者移除元素\n\n（3）SET\n\n​\t\tvalue：包含字符串的无序收集器( unordered collection)，并且被包含的每个字符串都是独一无二、各不相同的\n\n​\t\toperate：添加、获取、移除单个元素；检查一个元素是否存在于集合中;计算交集、并集、差集；从集合里面随机获取元素\n\n（4）HASH\n\n​\t\tvalue：包含键值对的无序散列表\n\n​\t\toperate：添加、获取、移除单个键值对;获取所有键值对\n\n（5）ZSET（有序集合)\n\n​\t\tvalue：字符串成员( member)与浮点数分值( score）之间的有序映射，元素的排列顺序由分值的大小决定\n\n​\t\toperate：添加、获取、删除单个元素；根据分值范围（range)或者成员来获取元素\n\n# 2、常用命令\n\n## （1）STRING字符串命令\n\n- SET key-name value\t\t设置值\n- GET key-name\t\t获得值\n- DEL key-name\t\t删除值\n\n例子：\n\n```java\n127.0.0.1:6379> set hello world\nOK\n127.0.0.1:6379> get hello\n\"world\"\n127.0.0.1:6379> del hello\n(integer) 1\n127.0.0.1:6379> get hello\n(nil)\n```\n\n- INCR key-name\t\t将键存储的值加上1\n- DECR key-name\t\t将键存储的值减去1\n- INCRBY key-name amount\t\t将键存储的值加上整数amount\n- DECRBY key-name amount\t\t将键存储的值减去整数amount\n- INCRBYFLOAT key-name amount\t\t将键存储的值加上浮点数 amount，这个命令在Redis 2.6或以上的版本可用\n\n例子：\n\n```\n127.0.0.1:6379> set name 1\nOK\n127.0.0.1:6379> incr name\n(integer) 2\n127.0.0.1:6379> get name\n\"2\"\n127.0.0.1:6379> decr name\n(integer) 1\n127.0.0.1:6379> get name\n\"1\"\n127.0.0.1:6379> incrby name 2\n(integer) 3\n127.0.0.1:6379> get name\n\"3\"\n```\n\n\n\n## （2）LIST列表命令\n\n- RPUSH key-name value [value ...]\t\t将一个或多个值推入列表的右端\n- LPUSH key-name value [value ...]\t\t将一个或多个值推入列表的左端\n- RPOP key-name\t\t移除并返回列表最右端的元素\n- LPOP key-name\t\t移除并返回列表最左端的元素\n- LINDEX key-name offset\t\t返回列表中偏移量为offset的元素\n- LRANGE key-name start end\t\t返回列表从start偏移量到end偏移量范围内的所有元素,其中偏移量为start和偏移量为end 的元素也会包含在被返回的元素之内\n- LTRIM key-name start end\t\t对列表进行修剪，只保留从start偏移量到end偏移量范围内的元素，其中偏移量为start和偏移量为end的元素也会被保留\n\n例子：\n\n```java\n127.0.0.1:6379> rpush myList 1 2 3 4\n(integer) 4\n127.0.0.1:6379> lrange myList 0 4\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n127.0.0.1:6379> lrange myList 0 -1\n1) \"1\"\n2) \"2\"\n3) \"3\"\n4) \"4\"\n127.0.0.1:6379> lpush myList -1 0\n(integer) 6\n127.0.0.1:6379> lrange myList 0 -1\n1) \"0\"\n2) \"-1\"\n3) \"1\"\n4) \"2\"\n5) \"3\"\n6) \"4\"\n```\n\n- BLPOP key-name [ key-name ...] timeout\t\t从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现\n- BRPOP key-name [key-name ...] timeout\t\t从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现\n- RPOPLPUSH source-key dest-key\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素\n- BRPOPLPUSH source-key dest-key timeout\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素；如果source-key为空，那么在timeout秒之内阻塞并等待可弹出的元素出现\n\n```java\n127.0.0.1:6379> blpop myList 2\n1) \"myList\"\n2) \"0\"\n127.0.0.1:6379> blpop myList 2000\n1) \"myList\"\n2) \"-1\"\n127.0.0.1:6379> blpop myList 6000\n1) \"myList\"\n2) \"1\"\n127.0.0.1:6379> brpop myList 2\n1) \"myList\"\n2) \"4\"\n```\n\n# （3）SET集合命令\n\n- SADD key-name item [item ...]\t\t将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量\n- SREM key-name item [item ...]\t\t从集合里面移除一个或多个元素，并返回被移除元素的数量\n- SISMEMBER key-name item\t\t检查元素item是否存在于集合key-name 里\n- SCARD key-name\t\t返回集合包含的元素的数量\n- SMEMBERS key-name\t\t返回集合包含的所有元素\n- SRANDMEMBER key-name [count]\t\t从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复:当count为负数时，命令返回的随机元素可能会出现重复\n- SPOP key-name\t\t随机地移除集合中的一个元素，并返回被移除的元素\n- SMOVE source-key dest-key item\t\t如果集合source-key包含元素item，那么从集合source-key里面移除元素item，并将元素item添加到集合dest-key中;如果item被成功移除,那么命令返回1，否则返回0\n\n## 用于组合和处理多个集合的Redis命令\n\n- SDIFF key-name [key-name ...]\t\t返回那些存在于第一个集合、但不存在于其他集合中的元素（数学上的差集运算)\n- SDIFFSTORE dest-key key-name [key-name ...]\t\t将那些存在于第一个集合但并不存在于其他集合中的元素（数学上的差集运算）存储到dest-key键里面\n- SINTER key-name [key-name ...]\t\t返回那些同时存在于所有集合中的元素（数学上的交集运算)\n- SINTERSTORE dest-key key-name [key-name ...]\t\t将那些同时存在于所有集合的元素（数学上的交集运算)存储到dest-key键里面\n- SUNION key-name [key-name ...]\t\t返回那些至少存在于一个集合中的元素（数学上的并集计算)\n- SUNIONSTORE dest-key key-name [key-name ...]\t\t将那些至少存在于一个集合中的元素（数学上的并集计算)存储到dest-key键里面\n\n# （4）HASH散列命令\n\nHMGET key-name key [key ...]\t\t从散列里面获取一个或多个键的值\n\nHMSET key-name key value [key value ...]\t\t为散列里面的一个或多个键设置值\n\nHDEL key-name key [key ...]\t\t删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量\n\nHLEN key-name\t\t返回散列包含的键值对数量\n\n## 散列的更高级特性\n\nHEXISTS key-name key\t\t检查给定键是否存在于散列中HKEYS key-name———获取散列包含的所有键\n\nHKEYS key-name\t\t获取散列包含的所有键\n\nHVALS key-name\t\t获取散列包含的所有值\n\nHGETALL key-name\t\t获取散列包含的所有键值对\n\nHINCRBY key-name key increment\t\t将键key存储的值加上整数increment\n\nHINCRBYFLOAT key-name key increment\t\t将键key存储的值加上浮点数increment\n\n# （5）ZSET有序集合命令\n\nZADD key-name score member [score member ...]\t\t将带有给定分值的成员添加到有序集合里面\n\nZREM key-name member [member ...]\t\t从有序集合里面移除给定的成员，并返回被移除成员的数量\n\nZCARD key-name\t\t返回有序集合包含的成员数量\n\nZINCRBY key-name increment member\t\t将member成员的分值加上increment\n\nZCOUNT key-name min max——返回分值介于min和max之间的成员数量\n\nZRANK key-name member\t\t返回成员member在有序集合中的排名\n\nZSCORE key-name member\t\t返回成员member的分值\n\nZRANGE key-name start stop [WITHSCORES]\t\t返回有序集合中排名介于start和 stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回\n\n## 有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令\n\nZREVRANK key-name member\t\t返回有序集合里成员member的排名，成员按照分值从大到小排列\n\nZREVRANGE key-name start stop [WITHSCORES]\t\t返回有序集合给定排名范围内的成员，成员按照分值从大到小排列\n\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]\t\t返回有序集合中，分值介于min和max之间的所有成员\n\nZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]\t\t获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回它们\n\nZREMRANGEBYRANK key-name start stop\t\t移除有序集合中排名介于start和stop之间的所有成员\n\nZREMRANGEBYSCORE key-name min max\t\t移除有序集合中分值介于min和 max之间的所有成员\n\nZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight[ weight ...]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的交集运算\n\nZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight[ weight ...]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的并集运算\n","slug":"初识redis（1）","published":1,"updated":"2022-04-04T08:32:40.168Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0l007v7kt90jzlhazb","content":"<h1>1、Redis数据结构介绍</h1>\n<p>（1）STRING</p>\n<p>​\t\tvalue：可以是字符串、整数或者浮点数</p>\n<p>​\t\toperate：对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数执行自增（increment）或者自减(decrement)操作</p>\n<p>（2）LIST</p>\n<p>​\t\tvalue：一个链表，链表上的每个节点都包含了—个字符串</p>\n<p>​\t\toperate：从链表的两端推入或者弹出元素;根据偏移量对链表进行修剪(trim)；读取单个或者多个元素;根据值查找或者移除元素</p>\n<p>（3）SET</p>\n<p>​\t\tvalue：包含字符串的无序收集器( unordered collection)，并且被包含的每个字符串都是独一无二、各不相同的</p>\n<p>​\t\toperate：添加、获取、移除单个元素；检查一个元素是否存在于集合中;计算交集、并集、差集；从集合里面随机获取元素</p>\n<p>（4）HASH</p>\n<p>​\t\tvalue：包含键值对的无序散列表</p>\n<p>​\t\toperate：添加、获取、移除单个键值对;获取所有键值对</p>\n<p>（5）ZSET（有序集合)</p>\n<p>​\t\tvalue：字符串成员( member)与浮点数分值( score）之间的有序映射，元素的排列顺序由分值的大小决定</p>\n<p>​\t\toperate：添加、获取、删除单个元素；根据分值范围（range)或者成员来获取元素</p>\n<h1>2、常用命令</h1>\n<h2 id=\"（1）STRING字符串命令\">（1）STRING字符串命令</h2>\n<ul>\n<li>SET key-name value\t\t设置值</li>\n<li>GET key-name\t\t获得值</li>\n<li>DEL key-name\t\t删除值</li>\n</ul>\n<p>例子：</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; set hello world\nOK\n127.0.0.1:6379&gt; get hello\n&quot;world&quot;\n127.0.0.1:6379&gt; del hello\n(integer) 1\n127.0.0.1:6379&gt; get hello\n(nil)\n</code></pre>\n<ul>\n<li>INCR key-name\t\t将键存储的值加上1</li>\n<li>DECR key-name\t\t将键存储的值减去1</li>\n<li>INCRBY key-name amount\t\t将键存储的值加上整数amount</li>\n<li>DECRBY key-name amount\t\t将键存储的值减去整数amount</li>\n<li>INCRBYFLOAT key-name amount\t\t将键存储的值加上浮点数 amount，这个命令在Redis 2.6或以上的版本可用</li>\n</ul>\n<p>例子：</p>\n<pre><code>127.0.0.1:6379&gt; set name 1\nOK\n127.0.0.1:6379&gt; incr name\n(integer) 2\n127.0.0.1:6379&gt; get name\n&quot;2&quot;\n127.0.0.1:6379&gt; decr name\n(integer) 1\n127.0.0.1:6379&gt; get name\n&quot;1&quot;\n127.0.0.1:6379&gt; incrby name 2\n(integer) 3\n127.0.0.1:6379&gt; get name\n&quot;3&quot;\n</code></pre>\n<h2 id=\"（2）LIST列表命令\">（2）LIST列表命令</h2>\n<ul>\n<li>RPUSH key-name value [value …]\t\t将一个或多个值推入列表的右端</li>\n<li>LPUSH key-name value [value …]\t\t将一个或多个值推入列表的左端</li>\n<li>RPOP key-name\t\t移除并返回列表最右端的元素</li>\n<li>LPOP key-name\t\t移除并返回列表最左端的元素</li>\n<li>LINDEX key-name offset\t\t返回列表中偏移量为offset的元素</li>\n<li>LRANGE key-name start end\t\t返回列表从start偏移量到end偏移量范围内的所有元素,其中偏移量为start和偏移量为end 的元素也会包含在被返回的元素之内</li>\n<li>LTRIM key-name start end\t\t对列表进行修剪，只保留从start偏移量到end偏移量范围内的元素，其中偏移量为start和偏移量为end的元素也会被保留</li>\n</ul>\n<p>例子：</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; rpush myList 1 2 3 4\n(integer) 4\n127.0.0.1:6379&gt; lrange myList 0 4\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n127.0.0.1:6379&gt; lpush myList -1 0\n(integer) 6\n127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;0&quot;\n2) &quot;-1&quot;\n3) &quot;1&quot;\n4) &quot;2&quot;\n5) &quot;3&quot;\n6) &quot;4&quot;\n</code></pre>\n<ul>\n<li>BLPOP key-name [ key-name …] timeout\t\t从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现</li>\n<li>BRPOP key-name [key-name …] timeout\t\t从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现</li>\n<li>RPOPLPUSH source-key dest-key\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素</li>\n<li>BRPOPLPUSH source-key dest-key timeout\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素；如果source-key为空，那么在timeout秒之内阻塞并等待可弹出的元素出现</li>\n</ul>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; blpop myList 2\n1) &quot;myList&quot;\n2) &quot;0&quot;\n127.0.0.1:6379&gt; blpop myList 2000\n1) &quot;myList&quot;\n2) &quot;-1&quot;\n127.0.0.1:6379&gt; blpop myList 6000\n1) &quot;myList&quot;\n2) &quot;1&quot;\n127.0.0.1:6379&gt; brpop myList 2\n1) &quot;myList&quot;\n2) &quot;4&quot;\n</code></pre>\n<h1>（3）SET集合命令</h1>\n<ul>\n<li>SADD key-name item [item …]\t\t将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量</li>\n<li>SREM key-name item [item …]\t\t从集合里面移除一个或多个元素，并返回被移除元素的数量</li>\n<li>SISMEMBER key-name item\t\t检查元素item是否存在于集合key-name 里</li>\n<li>SCARD key-name\t\t返回集合包含的元素的数量</li>\n<li>SMEMBERS key-name\t\t返回集合包含的所有元素</li>\n<li>SRANDMEMBER key-name [count]\t\t从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复:当count为负数时，命令返回的随机元素可能会出现重复</li>\n<li>SPOP key-name\t\t随机地移除集合中的一个元素，并返回被移除的元素</li>\n<li>SMOVE source-key dest-key item\t\t如果集合source-key包含元素item，那么从集合source-key里面移除元素item，并将元素item添加到集合dest-key中;如果item被成功移除,那么命令返回1，否则返回0</li>\n</ul>\n<h2 id=\"用于组合和处理多个集合的Redis命令\">用于组合和处理多个集合的Redis命令</h2>\n<ul>\n<li>SDIFF key-name [key-name …]\t\t返回那些存在于第一个集合、但不存在于其他集合中的元素（数学上的差集运算)</li>\n<li>SDIFFSTORE dest-key key-name [key-name …]\t\t将那些存在于第一个集合但并不存在于其他集合中的元素（数学上的差集运算）存储到dest-key键里面</li>\n<li>SINTER key-name [key-name …]\t\t返回那些同时存在于所有集合中的元素（数学上的交集运算)</li>\n<li>SINTERSTORE dest-key key-name [key-name …]\t\t将那些同时存在于所有集合的元素（数学上的交集运算)存储到dest-key键里面</li>\n<li>SUNION key-name [key-name …]\t\t返回那些至少存在于一个集合中的元素（数学上的并集计算)</li>\n<li>SUNIONSTORE dest-key key-name [key-name …]\t\t将那些至少存在于一个集合中的元素（数学上的并集计算)存储到dest-key键里面</li>\n</ul>\n<h1>（4）HASH散列命令</h1>\n<p>HMGET key-name key [key …]\t\t从散列里面获取一个或多个键的值</p>\n<p>HMSET key-name key value [key value …]\t\t为散列里面的一个或多个键设置值</p>\n<p>HDEL key-name key [key …]\t\t删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量</p>\n<p>HLEN key-name\t\t返回散列包含的键值对数量</p>\n<h2 id=\"散列的更高级特性\">散列的更高级特性</h2>\n<p>HEXISTS key-name key\t\t检查给定键是否存在于散列中HKEYS key-name———获取散列包含的所有键</p>\n<p>HKEYS key-name\t\t获取散列包含的所有键</p>\n<p>HVALS key-name\t\t获取散列包含的所有值</p>\n<p>HGETALL key-name\t\t获取散列包含的所有键值对</p>\n<p>HINCRBY key-name key increment\t\t将键key存储的值加上整数increment</p>\n<p>HINCRBYFLOAT key-name key increment\t\t将键key存储的值加上浮点数increment</p>\n<h1>（5）ZSET有序集合命令</h1>\n<p>ZADD key-name score member [score member …]\t\t将带有给定分值的成员添加到有序集合里面</p>\n<p>ZREM key-name member [member …]\t\t从有序集合里面移除给定的成员，并返回被移除成员的数量</p>\n<p>ZCARD key-name\t\t返回有序集合包含的成员数量</p>\n<p>ZINCRBY key-name increment member\t\t将member成员的分值加上increment</p>\n<p>ZCOUNT key-name min max——返回分值介于min和max之间的成员数量</p>\n<p>ZRANK key-name member\t\t返回成员member在有序集合中的排名</p>\n<p>ZSCORE key-name member\t\t返回成员member的分值</p>\n<p>ZRANGE key-name start stop [WITHSCORES]\t\t返回有序集合中排名介于start和 stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回</p>\n<h2 id=\"有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令\">有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令</h2>\n<p>ZREVRANK key-name member\t\t返回有序集合里成员member的排名，成员按照分值从大到小排列</p>\n<p>ZREVRANGE key-name start stop [WITHSCORES]\t\t返回有序集合给定排名范围内的成员，成员按照分值从大到小排列</p>\n<p>ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]\t\t返回有序集合中，分值介于min和max之间的所有成员</p>\n<p>ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]\t\t获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回它们</p>\n<p>ZREMRANGEBYRANK key-name start stop\t\t移除有序集合中排名介于start和stop之间的所有成员</p>\n<p>ZREMRANGEBYSCORE key-name min max\t\t移除有序集合中分值介于min和 max之间的所有成员</p>\n<p>ZINTERSTORE dest-key key-count key [key …] [WEIGHTS weight[ weight …]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的交集运算</p>\n<p>ZUNIONSTORE dest-key key-count key [key …] [WEIGHTS weight[ weight …]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的并集运算</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、Redis数据结构介绍</h1>\n<p>（1）STRING</p>\n<p>​\t\tvalue：可以是字符串、整数或者浮点数</p>\n<p>​\t\toperate：对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数执行自增（increment）或者自减(decrement)操作</p>\n<p>（2）LIST</p>\n<p>​\t\tvalue：一个链表，链表上的每个节点都包含了—个字符串</p>\n<p>​\t\toperate：从链表的两端推入或者弹出元素;根据偏移量对链表进行修剪(trim)；读取单个或者多个元素;根据值查找或者移除元素</p>\n<p>（3）SET</p>\n<p>​\t\tvalue：包含字符串的无序收集器( unordered collection)，并且被包含的每个字符串都是独一无二、各不相同的</p>\n<p>​\t\toperate：添加、获取、移除单个元素；检查一个元素是否存在于集合中;计算交集、并集、差集；从集合里面随机获取元素</p>\n<p>（4）HASH</p>\n<p>​\t\tvalue：包含键值对的无序散列表</p>\n<p>​\t\toperate：添加、获取、移除单个键值对;获取所有键值对</p>\n<p>（5）ZSET（有序集合)</p>\n<p>​\t\tvalue：字符串成员( member)与浮点数分值( score）之间的有序映射，元素的排列顺序由分值的大小决定</p>\n<p>​\t\toperate：添加、获取、删除单个元素；根据分值范围（range)或者成员来获取元素</p>\n<h1>2、常用命令</h1>\n<h2 id=\"（1）STRING字符串命令\">（1）STRING字符串命令</h2>\n<ul>\n<li>SET key-name value\t\t设置值</li>\n<li>GET key-name\t\t获得值</li>\n<li>DEL key-name\t\t删除值</li>\n</ul>\n<p>例子：</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; set hello world\nOK\n127.0.0.1:6379&gt; get hello\n&quot;world&quot;\n127.0.0.1:6379&gt; del hello\n(integer) 1\n127.0.0.1:6379&gt; get hello\n(nil)\n</code></pre>\n<ul>\n<li>INCR key-name\t\t将键存储的值加上1</li>\n<li>DECR key-name\t\t将键存储的值减去1</li>\n<li>INCRBY key-name amount\t\t将键存储的值加上整数amount</li>\n<li>DECRBY key-name amount\t\t将键存储的值减去整数amount</li>\n<li>INCRBYFLOAT key-name amount\t\t将键存储的值加上浮点数 amount，这个命令在Redis 2.6或以上的版本可用</li>\n</ul>\n<p>例子：</p>\n<pre><code>127.0.0.1:6379&gt; set name 1\nOK\n127.0.0.1:6379&gt; incr name\n(integer) 2\n127.0.0.1:6379&gt; get name\n&quot;2&quot;\n127.0.0.1:6379&gt; decr name\n(integer) 1\n127.0.0.1:6379&gt; get name\n&quot;1&quot;\n127.0.0.1:6379&gt; incrby name 2\n(integer) 3\n127.0.0.1:6379&gt; get name\n&quot;3&quot;\n</code></pre>\n<h2 id=\"（2）LIST列表命令\">（2）LIST列表命令</h2>\n<ul>\n<li>RPUSH key-name value [value …]\t\t将一个或多个值推入列表的右端</li>\n<li>LPUSH key-name value [value …]\t\t将一个或多个值推入列表的左端</li>\n<li>RPOP key-name\t\t移除并返回列表最右端的元素</li>\n<li>LPOP key-name\t\t移除并返回列表最左端的元素</li>\n<li>LINDEX key-name offset\t\t返回列表中偏移量为offset的元素</li>\n<li>LRANGE key-name start end\t\t返回列表从start偏移量到end偏移量范围内的所有元素,其中偏移量为start和偏移量为end 的元素也会包含在被返回的元素之内</li>\n<li>LTRIM key-name start end\t\t对列表进行修剪，只保留从start偏移量到end偏移量范围内的元素，其中偏移量为start和偏移量为end的元素也会被保留</li>\n</ul>\n<p>例子：</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; rpush myList 1 2 3 4\n(integer) 4\n127.0.0.1:6379&gt; lrange myList 0 4\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;4&quot;\n127.0.0.1:6379&gt; lpush myList -1 0\n(integer) 6\n127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;0&quot;\n2) &quot;-1&quot;\n3) &quot;1&quot;\n4) &quot;2&quot;\n5) &quot;3&quot;\n6) &quot;4&quot;\n</code></pre>\n<ul>\n<li>BLPOP key-name [ key-name …] timeout\t\t从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现</li>\n<li>BRPOP key-name [key-name …] timeout\t\t从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现</li>\n<li>RPOPLPUSH source-key dest-key\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素</li>\n<li>BRPOPLPUSH source-key dest-key timeout\t\t从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表的最左端，并向用户返回这个元素；如果source-key为空，那么在timeout秒之内阻塞并等待可弹出的元素出现</li>\n</ul>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; blpop myList 2\n1) &quot;myList&quot;\n2) &quot;0&quot;\n127.0.0.1:6379&gt; blpop myList 2000\n1) &quot;myList&quot;\n2) &quot;-1&quot;\n127.0.0.1:6379&gt; blpop myList 6000\n1) &quot;myList&quot;\n2) &quot;1&quot;\n127.0.0.1:6379&gt; brpop myList 2\n1) &quot;myList&quot;\n2) &quot;4&quot;\n</code></pre>\n<h1>（3）SET集合命令</h1>\n<ul>\n<li>SADD key-name item [item …]\t\t将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量</li>\n<li>SREM key-name item [item …]\t\t从集合里面移除一个或多个元素，并返回被移除元素的数量</li>\n<li>SISMEMBER key-name item\t\t检查元素item是否存在于集合key-name 里</li>\n<li>SCARD key-name\t\t返回集合包含的元素的数量</li>\n<li>SMEMBERS key-name\t\t返回集合包含的所有元素</li>\n<li>SRANDMEMBER key-name [count]\t\t从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复:当count为负数时，命令返回的随机元素可能会出现重复</li>\n<li>SPOP key-name\t\t随机地移除集合中的一个元素，并返回被移除的元素</li>\n<li>SMOVE source-key dest-key item\t\t如果集合source-key包含元素item，那么从集合source-key里面移除元素item，并将元素item添加到集合dest-key中;如果item被成功移除,那么命令返回1，否则返回0</li>\n</ul>\n<h2 id=\"用于组合和处理多个集合的Redis命令\">用于组合和处理多个集合的Redis命令</h2>\n<ul>\n<li>SDIFF key-name [key-name …]\t\t返回那些存在于第一个集合、但不存在于其他集合中的元素（数学上的差集运算)</li>\n<li>SDIFFSTORE dest-key key-name [key-name …]\t\t将那些存在于第一个集合但并不存在于其他集合中的元素（数学上的差集运算）存储到dest-key键里面</li>\n<li>SINTER key-name [key-name …]\t\t返回那些同时存在于所有集合中的元素（数学上的交集运算)</li>\n<li>SINTERSTORE dest-key key-name [key-name …]\t\t将那些同时存在于所有集合的元素（数学上的交集运算)存储到dest-key键里面</li>\n<li>SUNION key-name [key-name …]\t\t返回那些至少存在于一个集合中的元素（数学上的并集计算)</li>\n<li>SUNIONSTORE dest-key key-name [key-name …]\t\t将那些至少存在于一个集合中的元素（数学上的并集计算)存储到dest-key键里面</li>\n</ul>\n<h1>（4）HASH散列命令</h1>\n<p>HMGET key-name key [key …]\t\t从散列里面获取一个或多个键的值</p>\n<p>HMSET key-name key value [key value …]\t\t为散列里面的一个或多个键设置值</p>\n<p>HDEL key-name key [key …]\t\t删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量</p>\n<p>HLEN key-name\t\t返回散列包含的键值对数量</p>\n<h2 id=\"散列的更高级特性\">散列的更高级特性</h2>\n<p>HEXISTS key-name key\t\t检查给定键是否存在于散列中HKEYS key-name———获取散列包含的所有键</p>\n<p>HKEYS key-name\t\t获取散列包含的所有键</p>\n<p>HVALS key-name\t\t获取散列包含的所有值</p>\n<p>HGETALL key-name\t\t获取散列包含的所有键值对</p>\n<p>HINCRBY key-name key increment\t\t将键key存储的值加上整数increment</p>\n<p>HINCRBYFLOAT key-name key increment\t\t将键key存储的值加上浮点数increment</p>\n<h1>（5）ZSET有序集合命令</h1>\n<p>ZADD key-name score member [score member …]\t\t将带有给定分值的成员添加到有序集合里面</p>\n<p>ZREM key-name member [member …]\t\t从有序集合里面移除给定的成员，并返回被移除成员的数量</p>\n<p>ZCARD key-name\t\t返回有序集合包含的成员数量</p>\n<p>ZINCRBY key-name increment member\t\t将member成员的分值加上increment</p>\n<p>ZCOUNT key-name min max——返回分值介于min和max之间的成员数量</p>\n<p>ZRANK key-name member\t\t返回成员member在有序集合中的排名</p>\n<p>ZSCORE key-name member\t\t返回成员member的分值</p>\n<p>ZRANGE key-name start stop [WITHSCORES]\t\t返回有序集合中排名介于start和 stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回</p>\n<h2 id=\"有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令\">有序集合的范围型数据获取命令和范围型数据删除命令，以及并集命令和交集命令</h2>\n<p>ZREVRANK key-name member\t\t返回有序集合里成员member的排名，成员按照分值从大到小排列</p>\n<p>ZREVRANGE key-name start stop [WITHSCORES]\t\t返回有序集合给定排名范围内的成员，成员按照分值从大到小排列</p>\n<p>ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]\t\t返回有序集合中，分值介于min和max之间的所有成员</p>\n<p>ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]\t\t获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回它们</p>\n<p>ZREMRANGEBYRANK key-name start stop\t\t移除有序集合中排名介于start和stop之间的所有成员</p>\n<p>ZREMRANGEBYSCORE key-name min max\t\t移除有序集合中分值介于min和 max之间的所有成员</p>\n<p>ZINTERSTORE dest-key key-count key [key …] [WEIGHTS weight[ weight …]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的交集运算</p>\n<p>ZUNIONSTORE dest-key key-count key [key …] [WEIGHTS weight[ weight …]] [AGGREGATE SUM|MIN|MAX]\t\t对给定的有序集合执行类似于集合的并集运算</p>\n"},{"title":"初识redis（3）-持久化","author":"ztq","date":"2021-08-01T14:43:00.000Z","_content":"\n# 1、Redis 数据持久化概念\n\nRedis 的数据持久化，即：将内存中的数据存储到硬盘（本文中亦称之为 “落地”）。Redis 提供了 RDB 和 AOF 两种持久化的方法：\n•\tRDB：基于特定的时间间隔将数据 “全量快照”，生成 RDB 文件并落地\n•\tAOF (Append Only File)：将 Redis 接收到命令以 “增量追加” 的方式，写入 AOF 文件\nRedis 允许使用任意一种持久化方法，亦允许同时使用或同时不使用。以下将阐述两者涉及的配置选项、命令以及优缺点。\n\n# 2、RDB 与 AOF 优缺点和选择\n\nRDB\n•\t非常适合于备份以及灾难恢复的场景\n•\t能够最大化 Redis 性能\n•\t相对于 AOF，RDB 文件在 Redis 启动时能够更快加载\n•\t若期望将数据丢失的可能性最小化，RDB 并不适用\nAOF\n•\t基于 “追加” 和 “文件同步” 的特性，AOF 具有更佳的 “持久化” 表现\n•\t对于相同的数据，AOF 文件大小通常将超过 RDB\n综合而言，如果能够承担一定程度的数据丢失风险，仅启用 RDB 持久化即可。但并不建议只启用 AOF 持久化，毕竟 RDB 文件更适合于数据备份。\n若 RDB 持久化和 AOF 持久化同时启用，Redis 启动时，将加载 AOF 文件，毕竟 AOF 具有更佳的 “持久化” 表现。\n\n# 3、RDB\n\n## 配置选项\n\n```java\nsave 900 1                       # RDB 落地选项，900 秒内有 1 次写入，即落地新的 RDB 文件。\n                                 # 允许多个 save 配置，满足任意配置即开始新的 RDB 文件落地；若无 save 配置，即表示关闭 RDB 数据持久化\n                                 # \nstop-writes-on-bgsave-error yes  # RDB 文件写入失败时，Redis 是否停止接收写命令，默认 yes，即停止\nrdbcompression yes               # 是否压缩 RDB 文件，默认 yes，即压缩\nrdbchecksum yes                  # 是否启用 RDB 文件校验，默认 yes，即生成 CRC64，写入 RDB 文件结尾\ndbfilename dump.rdb              # RDB 文件名称\ndir ./                           # RDB 文件路径（说明：配置与 AOF 共享）\n```\n\nRDB 相关的配置，应当结合业务的实际，例如：\n•\t当 RDB 文件写入失败时，若能够通过其他的运维手段进行及时处理，则无需开启 stop-writes-on-bgsave-error，以避免线上服务的中断\n•\t若开启 rdbcompression 与 rdbchecksum 选项，RDB 文件的落地、Redis 启动时的 RDB 文件加载，将产生额外的性能损耗\n\n## RDB 相关\n\n### •\tSAVE\n\n“同步” 创建 RDB 文件，SAVE 将阻塞 Redis，Redis 将不能响应其他任何命令，直到 RDB 文件完成创建与落地。\n\n### •\tBGSAVE\n\n“异步” 创建 RDB 文件，Redis 创建子进程：父进程继续提供服务，由子进程生成并落地 RDB 文件。\n当配置选项中任意的 save 配置条件满足时，Redis 将自动地 “触发” BGSAVE 命令。\n\n### •\tLASTSAVE\n\n获取最后一次成功落地 RDB 文件的 Unix 时间。\n\n# 4、AOF\n\n## 配置选项\n\n```java\nappendonly no                    # 是否启用 AOF 数据持久化，默认 no，即关闭\nappendfilename \"appendonly.aof\"  # AOF 文件名称\n# AOF 文件同步选项\nappendfsync everysec             \nno-appendfsync-on-rewrite no     \n# AOF 文件 rewrite 选项\nauto-aof-rewrite-percentage 100  \nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes           # 若 AOF 文件的结尾处损坏（由操作系统故障引起），Redis 启动时加载 AOF 文件，根据 aof-load-truncated 配置：\n                                 #   默认 yes：忽略 AOF 文件结尾处的损坏\n                                 #   no：Redis 进程退出\n                                 #\ndir ./                           # AOF 文件路径（说明：配置与 RDB 共享）\n```\n\n## AOF 相关\n\nAOF 文件同步，即通过系统调用 fsync()将 AOF 文件位于操作系统缓冲区的部分（取决于 write()的工作机制）写入硬盘。\n特别说明：fsync()并不确保缓冲区的内容一定能够写入硬盘，其工作机制取决于操作系统。\n\n### appendfsync 配置选项：\n\n•\talways：每次写入 AOF 文件，都进行一次 fsync() 系统调用\n•\teverysec：默认，每秒进行一次 fsync() 系统调用\n•\tno：不进行 fsync() 系统调用，完全由操作系统控制\n\n### no-appendfsync-on-rewrite 配置选项：\n\n•\tyes：即 Redis 进行 AOF 文件 rewrite 时（或落地 RDB 文件时），fsync() 系统调用暂停，以避免可能产生的阻塞\n•\tno：默认，即 Redis 进行 AOF 文件 rewrite 时，继续进行 fsync() 系统调用\n\n### AOF 文件 rewrite\n\n作为 Redis 命令的 “log”，AOF 文件的大小必须持续增长。Redis 提供 AOF 文件 rewrite 特性，能够移除 AOF 文件的冗余命令以减少 AOF 文件大小。\n例如：Redis 针对一个 key 执行了 100 次的 SET，AOF 文件仅保留最后一次 SET 命令即可。\n\n### BGREWRITEAOF 命令：\n\n“异步” 进行 Redis AOF 文件 rewrite\n•\t若 Redis 正在进行创建 RDB 文件，AOF 文件 rewrite 将等待 RDB 文件创建完成后开始\n•\t若 AOF 文件 rewrite 正在进行 ，BGREWRITEAOF 命令将不会开始新的 AOF 文件 rewrite\n\n### auto-aof-rewrite-percentage & auto-aof-rewrite-min-size 配置选项：\n\nAOF 文件自动 rewrite 机制，当 AOF 文件大小达到以下阈值，Redis 即自动开始 AOF 文件 rewrite：\n•\t增长百分比超过 auto-aof-rewrite-percentage（相对于上一次 rewrite 完成时的 AOF 文件）\n•\t超过 auto-aof-rewrite-min-size\n通过设置 auto-aof-rewrite-percentage 为 0，即关闭 AOF 文件的自动 rewrite。\n\n","source":"_posts/初识redis（3）-持久化.md","raw":"title: 初识redis（3）-持久化\nauthor: ztq\ntags:\n  - redis\ncategories:\n  - 数据库\ndate: 2021-08-01 22:43:00\n\n---\n\n# 1、Redis 数据持久化概念\n\nRedis 的数据持久化，即：将内存中的数据存储到硬盘（本文中亦称之为 “落地”）。Redis 提供了 RDB 和 AOF 两种持久化的方法：\n•\tRDB：基于特定的时间间隔将数据 “全量快照”，生成 RDB 文件并落地\n•\tAOF (Append Only File)：将 Redis 接收到命令以 “增量追加” 的方式，写入 AOF 文件\nRedis 允许使用任意一种持久化方法，亦允许同时使用或同时不使用。以下将阐述两者涉及的配置选项、命令以及优缺点。\n\n# 2、RDB 与 AOF 优缺点和选择\n\nRDB\n•\t非常适合于备份以及灾难恢复的场景\n•\t能够最大化 Redis 性能\n•\t相对于 AOF，RDB 文件在 Redis 启动时能够更快加载\n•\t若期望将数据丢失的可能性最小化，RDB 并不适用\nAOF\n•\t基于 “追加” 和 “文件同步” 的特性，AOF 具有更佳的 “持久化” 表现\n•\t对于相同的数据，AOF 文件大小通常将超过 RDB\n综合而言，如果能够承担一定程度的数据丢失风险，仅启用 RDB 持久化即可。但并不建议只启用 AOF 持久化，毕竟 RDB 文件更适合于数据备份。\n若 RDB 持久化和 AOF 持久化同时启用，Redis 启动时，将加载 AOF 文件，毕竟 AOF 具有更佳的 “持久化” 表现。\n\n# 3、RDB\n\n## 配置选项\n\n```java\nsave 900 1                       # RDB 落地选项，900 秒内有 1 次写入，即落地新的 RDB 文件。\n                                 # 允许多个 save 配置，满足任意配置即开始新的 RDB 文件落地；若无 save 配置，即表示关闭 RDB 数据持久化\n                                 # \nstop-writes-on-bgsave-error yes  # RDB 文件写入失败时，Redis 是否停止接收写命令，默认 yes，即停止\nrdbcompression yes               # 是否压缩 RDB 文件，默认 yes，即压缩\nrdbchecksum yes                  # 是否启用 RDB 文件校验，默认 yes，即生成 CRC64，写入 RDB 文件结尾\ndbfilename dump.rdb              # RDB 文件名称\ndir ./                           # RDB 文件路径（说明：配置与 AOF 共享）\n```\n\nRDB 相关的配置，应当结合业务的实际，例如：\n•\t当 RDB 文件写入失败时，若能够通过其他的运维手段进行及时处理，则无需开启 stop-writes-on-bgsave-error，以避免线上服务的中断\n•\t若开启 rdbcompression 与 rdbchecksum 选项，RDB 文件的落地、Redis 启动时的 RDB 文件加载，将产生额外的性能损耗\n\n## RDB 相关\n\n### •\tSAVE\n\n“同步” 创建 RDB 文件，SAVE 将阻塞 Redis，Redis 将不能响应其他任何命令，直到 RDB 文件完成创建与落地。\n\n### •\tBGSAVE\n\n“异步” 创建 RDB 文件，Redis 创建子进程：父进程继续提供服务，由子进程生成并落地 RDB 文件。\n当配置选项中任意的 save 配置条件满足时，Redis 将自动地 “触发” BGSAVE 命令。\n\n### •\tLASTSAVE\n\n获取最后一次成功落地 RDB 文件的 Unix 时间。\n\n# 4、AOF\n\n## 配置选项\n\n```java\nappendonly no                    # 是否启用 AOF 数据持久化，默认 no，即关闭\nappendfilename \"appendonly.aof\"  # AOF 文件名称\n# AOF 文件同步选项\nappendfsync everysec             \nno-appendfsync-on-rewrite no     \n# AOF 文件 rewrite 选项\nauto-aof-rewrite-percentage 100  \nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes           # 若 AOF 文件的结尾处损坏（由操作系统故障引起），Redis 启动时加载 AOF 文件，根据 aof-load-truncated 配置：\n                                 #   默认 yes：忽略 AOF 文件结尾处的损坏\n                                 #   no：Redis 进程退出\n                                 #\ndir ./                           # AOF 文件路径（说明：配置与 RDB 共享）\n```\n\n## AOF 相关\n\nAOF 文件同步，即通过系统调用 fsync()将 AOF 文件位于操作系统缓冲区的部分（取决于 write()的工作机制）写入硬盘。\n特别说明：fsync()并不确保缓冲区的内容一定能够写入硬盘，其工作机制取决于操作系统。\n\n### appendfsync 配置选项：\n\n•\talways：每次写入 AOF 文件，都进行一次 fsync() 系统调用\n•\teverysec：默认，每秒进行一次 fsync() 系统调用\n•\tno：不进行 fsync() 系统调用，完全由操作系统控制\n\n### no-appendfsync-on-rewrite 配置选项：\n\n•\tyes：即 Redis 进行 AOF 文件 rewrite 时（或落地 RDB 文件时），fsync() 系统调用暂停，以避免可能产生的阻塞\n•\tno：默认，即 Redis 进行 AOF 文件 rewrite 时，继续进行 fsync() 系统调用\n\n### AOF 文件 rewrite\n\n作为 Redis 命令的 “log”，AOF 文件的大小必须持续增长。Redis 提供 AOF 文件 rewrite 特性，能够移除 AOF 文件的冗余命令以减少 AOF 文件大小。\n例如：Redis 针对一个 key 执行了 100 次的 SET，AOF 文件仅保留最后一次 SET 命令即可。\n\n### BGREWRITEAOF 命令：\n\n“异步” 进行 Redis AOF 文件 rewrite\n•\t若 Redis 正在进行创建 RDB 文件，AOF 文件 rewrite 将等待 RDB 文件创建完成后开始\n•\t若 AOF 文件 rewrite 正在进行 ，BGREWRITEAOF 命令将不会开始新的 AOF 文件 rewrite\n\n### auto-aof-rewrite-percentage & auto-aof-rewrite-min-size 配置选项：\n\nAOF 文件自动 rewrite 机制，当 AOF 文件大小达到以下阈值，Redis 即自动开始 AOF 文件 rewrite：\n•\t增长百分比超过 auto-aof-rewrite-percentage（相对于上一次 rewrite 完成时的 AOF 文件）\n•\t超过 auto-aof-rewrite-min-size\n通过设置 auto-aof-rewrite-percentage 为 0，即关闭 AOF 文件的自动 rewrite。\n\n","slug":"初识redis（3）-持久化","published":1,"updated":"2022-04-04T08:32:40.169Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0l007y7kt9hsdjdwre","content":"<h1>1、Redis 数据持久化概念</h1>\n<p>Redis 的数据持久化，即：将内存中的数据存储到硬盘（本文中亦称之为 “落地”）。Redis 提供了 RDB 和 AOF 两种持久化的方法：<br>\n•\tRDB：基于特定的时间间隔将数据 “全量快照”，生成 RDB 文件并落地<br>\n•\tAOF (Append Only File)：将 Redis 接收到命令以 “增量追加” 的方式，写入 AOF 文件<br>\nRedis 允许使用任意一种持久化方法，亦允许同时使用或同时不使用。以下将阐述两者涉及的配置选项、命令以及优缺点。</p>\n<h1>2、RDB 与 AOF 优缺点和选择</h1>\n<p>RDB<br>\n•\t非常适合于备份以及灾难恢复的场景<br>\n•\t能够最大化 Redis 性能<br>\n•\t相对于 AOF，RDB 文件在 Redis 启动时能够更快加载<br>\n•\t若期望将数据丢失的可能性最小化，RDB 并不适用<br>\nAOF<br>\n•\t基于 “追加” 和 “文件同步” 的特性，AOF 具有更佳的 “持久化” 表现<br>\n•\t对于相同的数据，AOF 文件大小通常将超过 RDB<br>\n综合而言，如果能够承担一定程度的数据丢失风险，仅启用 RDB 持久化即可。但并不建议只启用 AOF 持久化，毕竟 RDB 文件更适合于数据备份。<br>\n若 RDB 持久化和 AOF 持久化同时启用，Redis 启动时，将加载 AOF 文件，毕竟 AOF 具有更佳的 “持久化” 表现。</p>\n<h1>3、RDB</h1>\n<h2 id=\"配置选项\">配置选项</h2>\n<pre><code class=\"language-java\">save 900 1                       # RDB 落地选项，900 秒内有 1 次写入，即落地新的 RDB 文件。\n                                 # 允许多个 save 配置，满足任意配置即开始新的 RDB 文件落地；若无 save 配置，即表示关闭 RDB 数据持久化\n                                 # \nstop-writes-on-bgsave-error yes  # RDB 文件写入失败时，Redis 是否停止接收写命令，默认 yes，即停止\nrdbcompression yes               # 是否压缩 RDB 文件，默认 yes，即压缩\nrdbchecksum yes                  # 是否启用 RDB 文件校验，默认 yes，即生成 CRC64，写入 RDB 文件结尾\ndbfilename dump.rdb              # RDB 文件名称\ndir ./                           # RDB 文件路径（说明：配置与 AOF 共享）\n</code></pre>\n<p>RDB 相关的配置，应当结合业务的实际，例如：<br>\n•\t当 RDB 文件写入失败时，若能够通过其他的运维手段进行及时处理，则无需开启 stop-writes-on-bgsave-error，以避免线上服务的中断<br>\n•\t若开启 rdbcompression 与 rdbchecksum 选项，RDB 文件的落地、Redis 启动时的 RDB 文件加载，将产生额外的性能损耗</p>\n<h2 id=\"RDB-相关\">RDB 相关</h2>\n<h3 id=\"•SAVE\">•\tSAVE</h3>\n<p>“同步” 创建 RDB 文件，SAVE 将阻塞 Redis，Redis 将不能响应其他任何命令，直到 RDB 文件完成创建与落地。</p>\n<h3 id=\"•BGSAVE\">•\tBGSAVE</h3>\n<p>“异步” 创建 RDB 文件，Redis 创建子进程：父进程继续提供服务，由子进程生成并落地 RDB 文件。<br>\n当配置选项中任意的 save 配置条件满足时，Redis 将自动地 “触发” BGSAVE 命令。</p>\n<h3 id=\"•LASTSAVE\">•\tLASTSAVE</h3>\n<p>获取最后一次成功落地 RDB 文件的 Unix 时间。</p>\n<h1>4、AOF</h1>\n<h2 id=\"配置选项-2\">配置选项</h2>\n<pre><code class=\"language-java\">appendonly no                    # 是否启用 AOF 数据持久化，默认 no，即关闭\nappendfilename &quot;appendonly.aof&quot;  # AOF 文件名称\n# AOF 文件同步选项\nappendfsync everysec             \nno-appendfsync-on-rewrite no     \n# AOF 文件 rewrite 选项\nauto-aof-rewrite-percentage 100  \nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes           # 若 AOF 文件的结尾处损坏（由操作系统故障引起），Redis 启动时加载 AOF 文件，根据 aof-load-truncated 配置：\n                                 #   默认 yes：忽略 AOF 文件结尾处的损坏\n                                 #   no：Redis 进程退出\n                                 #\ndir ./                           # AOF 文件路径（说明：配置与 RDB 共享）\n</code></pre>\n<h2 id=\"AOF-相关\">AOF 相关</h2>\n<p>AOF 文件同步，即通过系统调用 fsync()将 AOF 文件位于操作系统缓冲区的部分（取决于 write()的工作机制）写入硬盘。<br>\n特别说明：fsync()并不确保缓冲区的内容一定能够写入硬盘，其工作机制取决于操作系统。</p>\n<h3 id=\"appendfsync-配置选项：\">appendfsync 配置选项：</h3>\n<p>•\talways：每次写入 AOF 文件，都进行一次 fsync() 系统调用<br>\n•\teverysec：默认，每秒进行一次 fsync() 系统调用<br>\n•\tno：不进行 fsync() 系统调用，完全由操作系统控制</p>\n<h3 id=\"no-appendfsync-on-rewrite-配置选项：\">no-appendfsync-on-rewrite 配置选项：</h3>\n<p>•\tyes：即 Redis 进行 AOF 文件 rewrite 时（或落地 RDB 文件时），fsync() 系统调用暂停，以避免可能产生的阻塞<br>\n•\tno：默认，即 Redis 进行 AOF 文件 rewrite 时，继续进行 fsync() 系统调用</p>\n<h3 id=\"AOF-文件-rewrite\">AOF 文件 rewrite</h3>\n<p>作为 Redis 命令的 “log”，AOF 文件的大小必须持续增长。Redis 提供 AOF 文件 rewrite 特性，能够移除 AOF 文件的冗余命令以减少 AOF 文件大小。<br>\n例如：Redis 针对一个 key 执行了 100 次的 SET，AOF 文件仅保留最后一次 SET 命令即可。</p>\n<h3 id=\"BGREWRITEAOF-命令：\">BGREWRITEAOF 命令：</h3>\n<p>“异步” 进行 Redis AOF 文件 rewrite<br>\n•\t若 Redis 正在进行创建 RDB 文件，AOF 文件 rewrite 将等待 RDB 文件创建完成后开始<br>\n•\t若 AOF 文件 rewrite 正在进行 ，BGREWRITEAOF 命令将不会开始新的 AOF 文件 rewrite</p>\n<h3 id=\"auto-aof-rewrite-percentage-auto-aof-rewrite-min-size-配置选项：\">auto-aof-rewrite-percentage &amp; auto-aof-rewrite-min-size 配置选项：</h3>\n<p>AOF 文件自动 rewrite 机制，当 AOF 文件大小达到以下阈值，Redis 即自动开始 AOF 文件 rewrite：<br>\n•\t增长百分比超过 auto-aof-rewrite-percentage（相对于上一次 rewrite 完成时的 AOF 文件）<br>\n•\t超过 auto-aof-rewrite-min-size<br>\n通过设置 auto-aof-rewrite-percentage 为 0，即关闭 AOF 文件的自动 rewrite。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、Redis 数据持久化概念</h1>\n<p>Redis 的数据持久化，即：将内存中的数据存储到硬盘（本文中亦称之为 “落地”）。Redis 提供了 RDB 和 AOF 两种持久化的方法：<br>\n•\tRDB：基于特定的时间间隔将数据 “全量快照”，生成 RDB 文件并落地<br>\n•\tAOF (Append Only File)：将 Redis 接收到命令以 “增量追加” 的方式，写入 AOF 文件<br>\nRedis 允许使用任意一种持久化方法，亦允许同时使用或同时不使用。以下将阐述两者涉及的配置选项、命令以及优缺点。</p>\n<h1>2、RDB 与 AOF 优缺点和选择</h1>\n<p>RDB<br>\n•\t非常适合于备份以及灾难恢复的场景<br>\n•\t能够最大化 Redis 性能<br>\n•\t相对于 AOF，RDB 文件在 Redis 启动时能够更快加载<br>\n•\t若期望将数据丢失的可能性最小化，RDB 并不适用<br>\nAOF<br>\n•\t基于 “追加” 和 “文件同步” 的特性，AOF 具有更佳的 “持久化” 表现<br>\n•\t对于相同的数据，AOF 文件大小通常将超过 RDB<br>\n综合而言，如果能够承担一定程度的数据丢失风险，仅启用 RDB 持久化即可。但并不建议只启用 AOF 持久化，毕竟 RDB 文件更适合于数据备份。<br>\n若 RDB 持久化和 AOF 持久化同时启用，Redis 启动时，将加载 AOF 文件，毕竟 AOF 具有更佳的 “持久化” 表现。</p>\n<h1>3、RDB</h1>\n<h2 id=\"配置选项\">配置选项</h2>\n<pre><code class=\"language-java\">save 900 1                       # RDB 落地选项，900 秒内有 1 次写入，即落地新的 RDB 文件。\n                                 # 允许多个 save 配置，满足任意配置即开始新的 RDB 文件落地；若无 save 配置，即表示关闭 RDB 数据持久化\n                                 # \nstop-writes-on-bgsave-error yes  # RDB 文件写入失败时，Redis 是否停止接收写命令，默认 yes，即停止\nrdbcompression yes               # 是否压缩 RDB 文件，默认 yes，即压缩\nrdbchecksum yes                  # 是否启用 RDB 文件校验，默认 yes，即生成 CRC64，写入 RDB 文件结尾\ndbfilename dump.rdb              # RDB 文件名称\ndir ./                           # RDB 文件路径（说明：配置与 AOF 共享）\n</code></pre>\n<p>RDB 相关的配置，应当结合业务的实际，例如：<br>\n•\t当 RDB 文件写入失败时，若能够通过其他的运维手段进行及时处理，则无需开启 stop-writes-on-bgsave-error，以避免线上服务的中断<br>\n•\t若开启 rdbcompression 与 rdbchecksum 选项，RDB 文件的落地、Redis 启动时的 RDB 文件加载，将产生额外的性能损耗</p>\n<h2 id=\"RDB-相关\">RDB 相关</h2>\n<h3 id=\"•SAVE\">•\tSAVE</h3>\n<p>“同步” 创建 RDB 文件，SAVE 将阻塞 Redis，Redis 将不能响应其他任何命令，直到 RDB 文件完成创建与落地。</p>\n<h3 id=\"•BGSAVE\">•\tBGSAVE</h3>\n<p>“异步” 创建 RDB 文件，Redis 创建子进程：父进程继续提供服务，由子进程生成并落地 RDB 文件。<br>\n当配置选项中任意的 save 配置条件满足时，Redis 将自动地 “触发” BGSAVE 命令。</p>\n<h3 id=\"•LASTSAVE\">•\tLASTSAVE</h3>\n<p>获取最后一次成功落地 RDB 文件的 Unix 时间。</p>\n<h1>4、AOF</h1>\n<h2 id=\"配置选项-2\">配置选项</h2>\n<pre><code class=\"language-java\">appendonly no                    # 是否启用 AOF 数据持久化，默认 no，即关闭\nappendfilename &quot;appendonly.aof&quot;  # AOF 文件名称\n# AOF 文件同步选项\nappendfsync everysec             \nno-appendfsync-on-rewrite no     \n# AOF 文件 rewrite 选项\nauto-aof-rewrite-percentage 100  \nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes           # 若 AOF 文件的结尾处损坏（由操作系统故障引起），Redis 启动时加载 AOF 文件，根据 aof-load-truncated 配置：\n                                 #   默认 yes：忽略 AOF 文件结尾处的损坏\n                                 #   no：Redis 进程退出\n                                 #\ndir ./                           # AOF 文件路径（说明：配置与 RDB 共享）\n</code></pre>\n<h2 id=\"AOF-相关\">AOF 相关</h2>\n<p>AOF 文件同步，即通过系统调用 fsync()将 AOF 文件位于操作系统缓冲区的部分（取决于 write()的工作机制）写入硬盘。<br>\n特别说明：fsync()并不确保缓冲区的内容一定能够写入硬盘，其工作机制取决于操作系统。</p>\n<h3 id=\"appendfsync-配置选项：\">appendfsync 配置选项：</h3>\n<p>•\talways：每次写入 AOF 文件，都进行一次 fsync() 系统调用<br>\n•\teverysec：默认，每秒进行一次 fsync() 系统调用<br>\n•\tno：不进行 fsync() 系统调用，完全由操作系统控制</p>\n<h3 id=\"no-appendfsync-on-rewrite-配置选项：\">no-appendfsync-on-rewrite 配置选项：</h3>\n<p>•\tyes：即 Redis 进行 AOF 文件 rewrite 时（或落地 RDB 文件时），fsync() 系统调用暂停，以避免可能产生的阻塞<br>\n•\tno：默认，即 Redis 进行 AOF 文件 rewrite 时，继续进行 fsync() 系统调用</p>\n<h3 id=\"AOF-文件-rewrite\">AOF 文件 rewrite</h3>\n<p>作为 Redis 命令的 “log”，AOF 文件的大小必须持续增长。Redis 提供 AOF 文件 rewrite 特性，能够移除 AOF 文件的冗余命令以减少 AOF 文件大小。<br>\n例如：Redis 针对一个 key 执行了 100 次的 SET，AOF 文件仅保留最后一次 SET 命令即可。</p>\n<h3 id=\"BGREWRITEAOF-命令：\">BGREWRITEAOF 命令：</h3>\n<p>“异步” 进行 Redis AOF 文件 rewrite<br>\n•\t若 Redis 正在进行创建 RDB 文件，AOF 文件 rewrite 将等待 RDB 文件创建完成后开始<br>\n•\t若 AOF 文件 rewrite 正在进行 ，BGREWRITEAOF 命令将不会开始新的 AOF 文件 rewrite</p>\n<h3 id=\"auto-aof-rewrite-percentage-auto-aof-rewrite-min-size-配置选项：\">auto-aof-rewrite-percentage &amp; auto-aof-rewrite-min-size 配置选项：</h3>\n<p>AOF 文件自动 rewrite 机制，当 AOF 文件大小达到以下阈值，Redis 即自动开始 AOF 文件 rewrite：<br>\n•\t增长百分比超过 auto-aof-rewrite-percentage（相对于上一次 rewrite 完成时的 AOF 文件）<br>\n•\t超过 auto-aof-rewrite-min-size<br>\n通过设置 auto-aof-rewrite-percentage 为 0，即关闭 AOF 文件的自动 rewrite。</p>\n"},{"title":"初识redis（2）-基本特性","author":"ztq","date":"2021-08-01T14:08:00.000Z","_content":"\n# 1、排序\n\n​\t\tRedis 的排序操作和其他编程语言的排序操作一样，都可以根据某种比较规则对一系列元素进行有序的排列。负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5种键里面存储着的数据，对列表、集合以及有序集合进行排序。如果读者之前曾经使用过关系数据库的话，那么可以将SORT命令看作是SQL语言里的order by子句。\n\nSORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GETpattern ...] ] [ASC|DESC] [ALPHA] [STORE dest-key] 根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果\n\n```java\n127.0.0.1:6379> lrange myList 0 -1\n1) \"10\"\n2) \"-1\"\n3) \"2\"\n4) \"3\"\n127.0.0.1:6379> sort myList asc\n1) \"-1\"\n2) \"2\"\n3) \"3\"\n4) \"10\"\n```\n\n# 2、事务\n\n​\t\t有时候为了同时处理多个结构，我们需要向Redis 发送多个命令。尽管Redis有几个可以在两个键之间复制或者移动元素的命令，但却没有那种可以在两个不同类型之间移动元素的命令(虽然可以使用ZUNIONSTORE命令将元素从一个集合复制到一个有序集合)。为了对相同或者不同类型的多个键执行操作，Redis有5个命令可以让用户在不被打断(interruption )的情况下对多个键执行操作，它们分别是WATCH、MULTI、EXEC、UNWATCH和 DISCARD。\n\n​\t\tRedis的基本事务( basic transaction)需要用到MULTI命令和EXEC命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令。和关系数据库那种可以在执行的过程中进行回滚( rollback)的事务不同，在Redis里面，被MULTI命令和EXEC命令包围的所有命令会一个接-一个地执行， 直到所有命令都执行完毕为止。当一个事务执行完毕之后，Redis 才会处理其他客户端的命令。\n\n# 3、键的过期时间\n\n​\t\t在使用Redis存储数据的时候，有些数据可能在某个时间点之后就不再有用了，用户可以使用DEL命令显式地删除这些无用数据,也可以通过Redis的过期时间(expiration)特性来让一个键在给定的时限(timeout)之后自动被删除。当我们说一个键“带有生存时间(time to live)”或者一个键“会在特定时间之后过期(expire)”时，我们指的是Redis会在这个键的过期时间到达时自动删除该键。\n\n- PERSIST key-name\t\t移除键的过期时间\n- TTL key-name\t\t查看给定键距离过期还有多少秒\n- EXPIRE key-name seconds\t\t让给定键在指定的秒数之后过期\n- EXPIREAT key-name timestamp\t\t将给定键的过期时间设置为给定的UNIX时间戳\n- PTTL key-name\t\t查看给定键距离过期时间还有多少毫秒，这个命令在Redis 2.6或以上版本可用\n- PEXPIRE key-name milliseconds\t\t让给定键在指定的毫秒数之后过期，这个命令在Redis 2.6或以上版本可用\n- PEXPIREAT key-name timestamp-milliseconds\t\t将一个毫秒级精度的UNIX时间戳设置为给定键的过期时间，这个命令在 Redis 2.6或以上版本可用\n\n","source":"_posts/初识redis（2）.md","raw":"title: 初识redis（2）-基本特性\nauthor: ztq\ntags:\n  - redis\ncategories:\n  - 数据库\ndate: 2021-08-01 22:08:00\n---\n\n# 1、排序\n\n​\t\tRedis 的排序操作和其他编程语言的排序操作一样，都可以根据某种比较规则对一系列元素进行有序的排列。负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5种键里面存储着的数据，对列表、集合以及有序集合进行排序。如果读者之前曾经使用过关系数据库的话，那么可以将SORT命令看作是SQL语言里的order by子句。\n\nSORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GETpattern ...] ] [ASC|DESC] [ALPHA] [STORE dest-key] 根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果\n\n```java\n127.0.0.1:6379> lrange myList 0 -1\n1) \"10\"\n2) \"-1\"\n3) \"2\"\n4) \"3\"\n127.0.0.1:6379> sort myList asc\n1) \"-1\"\n2) \"2\"\n3) \"3\"\n4) \"10\"\n```\n\n# 2、事务\n\n​\t\t有时候为了同时处理多个结构，我们需要向Redis 发送多个命令。尽管Redis有几个可以在两个键之间复制或者移动元素的命令，但却没有那种可以在两个不同类型之间移动元素的命令(虽然可以使用ZUNIONSTORE命令将元素从一个集合复制到一个有序集合)。为了对相同或者不同类型的多个键执行操作，Redis有5个命令可以让用户在不被打断(interruption )的情况下对多个键执行操作，它们分别是WATCH、MULTI、EXEC、UNWATCH和 DISCARD。\n\n​\t\tRedis的基本事务( basic transaction)需要用到MULTI命令和EXEC命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令。和关系数据库那种可以在执行的过程中进行回滚( rollback)的事务不同，在Redis里面，被MULTI命令和EXEC命令包围的所有命令会一个接-一个地执行， 直到所有命令都执行完毕为止。当一个事务执行完毕之后，Redis 才会处理其他客户端的命令。\n\n# 3、键的过期时间\n\n​\t\t在使用Redis存储数据的时候，有些数据可能在某个时间点之后就不再有用了，用户可以使用DEL命令显式地删除这些无用数据,也可以通过Redis的过期时间(expiration)特性来让一个键在给定的时限(timeout)之后自动被删除。当我们说一个键“带有生存时间(time to live)”或者一个键“会在特定时间之后过期(expire)”时，我们指的是Redis会在这个键的过期时间到达时自动删除该键。\n\n- PERSIST key-name\t\t移除键的过期时间\n- TTL key-name\t\t查看给定键距离过期还有多少秒\n- EXPIRE key-name seconds\t\t让给定键在指定的秒数之后过期\n- EXPIREAT key-name timestamp\t\t将给定键的过期时间设置为给定的UNIX时间戳\n- PTTL key-name\t\t查看给定键距离过期时间还有多少毫秒，这个命令在Redis 2.6或以上版本可用\n- PEXPIRE key-name milliseconds\t\t让给定键在指定的毫秒数之后过期，这个命令在Redis 2.6或以上版本可用\n- PEXPIREAT key-name timestamp-milliseconds\t\t将一个毫秒级精度的UNIX时间戳设置为给定键的过期时间，这个命令在 Redis 2.6或以上版本可用\n\n","slug":"初识redis（2）","published":1,"updated":"2022-04-04T08:32:40.169Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0o00827kt9h3rwenli","content":"<h1>1、排序</h1>\n<p>​\t\tRedis 的排序操作和其他编程语言的排序操作一样，都可以根据某种比较规则对一系列元素进行有序的排列。负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5种键里面存储着的数据，对列表、集合以及有序集合进行排序。如果读者之前曾经使用过关系数据库的话，那么可以将SORT命令看作是SQL语言里的order by子句。</p>\n<p>SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GETpattern …] ] [ASC|DESC] [ALPHA] [STORE dest-key] 根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;10&quot;\n2) &quot;-1&quot;\n3) &quot;2&quot;\n4) &quot;3&quot;\n127.0.0.1:6379&gt; sort myList asc\n1) &quot;-1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;10&quot;\n</code></pre>\n<h1>2、事务</h1>\n<p>​\t\t有时候为了同时处理多个结构，我们需要向Redis 发送多个命令。尽管Redis有几个可以在两个键之间复制或者移动元素的命令，但却没有那种可以在两个不同类型之间移动元素的命令(虽然可以使用ZUNIONSTORE命令将元素从一个集合复制到一个有序集合)。为了对相同或者不同类型的多个键执行操作，Redis有5个命令可以让用户在不被打断(interruption )的情况下对多个键执行操作，它们分别是WATCH、MULTI、EXEC、UNWATCH和 DISCARD。</p>\n<p>​\t\tRedis的基本事务( basic transaction)需要用到MULTI命令和EXEC命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令。和关系数据库那种可以在执行的过程中进行回滚( rollback)的事务不同，在Redis里面，被MULTI命令和EXEC命令包围的所有命令会一个接-一个地执行， 直到所有命令都执行完毕为止。当一个事务执行完毕之后，Redis 才会处理其他客户端的命令。</p>\n<h1>3、键的过期时间</h1>\n<p>​\t\t在使用Redis存储数据的时候，有些数据可能在某个时间点之后就不再有用了，用户可以使用DEL命令显式地删除这些无用数据,也可以通过Redis的过期时间(expiration)特性来让一个键在给定的时限(timeout)之后自动被删除。当我们说一个键“带有生存时间(time to live)”或者一个键“会在特定时间之后过期(expire)”时，我们指的是Redis会在这个键的过期时间到达时自动删除该键。</p>\n<ul>\n<li>PERSIST key-name\t\t移除键的过期时间</li>\n<li>TTL key-name\t\t查看给定键距离过期还有多少秒</li>\n<li>EXPIRE key-name seconds\t\t让给定键在指定的秒数之后过期</li>\n<li>EXPIREAT key-name timestamp\t\t将给定键的过期时间设置为给定的UNIX时间戳</li>\n<li>PTTL key-name\t\t查看给定键距离过期时间还有多少毫秒，这个命令在Redis 2.6或以上版本可用</li>\n<li>PEXPIRE key-name milliseconds\t\t让给定键在指定的毫秒数之后过期，这个命令在Redis 2.6或以上版本可用</li>\n<li>PEXPIREAT key-name timestamp-milliseconds\t\t将一个毫秒级精度的UNIX时间戳设置为给定键的过期时间，这个命令在 Redis 2.6或以上版本可用</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、排序</h1>\n<p>​\t\tRedis 的排序操作和其他编程语言的排序操作一样，都可以根据某种比较规则对一系列元素进行有序的排列。负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5种键里面存储着的数据，对列表、集合以及有序集合进行排序。如果读者之前曾经使用过关系数据库的话，那么可以将SORT命令看作是SQL语言里的order by子句。</p>\n<p>SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GETpattern …] ] [ASC|DESC] [ALPHA] [STORE dest-key] 根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果</p>\n<pre><code class=\"language-java\">127.0.0.1:6379&gt; lrange myList 0 -1\n1) &quot;10&quot;\n2) &quot;-1&quot;\n3) &quot;2&quot;\n4) &quot;3&quot;\n127.0.0.1:6379&gt; sort myList asc\n1) &quot;-1&quot;\n2) &quot;2&quot;\n3) &quot;3&quot;\n4) &quot;10&quot;\n</code></pre>\n<h1>2、事务</h1>\n<p>​\t\t有时候为了同时处理多个结构，我们需要向Redis 发送多个命令。尽管Redis有几个可以在两个键之间复制或者移动元素的命令，但却没有那种可以在两个不同类型之间移动元素的命令(虽然可以使用ZUNIONSTORE命令将元素从一个集合复制到一个有序集合)。为了对相同或者不同类型的多个键执行操作，Redis有5个命令可以让用户在不被打断(interruption )的情况下对多个键执行操作，它们分别是WATCH、MULTI、EXEC、UNWATCH和 DISCARD。</p>\n<p>​\t\tRedis的基本事务( basic transaction)需要用到MULTI命令和EXEC命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令。和关系数据库那种可以在执行的过程中进行回滚( rollback)的事务不同，在Redis里面，被MULTI命令和EXEC命令包围的所有命令会一个接-一个地执行， 直到所有命令都执行完毕为止。当一个事务执行完毕之后，Redis 才会处理其他客户端的命令。</p>\n<h1>3、键的过期时间</h1>\n<p>​\t\t在使用Redis存储数据的时候，有些数据可能在某个时间点之后就不再有用了，用户可以使用DEL命令显式地删除这些无用数据,也可以通过Redis的过期时间(expiration)特性来让一个键在给定的时限(timeout)之后自动被删除。当我们说一个键“带有生存时间(time to live)”或者一个键“会在特定时间之后过期(expire)”时，我们指的是Redis会在这个键的过期时间到达时自动删除该键。</p>\n<ul>\n<li>PERSIST key-name\t\t移除键的过期时间</li>\n<li>TTL key-name\t\t查看给定键距离过期还有多少秒</li>\n<li>EXPIRE key-name seconds\t\t让给定键在指定的秒数之后过期</li>\n<li>EXPIREAT key-name timestamp\t\t将给定键的过期时间设置为给定的UNIX时间戳</li>\n<li>PTTL key-name\t\t查看给定键距离过期时间还有多少毫秒，这个命令在Redis 2.6或以上版本可用</li>\n<li>PEXPIRE key-name milliseconds\t\t让给定键在指定的毫秒数之后过期，这个命令在Redis 2.6或以上版本可用</li>\n<li>PEXPIREAT key-name timestamp-milliseconds\t\t将一个毫秒级精度的UNIX时间戳设置为给定键的过期时间，这个命令在 Redis 2.6或以上版本可用</li>\n</ul>\n"},{"title":"初识redis（4）-主从架构","author":"ztq","date":"2021-08-01T16:13:00.000Z","_content":"\n# 1、Redis “主 - 从” 机制\n\nRedis 提供 “主 - 从” 的数据复制：“从” Redis 即作为 “主” Redis 的数据副本。“从” Redis，既能够用于读性能的扩展，亦能够作为数据备份的一种手段。\n\n同时，Redis 支持 [Redis Sentinel](https://redis.io/topics/sentinel)，实现 “主 - 从” 监控、故障迁移，限于篇幅，本文不予以展开。\n\n# 2、工作机制\n\n## “主 **-** 从” 数据复制的基本工作机制\n\n- 已建立的 “主” - “从” 连接，“主”     Redis 不断地将命令发送到 “从” Redis\n- 若连接中断（例如：网络问题），“从” Redis 将尝试重新建立连接，并尝试 “半 - 重新同步”\n- 若无法进行 “半 - 重新同步”，“从” Redis 将尝试进行 “重新同步”（“主 - 从” 连接首次建立，亦执行 “重新同步”）\n\n## 关于 “半 - 重新同步” & “重新同步”\n\n- “半 - 重新同步”：“从” Redis 将尝试获取连接中断期间于 “主” Redis 执行的命令（存储于 backlog）\n- “重新同步”\n  - “主” Redis 创建数据快照（RDB 文件）、同步到 “从” Redis，开始将 “主” Redis 执行的命令发送到 “从” Redis\n  - “从” Redis 丢弃当前数据，加载 “主” Redis 的 RDB 文件，开始执行 “主” Redis 发送的命令\n\n“数据复制” 对于 “主” Redis 全部是异步的；对于 “从” Redis，大部分是异步的，但 “重新同步” 涉及 “丢弃当前数据，加载 RDB 文件”，将引起 “短暂中断”。\n\n# 3、“主 - 从” 配置\n\n```java\nslaveof master_ip master_port  # “从” Redis 配置：“主” Redis - IP &  port\nmasterauth master_password     # “从” Redis 配置：“主” Redis - 密码\n\nslave-serve-stale-data yes     # “从” Redis 配置：当 “主 - 从” 连接中断或 “从” Redis 正在进行初始化同步，“从” Redis 是否提供服务：\n                               #   yes: 默认，以 “从” Redis 当前数据提供服务\n                               #   no: 对于接收到的命令，“从” Redis 返回 “SYNC in progress”（INFO、SLAVEOF 命令除外）\n                               #\nslave-read-only yes            # “从” Redis 配置：是否 “只读”，默认 yes\n\n\n“主” Redis 配置：根据已连接的 “从” Redis 情况，“主” Redis 是否接收 “写命令”\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n表示：最少有 3 个已连接的 “从” Redis，且延迟小于等于 10 秒\n\nmin-slaves-to-write 3          # 默认 0，即无论 “从” Redis 的连接情况，始终接收 “写命令”\nmin-slaves-max-lag 10\n```\n\n以上的代码，仅列出了部分关键的配置。其他类似于：diskless 复制、backlog 配置，限于篇幅，未能列出，详情内容请参考 [redis.conf for Redis 2.8](https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf)。\n\n# 4、“主 - 从” 命令\n\n1. SLAVEOF host port\n   将 Redis 配置作为 “从” Redis，其 “主” Redis 位置即为 host:port。\n2. SLAVEOF NO ONE\n   终止 “从” Redis 自 “主” Redis 的数据同步。\n   特别说明：SLAVEOF NO ONE 包含了 Redis 设计之初，关于 “自由” 的思想：“If slavery is not wrong, nothing is wrong. -- Abraham Lincoln”。\n\n# 5、“主 - 从” 链\n\n“从” Redis 能够作为其他 Redis 的 “主” Redis，由此构建级联结构的 “主 - 从” 链。并且，“主” Redis 能够与多个 “从” Redis 建立连接，建立 “树状” 结构。\n\n![image-20210802000052243](/img/image-20210802000052243.png)\n\n","source":"_posts/初识redis（4）-主从架构.md","raw":"title: 初识redis（4）-主从架构\nauthor: ztq\ntags:\n\n  - redis\ncategories:\n  - 数据库\ndate: 2021-08-02 00:13:00\n\n---\n\n# 1、Redis “主 - 从” 机制\n\nRedis 提供 “主 - 从” 的数据复制：“从” Redis 即作为 “主” Redis 的数据副本。“从” Redis，既能够用于读性能的扩展，亦能够作为数据备份的一种手段。\n\n同时，Redis 支持 [Redis Sentinel](https://redis.io/topics/sentinel)，实现 “主 - 从” 监控、故障迁移，限于篇幅，本文不予以展开。\n\n# 2、工作机制\n\n## “主 **-** 从” 数据复制的基本工作机制\n\n- 已建立的 “主” - “从” 连接，“主”     Redis 不断地将命令发送到 “从” Redis\n- 若连接中断（例如：网络问题），“从” Redis 将尝试重新建立连接，并尝试 “半 - 重新同步”\n- 若无法进行 “半 - 重新同步”，“从” Redis 将尝试进行 “重新同步”（“主 - 从” 连接首次建立，亦执行 “重新同步”）\n\n## 关于 “半 - 重新同步” & “重新同步”\n\n- “半 - 重新同步”：“从” Redis 将尝试获取连接中断期间于 “主” Redis 执行的命令（存储于 backlog）\n- “重新同步”\n  - “主” Redis 创建数据快照（RDB 文件）、同步到 “从” Redis，开始将 “主” Redis 执行的命令发送到 “从” Redis\n  - “从” Redis 丢弃当前数据，加载 “主” Redis 的 RDB 文件，开始执行 “主” Redis 发送的命令\n\n“数据复制” 对于 “主” Redis 全部是异步的；对于 “从” Redis，大部分是异步的，但 “重新同步” 涉及 “丢弃当前数据，加载 RDB 文件”，将引起 “短暂中断”。\n\n# 3、“主 - 从” 配置\n\n```java\nslaveof master_ip master_port  # “从” Redis 配置：“主” Redis - IP &  port\nmasterauth master_password     # “从” Redis 配置：“主” Redis - 密码\n\nslave-serve-stale-data yes     # “从” Redis 配置：当 “主 - 从” 连接中断或 “从” Redis 正在进行初始化同步，“从” Redis 是否提供服务：\n                               #   yes: 默认，以 “从” Redis 当前数据提供服务\n                               #   no: 对于接收到的命令，“从” Redis 返回 “SYNC in progress”（INFO、SLAVEOF 命令除外）\n                               #\nslave-read-only yes            # “从” Redis 配置：是否 “只读”，默认 yes\n\n\n“主” Redis 配置：根据已连接的 “从” Redis 情况，“主” Redis 是否接收 “写命令”\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n表示：最少有 3 个已连接的 “从” Redis，且延迟小于等于 10 秒\n\nmin-slaves-to-write 3          # 默认 0，即无论 “从” Redis 的连接情况，始终接收 “写命令”\nmin-slaves-max-lag 10\n```\n\n以上的代码，仅列出了部分关键的配置。其他类似于：diskless 复制、backlog 配置，限于篇幅，未能列出，详情内容请参考 [redis.conf for Redis 2.8](https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf)。\n\n# 4、“主 - 从” 命令\n\n1. SLAVEOF host port\n   将 Redis 配置作为 “从” Redis，其 “主” Redis 位置即为 host:port。\n2. SLAVEOF NO ONE\n   终止 “从” Redis 自 “主” Redis 的数据同步。\n   特别说明：SLAVEOF NO ONE 包含了 Redis 设计之初，关于 “自由” 的思想：“If slavery is not wrong, nothing is wrong. -- Abraham Lincoln”。\n\n# 5、“主 - 从” 链\n\n“从” Redis 能够作为其他 Redis 的 “主” Redis，由此构建级联结构的 “主 - 从” 链。并且，“主” Redis 能够与多个 “从” Redis 建立连接，建立 “树状” 结构。\n\n![image-20210802000052243](/img/image-20210802000052243.png)\n\n","slug":"初识redis（4）-主从架构","published":1,"updated":"2022-04-04T08:32:40.169Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0q00857kt9gazu1eec","content":"<h1>1、Redis “主 - 从” 机制</h1>\n<p>Redis 提供 “主 - 从” 的数据复制：“从” Redis 即作为 “主” Redis 的数据副本。“从” Redis，既能够用于读性能的扩展，亦能够作为数据备份的一种手段。</p>\n<p>同时，Redis 支持 <a href=\"https://redis.io/topics/sentinel\">Redis Sentinel</a>，实现 “主 - 从” 监控、故障迁移，限于篇幅，本文不予以展开。</p>\n<h1>2、工作机制</h1>\n<h2 id=\"“主-从”-数据复制的基本工作机制\">“主 <strong>-</strong> 从” 数据复制的基本工作机制</h2>\n<ul>\n<li>已建立的 “主” - “从” 连接，“主”     Redis 不断地将命令发送到 “从” Redis</li>\n<li>若连接中断（例如：网络问题），“从” Redis 将尝试重新建立连接，并尝试 “半 - 重新同步”</li>\n<li>若无法进行 “半 - 重新同步”，“从” Redis 将尝试进行 “重新同步”（“主 - 从” 连接首次建立，亦执行 “重新同步”）</li>\n</ul>\n<h2 id=\"关于-“半-重新同步”-“重新同步”\">关于 “半 - 重新同步” &amp; “重新同步”</h2>\n<ul>\n<li>“半 - 重新同步”：“从” Redis 将尝试获取连接中断期间于 “主” Redis 执行的命令（存储于 backlog）</li>\n<li>“重新同步”\n<ul>\n<li>“主” Redis 创建数据快照（RDB 文件）、同步到 “从” Redis，开始将 “主” Redis 执行的命令发送到 “从” Redis</li>\n<li>“从” Redis 丢弃当前数据，加载 “主” Redis 的 RDB 文件，开始执行 “主” Redis 发送的命令</li>\n</ul>\n</li>\n</ul>\n<p>“数据复制” 对于 “主” Redis 全部是异步的；对于 “从” Redis，大部分是异步的，但 “重新同步” 涉及 “丢弃当前数据，加载 RDB 文件”，将引起 “短暂中断”。</p>\n<h1>3、“主 - 从” 配置</h1>\n<pre><code class=\"language-java\">slaveof master_ip master_port  # “从” Redis 配置：“主” Redis - IP &amp;  port\nmasterauth master_password     # “从” Redis 配置：“主” Redis - 密码\n\nslave-serve-stale-data yes     # “从” Redis 配置：当 “主 - 从” 连接中断或 “从” Redis 正在进行初始化同步，“从” Redis 是否提供服务：\n                               #   yes: 默认，以 “从” Redis 当前数据提供服务\n                               #   no: 对于接收到的命令，“从” Redis 返回 “SYNC in progress”（INFO、SLAVEOF 命令除外）\n                               #\nslave-read-only yes            # “从” Redis 配置：是否 “只读”，默认 yes\n\n\n“主” Redis 配置：根据已连接的 “从” Redis 情况，“主” Redis 是否接收 “写命令”\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n表示：最少有 3 个已连接的 “从” Redis，且延迟小于等于 10 秒\n\nmin-slaves-to-write 3          # 默认 0，即无论 “从” Redis 的连接情况，始终接收 “写命令”\nmin-slaves-max-lag 10\n</code></pre>\n<p>以上的代码，仅列出了部分关键的配置。其他类似于：diskless 复制、backlog 配置，限于篇幅，未能列出，详情内容请参考 <a href=\"https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf\">redis.conf for Redis 2.8</a>。</p>\n<h1>4、“主 - 从” 命令</h1>\n<ol>\n<li>SLAVEOF host port<br>\n将 Redis 配置作为 “从” Redis，其 “主” Redis 位置即为 host:port。</li>\n<li>SLAVEOF NO ONE<br>\n终止 “从” Redis 自 “主” Redis 的数据同步。<br>\n特别说明：SLAVEOF NO ONE 包含了 Redis 设计之初，关于 “自由” 的思想：“If slavery is not wrong, nothing is wrong. – Abraham Lincoln”。</li>\n</ol>\n<h1>5、“主 - 从” 链</h1>\n<p>“从” Redis 能够作为其他 Redis 的 “主” Redis，由此构建级联结构的 “主 - 从” 链。并且，“主” Redis 能够与多个 “从” Redis 建立连接，建立 “树状” 结构。</p>\n<p><img src=\"/img/image-20210802000052243.png\" alt=\"image-20210802000052243\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、Redis “主 - 从” 机制</h1>\n<p>Redis 提供 “主 - 从” 的数据复制：“从” Redis 即作为 “主” Redis 的数据副本。“从” Redis，既能够用于读性能的扩展，亦能够作为数据备份的一种手段。</p>\n<p>同时，Redis 支持 <a href=\"https://redis.io/topics/sentinel\">Redis Sentinel</a>，实现 “主 - 从” 监控、故障迁移，限于篇幅，本文不予以展开。</p>\n<h1>2、工作机制</h1>\n<h2 id=\"“主-从”-数据复制的基本工作机制\">“主 <strong>-</strong> 从” 数据复制的基本工作机制</h2>\n<ul>\n<li>已建立的 “主” - “从” 连接，“主”     Redis 不断地将命令发送到 “从” Redis</li>\n<li>若连接中断（例如：网络问题），“从” Redis 将尝试重新建立连接，并尝试 “半 - 重新同步”</li>\n<li>若无法进行 “半 - 重新同步”，“从” Redis 将尝试进行 “重新同步”（“主 - 从” 连接首次建立，亦执行 “重新同步”）</li>\n</ul>\n<h2 id=\"关于-“半-重新同步”-“重新同步”\">关于 “半 - 重新同步” &amp; “重新同步”</h2>\n<ul>\n<li>“半 - 重新同步”：“从” Redis 将尝试获取连接中断期间于 “主” Redis 执行的命令（存储于 backlog）</li>\n<li>“重新同步”\n<ul>\n<li>“主” Redis 创建数据快照（RDB 文件）、同步到 “从” Redis，开始将 “主” Redis 执行的命令发送到 “从” Redis</li>\n<li>“从” Redis 丢弃当前数据，加载 “主” Redis 的 RDB 文件，开始执行 “主” Redis 发送的命令</li>\n</ul>\n</li>\n</ul>\n<p>“数据复制” 对于 “主” Redis 全部是异步的；对于 “从” Redis，大部分是异步的，但 “重新同步” 涉及 “丢弃当前数据，加载 RDB 文件”，将引起 “短暂中断”。</p>\n<h1>3、“主 - 从” 配置</h1>\n<pre><code class=\"language-java\">slaveof master_ip master_port  # “从” Redis 配置：“主” Redis - IP &amp;  port\nmasterauth master_password     # “从” Redis 配置：“主” Redis - 密码\n\nslave-serve-stale-data yes     # “从” Redis 配置：当 “主 - 从” 连接中断或 “从” Redis 正在进行初始化同步，“从” Redis 是否提供服务：\n                               #   yes: 默认，以 “从” Redis 当前数据提供服务\n                               #   no: 对于接收到的命令，“从” Redis 返回 “SYNC in progress”（INFO、SLAVEOF 命令除外）\n                               #\nslave-read-only yes            # “从” Redis 配置：是否 “只读”，默认 yes\n\n\n“主” Redis 配置：根据已连接的 “从” Redis 情况，“主” Redis 是否接收 “写命令”\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n表示：最少有 3 个已连接的 “从” Redis，且延迟小于等于 10 秒\n\nmin-slaves-to-write 3          # 默认 0，即无论 “从” Redis 的连接情况，始终接收 “写命令”\nmin-slaves-max-lag 10\n</code></pre>\n<p>以上的代码，仅列出了部分关键的配置。其他类似于：diskless 复制、backlog 配置，限于篇幅，未能列出，详情内容请参考 <a href=\"https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf\">redis.conf for Redis 2.8</a>。</p>\n<h1>4、“主 - 从” 命令</h1>\n<ol>\n<li>SLAVEOF host port<br>\n将 Redis 配置作为 “从” Redis，其 “主” Redis 位置即为 host:port。</li>\n<li>SLAVEOF NO ONE<br>\n终止 “从” Redis 自 “主” Redis 的数据同步。<br>\n特别说明：SLAVEOF NO ONE 包含了 Redis 设计之初，关于 “自由” 的思想：“If slavery is not wrong, nothing is wrong. – Abraham Lincoln”。</li>\n</ol>\n<h1>5、“主 - 从” 链</h1>\n<p>“从” Redis 能够作为其他 Redis 的 “主” Redis，由此构建级联结构的 “主 - 从” 链。并且，“主” Redis 能够与多个 “从” Redis 建立连接，建立 “树状” 结构。</p>\n<p><img src=\"/img/image-20210802000052243.png\" alt=\"image-20210802000052243\"></p>\n"},{"title":"初识redis（5）-内存调优","author":"ztq","date":"2021-08-01T16:14:00.000Z","_content":"\n# 1、优化 Redis 内存使用\n\n合理的 Redis 实例，内存的占有量不应当超过 60%，当内存使用率过高时，应该予以清理及优化。\n\n# 2、使用 ziplist & intset\n\n## ziplist 优化机制\n\nziplist 实现了 “紧凑” 的数据结构，通过尽可能减少非数据节点的占用，以提供内存密度。\n\n![image-20210802000308665](/img/image-20210802000308665.png)\n\n图中所示，ziplist 整体结构：\n•\tzl-bytes：整个 ziplist 占用内存的字节数\n•\tzl-tail：ziplist 尾节点距离起始地址的字节数\n•\tzl-len：ziplist 包含的节点数量\n•\tentry：节点\n•\tzl-end：ziplist 末端标记，固定 0xFF\nziplist 节点结构：\n•\tprevious-entry-length：前一个节点占用内存的字节数\n•\tencoding：节点编码，明确节点存储内容属于 “字节数组” 或整数，并明确长度（即占用的字节数）\n•\tcontent：节点存储内容\n“散列表”、“链表”、“有序集合”，使用 ziplist，受益于其 “紧凑” 的数据结构，相较于 hashtable、linkedlist、skiplist，能够有效减少内存占用。\n然而，受限于 “紧凑” 的数据结构，随着节点数量增长和节点大小膨胀，基于 ziplist 实现的 “散列表”、“链表”、“有序集合”，性能将显著下降。\n\n## intset 优化机制\n\nintset 使用整型数组作为存储的数据结构。通常，hashtable 实现的 Redis 集合，其成员以 “字符串” 结构进行存储，intset 由此能够显著降低内存使用。\n\n类似于 ziplist，同样受限于其整型数组，“集合” 成员数量的增长将引起 “集合” 性能的下降。\n\n# 3、ziplist & intset配置\n\n```java\n “散列表” 使用 ziplist 的限制条件：\n  - 成员数量不超过 hash-max-ziplist-entries\n  - 最大内存占用的成员，内存占用不超过 hash-max-ziplist-value (字节)\n两者必须同时具备，任意条件不满足，即无法使用 ziplist    \nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\n “链表” 使用 ziplist 的限制条件\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\n“有序集合” 使用 ziplist 的限制条件\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n“集合” 使用 intset 的限制条件:\n- 成员全部为 64 位有符号整数\n- 成员数量不超过 set-max-intset-entries\nset-max-intset-entries 512\n\n```\n\n附加说明：ziplist & intset 的限制条件，是基于内存占用和性能的综合考虑。\n\n# 4、数据分片\n\nRedis 作为任何单实例的数据服务，最终会遇到容量和性能瓶颈。前文阐述的 Redis “主 - 从”，即为常见且有效的扩展 Redis 读性能的方案。 “数据分片” 构建 Redis 集群。常用的方案包括：Redis Cluster、[twemproxy](https://github.com/twitter/twemproxy)、[Codis](https://github.com/CodisLabs)。\n\n## 分布式 “数据分片”\n\n分布式 “数据分片”：选取合适的方式将 Redis 数据分布于不同的实例，由此降低单实例的内存使用，实现优化。\n\n## 单实例 “数据分片”\n\n通常而言，单实例 “数据分片”，并不能直接降低 Redis 内存使用，需要结合 ziplist 等内存优化方式，以 “散列表” 为例：\n•\t以散列表的键作为 “数据分片” 的 “路由”，将单个内存占用量大的 “散列表” 分片到多个内存占有量小的 “散列表”\n•\t内存占有量小的 “散列表”（例如：“散列表” 成员数量少） 能够以 ziplist 方式减少内存占用\n由此，有效地实现内存使用的优化。\n\n## 基于业务进行优化\n\n基于业务，通常能够取得良好的 Redis 内存优化效果，例如：\n•\t尽可能短的 Redis 键，例如：以 “u_178” 替代 “user_id_178”\n•\t选择合适的 Redis 数据结构，例如：合理地选择 “散列表” 替代 “字符串”，若 “字符串” 数量较少，使用一个 “散列表” 替代，通常能够减少内存使用\n•\t减少存储于 Redis 的业务数据量\n\n# 5、数据分片配置\n\n“数据分片” - 使用 [Codis](https://github.com/CodisLabs)\n\n选择 Codis 的原因\nCodis 来自于 “豌豆荚”，相对于 twemproxy，选择 Codis 的原因：\n•\ttwemproxy 无法实现动态水平扩展\n•\tCodis 运行于多核机器能够获得更好的应用\n相对于 Redis Cluster，选择 Codis 的原因：\n•\tRedis Cluster 必须使用 Redis 3.0 以上版本的客户端\n•\tRedis Cluster 无法支持 pipeline\n\n\n\nCodis 架构\n\n![image-20210802000916321](/img/image-20210802000916321.png)\n\n图中所示，Codis 架构中引入了 codis-proxy，由 codis-proxy 基于 Redis key 计算分片，将命令转发到 codis-group，因此：对于绝大多数的命令，客户端对于 Codis 的接入是透明的。\n\n\n\nCodis 针对 Redis key 计算 CRC32，默认分为 1024 个 Slot，进而路由到特定的 codis-group，实现分片。\n除了 “数据分片”，Codis 的特性还包括：\n•\t提供了 codis-fe & codis-dashborad 作为集群管理工具\n•\t允许多个 codis-proxy，实现 proxy 层的高可用\n•\tcodis-group 支持 “主 - 从”，引入 redis-sentinel 实现 “主 - 从” 故障迁移\n必须说明的是：“数据分片” 扩展容量和性能的同时，亦限制了 Redis 若干方便的能力，例如：Codis 不支持事务、部分命令不支持。\n","source":"_posts/初识redis（5）-内存调优.md","raw":"title: 初识redis（5）-内存调优\nauthor: ztq\ntags:\n\n  - redis\ncategories:\n  - 数据库\ndate: 2021-08-02 00:14:00\n\n---\n\n# 1、优化 Redis 内存使用\n\n合理的 Redis 实例，内存的占有量不应当超过 60%，当内存使用率过高时，应该予以清理及优化。\n\n# 2、使用 ziplist & intset\n\n## ziplist 优化机制\n\nziplist 实现了 “紧凑” 的数据结构，通过尽可能减少非数据节点的占用，以提供内存密度。\n\n![image-20210802000308665](/img/image-20210802000308665.png)\n\n图中所示，ziplist 整体结构：\n•\tzl-bytes：整个 ziplist 占用内存的字节数\n•\tzl-tail：ziplist 尾节点距离起始地址的字节数\n•\tzl-len：ziplist 包含的节点数量\n•\tentry：节点\n•\tzl-end：ziplist 末端标记，固定 0xFF\nziplist 节点结构：\n•\tprevious-entry-length：前一个节点占用内存的字节数\n•\tencoding：节点编码，明确节点存储内容属于 “字节数组” 或整数，并明确长度（即占用的字节数）\n•\tcontent：节点存储内容\n“散列表”、“链表”、“有序集合”，使用 ziplist，受益于其 “紧凑” 的数据结构，相较于 hashtable、linkedlist、skiplist，能够有效减少内存占用。\n然而，受限于 “紧凑” 的数据结构，随着节点数量增长和节点大小膨胀，基于 ziplist 实现的 “散列表”、“链表”、“有序集合”，性能将显著下降。\n\n## intset 优化机制\n\nintset 使用整型数组作为存储的数据结构。通常，hashtable 实现的 Redis 集合，其成员以 “字符串” 结构进行存储，intset 由此能够显著降低内存使用。\n\n类似于 ziplist，同样受限于其整型数组，“集合” 成员数量的增长将引起 “集合” 性能的下降。\n\n# 3、ziplist & intset配置\n\n```java\n “散列表” 使用 ziplist 的限制条件：\n  - 成员数量不超过 hash-max-ziplist-entries\n  - 最大内存占用的成员，内存占用不超过 hash-max-ziplist-value (字节)\n两者必须同时具备，任意条件不满足，即无法使用 ziplist    \nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\n “链表” 使用 ziplist 的限制条件\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\n“有序集合” 使用 ziplist 的限制条件\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n“集合” 使用 intset 的限制条件:\n- 成员全部为 64 位有符号整数\n- 成员数量不超过 set-max-intset-entries\nset-max-intset-entries 512\n\n```\n\n附加说明：ziplist & intset 的限制条件，是基于内存占用和性能的综合考虑。\n\n# 4、数据分片\n\nRedis 作为任何单实例的数据服务，最终会遇到容量和性能瓶颈。前文阐述的 Redis “主 - 从”，即为常见且有效的扩展 Redis 读性能的方案。 “数据分片” 构建 Redis 集群。常用的方案包括：Redis Cluster、[twemproxy](https://github.com/twitter/twemproxy)、[Codis](https://github.com/CodisLabs)。\n\n## 分布式 “数据分片”\n\n分布式 “数据分片”：选取合适的方式将 Redis 数据分布于不同的实例，由此降低单实例的内存使用，实现优化。\n\n## 单实例 “数据分片”\n\n通常而言，单实例 “数据分片”，并不能直接降低 Redis 内存使用，需要结合 ziplist 等内存优化方式，以 “散列表” 为例：\n•\t以散列表的键作为 “数据分片” 的 “路由”，将单个内存占用量大的 “散列表” 分片到多个内存占有量小的 “散列表”\n•\t内存占有量小的 “散列表”（例如：“散列表” 成员数量少） 能够以 ziplist 方式减少内存占用\n由此，有效地实现内存使用的优化。\n\n## 基于业务进行优化\n\n基于业务，通常能够取得良好的 Redis 内存优化效果，例如：\n•\t尽可能短的 Redis 键，例如：以 “u_178” 替代 “user_id_178”\n•\t选择合适的 Redis 数据结构，例如：合理地选择 “散列表” 替代 “字符串”，若 “字符串” 数量较少，使用一个 “散列表” 替代，通常能够减少内存使用\n•\t减少存储于 Redis 的业务数据量\n\n# 5、数据分片配置\n\n“数据分片” - 使用 [Codis](https://github.com/CodisLabs)\n\n选择 Codis 的原因\nCodis 来自于 “豌豆荚”，相对于 twemproxy，选择 Codis 的原因：\n•\ttwemproxy 无法实现动态水平扩展\n•\tCodis 运行于多核机器能够获得更好的应用\n相对于 Redis Cluster，选择 Codis 的原因：\n•\tRedis Cluster 必须使用 Redis 3.0 以上版本的客户端\n•\tRedis Cluster 无法支持 pipeline\n\n\n\nCodis 架构\n\n![image-20210802000916321](/img/image-20210802000916321.png)\n\n图中所示，Codis 架构中引入了 codis-proxy，由 codis-proxy 基于 Redis key 计算分片，将命令转发到 codis-group，因此：对于绝大多数的命令，客户端对于 Codis 的接入是透明的。\n\n\n\nCodis 针对 Redis key 计算 CRC32，默认分为 1024 个 Slot，进而路由到特定的 codis-group，实现分片。\n除了 “数据分片”，Codis 的特性还包括：\n•\t提供了 codis-fe & codis-dashborad 作为集群管理工具\n•\t允许多个 codis-proxy，实现 proxy 层的高可用\n•\tcodis-group 支持 “主 - 从”，引入 redis-sentinel 实现 “主 - 从” 故障迁移\n必须说明的是：“数据分片” 扩展容量和性能的同时，亦限制了 Redis 若干方便的能力，例如：Codis 不支持事务、部分命令不支持。\n","slug":"初识redis（5）-内存调优","published":1,"updated":"2022-04-04T08:32:40.170Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0q00897kt939lo4nvc","content":"<h1>1、优化 Redis 内存使用</h1>\n<p>合理的 Redis 实例，内存的占有量不应当超过 60%，当内存使用率过高时，应该予以清理及优化。</p>\n<h1>2、使用 ziplist &amp; intset</h1>\n<h2 id=\"ziplist-优化机制\">ziplist 优化机制</h2>\n<p>ziplist 实现了 “紧凑” 的数据结构，通过尽可能减少非数据节点的占用，以提供内存密度。</p>\n<p><img src=\"/img/image-20210802000308665.png\" alt=\"image-20210802000308665\"></p>\n<p>图中所示，ziplist 整体结构：<br>\n•\tzl-bytes：整个 ziplist 占用内存的字节数<br>\n•\tzl-tail：ziplist 尾节点距离起始地址的字节数<br>\n•\tzl-len：ziplist 包含的节点数量<br>\n•\tentry：节点<br>\n•\tzl-end：ziplist 末端标记，固定 0xFF<br>\nziplist 节点结构：<br>\n•\tprevious-entry-length：前一个节点占用内存的字节数<br>\n•\tencoding：节点编码，明确节点存储内容属于 “字节数组” 或整数，并明确长度（即占用的字节数）<br>\n•\tcontent：节点存储内容<br>\n“散列表”、“链表”、“有序集合”，使用 ziplist，受益于其 “紧凑” 的数据结构，相较于 hashtable、linkedlist、skiplist，能够有效减少内存占用。<br>\n然而，受限于 “紧凑” 的数据结构，随着节点数量增长和节点大小膨胀，基于 ziplist 实现的 “散列表”、“链表”、“有序集合”，性能将显著下降。</p>\n<h2 id=\"intset-优化机制\">intset 优化机制</h2>\n<p>intset 使用整型数组作为存储的数据结构。通常，hashtable 实现的 Redis 集合，其成员以 “字符串” 结构进行存储，intset 由此能够显著降低内存使用。</p>\n<p>类似于 ziplist，同样受限于其整型数组，“集合” 成员数量的增长将引起 “集合” 性能的下降。</p>\n<h1>3、ziplist &amp; intset配置</h1>\n<pre><code class=\"language-java\"> “散列表” 使用 ziplist 的限制条件：\n  - 成员数量不超过 hash-max-ziplist-entries\n  - 最大内存占用的成员，内存占用不超过 hash-max-ziplist-value (字节)\n两者必须同时具备，任意条件不满足，即无法使用 ziplist    \nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\n “链表” 使用 ziplist 的限制条件\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\n“有序集合” 使用 ziplist 的限制条件\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n“集合” 使用 intset 的限制条件:\n- 成员全部为 64 位有符号整数\n- 成员数量不超过 set-max-intset-entries\nset-max-intset-entries 512\n\n</code></pre>\n<p>附加说明：ziplist &amp; intset 的限制条件，是基于内存占用和性能的综合考虑。</p>\n<h1>4、数据分片</h1>\n<p>Redis 作为任何单实例的数据服务，最终会遇到容量和性能瓶颈。前文阐述的 Redis “主 - 从”，即为常见且有效的扩展 Redis 读性能的方案。 “数据分片” 构建 Redis 集群。常用的方案包括：Redis Cluster、<a href=\"https://github.com/twitter/twemproxy\">twemproxy</a>、<a href=\"https://github.com/CodisLabs\">Codis</a>。</p>\n<h2 id=\"分布式-“数据分片”\">分布式 “数据分片”</h2>\n<p>分布式 “数据分片”：选取合适的方式将 Redis 数据分布于不同的实例，由此降低单实例的内存使用，实现优化。</p>\n<h2 id=\"单实例-“数据分片”\">单实例 “数据分片”</h2>\n<p>通常而言，单实例 “数据分片”，并不能直接降低 Redis 内存使用，需要结合 ziplist 等内存优化方式，以 “散列表” 为例：<br>\n•\t以散列表的键作为 “数据分片” 的 “路由”，将单个内存占用量大的 “散列表” 分片到多个内存占有量小的 “散列表”<br>\n•\t内存占有量小的 “散列表”（例如：“散列表” 成员数量少） 能够以 ziplist 方式减少内存占用<br>\n由此，有效地实现内存使用的优化。</p>\n<h2 id=\"基于业务进行优化\">基于业务进行优化</h2>\n<p>基于业务，通常能够取得良好的 Redis 内存优化效果，例如：<br>\n•\t尽可能短的 Redis 键，例如：以 “u_178” 替代 “user_id_178”<br>\n•\t选择合适的 Redis 数据结构，例如：合理地选择 “散列表” 替代 “字符串”，若 “字符串” 数量较少，使用一个 “散列表” 替代，通常能够减少内存使用<br>\n•\t减少存储于 Redis 的业务数据量</p>\n<h1>5、数据分片配置</h1>\n<p>“数据分片” - 使用 <a href=\"https://github.com/CodisLabs\">Codis</a></p>\n<p>选择 Codis 的原因<br>\nCodis 来自于 “豌豆荚”，相对于 twemproxy，选择 Codis 的原因：<br>\n•\ttwemproxy 无法实现动态水平扩展<br>\n•\tCodis 运行于多核机器能够获得更好的应用<br>\n相对于 Redis Cluster，选择 Codis 的原因：<br>\n•\tRedis Cluster 必须使用 Redis 3.0 以上版本的客户端<br>\n•\tRedis Cluster 无法支持 pipeline</p>\n<p>Codis 架构</p>\n<p><img src=\"/img/image-20210802000916321.png\" alt=\"image-20210802000916321\"></p>\n<p>图中所示，Codis 架构中引入了 codis-proxy，由 codis-proxy 基于 Redis key 计算分片，将命令转发到 codis-group，因此：对于绝大多数的命令，客户端对于 Codis 的接入是透明的。</p>\n<p>Codis 针对 Redis key 计算 CRC32，默认分为 1024 个 Slot，进而路由到特定的 codis-group，实现分片。<br>\n除了 “数据分片”，Codis 的特性还包括：<br>\n•\t提供了 codis-fe &amp; codis-dashborad 作为集群管理工具<br>\n•\t允许多个 codis-proxy，实现 proxy 层的高可用<br>\n•\tcodis-group 支持 “主 - 从”，引入 redis-sentinel 实现 “主 - 从” 故障迁移<br>\n必须说明的是：“数据分片” 扩展容量和性能的同时，亦限制了 Redis 若干方便的能力，例如：Codis 不支持事务、部分命令不支持。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、优化 Redis 内存使用</h1>\n<p>合理的 Redis 实例，内存的占有量不应当超过 60%，当内存使用率过高时，应该予以清理及优化。</p>\n<h1>2、使用 ziplist &amp; intset</h1>\n<h2 id=\"ziplist-优化机制\">ziplist 优化机制</h2>\n<p>ziplist 实现了 “紧凑” 的数据结构，通过尽可能减少非数据节点的占用，以提供内存密度。</p>\n<p><img src=\"/img/image-20210802000308665.png\" alt=\"image-20210802000308665\"></p>\n<p>图中所示，ziplist 整体结构：<br>\n•\tzl-bytes：整个 ziplist 占用内存的字节数<br>\n•\tzl-tail：ziplist 尾节点距离起始地址的字节数<br>\n•\tzl-len：ziplist 包含的节点数量<br>\n•\tentry：节点<br>\n•\tzl-end：ziplist 末端标记，固定 0xFF<br>\nziplist 节点结构：<br>\n•\tprevious-entry-length：前一个节点占用内存的字节数<br>\n•\tencoding：节点编码，明确节点存储内容属于 “字节数组” 或整数，并明确长度（即占用的字节数）<br>\n•\tcontent：节点存储内容<br>\n“散列表”、“链表”、“有序集合”，使用 ziplist，受益于其 “紧凑” 的数据结构，相较于 hashtable、linkedlist、skiplist，能够有效减少内存占用。<br>\n然而，受限于 “紧凑” 的数据结构，随着节点数量增长和节点大小膨胀，基于 ziplist 实现的 “散列表”、“链表”、“有序集合”，性能将显著下降。</p>\n<h2 id=\"intset-优化机制\">intset 优化机制</h2>\n<p>intset 使用整型数组作为存储的数据结构。通常，hashtable 实现的 Redis 集合，其成员以 “字符串” 结构进行存储，intset 由此能够显著降低内存使用。</p>\n<p>类似于 ziplist，同样受限于其整型数组，“集合” 成员数量的增长将引起 “集合” 性能的下降。</p>\n<h1>3、ziplist &amp; intset配置</h1>\n<pre><code class=\"language-java\"> “散列表” 使用 ziplist 的限制条件：\n  - 成员数量不超过 hash-max-ziplist-entries\n  - 最大内存占用的成员，内存占用不超过 hash-max-ziplist-value (字节)\n两者必须同时具备，任意条件不满足，即无法使用 ziplist    \nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\n “链表” 使用 ziplist 的限制条件\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\n“有序集合” 使用 ziplist 的限制条件\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n“集合” 使用 intset 的限制条件:\n- 成员全部为 64 位有符号整数\n- 成员数量不超过 set-max-intset-entries\nset-max-intset-entries 512\n\n</code></pre>\n<p>附加说明：ziplist &amp; intset 的限制条件，是基于内存占用和性能的综合考虑。</p>\n<h1>4、数据分片</h1>\n<p>Redis 作为任何单实例的数据服务，最终会遇到容量和性能瓶颈。前文阐述的 Redis “主 - 从”，即为常见且有效的扩展 Redis 读性能的方案。 “数据分片” 构建 Redis 集群。常用的方案包括：Redis Cluster、<a href=\"https://github.com/twitter/twemproxy\">twemproxy</a>、<a href=\"https://github.com/CodisLabs\">Codis</a>。</p>\n<h2 id=\"分布式-“数据分片”\">分布式 “数据分片”</h2>\n<p>分布式 “数据分片”：选取合适的方式将 Redis 数据分布于不同的实例，由此降低单实例的内存使用，实现优化。</p>\n<h2 id=\"单实例-“数据分片”\">单实例 “数据分片”</h2>\n<p>通常而言，单实例 “数据分片”，并不能直接降低 Redis 内存使用，需要结合 ziplist 等内存优化方式，以 “散列表” 为例：<br>\n•\t以散列表的键作为 “数据分片” 的 “路由”，将单个内存占用量大的 “散列表” 分片到多个内存占有量小的 “散列表”<br>\n•\t内存占有量小的 “散列表”（例如：“散列表” 成员数量少） 能够以 ziplist 方式减少内存占用<br>\n由此，有效地实现内存使用的优化。</p>\n<h2 id=\"基于业务进行优化\">基于业务进行优化</h2>\n<p>基于业务，通常能够取得良好的 Redis 内存优化效果，例如：<br>\n•\t尽可能短的 Redis 键，例如：以 “u_178” 替代 “user_id_178”<br>\n•\t选择合适的 Redis 数据结构，例如：合理地选择 “散列表” 替代 “字符串”，若 “字符串” 数量较少，使用一个 “散列表” 替代，通常能够减少内存使用<br>\n•\t减少存储于 Redis 的业务数据量</p>\n<h1>5、数据分片配置</h1>\n<p>“数据分片” - 使用 <a href=\"https://github.com/CodisLabs\">Codis</a></p>\n<p>选择 Codis 的原因<br>\nCodis 来自于 “豌豆荚”，相对于 twemproxy，选择 Codis 的原因：<br>\n•\ttwemproxy 无法实现动态水平扩展<br>\n•\tCodis 运行于多核机器能够获得更好的应用<br>\n相对于 Redis Cluster，选择 Codis 的原因：<br>\n•\tRedis Cluster 必须使用 Redis 3.0 以上版本的客户端<br>\n•\tRedis Cluster 无法支持 pipeline</p>\n<p>Codis 架构</p>\n<p><img src=\"/img/image-20210802000916321.png\" alt=\"image-20210802000916321\"></p>\n<p>图中所示，Codis 架构中引入了 codis-proxy，由 codis-proxy 基于 Redis key 计算分片，将命令转发到 codis-group，因此：对于绝大多数的命令，客户端对于 Codis 的接入是透明的。</p>\n<p>Codis 针对 Redis key 计算 CRC32，默认分为 1024 个 Slot，进而路由到特定的 codis-group，实现分片。<br>\n除了 “数据分片”，Codis 的特性还包括：<br>\n•\t提供了 codis-fe &amp; codis-dashborad 作为集群管理工具<br>\n•\t允许多个 codis-proxy，实现 proxy 层的高可用<br>\n•\tcodis-group 支持 “主 - 从”，引入 redis-sentinel 实现 “主 - 从” 故障迁移<br>\n必须说明的是：“数据分片” 扩展容量和性能的同时，亦限制了 Redis 若干方便的能力，例如：Codis 不支持事务、部分命令不支持。</p>\n"},{"title":"加密解密","author":"郑天祺","date":"2019-09-02T05:37:00.000Z","_content":"\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","source":"_posts/加密解密.md","raw":"title: 加密解密\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-02 13:37:00\n\n---\n\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","slug":"加密解密","published":1,"updated":"2022-04-04T08:32:40.170Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0r008c7kt9gyedhhoa","content":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2 id=\"（1）对称加密算法\">（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（2）非对称加密算法\">（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（3）混合加密机制\">（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt=\"\"></p>\n<h3 id=\"加密过程\">加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3 id=\"解密过程\">解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2 id=\"（4）为什么需要混合加密机制？\">（4）为什么需要混合加密机制？</h2>\n<h3 id=\"安全？速度快？\">安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2 id=\"（1）对称加密算法\">（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（2）非对称加密算法\">（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（3）混合加密机制\">（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt=\"\"></p>\n<h3 id=\"加密过程\">加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3 id=\"解密过程\">解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2 id=\"（4）为什么需要混合加密机制？\">（4）为什么需要混合加密机制？</h2>\n<h3 id=\"安全？速度快？\">安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n"},{"title":"单例模式","author":"郑天祺","date":"2020-01-03T08:24:00.000Z","_content":"\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","source":"_posts/单例模式.md","raw":"title: 单例模式\nauthor: 郑天祺\ntags:\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 16:24:00\n\n---\n\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","slug":"单例模式","published":1,"updated":"2022-04-04T08:32:40.170Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0s008f7kt9awvvhfyq","content":"<p>继之前的单例模式（<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215%EF%BC%89%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%EF%BC%8C%E8%B6%8A%E7%9C%8B%E8%B6%8A%E5%AE%B9%E6%98%93%E4%B8%8D%E6%98%8E%E7%99%BD%E4%BA%86%5B%E5%93%AD%E5%93%AD%5D%EF%BC%9A\">https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</a></p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。<br>\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（<a href=\"https://zhengtianqi.github.io/2019/09/01/%E6%B1%A0%E5%8C%96%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0/%EF%BC%89%E3%80%82\">https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</a></p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>继之前的单例模式（<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215%EF%BC%89%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%EF%BC%8C%E8%B6%8A%E7%9C%8B%E8%B6%8A%E5%AE%B9%E6%98%93%E4%B8%8D%E6%98%8E%E7%99%BD%E4%BA%86%5B%E5%93%AD%E5%93%AD%5D%EF%BC%9A\">https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</a></p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。<br>\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（<a href=\"https://zhengtianqi.github.io/2019/09/01/%E6%B1%A0%E5%8C%96%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0/%EF%BC%89%E3%80%82\">https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</a></p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n"},{"title":"可信与可信计算","author":"郑天祺","date":"2019-09-28T08:55:00.000Z","_content":"\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","source":"_posts/可信与可信计算.md","raw":"title: 可信与可信计算\nauthor: 郑天祺\ntags:\n  - 可信计算\ncategories:\n  - 可信\ndate: 2019-09-28 16:55:00\n\n---\n\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","slug":"可信与可信计算","published":1,"updated":"2022-04-04T08:32:40.171Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0s008k7kt990hoel8z","content":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n"},{"title":"可信基本概念","author":"郑天祺","date":"2019-09-01T06:46:00.000Z","_content":"\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","source":"_posts/可信基本概念.md","raw":"title: 可信基本概念\nauthor: 郑天祺\ntags:\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-01 14:46:00\n\n---\n\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","slug":"可信基本概念","published":1,"updated":"2022-04-04T08:32:40.171Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0t008n7kt9a17rga93","content":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2 id=\"（1）可信1-0（软件容错）\">（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2 id=\"（2）可信2-0（硬件可信）\">（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2 id=\"（3）可信3-0（主动防御体系）\">（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3 id=\"（a）TPCM\">（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3 id=\"（b）-可信基础支撑软件框架\">（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3 id=\"（c）三层三元对等的可信连接框架\">（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3 id=\"（d）加密算法均自主设计\">（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3 id=\"（f）主动免疫可信架构信任链传递示意图：\">（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt=\"\"></p>\n<h2 id=\"2、可信的应用\">2、可信的应用</h2>\n<h2 id=\"（1）基础架构图\">（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt=\"\"></p>\n<h2 id=\"（2）可信的安全保障机制\">（2）可信的安全保障机制</h2>\n<h3 id=\"（a）运行环境\">（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3 id=\"（b）监控技术\">（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3 id=\"（c）隔离技术\">（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3 id=\"（d）接入技术\">（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2 id=\"（1）可信1-0（软件容错）\">（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2 id=\"（2）可信2-0（硬件可信）\">（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2 id=\"（3）可信3-0（主动防御体系）\">（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3 id=\"（a）TPCM\">（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3 id=\"（b）-可信基础支撑软件框架\">（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3 id=\"（c）三层三元对等的可信连接框架\">（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3 id=\"（d）加密算法均自主设计\">（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3 id=\"（f）主动免疫可信架构信任链传递示意图：\">（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt=\"\"></p>\n<h2 id=\"2、可信的应用\">2、可信的应用</h2>\n<h2 id=\"（1）基础架构图\">（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt=\"\"></p>\n<h2 id=\"（2）可信的安全保障机制\">（2）可信的安全保障机制</h2>\n<h3 id=\"（a）运行环境\">（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3 id=\"（b）监控技术\">（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3 id=\"（c）隔离技术\">（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3 id=\"（d）接入技术\">（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n"},{"title":"可靠性和容错技术","author":"郑天祺","date":"2019-10-02T05:39:00.000Z","_content":"\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","source":"_posts/可靠性和容错技术.md","raw":"title: 可靠性和容错技术\nauthor: 郑天祺\ntags:\n  - 可靠\n  - 容错\ncategories:\n  - 可信\ndate: 2019-10-02 13:39:00\n\n---\n\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","slug":"可靠性和容错技术","published":1,"updated":"2022-04-04T08:32:40.172Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0u008r7kt9btl5dhzs","content":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n"},{"title":"可重入锁","author":"郑天祺","date":"2019-08-31T05:05:00.000Z","_content":"\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","source":"_posts/可重入锁.md","raw":"title: 可重入锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:05:00\n\n---\n\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","slug":"可重入锁","published":1,"updated":"2022-04-04T08:32:40.171Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0v008v7kt97alqblc5","content":"<h2 id=\"1、可重入锁：\">1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。<br>\n​\t“独占”，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2 id=\"2、Synchronized和ReentrantLock\">2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable &#123;\n     public synchronized void get() &#123;\n         System.out.println(Thread.currentThread().getName());\n         set();\n     &#125;\n     public synchronized void set() &#123;\n         System.out.println(Thread.currentThread().getName());\n     &#125;\n     @Override\n     public void run() &#123;\n         get();\n     &#125;\n     public static void main(String[] args) &#123;\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     &#125;\n &#125;\n\n \n\npublic class ReentrantLockTest implements Runnable &#123;\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     &#125;\n\n    public void set() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     &#125;\n\n    @Override\n     public void run() &#123;\n         get();\n     &#125;\n\n    public static void main(String[] args) &#123;\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     &#125;\n &#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、可重入锁：\">1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。<br>\n​\t“独占”，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2 id=\"2、Synchronized和ReentrantLock\">2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable &#123;\n     public synchronized void get() &#123;\n         System.out.println(Thread.currentThread().getName());\n         set();\n     &#125;\n     public synchronized void set() &#123;\n         System.out.println(Thread.currentThread().getName());\n     &#125;\n     @Override\n     public void run() &#123;\n         get();\n     &#125;\n     public static void main(String[] args) &#123;\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     &#125;\n &#125;\n\n \n\npublic class ReentrantLockTest implements Runnable &#123;\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     &#125;\n\n    public void set() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     &#125;\n\n    @Override\n     public void run() &#123;\n         get();\n     &#125;\n\n    public static void main(String[] args) &#123;\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     &#125;\n &#125;\n\n \n</code></pre>\n"},{"title":"图解公钥私钥","author":"郑天祺","date":"2019-09-24T13:32:00.000Z","_content":"\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","source":"_posts/图解公钥与私钥.md","raw":"title: 图解公钥私钥\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-24 21:32:00\n---\n\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","slug":"图解公钥与私钥","published":1,"updated":"2022-04-04T08:32:40.172Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0w008z7kt91evi4d5n","content":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n"},{"title":"基于JavaAgent的全链路监控（1）","author":"郑天祺","date":"2020-07-17T05:11:00.000Z","_content":"\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（1）.md","raw":"title: 基于JavaAgent的全链路监控（1）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-17 13:11:00\n\n---\n\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","slug":"基于JavaAgent的全链路监控（1）","published":1,"updated":"2022-04-04T08:32:40.173Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0x00937kt97hj1dxs7","content":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2 id=\"（1）建立maven的空java项目\">（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2 id=\"（2）MANIFEST-MF-文件\">（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"（3）写一个main函数\">（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n\n&#125;\n</code></pre>\n<h2 id=\"（4）打包\">（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2 id=\"（1）主程序运行前\">（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2 id=\"（2）主程序运行后加载\">（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;$&#123;java.home&#125;/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException &#123;\n        VirtualMachine virtualMachine = null;\n        try &#123;\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        &#125; finally &#123;\n            if (virtualMachine != null) &#123;\n                virtualMachine.detach();\n            &#125;\n        &#125;\n\n    &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2 id=\"（1）建立maven的空java项目\">（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2 id=\"（2）MANIFEST-MF-文件\">（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"（3）写一个main函数\">（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n\n&#125;\n</code></pre>\n<h2 id=\"（4）打包\">（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2 id=\"（1）主程序运行前\">（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2 id=\"（2）主程序运行后加载\">（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;$&#123;java.home&#125;/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException &#123;\n        VirtualMachine virtualMachine = null;\n        try &#123;\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        &#125; finally &#123;\n            if (virtualMachine != null) &#123;\n                virtualMachine.detach();\n            &#125;\n        &#125;\n\n    &#125;\n</code></pre>\n"},{"title":"基于JavaAgent的全链路监控（2）","author":"郑天祺","date":"2020-07-19T08:54:00.000Z","_content":"\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","source":"_posts/基于JavaAgent的全链路监控（2）.md","raw":"title: 基于JavaAgent的全链路监控（2）\nauthor: 郑天祺\ntags:\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 16:54:00\n\n---\n\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","slug":"基于JavaAgent的全链路监控（2）","published":1,"updated":"2022-04-04T08:32:40.173Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0y00967kt9dxpk1sxw","content":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2 id=\"1、介绍\">1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2 id=\"2、pom-xml\">2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;javassist.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2 id=\"3、MethodCostTime-java\">3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime &#123;\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception &#123;\n        long start = System.currentTimeMillis();\n        try &#123;\n            // 原有函数执行\n            return callable.call();\n        &#125; finally &#123;\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、MyAgent-java\">4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; &#123;\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        &#125;;\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() &#123;\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) &#123;\n\n            &#125;\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) &#123;\n\n            &#125;\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n        &#125;;\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"5、MANIFEST-MF\">5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"6、测试\">6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2 id=\"1、介绍\">1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2 id=\"2、pom-xml\">2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;javassist.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2 id=\"3、MethodCostTime-java\">3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime &#123;\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception &#123;\n        long start = System.currentTimeMillis();\n        try &#123;\n            // 原有函数执行\n            return callable.call();\n        &#125; finally &#123;\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、MyAgent-java\">4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; &#123;\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        &#125;;\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() &#123;\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) &#123;\n\n            &#125;\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) &#123;\n\n            &#125;\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n        &#125;;\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"5、MANIFEST-MF\">5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"6、测试\">6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n"},{"title":"基于JavaAgent的全链路监控（3）","author":"郑天祺","date":"2020-07-19T09:25:00.000Z","_content":"\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（3）.md","raw":"title: 基于JavaAgent的全链路监控（3）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 17:25:00\n\n---\n\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","slug":"基于JavaAgent的全链路监控（3）","published":1,"updated":"2022-04-04T08:32:40.173Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0y00997kt9c29f118b","content":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2 id=\"（1）MyAgent-java\">（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        &#123;\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        &#125;, 0L, 1000L, TimeUnit.MILLISECONDS);\n     &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2 id=\"（1）MyAgent-java\">（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        &#123;\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        &#125;, 0L, 1000L, TimeUnit.MILLISECONDS);\n     &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"对象存储与指针压缩","author":"郑天祺","date":"2019-11-20T11:50:00.000Z","_content":"\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","source":"_posts/对象存储与指针压缩.md","raw":"title: 对象存储与指针压缩\nauthor: 郑天祺\ntags:\n  - 内存模型\ncategories:\n  - java基础\ndate: 2019-11-20 19:50:00\n\n---\n\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","slug":"对象存储与指针压缩","published":1,"updated":"2022-04-04T08:32:40.174Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno0z009d7kt9foqi3dul","content":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：<a href=\"http://www.ideabuffer.cn/2017/05/06/Java%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/\">http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：<a href=\"http://www.ideabuffer.cn/2017/05/06/Java%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/\">http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</a></p>\n"},{"title":"并发编程总结","author":"郑天祺","date":"2020-11-17T08:57:00.000Z","_content":"\n1、Synchronized\n\n​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。\n\n​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。\n\n​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。\n\n​\t\tSynchronize是非公平锁。\n\n2、Synchronized锁对象\n\n​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。\n（2）如果没有明确指定:\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。\n\n3、Synchronized可重入锁\n\n​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。\n\n4、JVM对java的原生锁的优化\n\n​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。\n\n​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁\n\n​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。\n\n​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。\n\n​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。\n\n​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。\n\n5、Synchronize 和 ReentrantLock实现原理的不同\n\nSynchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。\n\n6、AQS框架\n\n​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。\n\n（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。\n\n（2）\n\n","source":"_posts/并发编程总结.md","raw":"title: 并发编程总结\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - 面试\ndate: 2020-11-17 16:57:00\n\n---\n\n1、Synchronized\n\n​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。\n\n​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。\n\n​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。\n\n​\t\tSynchronize是非公平锁。\n\n2、Synchronized锁对象\n\n​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。\n（2）如果没有明确指定:\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。\n\n3、Synchronized可重入锁\n\n​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。\n\n4、JVM对java的原生锁的优化\n\n​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。\n\n​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁\n\n​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。\n\n​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。\n\n​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。\n\n​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。\n\n5、Synchronize 和 ReentrantLock实现原理的不同\n\nSynchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。\n\n6、AQS框架\n\n​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。\n\n（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。\n\n（2）\n\n","slug":"并发编程总结","published":1,"updated":"2022-04-04T08:32:40.174Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno11009g7kt9esm458fl","content":"<p>1、Synchronized</p>\n<p>​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。</p>\n<p>​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。</p>\n<p>​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。</p>\n<p>​\t\tSynchronize是非公平锁。</p>\n<p>2、Synchronized锁对象</p>\n<p>​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。<br>\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。<br>\n（2）如果没有明确指定:<br>\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;<br>\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。<br>\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。</p>\n<p>3、Synchronized可重入锁</p>\n<p>​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。<br>\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。</p>\n<p>4、JVM对java的原生锁的优化</p>\n<p>​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。</p>\n<p>​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁</p>\n<p>​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。</p>\n<p>​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。</p>\n<p>​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。</p>\n<p>​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。</p>\n<p>5、Synchronize 和 ReentrantLock实现原理的不同</p>\n<p>Synchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。</p>\n<p>6、AQS框架</p>\n<p>​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。</p>\n<p>（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。</p>\n<p>（2）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、Synchronized</p>\n<p>​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。</p>\n<p>​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。</p>\n<p>​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。</p>\n<p>​\t\tSynchronize是非公平锁。</p>\n<p>2、Synchronized锁对象</p>\n<p>​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。<br>\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。<br>\n（2）如果没有明确指定:<br>\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;<br>\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。<br>\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。</p>\n<p>3、Synchronized可重入锁</p>\n<p>​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。<br>\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。</p>\n<p>4、JVM对java的原生锁的优化</p>\n<p>​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。</p>\n<p>​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁</p>\n<p>​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。</p>\n<p>​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。</p>\n<p>​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。</p>\n<p>​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。</p>\n<p>5、Synchronize 和 ReentrantLock实现原理的不同</p>\n<p>Synchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。</p>\n<p>6、AQS框架</p>\n<p>​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。</p>\n<p>（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。</p>\n<p>（2）</p>\n"},{"title":"悲观锁、乐观锁","author":"郑天祺","date":"2019-08-31T05:16:00.000Z","_content":"\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","source":"_posts/悲观锁、乐观锁.md","raw":"title: 悲观锁、乐观锁\nauthor: 郑天祺\ntags:\n  - 锁\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 13:16:00\n\n---\n\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","slug":"悲观锁、乐观锁","published":1,"updated":"2022-04-04T08:32:40.174Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno12009k7kt907y77qop","content":"<h2 id=\"1、悲观锁\">1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2 id=\"2、缺点\">2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2 id=\"3、实现方法\">3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=“123” for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2 id=\"4、使用场景举例\">4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3 id=\"0、乐观锁：\">0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3 id=\"1、乐观锁：\">1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3 id=\"2、优缺点：\">2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3 id=\"3、步骤\">3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#&#123;id&#125;\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#&#123;id&#125; and versio&#123;139&#125;&#125;;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、悲观锁\">1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2 id=\"2、缺点\">2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2 id=\"3、实现方法\">3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=“123” for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2 id=\"4、使用场景举例\">4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3 id=\"0、乐观锁：\">0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3 id=\"1、乐观锁：\">1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3 id=\"2、优缺点：\">2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3 id=\"3、步骤\">3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#&#123;id&#125;\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#&#123;id&#125; and versio&#123;139&#125;&#125;;\n</code></pre>\n"},{"title":"手写一个简单Autowired","author":"郑天祺","date":"2020-09-11T02:49:00.000Z","_content":"\n# 1、按照惯例\n\n首先写一个controller和service\n\n```java\npackage cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController {\n    @Autowired\n    private UserService userService;\n    \n}\n\n```\n\n```java\npackage cn.edu.bjut.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n\n```\n\n# 2、Autowired注解的定义\n\n```java\npackage cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired {\n\n}\n\n```\n\n# 3、注入的方法\n\n利用反射\n\n```java\npackage cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired {\n    public static void main(String[] args) {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -> {\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) {\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class<?> type = field.getType();\n                try {\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                } catch (InstantiationException e) {\n                    e.printStackTrace();\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                }\n            }\n        });\n    }\n}\n```\n\n","source":"_posts/手写一个简单Autowired.md","raw":"title: 手写一个简单Autowired\nauthor: 郑天祺\ntags:\n  - spring\ncategories:\n  - spring\ndate: 2020-09-11 10:49:00\n\n---\n\n# 1、按照惯例\n\n首先写一个controller和service\n\n```java\npackage cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController {\n    @Autowired\n    private UserService userService;\n    \n}\n\n```\n\n```java\npackage cn.edu.bjut.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n\n```\n\n# 2、Autowired注解的定义\n\n```java\npackage cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired {\n\n}\n\n```\n\n# 3、注入的方法\n\n利用反射\n\n```java\npackage cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired {\n    public static void main(String[] args) {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -> {\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) {\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class<?> type = field.getType();\n                try {\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                } catch (InstantiationException e) {\n                    e.printStackTrace();\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                }\n            }\n        });\n    }\n}\n```\n\n","slug":"手写一个简单Autowired","published":1,"updated":"2022-04-04T08:32:40.175Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno12009n7kt92nvja4mx","content":"<h1>1、按照惯例</h1>\n<p>首先写一个controller和service</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController &#123;\n    @Autowired\n    private UserService userService;\n    \n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>2、Autowired注解的定义</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired &#123;\n\n&#125;\n\n</code></pre>\n<h1>3、注入的方法</h1>\n<p>利用反射</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired &#123;\n    public static void main(String[] args) &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -&gt; &#123;\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) &#123;\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class&lt;?&gt; type = field.getType();\n                try &#123;\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                &#125; catch (InstantiationException e) &#123;\n                    e.printStackTrace();\n                &#125; catch (IllegalAccessException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、按照惯例</h1>\n<p>首先写一个controller和service</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController &#123;\n    @Autowired\n    private UserService userService;\n    \n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>2、Autowired注解的定义</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired &#123;\n\n&#125;\n\n</code></pre>\n<h1>3、注入的方法</h1>\n<p>利用反射</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired &#123;\n    public static void main(String[] args) &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -&gt; &#123;\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) &#123;\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class&lt;?&gt; type = field.getType();\n                try &#123;\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                &#125; catch (InstantiationException e) &#123;\n                    e.printStackTrace();\n                &#125; catch (IllegalAccessException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"排序之比较器Comparable<T>","author":"郑天祺","date":"2020-01-02T02:27:00.000Z","_content":"\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","source":"_posts/排序之比较器.md","raw":"title: 排序之比较器Comparable<T>\nauthor: 郑天祺\ntags:\n  - java\n  - 数据结构\n  - ''\ncategories:\n  - java基础\ndate: 2020-01-02 10:27:00\n---\n\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","slug":"排序之比较器","published":1,"updated":"2022-04-04T08:32:40.175Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno13009r7kt91nj24flk","content":"<h1>一、Comparable<T>比较器的使用</h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：</p>\n<p>​\t\t\t1、实体继承Comparable<T></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; &#123;\n    private String name;\n    private int age;\n\n    public Student(String name, int age) &#123;\n        this.name = name;\n        this.age = age;\n    &#125;\n\n    @Override\n    public int compareTo(Student o) &#123;\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    &#125;\n\n    public void print() &#123;\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = &#123;stu1, stu2, stu3, stu4&#125;;\n\n        Arrays.sort(students);\n        for (Student stu : students) &#123;\n            stu.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>二、Comparable<T>比较器的原理</h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable &#123;\n\n    public static class BinaryTree&lt;T&gt; &#123;\n        class Node &#123;\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) &#123;\n                this.data = data;\n            &#125;\n\n            void addNode(Node newNode) &#123;\n                if (newNode.data.compareTo((T) this.data) &lt; 0) &#123;\n                    if (this.left == null) &#123;\n                        this.left = newNode;\n                    &#125; else &#123;\n                        this.left.addNode(newNode);\n                    &#125;\n                &#125;\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) &#123;\n                    if (this.right == null) &#123;\n                        this.right = newNode;\n                    &#125; else &#123;\n                        this.right.addNode(newNode);\n                    &#125;\n                &#125;\n            &#125;\n\n            void print() &#123;\n                if (this.left != null) &#123;\n                    left.print();\n                &#125;\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) &#123;\n                    this.right.print();\n                &#125;\n            &#125;\n        &#125;\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) &#123;\n            Node newNode = new Node(data);\n            if (root == null) &#123;\n                root = newNode;\n            &#125; else &#123;\n                root.addNode(newNode);\n            &#125;\n        &#125;\n\n        public void print() &#123;\n            this.root.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t&#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparable<T>比较器的使用</h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：</p>\n<p>​\t\t\t1、实体继承Comparable<T></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; &#123;\n    private String name;\n    private int age;\n\n    public Student(String name, int age) &#123;\n        this.name = name;\n        this.age = age;\n    &#125;\n\n    @Override\n    public int compareTo(Student o) &#123;\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    &#125;\n\n    public void print() &#123;\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = &#123;stu1, stu2, stu3, stu4&#125;;\n\n        Arrays.sort(students);\n        for (Student stu : students) &#123;\n            stu.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>二、Comparable<T>比较器的原理</h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable &#123;\n\n    public static class BinaryTree&lt;T&gt; &#123;\n        class Node &#123;\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) &#123;\n                this.data = data;\n            &#125;\n\n            void addNode(Node newNode) &#123;\n                if (newNode.data.compareTo((T) this.data) &lt; 0) &#123;\n                    if (this.left == null) &#123;\n                        this.left = newNode;\n                    &#125; else &#123;\n                        this.left.addNode(newNode);\n                    &#125;\n                &#125;\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) &#123;\n                    if (this.right == null) &#123;\n                        this.right = newNode;\n                    &#125; else &#123;\n                        this.right.addNode(newNode);\n                    &#125;\n                &#125;\n            &#125;\n\n            void print() &#123;\n                if (this.left != null) &#123;\n                    left.print();\n                &#125;\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) &#123;\n                    this.right.print();\n                &#125;\n            &#125;\n        &#125;\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) &#123;\n            Node newNode = new Node(data);\n            if (root == null) &#123;\n                root = newNode;\n            &#125; else &#123;\n                root.addNode(newNode);\n            &#125;\n        &#125;\n\n        public void print() &#123;\n            this.root.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t&#125;\n&#125;\n</code></pre>\n"},{"title":"排序之比较器Comparator<T>","author":"郑天祺","date":"2020-01-02T03:34:00.000Z","_content":"\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","source":"_posts/排序之比较器Comparator-T.md","raw":"title: 排序之比较器Comparator<T>\nauthor: 郑天祺\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2020-01-02 11:34:00\n\n---\n\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","slug":"排序之比较器Comparator-T","published":1,"updated":"2022-04-04T08:32:40.175Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno14009u7kt93sfa2kde","content":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) &#123;\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() &#123;\n            @Override\n            public int compare(Student o1, Student o2) &#123;\n                return o1.getAge() - o2.getAge();\n            &#125;\n        &#125;);\n    &#125;\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) &#123;\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() &#123;\n            @Override\n            public int compare(Student o1, Student o2) &#123;\n                return o1.getAge() - o2.getAge();\n            &#125;\n        &#125;);\n    &#125;\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n"},{"title":"数字签名","author":"郑天祺","date":"2019-09-01T12:23:00.000Z","_content":"# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","source":"_posts/数字签名.md","raw":"title: 数字签名\nauthor: 郑天祺\ntags:\n  - 可信\n  - 加密算法\ncategories:\n  - 可信\ndate: 2019-09-01 20:23:00\n---\n# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","slug":"数字签名","published":1,"updated":"2022-04-04T08:32:40.176Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno15009y7kt91kn45bf9","content":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2 id=\"（1）椭圆曲线数字签名算法\">（1）椭圆曲线数字签名算法:</h2>\n<h3 id=\"生成数字签名\">生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3 id=\"验证数字签名\">验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3 id=\"验证方法解释\">验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3 id=\"原理解释：\">原理解释：</h3>\n<p><a href=\"https://www.cnblogs.com/wsonepiece/p/3977021.html\">https://www.cnblogs.com/wsonepiece/p/3977021.html</a></p>\n<h2 id=\"（2）Schnorr数字签名算法\">（2）Schnorr数字签名算法</h2>\n<h3 id=\"生成数字签名-2\">生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3 id=\"验证数字签名-2\">验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3 id=\"验证方法解释-2\">验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2 id=\"（1）椭圆曲线数字签名算法\">（1）椭圆曲线数字签名算法:</h2>\n<h3 id=\"生成数字签名\">生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3 id=\"验证数字签名\">验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3 id=\"验证方法解释\">验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3 id=\"原理解释：\">原理解释：</h3>\n<p><a href=\"https://www.cnblogs.com/wsonepiece/p/3977021.html\">https://www.cnblogs.com/wsonepiece/p/3977021.html</a></p>\n<h2 id=\"（2）Schnorr数字签名算法\">（2）Schnorr数字签名算法</h2>\n<h3 id=\"生成数字签名-2\">生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3 id=\"验证数字签名-2\">验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3 id=\"验证方法解释-2\">验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n"},{"title":"文件上传之Content-Type","author":"郑天祺","date":"2019-08-31T08:15:00.000Z","_content":"\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","source":"_posts/文件上传.md","raw":"title: 文件上传之Content-Type\nauthor: 郑天祺\ntags:\n  - 文件上传\ncategories:\n  - java基础\ndate: 2019-08-31 16:15:00\n\n---\n\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","slug":"文件上传","published":1,"updated":"2022-04-04T08:32:40.177Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1600a17kt91xxddepo","content":"<h2 id=\"1、Content-Type介绍\">1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3 id=\"1-1、常见的页面资源类型\">1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3 id=\"1-2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\">1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file)&#123;\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n&#125;\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) &#123;\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n&#125;\n</code></pre>\n<p>​</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、Content-Type介绍\">1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3 id=\"1-1、常见的页面资源类型\">1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3 id=\"1-2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\">1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file)&#123;\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n&#125;\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) &#123;\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n&#125;\n</code></pre>\n<p>​</p>\n"},{"title":"池化之线程池","author":"郑天祺","date":"2019-09-01T02:14:00.000Z","_content":"\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池好处\n\n几乎所有需要异步或者并发执行任务的程序都可以使用线程池。\n\n合理使用会给我们带来以下好处。\n\n降低系统消耗：重复利用已经创建的线程降低线程创建和销毁造成的资源消耗。\n\n提高响应速度：当任务到达时，任务不需要等到线程创建就可以立即执行。\n\n提供线程可以管理性：可以通过设置合理分配、调优、监控。\n\n## 3、线程池工作流程\n\n1、判断核心线程池里的线程是否都有在执行任务，否->创建一个新工作线程来执行任务。是->走下个流程。\n\n2、判断工作队列是否已满，否->新任务存储在这个工作队列里，是->走下个流程。\n\n3、判断线程池里的线程是否都在工作状态，否->创建一个新的工作线程来执行任务，是->走下个流程。\n\n4、按照设置的策略来处理无法执行的任务。\n\n\n\n## 4、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n### 1.corePoolSize：\n\n核心线程池大小，当提交一个任务时，线程池会创建一个线程来执行任务，即使其他空闲的核心线程能够执行新任务也会创建，等待需要执行的任务数大于线程核心大小就不会继续创建。\n\n### 2.maximumPoolSize：\n\n线程池最大数，允许创建的最大线程数，如果队列满了，并且已经创建的线程数小于最大线程数，则会创建新的线程执行任务。如果是无界队列，这个参数基本没用。\n\n### 3.keepAliveTime：\n\n线程保持活动时间，线程池工作线程空闲后，保持存活的时间，所以如果任务很多，并且每个任务执行时间较短，可以调大时间，提高线程利用率。\n\n### 4.unit：\n\n线程保持活动时间单位，天(DAYS)、小时(HOURS)、分钟(MINUTES、毫秒MILLISECONDS).微秒(MICROSECONDS)、纳秒(NANOSECONDS)\n\n### 5.workQueue：\n\n任务队列，保存等待执行的任务的阻塞队列。一般来说可以选择如下阻塞队列：\n\n   (1) ArrayBlockingQueue:基于数组的有界阻塞队列。\n\n​    (2)LinkedBlockingQueue:基于链表的阻塞队列。\n\n​    (3)SynchronizedQueue:一个不存储元素的阻塞队列。\n\n​    (4)PriorityBlockingQueue:一个具有优先级的阻塞队列。\n\n### 6.threadFactory：\n\n设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。\n\n### 7.handler：\n\n饱和策略也叫拒绝策略。当队列和线程池都满了，即达到饱和状态。所以需要采取策略来处理新的任务。默认策略是AbortPblicy\n\n (1)AbortPolicy:直接抛出异常。\n\n (2)CallerRunsPolicy:调用者所在的线程来运行任务。\n\n (3)DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。\n\n (4)DiscardPolicy:不处理，直接丢掉。\n\n当然可以根据自己的应用场景，实现RejectedExecutionHandler接口自定义策略。\n\n\n\n## 5、向线程池提交任务\n\n可以使用execute()和submit()两种方式提交任务。\n\nexecute():无返回值，所以无法判断任务是否被执行成功。\n\nsubmit(:用于提交需要有返回值的任务。线程池返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()来获取返回值，get()方法会阻塞当前线程知道任务完成。get(long timeout,TimeUnit unit)可以设置超时时间。\n\n## 6、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n可以通过shutdown()或shutdownNow()来关闭线程池。\n\n它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt来中断线程，所以无法响应终端的任务可以能永远无法停止\n\nshutdownNow首先将线程池状态设置成STOP;然后尝试停止所有的正在执行或者暂停的线程，并返回等待执行任务的列表。\n\nshutdown只是将线程池的状态设置成shutdown状态，然后中断所有没有正在执行任务的线程。\n\n只要调用两者之一，isShutdown就会返回true,当所有任务都已关闭，isTerminaed就会返回true。一般来说调用shutdown方法来关闭线程池，如果任务不一定要执行完，可以直接调用shutdownNow方法。\n\n## 7、线程池如何配置合理\n\n配置线程池可以从以下几个方面考虑。\n\n任务是cpu密集型、IO密集型或者混合型·任务优先级，高中低。\n\n任务时间执行长短。\n\n任务依赖性:是否依赖其他系统资源。\n\ncpu密集型可以配置可能小的线程,比如n+1个线程。io密集型可以配置较多的线程，如2n个线程。\n\n混合型可以拆成io密集型任务和cpu密集型任务，\n\n如果两个任务执行时间相差大，否->分解后执行吞吐量将高于串行执行吞吐量。否->没必要分解。\n\n可以通过Runtime.getRuntime().availableProcessors()来获取cpu个数。建议使用有界队列，增加系统的预警能力和稳定性。\n\n## 8、JDK线程示例\n\n### （0）FixedThreadPool\n\n可重用固定线程数的线程池。查看源码：\n\n```java\npublic static ExecutorService newFixedThreadPool(int nThreads) {\n    return new ThreadPoolExecutor(nThreads, nThreads,\n                                  0L, TimeUnit.MILLISECONDS,\n                                  new LinkedBlockingQueue<Runnable>());\n}\n```\n\ncorePoolSize和maxPoolSize都被设置成我们设置的nThreads。\n当线程池中的线程数大于corePoolSize ,keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止，如果设为0，表示多余的空闲线程会立即终止。\n工作流程:\n\n1.当前线程少于corePoolSize,创建新线程执行任务。\n\n2.当前运行线程等于corePoolSize,将任务加入LinkedBlockingQueue。\n\n3.线程执行完1中的任务，会循环反复从LinkedBlockingQueue获取任务来执行。LinkedBlockingQueue作为线程池工作队列(默认容量Integer.MAX_VALUE)。因此可能会造成如下。\n\n\n\n1.当线程数等于corePoolSize时，新任务将在队列中等待，因为线程池中的线程不会超过corePoolSize。\n\n2.maxnumPoolSize等于说是一个无效参数。\n\n3.keepAliveTime等于说也是一个无效参数。\n\n4.运行中的FixedThreadPool(未执行shundown或shundownNow)则不会调用拒绝策略。\n\n5.由于任务可以不停的加到队列，当任务越来越多时很容易造成OOM。\n\n### （1）SingleThreadExecutor\n\n是使用单个worker线程的Executor。查看源码:\n\n```java\n    public static ExecutorService newSingleThreadExecutor() {\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue<Runnable>()));\n    }\n\n    public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) {\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue<Runnable>(),\n                                    threadFactory));\n    }\n```\n\ncorePoolSize和maxnumPoolSize被设置为1。其他参数和FixedThreadPool相同。执行流程以及造成的影响同FixedThreadPool。\n\n### （2）CachedThreadPool\n\n根据需要创建新线程的线程池。查看源码:\n\n```java\npublic static ExecutorService newCachedThreadPool() {\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue<Runnable>());\n    }    \npublic static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) {\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue<Runnable>(),\n                                      threadFactory);\n    }\n```\n\ncorePoolSize设置为0,maxmumPoolSize为Integer.MAX_VALUE。keepAliveTime为60秒。工作流程:\n\n1.首先执行SynchronousQueue.offer (Runnable task)。如果当前maximumPool中有空闲线程正在执行SynchronousQueue.pol(keepAliveTIme,TimeUnit.NANOSECONDS)，那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute方法执行完成；否则执行下面的步骤2。\n\n2.当初始maximumPool为空或者maximumPool中当前没有空闲线程时，将没有线程执行SynchronousQueue.poll (keepAliveTime, TimeUnit.NANOSECONDS)。这种情况下，步骤1将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。\n\n3.在步骤2中新创建的线程将任务执行完后，会执行SynchronousQueue.poll (keepAliveTime,TimeUnit.NANOSECONDS)。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务(主线程执行步骤1)，那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。\n\n","source":"_posts/池化之线程池.md","raw":"title: 池化之线程池\nauthor: 郑天祺\ntags:\n  - java\n  - 多线程\ncategories:\n  - java基础\ndate: 2019-09-01 10:14:00\n\n---\n\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池好处\n\n几乎所有需要异步或者并发执行任务的程序都可以使用线程池。\n\n合理使用会给我们带来以下好处。\n\n降低系统消耗：重复利用已经创建的线程降低线程创建和销毁造成的资源消耗。\n\n提高响应速度：当任务到达时，任务不需要等到线程创建就可以立即执行。\n\n提供线程可以管理性：可以通过设置合理分配、调优、监控。\n\n## 3、线程池工作流程\n\n1、判断核心线程池里的线程是否都有在执行任务，否->创建一个新工作线程来执行任务。是->走下个流程。\n\n2、判断工作队列是否已满，否->新任务存储在这个工作队列里，是->走下个流程。\n\n3、判断线程池里的线程是否都在工作状态，否->创建一个新的工作线程来执行任务，是->走下个流程。\n\n4、按照设置的策略来处理无法执行的任务。\n\n\n\n## 4、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n### 1.corePoolSize：\n\n核心线程池大小，当提交一个任务时，线程池会创建一个线程来执行任务，即使其他空闲的核心线程能够执行新任务也会创建，等待需要执行的任务数大于线程核心大小就不会继续创建。\n\n### 2.maximumPoolSize：\n\n线程池最大数，允许创建的最大线程数，如果队列满了，并且已经创建的线程数小于最大线程数，则会创建新的线程执行任务。如果是无界队列，这个参数基本没用。\n\n### 3.keepAliveTime：\n\n线程保持活动时间，线程池工作线程空闲后，保持存活的时间，所以如果任务很多，并且每个任务执行时间较短，可以调大时间，提高线程利用率。\n\n### 4.unit：\n\n线程保持活动时间单位，天(DAYS)、小时(HOURS)、分钟(MINUTES、毫秒MILLISECONDS).微秒(MICROSECONDS)、纳秒(NANOSECONDS)\n\n### 5.workQueue：\n\n任务队列，保存等待执行的任务的阻塞队列。一般来说可以选择如下阻塞队列：\n\n   (1) ArrayBlockingQueue:基于数组的有界阻塞队列。\n\n​    (2)LinkedBlockingQueue:基于链表的阻塞队列。\n\n​    (3)SynchronizedQueue:一个不存储元素的阻塞队列。\n\n​    (4)PriorityBlockingQueue:一个具有优先级的阻塞队列。\n\n### 6.threadFactory：\n\n设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。\n\n### 7.handler：\n\n饱和策略也叫拒绝策略。当队列和线程池都满了，即达到饱和状态。所以需要采取策略来处理新的任务。默认策略是AbortPblicy\n\n (1)AbortPolicy:直接抛出异常。\n\n (2)CallerRunsPolicy:调用者所在的线程来运行任务。\n\n (3)DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。\n\n (4)DiscardPolicy:不处理，直接丢掉。\n\n当然可以根据自己的应用场景，实现RejectedExecutionHandler接口自定义策略。\n\n\n\n## 5、向线程池提交任务\n\n可以使用execute()和submit()两种方式提交任务。\n\nexecute():无返回值，所以无法判断任务是否被执行成功。\n\nsubmit(:用于提交需要有返回值的任务。线程池返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()来获取返回值，get()方法会阻塞当前线程知道任务完成。get(long timeout,TimeUnit unit)可以设置超时时间。\n\n## 6、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n可以通过shutdown()或shutdownNow()来关闭线程池。\n\n它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt来中断线程，所以无法响应终端的任务可以能永远无法停止\n\nshutdownNow首先将线程池状态设置成STOP;然后尝试停止所有的正在执行或者暂停的线程，并返回等待执行任务的列表。\n\nshutdown只是将线程池的状态设置成shutdown状态，然后中断所有没有正在执行任务的线程。\n\n只要调用两者之一，isShutdown就会返回true,当所有任务都已关闭，isTerminaed就会返回true。一般来说调用shutdown方法来关闭线程池，如果任务不一定要执行完，可以直接调用shutdownNow方法。\n\n## 7、线程池如何配置合理\n\n配置线程池可以从以下几个方面考虑。\n\n任务是cpu密集型、IO密集型或者混合型·任务优先级，高中低。\n\n任务时间执行长短。\n\n任务依赖性:是否依赖其他系统资源。\n\ncpu密集型可以配置可能小的线程,比如n+1个线程。io密集型可以配置较多的线程，如2n个线程。\n\n混合型可以拆成io密集型任务和cpu密集型任务，\n\n如果两个任务执行时间相差大，否->分解后执行吞吐量将高于串行执行吞吐量。否->没必要分解。\n\n可以通过Runtime.getRuntime().availableProcessors()来获取cpu个数。建议使用有界队列，增加系统的预警能力和稳定性。\n\n## 8、JDK线程示例\n\n### （0）FixedThreadPool\n\n可重用固定线程数的线程池。查看源码：\n\n```java\npublic static ExecutorService newFixedThreadPool(int nThreads) {\n    return new ThreadPoolExecutor(nThreads, nThreads,\n                                  0L, TimeUnit.MILLISECONDS,\n                                  new LinkedBlockingQueue<Runnable>());\n}\n```\n\ncorePoolSize和maxPoolSize都被设置成我们设置的nThreads。\n当线程池中的线程数大于corePoolSize ,keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止，如果设为0，表示多余的空闲线程会立即终止。\n工作流程:\n\n1.当前线程少于corePoolSize,创建新线程执行任务。\n\n2.当前运行线程等于corePoolSize,将任务加入LinkedBlockingQueue。\n\n3.线程执行完1中的任务，会循环反复从LinkedBlockingQueue获取任务来执行。LinkedBlockingQueue作为线程池工作队列(默认容量Integer.MAX_VALUE)。因此可能会造成如下。\n\n\n\n1.当线程数等于corePoolSize时，新任务将在队列中等待，因为线程池中的线程不会超过corePoolSize。\n\n2.maxnumPoolSize等于说是一个无效参数。\n\n3.keepAliveTime等于说也是一个无效参数。\n\n4.运行中的FixedThreadPool(未执行shundown或shundownNow)则不会调用拒绝策略。\n\n5.由于任务可以不停的加到队列，当任务越来越多时很容易造成OOM。\n\n### （1）SingleThreadExecutor\n\n是使用单个worker线程的Executor。查看源码:\n\n```java\n    public static ExecutorService newSingleThreadExecutor() {\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue<Runnable>()));\n    }\n\n    public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) {\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue<Runnable>(),\n                                    threadFactory));\n    }\n```\n\ncorePoolSize和maxnumPoolSize被设置为1。其他参数和FixedThreadPool相同。执行流程以及造成的影响同FixedThreadPool。\n\n### （2）CachedThreadPool\n\n根据需要创建新线程的线程池。查看源码:\n\n```java\npublic static ExecutorService newCachedThreadPool() {\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue<Runnable>());\n    }    \npublic static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) {\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue<Runnable>(),\n                                      threadFactory);\n    }\n```\n\ncorePoolSize设置为0,maxmumPoolSize为Integer.MAX_VALUE。keepAliveTime为60秒。工作流程:\n\n1.首先执行SynchronousQueue.offer (Runnable task)。如果当前maximumPool中有空闲线程正在执行SynchronousQueue.pol(keepAliveTIme,TimeUnit.NANOSECONDS)，那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute方法执行完成；否则执行下面的步骤2。\n\n2.当初始maximumPool为空或者maximumPool中当前没有空闲线程时，将没有线程执行SynchronousQueue.poll (keepAliveTime, TimeUnit.NANOSECONDS)。这种情况下，步骤1将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。\n\n3.在步骤2中新创建的线程将任务执行完后，会执行SynchronousQueue.poll (keepAliveTime,TimeUnit.NANOSECONDS)。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务(主线程执行步骤1)，那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。\n\n","slug":"池化之线程池","published":1,"updated":"2022-04-08T14:45:32.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1700a57kt99xgw4uaa","content":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2 id=\"1、池化技术\">1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2 id=\"2、线程池好处\">2、线程池好处</h2>\n<p>几乎所有需要异步或者并发执行任务的程序都可以使用线程池。</p>\n<p>合理使用会给我们带来以下好处。</p>\n<p>降低系统消耗：重复利用已经创建的线程降低线程创建和销毁造成的资源消耗。</p>\n<p>提高响应速度：当任务到达时，任务不需要等到线程创建就可以立即执行。</p>\n<p>提供线程可以管理性：可以通过设置合理分配、调优、监控。</p>\n<h2 id=\"3、线程池工作流程\">3、线程池工作流程</h2>\n<p>1、判断核心线程池里的线程是否都有在执行任务，否-&gt;创建一个新工作线程来执行任务。是-&gt;走下个流程。</p>\n<p>2、判断工作队列是否已满，否-&gt;新任务存储在这个工作队列里，是-&gt;走下个流程。</p>\n<p>3、判断线程池里的线程是否都在工作状态，否-&gt;创建一个新的工作线程来执行任务，是-&gt;走下个流程。</p>\n<p>4、按照设置的策略来处理无法执行的任务。</p>\n<h2 id=\"4、线程池的创建\">4、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h3 id=\"1-corePoolSize：\">1.corePoolSize：</h3>\n<p>核心线程池大小，当提交一个任务时，线程池会创建一个线程来执行任务，即使其他空闲的核心线程能够执行新任务也会创建，等待需要执行的任务数大于线程核心大小就不会继续创建。</p>\n<h3 id=\"2-maximumPoolSize：\">2.maximumPoolSize：</h3>\n<p>线程池最大数，允许创建的最大线程数，如果队列满了，并且已经创建的线程数小于最大线程数，则会创建新的线程执行任务。如果是无界队列，这个参数基本没用。</p>\n<h3 id=\"3-keepAliveTime：\">3.keepAliveTime：</h3>\n<p>线程保持活动时间，线程池工作线程空闲后，保持存活的时间，所以如果任务很多，并且每个任务执行时间较短，可以调大时间，提高线程利用率。</p>\n<h3 id=\"4-unit：\">4.unit：</h3>\n<p>线程保持活动时间单位，天(DAYS)、小时(HOURS)、分钟(MINUTES、毫秒MILLISECONDS).微秒(MICROSECONDS)、纳秒(NANOSECONDS)</p>\n<h3 id=\"5-workQueue：\">5.workQueue：</h3>\n<p>任务队列，保存等待执行的任务的阻塞队列。一般来说可以选择如下阻塞队列：</p>\n<p>(1) ArrayBlockingQueue:基于数组的有界阻塞队列。</p>\n<p>​    (2)LinkedBlockingQueue:基于链表的阻塞队列。</p>\n<p>​    (3)SynchronizedQueue:一个不存储元素的阻塞队列。</p>\n<p>​    (4)PriorityBlockingQueue:一个具有优先级的阻塞队列。</p>\n<h3 id=\"6-threadFactory：\">6.threadFactory：</h3>\n<p>设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。</p>\n<h3 id=\"7-handler：\">7.handler：</h3>\n<p>饱和策略也叫拒绝策略。当队列和线程池都满了，即达到饱和状态。所以需要采取策略来处理新的任务。默认策略是AbortPblicy</p>\n<p>(1)AbortPolicy:直接抛出异常。</p>\n<p>(2)CallerRunsPolicy:调用者所在的线程来运行任务。</p>\n<p>(3)DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。</p>\n<p>(4)DiscardPolicy:不处理，直接丢掉。</p>\n<p>当然可以根据自己的应用场景，实现RejectedExecutionHandler接口自定义策略。</p>\n<h2 id=\"5、向线程池提交任务\">5、向线程池提交任务</h2>\n<p>可以使用execute()和submit()两种方式提交任务。</p>\n<p>execute():无返回值，所以无法判断任务是否被执行成功。</p>\n<p>submit(:用于提交需要有返回值的任务。线程池返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()来获取返回值，get()方法会阻塞当前线程知道任务完成。get(long timeout,TimeUnit unit)可以设置超时时间。</p>\n<h2 id=\"6、线程池的关闭\">6、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n<p>可以通过shutdown()或shutdownNow()来关闭线程池。</p>\n<p>它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt来中断线程，所以无法响应终端的任务可以能永远无法停止</p>\n<p>shutdownNow首先将线程池状态设置成STOP;然后尝试停止所有的正在执行或者暂停的线程，并返回等待执行任务的列表。</p>\n<p>shutdown只是将线程池的状态设置成shutdown状态，然后中断所有没有正在执行任务的线程。</p>\n<p>只要调用两者之一，isShutdown就会返回true,当所有任务都已关闭，isTerminaed就会返回true。一般来说调用shutdown方法来关闭线程池，如果任务不一定要执行完，可以直接调用shutdownNow方法。</p>\n<h2 id=\"7、线程池如何配置合理\">7、线程池如何配置合理</h2>\n<p>配置线程池可以从以下几个方面考虑。</p>\n<p>任务是cpu密集型、IO密集型或者混合型·任务优先级，高中低。</p>\n<p>任务时间执行长短。</p>\n<p>任务依赖性:是否依赖其他系统资源。</p>\n<p>cpu密集型可以配置可能小的线程,比如n+1个线程。io密集型可以配置较多的线程，如2n个线程。</p>\n<p>混合型可以拆成io密集型任务和cpu密集型任务，</p>\n<p>如果两个任务执行时间相差大，否-&gt;分解后执行吞吐量将高于串行执行吞吐量。否-&gt;没必要分解。</p>\n<p>可以通过Runtime.getRuntime().availableProcessors()来获取cpu个数。建议使用有界队列，增加系统的预警能力和稳定性。</p>\n<h2 id=\"8、JDK线程示例\">8、JDK线程示例</h2>\n<h3 id=\"（0）FixedThreadPool\">（0）FixedThreadPool</h3>\n<p>可重用固定线程数的线程池。查看源码：</p>\n<pre><code class=\"language-java\">public static ExecutorService newFixedThreadPool(int nThreads) &#123;\n    return new ThreadPoolExecutor(nThreads, nThreads,\n                                  0L, TimeUnit.MILLISECONDS,\n                                  new LinkedBlockingQueue&lt;Runnable&gt;());\n&#125;\n</code></pre>\n<p>corePoolSize和maxPoolSize都被设置成我们设置的nThreads。<br>\n当线程池中的线程数大于corePoolSize ,keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止，如果设为0，表示多余的空闲线程会立即终止。<br>\n工作流程:</p>\n<p>1.当前线程少于corePoolSize,创建新线程执行任务。</p>\n<p>2.当前运行线程等于corePoolSize,将任务加入LinkedBlockingQueue。</p>\n<p>3.线程执行完1中的任务，会循环反复从LinkedBlockingQueue获取任务来执行。LinkedBlockingQueue作为线程池工作队列(默认容量Integer.MAX_VALUE)。因此可能会造成如下。</p>\n<p>1.当线程数等于corePoolSize时，新任务将在队列中等待，因为线程池中的线程不会超过corePoolSize。</p>\n<p>2.maxnumPoolSize等于说是一个无效参数。</p>\n<p>3.keepAliveTime等于说也是一个无效参数。</p>\n<p>4.运行中的FixedThreadPool(未执行shundown或shundownNow)则不会调用拒绝策略。</p>\n<p>5.由于任务可以不停的加到队列，当任务越来越多时很容易造成OOM。</p>\n<h3 id=\"（1）SingleThreadExecutor\">（1）SingleThreadExecutor</h3>\n<p>是使用单个worker线程的Executor。查看源码:</p>\n<pre><code class=\"language-java\">    public static ExecutorService newSingleThreadExecutor() &#123;\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue&lt;Runnable&gt;()));\n    &#125;\n\n    public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123;\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue&lt;Runnable&gt;(),\n                                    threadFactory));\n    &#125;\n</code></pre>\n<p>corePoolSize和maxnumPoolSize被设置为1。其他参数和FixedThreadPool相同。执行流程以及造成的影响同FixedThreadPool。</p>\n<h3 id=\"（2）CachedThreadPool\">（2）CachedThreadPool</h3>\n<p>根据需要创建新线程的线程池。查看源码:</p>\n<pre><code class=\"language-java\">public static ExecutorService newCachedThreadPool() &#123;\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue&lt;Runnable&gt;());\n    &#125;    \npublic static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123;\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue&lt;Runnable&gt;(),\n                                      threadFactory);\n    &#125;\n</code></pre>\n<p>corePoolSize设置为0,maxmumPoolSize为Integer.MAX_VALUE。keepAliveTime为60秒。工作流程:</p>\n<p>1.首先执行SynchronousQueue.offer (Runnable task)。如果当前maximumPool中有空闲线程正在执行SynchronousQueue.pol(keepAliveTIme,TimeUnit.NANOSECONDS)，那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute方法执行完成；否则执行下面的步骤2。</p>\n<p>2.当初始maximumPool为空或者maximumPool中当前没有空闲线程时，将没有线程执行SynchronousQueue.poll (keepAliveTime, TimeUnit.NANOSECONDS)。这种情况下，步骤1将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。</p>\n<p>3.在步骤2中新创建的线程将任务执行完后，会执行SynchronousQueue.poll (keepAliveTime,TimeUnit.NANOSECONDS)。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务(主线程执行步骤1)，那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2 id=\"1、池化技术\">1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2 id=\"2、线程池好处\">2、线程池好处</h2>\n<p>几乎所有需要异步或者并发执行任务的程序都可以使用线程池。</p>\n<p>合理使用会给我们带来以下好处。</p>\n<p>降低系统消耗：重复利用已经创建的线程降低线程创建和销毁造成的资源消耗。</p>\n<p>提高响应速度：当任务到达时，任务不需要等到线程创建就可以立即执行。</p>\n<p>提供线程可以管理性：可以通过设置合理分配、调优、监控。</p>\n<h2 id=\"3、线程池工作流程\">3、线程池工作流程</h2>\n<p>1、判断核心线程池里的线程是否都有在执行任务，否-&gt;创建一个新工作线程来执行任务。是-&gt;走下个流程。</p>\n<p>2、判断工作队列是否已满，否-&gt;新任务存储在这个工作队列里，是-&gt;走下个流程。</p>\n<p>3、判断线程池里的线程是否都在工作状态，否-&gt;创建一个新的工作线程来执行任务，是-&gt;走下个流程。</p>\n<p>4、按照设置的策略来处理无法执行的任务。</p>\n<h2 id=\"4、线程池的创建\">4、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h3 id=\"1-corePoolSize：\">1.corePoolSize：</h3>\n<p>核心线程池大小，当提交一个任务时，线程池会创建一个线程来执行任务，即使其他空闲的核心线程能够执行新任务也会创建，等待需要执行的任务数大于线程核心大小就不会继续创建。</p>\n<h3 id=\"2-maximumPoolSize：\">2.maximumPoolSize：</h3>\n<p>线程池最大数，允许创建的最大线程数，如果队列满了，并且已经创建的线程数小于最大线程数，则会创建新的线程执行任务。如果是无界队列，这个参数基本没用。</p>\n<h3 id=\"3-keepAliveTime：\">3.keepAliveTime：</h3>\n<p>线程保持活动时间，线程池工作线程空闲后，保持存活的时间，所以如果任务很多，并且每个任务执行时间较短，可以调大时间，提高线程利用率。</p>\n<h3 id=\"4-unit：\">4.unit：</h3>\n<p>线程保持活动时间单位，天(DAYS)、小时(HOURS)、分钟(MINUTES、毫秒MILLISECONDS).微秒(MICROSECONDS)、纳秒(NANOSECONDS)</p>\n<h3 id=\"5-workQueue：\">5.workQueue：</h3>\n<p>任务队列，保存等待执行的任务的阻塞队列。一般来说可以选择如下阻塞队列：</p>\n<p>(1) ArrayBlockingQueue:基于数组的有界阻塞队列。</p>\n<p>​    (2)LinkedBlockingQueue:基于链表的阻塞队列。</p>\n<p>​    (3)SynchronizedQueue:一个不存储元素的阻塞队列。</p>\n<p>​    (4)PriorityBlockingQueue:一个具有优先级的阻塞队列。</p>\n<h3 id=\"6-threadFactory：\">6.threadFactory：</h3>\n<p>设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。</p>\n<h3 id=\"7-handler：\">7.handler：</h3>\n<p>饱和策略也叫拒绝策略。当队列和线程池都满了，即达到饱和状态。所以需要采取策略来处理新的任务。默认策略是AbortPblicy</p>\n<p>(1)AbortPolicy:直接抛出异常。</p>\n<p>(2)CallerRunsPolicy:调用者所在的线程来运行任务。</p>\n<p>(3)DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。</p>\n<p>(4)DiscardPolicy:不处理，直接丢掉。</p>\n<p>当然可以根据自己的应用场景，实现RejectedExecutionHandler接口自定义策略。</p>\n<h2 id=\"5、向线程池提交任务\">5、向线程池提交任务</h2>\n<p>可以使用execute()和submit()两种方式提交任务。</p>\n<p>execute():无返回值，所以无法判断任务是否被执行成功。</p>\n<p>submit(:用于提交需要有返回值的任务。线程池返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()来获取返回值，get()方法会阻塞当前线程知道任务完成。get(long timeout,TimeUnit unit)可以设置超时时间。</p>\n<h2 id=\"6、线程池的关闭\">6、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n<p>可以通过shutdown()或shutdownNow()来关闭线程池。</p>\n<p>它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt来中断线程，所以无法响应终端的任务可以能永远无法停止</p>\n<p>shutdownNow首先将线程池状态设置成STOP;然后尝试停止所有的正在执行或者暂停的线程，并返回等待执行任务的列表。</p>\n<p>shutdown只是将线程池的状态设置成shutdown状态，然后中断所有没有正在执行任务的线程。</p>\n<p>只要调用两者之一，isShutdown就会返回true,当所有任务都已关闭，isTerminaed就会返回true。一般来说调用shutdown方法来关闭线程池，如果任务不一定要执行完，可以直接调用shutdownNow方法。</p>\n<h2 id=\"7、线程池如何配置合理\">7、线程池如何配置合理</h2>\n<p>配置线程池可以从以下几个方面考虑。</p>\n<p>任务是cpu密集型、IO密集型或者混合型·任务优先级，高中低。</p>\n<p>任务时间执行长短。</p>\n<p>任务依赖性:是否依赖其他系统资源。</p>\n<p>cpu密集型可以配置可能小的线程,比如n+1个线程。io密集型可以配置较多的线程，如2n个线程。</p>\n<p>混合型可以拆成io密集型任务和cpu密集型任务，</p>\n<p>如果两个任务执行时间相差大，否-&gt;分解后执行吞吐量将高于串行执行吞吐量。否-&gt;没必要分解。</p>\n<p>可以通过Runtime.getRuntime().availableProcessors()来获取cpu个数。建议使用有界队列，增加系统的预警能力和稳定性。</p>\n<h2 id=\"8、JDK线程示例\">8、JDK线程示例</h2>\n<h3 id=\"（0）FixedThreadPool\">（0）FixedThreadPool</h3>\n<p>可重用固定线程数的线程池。查看源码：</p>\n<pre><code class=\"language-java\">public static ExecutorService newFixedThreadPool(int nThreads) &#123;\n    return new ThreadPoolExecutor(nThreads, nThreads,\n                                  0L, TimeUnit.MILLISECONDS,\n                                  new LinkedBlockingQueue&lt;Runnable&gt;());\n&#125;\n</code></pre>\n<p>corePoolSize和maxPoolSize都被设置成我们设置的nThreads。<br>\n当线程池中的线程数大于corePoolSize ,keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止，如果设为0，表示多余的空闲线程会立即终止。<br>\n工作流程:</p>\n<p>1.当前线程少于corePoolSize,创建新线程执行任务。</p>\n<p>2.当前运行线程等于corePoolSize,将任务加入LinkedBlockingQueue。</p>\n<p>3.线程执行完1中的任务，会循环反复从LinkedBlockingQueue获取任务来执行。LinkedBlockingQueue作为线程池工作队列(默认容量Integer.MAX_VALUE)。因此可能会造成如下。</p>\n<p>1.当线程数等于corePoolSize时，新任务将在队列中等待，因为线程池中的线程不会超过corePoolSize。</p>\n<p>2.maxnumPoolSize等于说是一个无效参数。</p>\n<p>3.keepAliveTime等于说也是一个无效参数。</p>\n<p>4.运行中的FixedThreadPool(未执行shundown或shundownNow)则不会调用拒绝策略。</p>\n<p>5.由于任务可以不停的加到队列，当任务越来越多时很容易造成OOM。</p>\n<h3 id=\"（1）SingleThreadExecutor\">（1）SingleThreadExecutor</h3>\n<p>是使用单个worker线程的Executor。查看源码:</p>\n<pre><code class=\"language-java\">    public static ExecutorService newSingleThreadExecutor() &#123;\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue&lt;Runnable&gt;()));\n    &#125;\n\n    public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123;\n        return new FinalizableDelegatedExecutorService\n            (new ThreadPoolExecutor(1, 1,\n                                    0L, TimeUnit.MILLISECONDS,\n                                    new LinkedBlockingQueue&lt;Runnable&gt;(),\n                                    threadFactory));\n    &#125;\n</code></pre>\n<p>corePoolSize和maxnumPoolSize被设置为1。其他参数和FixedThreadPool相同。执行流程以及造成的影响同FixedThreadPool。</p>\n<h3 id=\"（2）CachedThreadPool\">（2）CachedThreadPool</h3>\n<p>根据需要创建新线程的线程池。查看源码:</p>\n<pre><code class=\"language-java\">public static ExecutorService newCachedThreadPool() &#123;\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue&lt;Runnable&gt;());\n    &#125;    \npublic static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123;\n        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,\n                                      60L, TimeUnit.SECONDS,\n                                      new SynchronousQueue&lt;Runnable&gt;(),\n                                      threadFactory);\n    &#125;\n</code></pre>\n<p>corePoolSize设置为0,maxmumPoolSize为Integer.MAX_VALUE。keepAliveTime为60秒。工作流程:</p>\n<p>1.首先执行SynchronousQueue.offer (Runnable task)。如果当前maximumPool中有空闲线程正在执行SynchronousQueue.pol(keepAliveTIme,TimeUnit.NANOSECONDS)，那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute方法执行完成；否则执行下面的步骤2。</p>\n<p>2.当初始maximumPool为空或者maximumPool中当前没有空闲线程时，将没有线程执行SynchronousQueue.poll (keepAliveTime, TimeUnit.NANOSECONDS)。这种情况下，步骤1将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。</p>\n<p>3.在步骤2中新创建的线程将任务执行完后，会执行SynchronousQueue.poll (keepAliveTime,TimeUnit.NANOSECONDS)。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务(主线程执行步骤1)，那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。</p>\n"},{"title":"用户行为特征提取","author":"郑天祺","date":"2019-12-23T03:10:00.000Z","_content":"\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","source":"_posts/特征提取-简单流程.md","raw":"title: 用户行为特征提取\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-23 11:10:00\n---\n\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","slug":"特征提取-简单流程","published":1,"updated":"2022-04-04T08:32:40.177Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1800a87kt96ombgy8m","content":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2 id=\"（1）、时延\">（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2 id=\"（2）、吞吐量\">（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2 id=\"（3）、应用：\">（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2 id=\"（1）统计结果\">（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2 id=\"（2）分析项目的目的\">（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2 id=\"（1）数据源-与-统计指标（KPI）分析\">（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2 id=\"（2）数据源-与-统计指标（KPI）的关系\">（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2 id=\"（1）流程设计\">（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2 id=\"（2）统计指标设计\">（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况：<a href=\"https://blog.csdn.net/weixin_35353187/article/details/84303518\">https://blog.csdn.net/weixin_35353187/article/details/84303518</a></p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2 id=\"（1）、时延\">（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2 id=\"（2）、吞吐量\">（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2 id=\"（3）、应用：\">（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2 id=\"（1）统计结果\">（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2 id=\"（2）分析项目的目的\">（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2 id=\"（1）数据源-与-统计指标（KPI）分析\">（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2 id=\"（2）数据源-与-统计指标（KPI）的关系\">（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2 id=\"（1）流程设计\">（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2 id=\"（2）统计指标设计\">（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况：<a href=\"https://blog.csdn.net/weixin_35353187/article/details/84303518\">https://blog.csdn.net/weixin_35353187/article/details/84303518</a></p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n"},{"title":"理解IO阻塞与非阻塞","author":"郑天祺","date":"2019-08-30T09:34:00.000Z","_content":"\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll、poll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n### （1）select 时间复杂度O(n)\n\n#### 过程 \n\n（1）从用户空间拷贝fd_set到内核空间\n\n（2）注册回调函数\n\n（3）遍历所有fd，调用其对应的poll方法\n\n（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n\n（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列\n\n（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值\n\n（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n\n（8）把fd_set从内核空间拷贝到用户空间。\n\n#### 总结\n\n​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。\n\n​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。\n\n​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （2）poll 时间复杂度O(n)\n\n#### 过程\n\n```java\nwhile true  \n{  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    {  \n        if i has data  \n        read until unavailable  \n    }  \n} \n```\n\n#### 总结\n\n​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （3）epoll 时间复杂度O(1)\n\n#### 过程\n\n```java\n{  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    {  \n        read or write till  \n    }  \n}  \n```\n\n\n\n#### 总结\n\n​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。\n\n​\t\t内核和用户空间共享一块内存来实现的\n\n​\t优点：\n\n​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）\n\n​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。\n\n​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。\n\n#### 总结：\n\n​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。\n\n## 8、多路复用的好处\n\n​\t\tselect，poll，epoll都是IO多路复用的机制。\n\n​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。\n\n​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销\n\n​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。","source":"_posts/理解IO阻塞与非阻塞.md","raw":"title: 理解IO阻塞与非阻塞\nauthor: 郑天祺\ntags:\n  - IO\n  - 阻塞与非阻塞\ncategories:\n  - 网络\ndate: 2019-08-30 17:34:00\n\n---\n\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll、poll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n### （1）select 时间复杂度O(n)\n\n#### 过程 \n\n（1）从用户空间拷贝fd_set到内核空间\n\n（2）注册回调函数\n\n（3）遍历所有fd，调用其对应的poll方法\n\n（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n\n（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列\n\n（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值\n\n（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n\n（8）把fd_set从内核空间拷贝到用户空间。\n\n#### 总结\n\n​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。\n\n​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。\n\n​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （2）poll 时间复杂度O(n)\n\n#### 过程\n\n```java\nwhile true  \n{  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    {  \n        if i has data  \n        read until unavailable  \n    }  \n} \n```\n\n#### 总结\n\n​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （3）epoll 时间复杂度O(1)\n\n#### 过程\n\n```java\n{  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    {  \n        read or write till  \n    }  \n}  \n```\n\n\n\n#### 总结\n\n​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。\n\n​\t\t内核和用户空间共享一块内存来实现的\n\n​\t优点：\n\n​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）\n\n​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。\n\n​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。\n\n#### 总结：\n\n​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。\n\n## 8、多路复用的好处\n\n​\t\tselect，poll，epoll都是IO多路复用的机制。\n\n​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。\n\n​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销\n\n​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。","slug":"理解IO阻塞与非阻塞","published":1,"updated":"2022-04-04T08:32:40.177Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1900ac7kt9ckli24fx","content":"<h2 id=\"1、饭店吃饭的例子\">1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 – 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 – 【非阻塞I/O 】</p>\n<h2 id=\"2、阻塞与非阻塞调用对比\">2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h2 id=\"3、阻塞IO\">3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"4、非阻塞IO\">4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"5、I-O复用模型\">5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2 id=\"6、I-O复用select模型\">6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"7、select、epoll、poll模型对比\">7、select、epoll、poll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h3 id=\"（1）select-时间复杂度O-n\">（1）select 时间复杂度O(n)</h3>\n<h4 id=\"过程\">过程</h4>\n<p>（1）从用户空间拷贝fd_set到内核空间</p>\n<p>（2）注册回调函数</p>\n<p>（3）遍历所有fd，调用其对应的poll方法</p>\n<p>（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。</p>\n<p>（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列</p>\n<p>（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值</p>\n<p>（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。</p>\n<p>（8）把fd_set从内核空间拷贝到用户空间。</p>\n<h4 id=\"总结\">总结</h4>\n<p>​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。</p>\n<p>​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。</p>\n<p>​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（2）poll-时间复杂度O-n\">（2）poll 时间复杂度O(n)</h3>\n<h4 id=\"过程-2\">过程</h4>\n<pre><code class=\"language-java\">while true  \n&#123;  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    &#123;  \n        if i has data  \n        read until unavailable  \n    &#125;  \n&#125; \n</code></pre>\n<h4 id=\"总结-2\">总结</h4>\n<p>​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（3）epoll-时间复杂度O-1\">（3）epoll 时间复杂度O(1)</h3>\n<h4 id=\"过程-3\">过程</h4>\n<pre><code class=\"language-java\">&#123;  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    &#123;  \n        read or write till  \n    &#125;  \n&#125;  \n</code></pre>\n<h4 id=\"总结-3\">总结</h4>\n<p>​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。</p>\n<p>​\t\t内核和用户空间共享一块内存来实现的</p>\n<p>​\t优点：</p>\n<p>​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）</p>\n<p>​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。</p>\n<p>​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。</p>\n<h4 id=\"总结：\">总结：</h4>\n<p>​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。</p>\n<h2 id=\"8、多路复用的好处\">8、多路复用的好处</h2>\n<p>​\t\tselect，poll，epoll都是IO多路复用的机制。</p>\n<p>​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。</p>\n<p>​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n<p>​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、饭店吃饭的例子\">1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 – 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 – 【非阻塞I/O 】</p>\n<h2 id=\"2、阻塞与非阻塞调用对比\">2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h2 id=\"3、阻塞IO\">3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"4、非阻塞IO\">4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"5、I-O复用模型\">5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2 id=\"6、I-O复用select模型\">6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"7、select、epoll、poll模型对比\">7、select、epoll、poll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h3 id=\"（1）select-时间复杂度O-n\">（1）select 时间复杂度O(n)</h3>\n<h4 id=\"过程\">过程</h4>\n<p>（1）从用户空间拷贝fd_set到内核空间</p>\n<p>（2）注册回调函数</p>\n<p>（3）遍历所有fd，调用其对应的poll方法</p>\n<p>（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。</p>\n<p>（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列</p>\n<p>（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值</p>\n<p>（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。</p>\n<p>（8）把fd_set从内核空间拷贝到用户空间。</p>\n<h4 id=\"总结\">总结</h4>\n<p>​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。</p>\n<p>​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。</p>\n<p>​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（2）poll-时间复杂度O-n\">（2）poll 时间复杂度O(n)</h3>\n<h4 id=\"过程-2\">过程</h4>\n<pre><code class=\"language-java\">while true  \n&#123;  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    &#123;  \n        if i has data  \n        read until unavailable  \n    &#125;  \n&#125; \n</code></pre>\n<h4 id=\"总结-2\">总结</h4>\n<p>​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（3）epoll-时间复杂度O-1\">（3）epoll 时间复杂度O(1)</h3>\n<h4 id=\"过程-3\">过程</h4>\n<pre><code class=\"language-java\">&#123;  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    &#123;  \n        read or write till  \n    &#125;  \n&#125;  \n</code></pre>\n<h4 id=\"总结-3\">总结</h4>\n<p>​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。</p>\n<p>​\t\t内核和用户空间共享一块内存来实现的</p>\n<p>​\t优点：</p>\n<p>​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）</p>\n<p>​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。</p>\n<p>​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。</p>\n<h4 id=\"总结：\">总结：</h4>\n<p>​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。</p>\n<h2 id=\"8、多路复用的好处\">8、多路复用的好处</h2>\n<p>​\t\tselect，poll，epoll都是IO多路复用的机制。</p>\n<p>​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。</p>\n<p>​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n<p>​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。</p>\n"},{"title":"直方图","author":"ztq","date":"2021-04-17T10:27:00.000Z","_content":"\n## 一、边缘直方图\n\n如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。\n\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n\n\n### 1、使用seaborn实现边缘直方图\n\n（1）格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.jointplot)\n```\n\n    Help on function jointplot in module seaborn.axisgrid:\n    \n    jointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n        Draw a plot of two variables with bivariate and univariate graphs.\n        \n        This function provides a convenient interface to the :class:`JointGrid`\n        class, with several canned plot kinds. This is intended to be a fairly\n        lightweight wrapper; if you need more flexibility, you should use\n        :class:`JointGrid` directly.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        kind : { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\" }\n            Kind of plot to draw. See the examples for references to the underlying functions.\n        color : :mod:`matplotlib color <matplotlib.colors>`\n            Single color specification for when hue mapping is not used. Otherwise, the\n            plot will try to hook into the matplotlib property cycle.\n        height : numeric\n            Size of the figure (it will be square).\n        ratio : numeric\n            Ratio of joint axes height to marginal axes height.\n        space : numeric\n            Space between the joint and marginal axes\n        dropna : bool\n            If True, remove observations that are missing from ``x`` and ``y``.\n        {x, y}lim : pairs of numbers\n            Axis limits to set before plotting.\n        marginal_ticks : bool\n            If False, suppress ticks on the count/density axis of the marginal plots.\n        {joint, marginal}_kws : dicts\n            Additional keyword arguments for the plot components.\n        hue : vector or key in ``data``\n            Semantic variable that is mapped to determine the color of plot elements.\n            Semantic variable that is mapped to determine the color of plot elements.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        kwargs\n            Additional keyword arguments are passed to the function used to\n            draw the plot on the joint Axes, superseding items in the\n            ``joint_kws`` dictionary.\n        \n        Returns\n        -------\n        :class:`JointGrid`\n            An object managing multiple subplots that correspond to joint and marginal axes\n            for plotting a bivariate relationship or distribution.\n        \n        See Also\n        --------\n        JointGrid : Set up a figure with joint and marginal views on bivariate data.\n        PairGrid : Set up a figure with joint and marginal views on multiple variables.\n        jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/jointplot.rst\n\n\n​    \n\n（2）举例\n\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n\n\n\n```python\nimport seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(\"tips\")\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\")\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c25dcf10>\n\n\n\n\n![png](/img/output_8_111.png)\n    \n\n\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n\n\n示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线\n\n\n```python\nimport seaborn as sns\n  \ntips = sns.load_dataset(\"tips\")\n  \n# here \"*\" is used as a marker for scatterplot\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"reg\")\n\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295ada536a0>\n\n\n\n\n![png](/img/output_11_111.png)\n    \n\n\nkind的其他取值\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='scatter')\n#默认kind=\"scatter\"\n#sns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex')\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='kde')\n\nplt.show()\n```\n\n\n![png](/img/output_13_011.png)\n    \n\n\n\n\n![png](/img/output_13_111.png)\n    \n\n\n使用marginal_kws修改直方图的样式\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n```\n\n\n![png](/img/output_15_011.png)\n    \n\n\n### 2、使用plotly express实现边缘直方图\n\n(1)plotly express简单介绍及安装\n\nPlotly Express 是一个新的高级 Python 可视化库：它是 Plotly.py 的高级封装，它为复杂的图表提供了一个简单的语法。 \n\n受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 Plotly.py 一样。\n\nPlotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。 \n\n最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！\n\n用 pip install plotly_express 命令可以安装 Plotly Express。\n\n\n(2)格式\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=\"\",marginal_y=\"\"）\n（2）示例\n\n<strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong>\n\n## 二、边缘箱线图\n\n边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数\n\n### 1、使用matplotlib实现边缘箱线图\n\n\n```python\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"Data\\mpg_ggplot2.csv\")\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"Set1\", edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=\"v\")\nsns.boxplot(df.displ, ax=ax_bottom, orient=\"h\")\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n      warnings.warn(single_var_warning.format(\"Vertical\", \"x\"))\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n\n\n\n\n![png](/img/output_26_111.png)\n    \n\n\n### 2、使用seaborn实现边缘箱线图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=\"Data\\palmer_penguin_species.tsv.txt\"\n# penguins_df = pd.read_csv(penguins_data, sep=\"\\t\")\npenguins=sns.load_dataset(\"penguins\")\npenguins.head()\n\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\n# # set plot context to set plot sizes \n# sns.set_context(\"talk\", font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=\"culmen_length_mm\",\n#                   y=\"culmen_depth_mm\")\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(\"Scatterplot_with_marginal_boxplot_Seaborn.png\",\n# #                    format='png',dpi=150)\n# plt.show()\n```\n\n\n```python\nimport seaborn\nseaborn.__version__\n```\n\n\n\n\n    '0.11.0'\n\n\n需要更的到0.11.0版本才可以正常使用\npip install seaborn==0.11.0\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2af95e0>\n\n\n\n\n![png](/img/output_32_111.png)\n    \n\n\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2ec4160>\n\n\n\n\n​    \n![png](/img/output_33_111.png)\n​    \n\n\n\n```python\n\n```","source":"_posts/直方图.md","raw":"title: 直方图\nauthor: ztq\ntags:\n  - python\ncategories:\n  - python基础\n  - ''\ndate: 2021-04-17 18:27:00\n\n---\n\n## 一、边缘直方图\n\n如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。\n\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n\n\n### 1、使用seaborn实现边缘直方图\n\n（1）格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.jointplot)\n```\n\n    Help on function jointplot in module seaborn.axisgrid:\n    \n    jointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n        Draw a plot of two variables with bivariate and univariate graphs.\n        \n        This function provides a convenient interface to the :class:`JointGrid`\n        class, with several canned plot kinds. This is intended to be a fairly\n        lightweight wrapper; if you need more flexibility, you should use\n        :class:`JointGrid` directly.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        kind : { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\" }\n            Kind of plot to draw. See the examples for references to the underlying functions.\n        color : :mod:`matplotlib color <matplotlib.colors>`\n            Single color specification for when hue mapping is not used. Otherwise, the\n            plot will try to hook into the matplotlib property cycle.\n        height : numeric\n            Size of the figure (it will be square).\n        ratio : numeric\n            Ratio of joint axes height to marginal axes height.\n        space : numeric\n            Space between the joint and marginal axes\n        dropna : bool\n            If True, remove observations that are missing from ``x`` and ``y``.\n        {x, y}lim : pairs of numbers\n            Axis limits to set before plotting.\n        marginal_ticks : bool\n            If False, suppress ticks on the count/density axis of the marginal plots.\n        {joint, marginal}_kws : dicts\n            Additional keyword arguments for the plot components.\n        hue : vector or key in ``data``\n            Semantic variable that is mapped to determine the color of plot elements.\n            Semantic variable that is mapped to determine the color of plot elements.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        kwargs\n            Additional keyword arguments are passed to the function used to\n            draw the plot on the joint Axes, superseding items in the\n            ``joint_kws`` dictionary.\n        \n        Returns\n        -------\n        :class:`JointGrid`\n            An object managing multiple subplots that correspond to joint and marginal axes\n            for plotting a bivariate relationship or distribution.\n        \n        See Also\n        --------\n        JointGrid : Set up a figure with joint and marginal views on bivariate data.\n        PairGrid : Set up a figure with joint and marginal views on multiple variables.\n        jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/jointplot.rst\n\n\n​    \n\n（2）举例\n\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n\n\n\n```python\nimport seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(\"tips\")\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\")\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c25dcf10>\n\n\n\n\n![png](/img/output_8_111.png)\n    \n\n\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n\n\n示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线\n\n\n```python\nimport seaborn as sns\n  \ntips = sns.load_dataset(\"tips\")\n  \n# here \"*\" is used as a marker for scatterplot\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"reg\")\n\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295ada536a0>\n\n\n\n\n![png](/img/output_11_111.png)\n    \n\n\nkind的其他取值\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='scatter')\n#默认kind=\"scatter\"\n#sns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex')\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='kde')\n\nplt.show()\n```\n\n\n![png](/img/output_13_011.png)\n    \n\n\n\n\n![png](/img/output_13_111.png)\n    \n\n\n使用marginal_kws修改直方图的样式\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n```\n\n\n![png](/img/output_15_011.png)\n    \n\n\n### 2、使用plotly express实现边缘直方图\n\n(1)plotly express简单介绍及安装\n\nPlotly Express 是一个新的高级 Python 可视化库：它是 Plotly.py 的高级封装，它为复杂的图表提供了一个简单的语法。 \n\n受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 Plotly.py 一样。\n\nPlotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。 \n\n最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！\n\n用 pip install plotly_express 命令可以安装 Plotly Express。\n\n\n(2)格式\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=\"\",marginal_y=\"\"）\n（2）示例\n\n<strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong>\n\n## 二、边缘箱线图\n\n边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数\n\n### 1、使用matplotlib实现边缘箱线图\n\n\n```python\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"Data\\mpg_ggplot2.csv\")\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"Set1\", edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=\"v\")\nsns.boxplot(df.displ, ax=ax_bottom, orient=\"h\")\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n      warnings.warn(single_var_warning.format(\"Vertical\", \"x\"))\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n\n\n\n\n![png](/img/output_26_111.png)\n    \n\n\n### 2、使用seaborn实现边缘箱线图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=\"Data\\palmer_penguin_species.tsv.txt\"\n# penguins_df = pd.read_csv(penguins_data, sep=\"\\t\")\npenguins=sns.load_dataset(\"penguins\")\npenguins.head()\n\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\n# # set plot context to set plot sizes \n# sns.set_context(\"talk\", font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=\"culmen_length_mm\",\n#                   y=\"culmen_depth_mm\")\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(\"Scatterplot_with_marginal_boxplot_Seaborn.png\",\n# #                    format='png',dpi=150)\n# plt.show()\n```\n\n\n```python\nimport seaborn\nseaborn.__version__\n```\n\n\n\n\n    '0.11.0'\n\n\n需要更的到0.11.0版本才可以正常使用\npip install seaborn==0.11.0\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2af95e0>\n\n\n\n\n![png](/img/output_32_111.png)\n    \n\n\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2ec4160>\n\n\n\n\n​    \n![png](/img/output_33_111.png)\n​    \n\n\n\n```python\n\n```","slug":"直方图","published":1,"updated":"2022-04-04T08:32:40.178Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1900af7kt91f6vdfhq","content":"<h2 id=\"一、边缘直方图\">一、边缘直方图</h2>\n<p>如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。</p>\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n<h3 id=\"1、使用seaborn实现边缘直方图\">1、使用seaborn实现边缘直方图</h3>\n<p>（1）格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.jointplot)\n</code></pre>\n<pre><code>Help on function jointplot in module seaborn.axisgrid:\n\njointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n    Draw a plot of two variables with bivariate and univariate graphs.\n    \n    This function provides a convenient interface to the :class:`JointGrid`\n    class, with several canned plot kinds. This is intended to be a fairly\n    lightweight wrapper; if you need more flexibility, you should use\n    :class:`JointGrid` directly.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    kind : &#123; &quot;scatter&quot; | &quot;kde&quot; | &quot;hist&quot; | &quot;hex&quot; | &quot;reg&quot; | &quot;resid&quot; &#125;\n        Kind of plot to draw. See the examples for references to the underlying functions.\n    color : :mod:`matplotlib color &lt;matplotlib.colors&gt;`\n        Single color specification for when hue mapping is not used. Otherwise, the\n        plot will try to hook into the matplotlib property cycle.\n    height : numeric\n        Size of the figure (it will be square).\n    ratio : numeric\n        Ratio of joint axes height to marginal axes height.\n    space : numeric\n        Space between the joint and marginal axes\n    dropna : bool\n        If True, remove observations that are missing from ``x`` and ``y``.\n    &#123;x, y&#125;lim : pairs of numbers\n        Axis limits to set before plotting.\n    marginal_ticks : bool\n        If False, suppress ticks on the count/density axis of the marginal plots.\n    &#123;joint, marginal&#125;_kws : dicts\n        Additional keyword arguments for the plot components.\n    hue : vector or key in ``data``\n        Semantic variable that is mapped to determine the color of plot elements.\n        Semantic variable that is mapped to determine the color of plot elements.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    kwargs\n        Additional keyword arguments are passed to the function used to\n        draw the plot on the joint Axes, superseding items in the\n        ``joint_kws`` dictionary.\n    \n    Returns\n    -------\n    :class:`JointGrid`\n        An object managing multiple subplots that correspond to joint and marginal axes\n        for plotting a bivariate relationship or distribution.\n    \n    See Also\n    --------\n    JointGrid : Set up a figure with joint and marginal views on bivariate data.\n    PairGrid : Set up a figure with joint and marginal views on multiple variables.\n    jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/jointplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）举例</p>\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n<pre><code class=\"language-python\">import seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c25dcf10&gt;\n</code></pre>\n<p><img src=\"/img/output_8_111.png\" alt=\"png\"></p>\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n<p>示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线</p>\n<pre><code class=\"language-python\">import seaborn as sns\n  \ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# here &quot;*&quot; is used as a marker for scatterplot\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;, kind=&quot;reg&quot;)\n\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295ada536a0&gt;\n</code></pre>\n<p><img src=\"/img/output_11_111.png\" alt=\"png\"></p>\n<p>kind的其他取值</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='scatter')\n#默认kind=&quot;scatter&quot;\n#sns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex')\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='kde')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_011.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_13_111.png\" alt=\"png\"></p>\n<p>使用marginal_kws修改直方图的样式</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_15_011.png\" alt=\"png\"></p>\n<h3 id=\"2、使用plotly-express实现边缘直方图\">2、使用plotly express实现边缘直方图</h3>\n<p>(1)plotly express简单介绍及安装</p>\n<p>Plotly Express 是一个新的高级 Python 可视化库：它是 <a href=\"http://Plotly.py\">Plotly.py</a> 的高级封装，它为复杂的图表提供了一个简单的语法。</p>\n<p>受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 <a href=\"http://Plotly.py\">Plotly.py</a> 一样。</p>\n<p>Plotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。</p>\n<p>最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！</p>\n<p>用 pip install plotly_express 命令可以安装 Plotly Express。</p>\n<p>(2)格式<br>\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）<br>\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=“”,marginal_y=“”）<br>\n（2）示例</p>\n<p><strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong></p>\n<h2 id=\"二、边缘箱线图\">二、边缘箱线图</h2>\n<p>边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数</p>\n<h3 id=\"1、使用matplotlib实现边缘箱线图\">1、使用matplotlib实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(&quot;Data\\mpg_ggplot2.csv&quot;)\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=&quot;Set1&quot;, edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=&quot;v&quot;)\nsns.boxplot(df.displ, ax=ax_bottom, orient=&quot;h&quot;)\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n  warnings.warn(single_var_warning.format(&quot;Vertical&quot;, &quot;x&quot;))\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n</code></pre>\n<p><img src=\"/img/output_26_111.png\" alt=\"png\"></p>\n<h3 id=\"2、使用seaborn实现边缘箱线图\">2、使用seaborn实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=&quot;Data\\palmer_penguin_species.tsv.txt&quot;\n# penguins_df = pd.read_csv(penguins_data, sep=&quot;\\t&quot;)\npenguins=sns.load_dataset(&quot;penguins&quot;)\npenguins.head()\n\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\"># # set plot context to set plot sizes \n# sns.set_context(&quot;talk&quot;, font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=&quot;culmen_length_mm&quot;,\n#                   y=&quot;culmen_depth_mm&quot;)\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(&quot;Scatterplot_with_marginal_boxplot_Seaborn.png&quot;,\n# #                    format='png',dpi=150)\n# plt.show()\n</code></pre>\n<pre><code class=\"language-python\">import seaborn\nseaborn.__version__\n</code></pre>\n<pre><code>'0.11.0'\n</code></pre>\n<p>需要更的到0.11.0版本才可以正常使用<br>\npip install seaborn==0.11.0</p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2af95e0&gt;\n</code></pre>\n<p><img src=\"/img/output_32_111.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2ec4160&gt;\n</code></pre>\n<p>​<br>\n<img src=\"/img/output_33_111.png\" alt=\"png\"><br>\n​</p>\n<pre><code class=\"language-python\">\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、边缘直方图\">一、边缘直方图</h2>\n<p>如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。</p>\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n<h3 id=\"1、使用seaborn实现边缘直方图\">1、使用seaborn实现边缘直方图</h3>\n<p>（1）格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.jointplot)\n</code></pre>\n<pre><code>Help on function jointplot in module seaborn.axisgrid:\n\njointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n    Draw a plot of two variables with bivariate and univariate graphs.\n    \n    This function provides a convenient interface to the :class:`JointGrid`\n    class, with several canned plot kinds. This is intended to be a fairly\n    lightweight wrapper; if you need more flexibility, you should use\n    :class:`JointGrid` directly.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    kind : &#123; &quot;scatter&quot; | &quot;kde&quot; | &quot;hist&quot; | &quot;hex&quot; | &quot;reg&quot; | &quot;resid&quot; &#125;\n        Kind of plot to draw. See the examples for references to the underlying functions.\n    color : :mod:`matplotlib color &lt;matplotlib.colors&gt;`\n        Single color specification for when hue mapping is not used. Otherwise, the\n        plot will try to hook into the matplotlib property cycle.\n    height : numeric\n        Size of the figure (it will be square).\n    ratio : numeric\n        Ratio of joint axes height to marginal axes height.\n    space : numeric\n        Space between the joint and marginal axes\n    dropna : bool\n        If True, remove observations that are missing from ``x`` and ``y``.\n    &#123;x, y&#125;lim : pairs of numbers\n        Axis limits to set before plotting.\n    marginal_ticks : bool\n        If False, suppress ticks on the count/density axis of the marginal plots.\n    &#123;joint, marginal&#125;_kws : dicts\n        Additional keyword arguments for the plot components.\n    hue : vector or key in ``data``\n        Semantic variable that is mapped to determine the color of plot elements.\n        Semantic variable that is mapped to determine the color of plot elements.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    kwargs\n        Additional keyword arguments are passed to the function used to\n        draw the plot on the joint Axes, superseding items in the\n        ``joint_kws`` dictionary.\n    \n    Returns\n    -------\n    :class:`JointGrid`\n        An object managing multiple subplots that correspond to joint and marginal axes\n        for plotting a bivariate relationship or distribution.\n    \n    See Also\n    --------\n    JointGrid : Set up a figure with joint and marginal views on bivariate data.\n    PairGrid : Set up a figure with joint and marginal views on multiple variables.\n    jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/jointplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）举例</p>\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n<pre><code class=\"language-python\">import seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c25dcf10&gt;\n</code></pre>\n<p><img src=\"/img/output_8_111.png\" alt=\"png\"></p>\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n<p>示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线</p>\n<pre><code class=\"language-python\">import seaborn as sns\n  \ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# here &quot;*&quot; is used as a marker for scatterplot\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;, kind=&quot;reg&quot;)\n\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295ada536a0&gt;\n</code></pre>\n<p><img src=\"/img/output_11_111.png\" alt=\"png\"></p>\n<p>kind的其他取值</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='scatter')\n#默认kind=&quot;scatter&quot;\n#sns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex')\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='kde')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_011.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_13_111.png\" alt=\"png\"></p>\n<p>使用marginal_kws修改直方图的样式</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_15_011.png\" alt=\"png\"></p>\n<h3 id=\"2、使用plotly-express实现边缘直方图\">2、使用plotly express实现边缘直方图</h3>\n<p>(1)plotly express简单介绍及安装</p>\n<p>Plotly Express 是一个新的高级 Python 可视化库：它是 <a href=\"http://Plotly.py\">Plotly.py</a> 的高级封装，它为复杂的图表提供了一个简单的语法。</p>\n<p>受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 <a href=\"http://Plotly.py\">Plotly.py</a> 一样。</p>\n<p>Plotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。</p>\n<p>最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！</p>\n<p>用 pip install plotly_express 命令可以安装 Plotly Express。</p>\n<p>(2)格式<br>\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）<br>\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=“”,marginal_y=“”）<br>\n（2）示例</p>\n<p><strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong></p>\n<h2 id=\"二、边缘箱线图\">二、边缘箱线图</h2>\n<p>边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数</p>\n<h3 id=\"1、使用matplotlib实现边缘箱线图\">1、使用matplotlib实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(&quot;Data\\mpg_ggplot2.csv&quot;)\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=&quot;Set1&quot;, edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=&quot;v&quot;)\nsns.boxplot(df.displ, ax=ax_bottom, orient=&quot;h&quot;)\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n  warnings.warn(single_var_warning.format(&quot;Vertical&quot;, &quot;x&quot;))\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n</code></pre>\n<p><img src=\"/img/output_26_111.png\" alt=\"png\"></p>\n<h3 id=\"2、使用seaborn实现边缘箱线图\">2、使用seaborn实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=&quot;Data\\palmer_penguin_species.tsv.txt&quot;\n# penguins_df = pd.read_csv(penguins_data, sep=&quot;\\t&quot;)\npenguins=sns.load_dataset(&quot;penguins&quot;)\npenguins.head()\n\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\"># # set plot context to set plot sizes \n# sns.set_context(&quot;talk&quot;, font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=&quot;culmen_length_mm&quot;,\n#                   y=&quot;culmen_depth_mm&quot;)\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(&quot;Scatterplot_with_marginal_boxplot_Seaborn.png&quot;,\n# #                    format='png',dpi=150)\n# plt.show()\n</code></pre>\n<pre><code class=\"language-python\">import seaborn\nseaborn.__version__\n</code></pre>\n<pre><code>'0.11.0'\n</code></pre>\n<p>需要更的到0.11.0版本才可以正常使用<br>\npip install seaborn==0.11.0</p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2af95e0&gt;\n</code></pre>\n<p><img src=\"/img/output_32_111.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2ec4160&gt;\n</code></pre>\n<p>​<br>\n<img src=\"/img/output_33_111.png\" alt=\"png\"><br>\n​</p>\n<pre><code class=\"language-python\">\n</code></pre>\n"},{"title":"线程相关的知识","author":"郑天祺","date":"2019-11-20T11:46:00.000Z","_content":"\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","source":"_posts/线程相关的知识.md","raw":"title: 线程相关的知识\nauthor: 郑天祺\ntags:\n  - 线程\ncategories:\n  - java基础\ndate: 2019-11-20 19:46:00\n\n---\n\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","slug":"线程相关的知识","published":1,"updated":"2022-04-04T08:32:40.178Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1a00aj7kt90sdj56at","content":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n"},{"title":"自旋锁","author":"郑天祺","date":"2019-08-31T04:54:00.000Z","_content":"\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","source":"_posts/自旋锁.md","raw":"title: 自旋锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 12:54:00\n\n---\n\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","slug":"自旋锁","published":1,"updated":"2022-04-04T08:32:40.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1b00an7kt94i2w4c4h","content":"<h1>自旋锁</h1>\n<h2 id=\"1、自旋锁概念（spinlock）\">1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2 id=\"2、自旋锁的优点\">2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2 id=\"3、自旋锁应用\">3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2 id=\"4、简单自旋锁的实现-：\">4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock &#123;\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() &#123;\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) &#123;\n         &#125;\n     &#125;\n\n   public void unLock() &#123;\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) &#123;\n         &#125;\n         ref.set(null);\n     &#125;\n &#125;\n\ntest：\n\npublic class SimpleSpinLockTest &#123;\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"5、可重入的自旋锁和不可重入的自旋锁-：\">5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2 id=\"6、-另有三种常见的形式\">6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：<a href=\"https://www.cnblogs.com/stevenczp/p/7136416.html\">https://www.cnblogs.com/stevenczp/p/7136416.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>自旋锁</h1>\n<h2 id=\"1、自旋锁概念（spinlock）\">1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2 id=\"2、自旋锁的优点\">2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2 id=\"3、自旋锁应用\">3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2 id=\"4、简单自旋锁的实现-：\">4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock &#123;\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() &#123;\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) &#123;\n         &#125;\n     &#125;\n\n   public void unLock() &#123;\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) &#123;\n         &#125;\n         ref.set(null);\n     &#125;\n &#125;\n\ntest：\n\npublic class SimpleSpinLockTest &#123;\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"5、可重入的自旋锁和不可重入的自旋锁-：\">5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2 id=\"6、-另有三种常见的形式\">6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：<a href=\"https://www.cnblogs.com/stevenczp/p/7136416.html\">https://www.cnblogs.com/stevenczp/p/7136416.html</a></p>\n"},{"title":"读写锁","author":"郑天祺","date":"2019-08-31T05:08:00.000Z","_content":"\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","source":"_posts/读写锁.md","raw":"title: 读写锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:08:00\n\n---\n\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","slug":"读写锁","published":1,"updated":"2022-04-04T08:32:40.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1c00ar7kt91ytx2o67","content":"<h1>4、读写锁</h1>\n<h2 id=\"1、读写锁介绍：\">1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，<br>\n​         读-写不能共存，<br>\n​         写-写不能共存。</p>\n<p>连接：<a href=\"https://blog.csdn.net/j080624/article/details/82790372%E3%80%81https://ifeve.com/read-write-locks/\">https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</a></p>\n<h2 id=\"2、总结：\">2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2 id=\"3、写锁的获取：\">3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) &#123;\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) &#123;\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     &#125;\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n &#125;\n\n \n\n static int exclusiveCount(int c) &#123; \n\n        return c &amp; EXCLUSIVE_MASK;\n\n &#125;\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    &#123; \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n&#125;\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2 id=\"4、写锁的释放：\">4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) &#123;\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n &#125;\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2 id=\"5、读锁的获取\">5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) &#123;\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) &#123;\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         &#125; else if (firstReader == current) &#123;\n             firstReaderHoldCount++;\n         &#125; else &#123;\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         &#125;\n         return 1;\n     &#125;\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n &#125;\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2 id=\"6、读锁的释放\">6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) &#123;\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     &#125; else &#123;\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) &#123;\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         &#125;\n         --rh.count;\n     &#125;     for (;;) &#123;\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"7、锁降级\">7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() &#123;\n         rwl.readLock().lock();\n         if (!cacheValid) &#123;\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try &#123;\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) &#123;\n                     data = ...\n             cacheValid = true;\n           &#125;\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         &#125; finally &#123;\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         &#125;\n       &#125;\n       try &#123;\n         use(data);\n       &#125; finally &#123;\n         rwl.readLock().unlock();\n       &#125;\n     &#125;\n &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>4、读写锁</h1>\n<h2 id=\"1、读写锁介绍：\">1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，<br>\n​         读-写不能共存，<br>\n​         写-写不能共存。</p>\n<p>连接：<a href=\"https://blog.csdn.net/j080624/article/details/82790372%E3%80%81https://ifeve.com/read-write-locks/\">https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</a></p>\n<h2 id=\"2、总结：\">2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2 id=\"3、写锁的获取：\">3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) &#123;\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) &#123;\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     &#125;\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n &#125;\n\n \n\n static int exclusiveCount(int c) &#123; \n\n        return c &amp; EXCLUSIVE_MASK;\n\n &#125;\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    &#123; \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n&#125;\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2 id=\"4、写锁的释放：\">4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) &#123;\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n &#125;\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2 id=\"5、读锁的获取\">5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) &#123;\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) &#123;\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         &#125; else if (firstReader == current) &#123;\n             firstReaderHoldCount++;\n         &#125; else &#123;\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         &#125;\n         return 1;\n     &#125;\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n &#125;\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2 id=\"6、读锁的释放\">6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) &#123;\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     &#125; else &#123;\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) &#123;\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         &#125;\n         --rh.count;\n     &#125;     for (;;) &#123;\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"7、锁降级\">7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() &#123;\n         rwl.readLock().lock();\n         if (!cacheValid) &#123;\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try &#123;\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) &#123;\n                     data = ...\n             cacheValid = true;\n           &#125;\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         &#125; finally &#123;\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         &#125;\n       &#125;\n       try &#123;\n         use(data);\n       &#125; finally &#123;\n         rwl.readLock().unlock();\n       &#125;\n     &#125;\n &#125;\n</code></pre>\n"},{"title":"责任链模式","author":"郑天祺","date":"2020-01-03T07:21:00.000Z","_content":"\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","source":"_posts/责任链模式.md","raw":"title: 责任链模式\nauthor: 郑天祺\ntags:\n\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 15:21:00\n\n---\n\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","slug":"责任链模式","published":1,"updated":"2022-04-04T08:32:40.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1c00au7kt97emwh7mt","content":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90487984\">https://blog.csdn.net/qq_23034755/article/details/90487984</a></p>\n<p>单例模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215\">https://blog.csdn.net/qq_23034755/article/details/90547215</a></p>\n<p>观察者模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90705205\">https://blog.csdn.net/qq_23034755/article/details/90705205</a></p>\n<p>发布订阅模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/91340383\">https://blog.csdn.net/qq_23034755/article/details/91340383</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90487984\">https://blog.csdn.net/qq_23034755/article/details/90487984</a></p>\n<p>单例模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215\">https://blog.csdn.net/qq_23034755/article/details/90547215</a></p>\n<p>观察者模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90705205\">https://blog.csdn.net/qq_23034755/article/details/90705205</a></p>\n<p>发布订阅模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/91340383\">https://blog.csdn.net/qq_23034755/article/details/91340383</a></p>\n"},{"title":"重放攻击","author":"郑天祺","date":"2019-09-04T11:54:00.000Z","_content":"\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","source":"_posts/重放攻击.md","raw":"title: 重放攻击\nauthor: 郑天祺\ntags:\n  - 网络安全\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-04 19:54:00\n\n---\n\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","slug":"重放攻击","published":1,"updated":"2022-04-04T08:32:40.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1d00ay7kt95sq5gj2o","content":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n"},{"title":"运算符","author":"郑天祺","date":"2020-09-01T01:24:00.000Z","_content":"\n## ^ 位异或运算\n\n运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1\n\n比如：8^11\n\n8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(\"0011\",2)=3","source":"_posts/运算符.md","raw":"title: 运算符\nauthor: 郑天祺\ntags:\n  - 运算符\ncategories:\n  - java基础\ndate: 2020-09-01 09:24:00\n\n---\n\n## ^ 位异或运算\n\n运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1\n\n比如：8^11\n\n8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(\"0011\",2)=3","slug":"运算符","published":1,"updated":"2022-04-04T08:32:40.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1e00b17kt9g82ta19v","content":"<h2 id=\"位异或运算\">^ 位异或运算</h2>\n<p>运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1</p>\n<p>比如：8^11</p>\n<p>8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(“0011”,2)=3</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"位异或运算\">^ 位异或运算</h2>\n<p>运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1</p>\n<p>比如：8^11</p>\n<p>8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(“0011”,2)=3</p>\n"},{"title":"轻量级锁","author":"郑天祺","date":"2019-08-31T07:08:00.000Z","_content":"\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","source":"_posts/轻量级锁.md","raw":"title: 轻量级锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 15:08:00\n\n---\n\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","slug":"轻量级锁","published":1,"updated":"2022-04-04T08:32:40.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1f00b47kt971y37yoj","content":"<h2 id=\"1、轻量级锁\">1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。<br>\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。<br>\n​     c) 将锁记录中的Owner指针指向锁对象。<br>\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2 id=\"2、锁消除\">2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2 id=\"3、锁膨胀\">3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、轻量级锁\">1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。<br>\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。<br>\n​     c) 将锁记录中的Owner指针指向锁对象。<br>\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2 id=\"2、锁消除\">2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2 id=\"3、锁膨胀\">3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n"},{"title":"锁粗化","author":"郑天祺","date":"2019-08-31T05:32:00.000Z","_content":"\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","source":"_posts/锁粗化.md","raw":"title: 锁粗化\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:32:00\n---\n\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","slug":"锁粗化","published":1,"updated":"2022-04-04T08:32:40.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1f00b77kt9b520amiu","content":"<p>转自：<a href=\"https://blog.csdn.net/qq_26222859/article/details/80546917\">https://blog.csdn.net/qq_26222859/article/details/80546917</a></p>\n<p>参考：<a href=\"https://www.jianshu.com/p/f05423a21e78\">https://www.jianshu.com/p/f05423a21e78</a></p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     synchronized(lock)&#123;\n         //do some thing\n     &#125;\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock)&#123;\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock)&#123;\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++)&#123;\n     synchronized(lock)&#123;\n     &#125;\n &#125;\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock)&#123;\n     for(int i=0;i&lt;size;i++)&#123;\n     &#125;\n &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>转自：<a href=\"https://blog.csdn.net/qq_26222859/article/details/80546917\">https://blog.csdn.net/qq_26222859/article/details/80546917</a></p>\n<p>参考：<a href=\"https://www.jianshu.com/p/f05423a21e78\">https://www.jianshu.com/p/f05423a21e78</a></p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     synchronized(lock)&#123;\n         //do some thing\n     &#125;\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock)&#123;\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock)&#123;\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++)&#123;\n     synchronized(lock)&#123;\n     &#125;\n &#125;\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock)&#123;\n     for(int i=0;i&lt;size;i++)&#123;\n     &#125;\n &#125;\n</code></pre>\n"},{"title":"阻塞锁","author":"郑天祺","date":"2019-08-31T05:00:00.000Z","_content":"\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","source":"_posts/阻塞锁.md","raw":"title: 阻塞锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:00:00\n\n---\n\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","slug":"阻塞锁","published":1,"updated":"2022-04-04T08:32:40.182Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno1g00ba7kt95m96hoyi","content":"<h1>阻塞锁</h1>\n<h2 id=\"1、阻塞锁优势\">1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。<br>\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2 id=\"2、阻塞锁的优势：\">2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2 id=\"3、阻塞锁应用：\">3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2 id=\"4、阻塞锁的简单实现：\">4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock &#123;\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode &#123;\n         private volatile Thread isLocked;\n     &#125;\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() &#123;\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) &#123;\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         &#125;\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     &#125;\n\npublic void unLock() &#123;\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) &#123;\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         &#125;\n         node = null;\n     &#125;\n &#125;\n</code></pre>\n<h3 id=\"5、demo：\">5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest &#123;\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     &#125;\n &#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>阻塞锁</h1>\n<h2 id=\"1、阻塞锁优势\">1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。<br>\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2 id=\"2、阻塞锁的优势：\">2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2 id=\"3、阻塞锁应用：\">3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2 id=\"4、阻塞锁的简单实现：\">4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock &#123;\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode &#123;\n         private volatile Thread isLocked;\n     &#125;\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() &#123;\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) &#123;\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         &#125;\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     &#125;\n\npublic void unLock() &#123;\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) &#123;\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         &#125;\n         node = null;\n     &#125;\n &#125;\n</code></pre>\n<h3 id=\"5、demo：\">5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest &#123;\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     &#125;\n &#125;\n\n \n</code></pre>\n"},{"title":"饼图","author":"ztq","date":"2021-04-30T07:20:00.000Z","_content":"\n# 1、基本饼图\n\n首先生成数据，然后讲数据分成两个数组x_data和y_data。\n饼图的data参数：遍历进行zip封装；\n饼图的标题参数：基本饼图-月度开支；\n设置饼图的位置和格式。\n\n```python\nimport pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame({\"消费\":[\"住宿\",\"餐饮\",\"交通\",\"服装\",\"红包\"],\"数据\":[2580,1300,500,900,1300]})\nx_data=df[\"消费\"].tolist()\ny_data=df[\"数据\"].tolist()\nc = (\n    Pie()\n    .add(\"\", [list(z) for z in zip(x_data, y_data)])   # zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"基本饼图-月度开支\"))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts(position=\"inside\",formatter=\"{b}:{d}%\"))  # 数据标签设置\n)\n\nc.render_notebook()\n```\n\n![image-20210430152325572](/img/image-20210430152325572.png)\n\n# 2、旭日图\n\n首先生成数据，用opts.SunburstItem()叠加生成内容，设置name和value\n绘制图片,Sunburst方法设置width、height\nseries_name=\"\"，设置系列名称\ndata_pair=data，设置数据\nradius=[a,b]为半径,a:最小半径，b最大半径\nset_global_opts()方法设置标题\nset_series_opts()方法设置标签\nname直接显示,光标放置在name上可显示value\n\n```python\nfrom pyecharts.charts import Sunburst\nfrom pyecharts import options as opts\n# 生成数据\ndata = [\n    opts.SunburstItem(\n        name=\"Grandpa\",\n        children=[\n            opts.SunburstItem(\n                name=\"Uncle Leo\",\n                value=15,\n                children=[\n                    opts.SunburstItem(name=\"Cousin Jack\", value=2),\n                    opts.SunburstItem(\n                        name=\"Cousin Mary\",\n                        value=5,\n                        children=[opts.SunburstItem(name=\"Jackson\", value=2)],\n                    ),\n                    opts.SunburstItem(name=\"Cousin Ben\", value=4),\n                ],\n            ),\n            opts.SunburstItem(\n                name=\"Father\",\n                value=10,\n                children=[\n                    opts.SunburstItem(name=\"Me\", value=5),\n                    opts.SunburstItem(name=\"Brother Peter\", value=1),\n                ],\n            ),\n        ],\n    ),\n    opts.SunburstItem(\n        name=\"Nancy\",\n        children=[\n            opts.SunburstItem(\n                name=\"Uncle Nike\",\n                children=[\n                    opts.SunburstItem(name=\"Cousin Betty\", value=1),\n                    opts.SunburstItem(name=\"Cousin Jenny\", value=2),\n                ],\n            )\n        ],\n    ),\n]\n# 绘制图片\nc = (\n    Sunburst(init_opts=opts.InitOpts(width=\"1000px\", height=\"600px\"))\n    .add(series_name=\"\", data_pair=data, radius=[0, \"90%\"])\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"旭日图-族谱\"))\n    .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}\"))\n)\n\nc.render_notebook()\n```\n\n![image-20210430152414489](/img/image-20210430152414489.png)\n\n# 3.环形图\n\n生成数据：pd.DataFrame()\nPie.add():zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n设置标题，标签设置\n\n```python\nimport pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame({\"消费\":[\"住宿\",\"餐饮\",\"交通\",\"服装\",\"红包\"],\"数据\":[2580,1300,500,900,1300]})\nx_data=df[\"消费\"].tolist()\ny_data=df[\"数据\"].tolist()\n\nc = (\n    Pie()\n    .add(\"\", [list(z) for z in zip(x_data, y_data)],radius = [\"50%\",\"90%\"])   # zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"环形图-月度开支\"))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts('position=\"inside\",formatter=\"{b}:{d}%\"'))  # 数据标签设置\n)\nc.render_notebook()\n```\n\n![image-20210430152457125](/img/image-20210430152457125.png)\n\n# 4.华夫饼图\n\nplt.figure()设置行数列数颜色\n\n```python\nimport matplotlib.pyplot as plt\nfrom pywaffle import Waffle\n\nplt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    columns=10,\n    values=[48, 46, 6]\n)\nplt.show()\n```\n\n![image-20210430152537381](/img/image-20210430152537381.png)\n\n# 5.堆叠条形图\n\n生成数据:pd.DataFrame(),其中\"Type\"包括x轴坐标标签和y轴百分比数据，\"AAA\"、\"add\"为堆叠属性\n绘制图片：设置图表画布宽度\n设置标签属性\n\n```python\nimport pandas as pd\nfrom pyecharts.charts import Bar,Grid\nfrom pyecharts import options as opts\n#生成数据\ndata = pd.DataFrame({\n    \"Type\": [\"person (29%)\", \"org (19.5%)\", \"location (12.1%)\", \"location/city (4.6%)\", \"org/spo/team (3.7%)\", \"org/company (3.3%)\", \n             \"time (3.1%) \", \"org/edu/ins (2.7%)\", \"building (2.0%)\", \"art (1.7%)\"],\n    \"AAA\": [93,78,77,78,61,56,79,35,37,17],\n    \"add\": [\"+5\",\"+4\",\"+9\",\"+8\",\"+26\",\"+15\",\"+17\",\"+12\",\"+21\",\"+4\"],\n})\n#绘制图片\nstack_bar = (\n    Bar(init_opts=opts.InitOpts(width=\"900px\", height=\"500px\"))#设置图表画布宽度\n    .add_xaxis(data[\"Type\"].tolist())\n    .add_yaxis(\"AAA\", data[\"AAA\"].tolist(),bar_min_width=1,bar_max_width=50,color=\"#a834a8\", stack=\"stack\")\n    .add_yaxis(\"BBB\", data[\"add\"].tolist(),color=\"#42b7bd\", stack=\"stack\",bar_min_width=11,is_selected=True)\n    #设置标签属性\n    .set_series_opts(\n        label_opts=opts.LabelOpts(position=\"inside\", color=\"white\", font_size=18,font_style=\"normal\",font_weight='normal',font_family='Times New Roman', formatter=\"{c}%\"))     \n    .set_global_opts(\n        legend_opts=opts.LegendOpts(textstyle_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight='bold')),#设置图例属性\n        #设置横纵坐标属性\n        xaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=\"Type\",axislabel_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight=\"normal\" ,rotate=19),interval=115,boundary_gap=['50%', '80%']),\n        yaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=\"Accurary\",axislabel_opts=opts.LabelOpts(font_size=18,font_style=\"normal\",font_weight=\"normal\" ,font_family='Times New Romanrial',formatter=\"{value}%\"))\n    )\n)\nstack_bar.render_notebook()\n```\n\n![image-20210430152636683](/img/image-20210430152636683.png)","source":"_posts/饼图.md","raw":"title: 饼图\nauthor: ztq\ntags:\n\n  - python\ncategories:\n  - python基础\ndate: 2021-04-30 15:20:00\n\n---\n\n# 1、基本饼图\n\n首先生成数据，然后讲数据分成两个数组x_data和y_data。\n饼图的data参数：遍历进行zip封装；\n饼图的标题参数：基本饼图-月度开支；\n设置饼图的位置和格式。\n\n```python\nimport pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame({\"消费\":[\"住宿\",\"餐饮\",\"交通\",\"服装\",\"红包\"],\"数据\":[2580,1300,500,900,1300]})\nx_data=df[\"消费\"].tolist()\ny_data=df[\"数据\"].tolist()\nc = (\n    Pie()\n    .add(\"\", [list(z) for z in zip(x_data, y_data)])   # zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"基本饼图-月度开支\"))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts(position=\"inside\",formatter=\"{b}:{d}%\"))  # 数据标签设置\n)\n\nc.render_notebook()\n```\n\n![image-20210430152325572](/img/image-20210430152325572.png)\n\n# 2、旭日图\n\n首先生成数据，用opts.SunburstItem()叠加生成内容，设置name和value\n绘制图片,Sunburst方法设置width、height\nseries_name=\"\"，设置系列名称\ndata_pair=data，设置数据\nradius=[a,b]为半径,a:最小半径，b最大半径\nset_global_opts()方法设置标题\nset_series_opts()方法设置标签\nname直接显示,光标放置在name上可显示value\n\n```python\nfrom pyecharts.charts import Sunburst\nfrom pyecharts import options as opts\n# 生成数据\ndata = [\n    opts.SunburstItem(\n        name=\"Grandpa\",\n        children=[\n            opts.SunburstItem(\n                name=\"Uncle Leo\",\n                value=15,\n                children=[\n                    opts.SunburstItem(name=\"Cousin Jack\", value=2),\n                    opts.SunburstItem(\n                        name=\"Cousin Mary\",\n                        value=5,\n                        children=[opts.SunburstItem(name=\"Jackson\", value=2)],\n                    ),\n                    opts.SunburstItem(name=\"Cousin Ben\", value=4),\n                ],\n            ),\n            opts.SunburstItem(\n                name=\"Father\",\n                value=10,\n                children=[\n                    opts.SunburstItem(name=\"Me\", value=5),\n                    opts.SunburstItem(name=\"Brother Peter\", value=1),\n                ],\n            ),\n        ],\n    ),\n    opts.SunburstItem(\n        name=\"Nancy\",\n        children=[\n            opts.SunburstItem(\n                name=\"Uncle Nike\",\n                children=[\n                    opts.SunburstItem(name=\"Cousin Betty\", value=1),\n                    opts.SunburstItem(name=\"Cousin Jenny\", value=2),\n                ],\n            )\n        ],\n    ),\n]\n# 绘制图片\nc = (\n    Sunburst(init_opts=opts.InitOpts(width=\"1000px\", height=\"600px\"))\n    .add(series_name=\"\", data_pair=data, radius=[0, \"90%\"])\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"旭日图-族谱\"))\n    .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}\"))\n)\n\nc.render_notebook()\n```\n\n![image-20210430152414489](/img/image-20210430152414489.png)\n\n# 3.环形图\n\n生成数据：pd.DataFrame()\nPie.add():zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n设置标题，标签设置\n\n```python\nimport pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame({\"消费\":[\"住宿\",\"餐饮\",\"交通\",\"服装\",\"红包\"],\"数据\":[2580,1300,500,900,1300]})\nx_data=df[\"消费\"].tolist()\ny_data=df[\"数据\"].tolist()\n\nc = (\n    Pie()\n    .add(\"\", [list(z) for z in zip(x_data, y_data)],radius = [\"50%\",\"90%\"])   # zip函数两个部分组合在一起list(zip(x,y))-----> [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"环形图-月度开支\"))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts('position=\"inside\",formatter=\"{b}:{d}%\"'))  # 数据标签设置\n)\nc.render_notebook()\n```\n\n![image-20210430152457125](/img/image-20210430152457125.png)\n\n# 4.华夫饼图\n\nplt.figure()设置行数列数颜色\n\n```python\nimport matplotlib.pyplot as plt\nfrom pywaffle import Waffle\n\nplt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    columns=10,\n    values=[48, 46, 6]\n)\nplt.show()\n```\n\n![image-20210430152537381](/img/image-20210430152537381.png)\n\n# 5.堆叠条形图\n\n生成数据:pd.DataFrame(),其中\"Type\"包括x轴坐标标签和y轴百分比数据，\"AAA\"、\"add\"为堆叠属性\n绘制图片：设置图表画布宽度\n设置标签属性\n\n```python\nimport pandas as pd\nfrom pyecharts.charts import Bar,Grid\nfrom pyecharts import options as opts\n#生成数据\ndata = pd.DataFrame({\n    \"Type\": [\"person (29%)\", \"org (19.5%)\", \"location (12.1%)\", \"location/city (4.6%)\", \"org/spo/team (3.7%)\", \"org/company (3.3%)\", \n             \"time (3.1%) \", \"org/edu/ins (2.7%)\", \"building (2.0%)\", \"art (1.7%)\"],\n    \"AAA\": [93,78,77,78,61,56,79,35,37,17],\n    \"add\": [\"+5\",\"+4\",\"+9\",\"+8\",\"+26\",\"+15\",\"+17\",\"+12\",\"+21\",\"+4\"],\n})\n#绘制图片\nstack_bar = (\n    Bar(init_opts=opts.InitOpts(width=\"900px\", height=\"500px\"))#设置图表画布宽度\n    .add_xaxis(data[\"Type\"].tolist())\n    .add_yaxis(\"AAA\", data[\"AAA\"].tolist(),bar_min_width=1,bar_max_width=50,color=\"#a834a8\", stack=\"stack\")\n    .add_yaxis(\"BBB\", data[\"add\"].tolist(),color=\"#42b7bd\", stack=\"stack\",bar_min_width=11,is_selected=True)\n    #设置标签属性\n    .set_series_opts(\n        label_opts=opts.LabelOpts(position=\"inside\", color=\"white\", font_size=18,font_style=\"normal\",font_weight='normal',font_family='Times New Roman', formatter=\"{c}%\"))     \n    .set_global_opts(\n        legend_opts=opts.LegendOpts(textstyle_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight='bold')),#设置图例属性\n        #设置横纵坐标属性\n        xaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=\"Type\",axislabel_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight=\"normal\" ,rotate=19),interval=115,boundary_gap=['50%', '80%']),\n        yaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=\"Accurary\",axislabel_opts=opts.LabelOpts(font_size=18,font_style=\"normal\",font_weight=\"normal\" ,font_family='Times New Romanrial',formatter=\"{value}%\"))\n    )\n)\nstack_bar.render_notebook()\n```\n\n![image-20210430152636683](/img/image-20210430152636683.png)","slug":"饼图","published":1,"updated":"2022-04-04T08:32:40.182Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2300f47kt93srt1ygq","content":"<h1>1、基本饼图</h1>\n<p>首先生成数据，然后讲数据分成两个数组x_data和y_data。<br>\n饼图的data参数：遍历进行zip封装；<br>\n饼图的标题参数：基本饼图-月度开支；<br>\n设置饼图的位置和格式。</p>\n<pre><code class=\"language-python\">import pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame(&#123;&quot;消费&quot;:[&quot;住宿&quot;,&quot;餐饮&quot;,&quot;交通&quot;,&quot;服装&quot;,&quot;红包&quot;],&quot;数据&quot;:[2580,1300,500,900,1300]&#125;)\nx_data=df[&quot;消费&quot;].tolist()\ny_data=df[&quot;数据&quot;].tolist()\nc = (\n    Pie()\n    .add(&quot;&quot;, [list(z) for z in zip(x_data, y_data)])   # zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;基本饼图-月度开支&quot;))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts(position=&quot;inside&quot;,formatter=&quot;&#123;b&#125;:&#123;d&#125;%&quot;))  # 数据标签设置\n)\n\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152325572.png\" alt=\"image-20210430152325572\"></p>\n<h1>2、旭日图</h1>\n<p>首先生成数据，用opts.SunburstItem()叠加生成内容，设置name和value<br>\n绘制图片,Sunburst方法设置width、height<br>\nseries_name=“”，设置系列名称<br>\ndata_pair=data，设置数据<br>\nradius=[a,b]为半径,a:最小半径，b最大半径<br>\nset_global_opts()方法设置标题<br>\nset_series_opts()方法设置标签<br>\nname直接显示,光标放置在name上可显示value</p>\n<pre><code class=\"language-python\">from pyecharts.charts import Sunburst\nfrom pyecharts import options as opts\n# 生成数据\ndata = [\n    opts.SunburstItem(\n        name=&quot;Grandpa&quot;,\n        children=[\n            opts.SunburstItem(\n                name=&quot;Uncle Leo&quot;,\n                value=15,\n                children=[\n                    opts.SunburstItem(name=&quot;Cousin Jack&quot;, value=2),\n                    opts.SunburstItem(\n                        name=&quot;Cousin Mary&quot;,\n                        value=5,\n                        children=[opts.SunburstItem(name=&quot;Jackson&quot;, value=2)],\n                    ),\n                    opts.SunburstItem(name=&quot;Cousin Ben&quot;, value=4),\n                ],\n            ),\n            opts.SunburstItem(\n                name=&quot;Father&quot;,\n                value=10,\n                children=[\n                    opts.SunburstItem(name=&quot;Me&quot;, value=5),\n                    opts.SunburstItem(name=&quot;Brother Peter&quot;, value=1),\n                ],\n            ),\n        ],\n    ),\n    opts.SunburstItem(\n        name=&quot;Nancy&quot;,\n        children=[\n            opts.SunburstItem(\n                name=&quot;Uncle Nike&quot;,\n                children=[\n                    opts.SunburstItem(name=&quot;Cousin Betty&quot;, value=1),\n                    opts.SunburstItem(name=&quot;Cousin Jenny&quot;, value=2),\n                ],\n            )\n        ],\n    ),\n]\n# 绘制图片\nc = (\n    Sunburst(init_opts=opts.InitOpts(width=&quot;1000px&quot;, height=&quot;600px&quot;))\n    .add(series_name=&quot;&quot;, data_pair=data, radius=[0, &quot;90%&quot;])\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;旭日图-族谱&quot;))\n    .set_series_opts(label_opts=opts.LabelOpts(formatter=&quot;&#123;b&#125;&quot;))\n)\n\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152414489.png\" alt=\"image-20210430152414489\"></p>\n<h1>3.环形图</h1>\n<p>生成数据：pd.DataFrame()<br>\nPie.add():zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]<br>\n设置标题，标签设置</p>\n<pre><code class=\"language-python\">import pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame(&#123;&quot;消费&quot;:[&quot;住宿&quot;,&quot;餐饮&quot;,&quot;交通&quot;,&quot;服装&quot;,&quot;红包&quot;],&quot;数据&quot;:[2580,1300,500,900,1300]&#125;)\nx_data=df[&quot;消费&quot;].tolist()\ny_data=df[&quot;数据&quot;].tolist()\n\nc = (\n    Pie()\n    .add(&quot;&quot;, [list(z) for z in zip(x_data, y_data)],radius = [&quot;50%&quot;,&quot;90%&quot;])   # zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;环形图-月度开支&quot;))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts('position=&quot;inside&quot;,formatter=&quot;&#123;b&#125;:&#123;d&#125;%&quot;'))  # 数据标签设置\n)\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152457125.png\" alt=\"image-20210430152457125\"></p>\n<h1>4.华夫饼图</h1>\n<p>plt.figure()设置行数列数颜色</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nfrom pywaffle import Waffle\n\nplt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    columns=10,\n    values=[48, 46, 6]\n)\nplt.show()\n</code></pre>\n<p><img src=\"/img/image-20210430152537381.png\" alt=\"image-20210430152537381\"></p>\n<h1>5.堆叠条形图</h1>\n<p>生成数据:pd.DataFrame(),其中&quot;Type&quot;包括x轴坐标标签和y轴百分比数据，“AAA”、&quot;add&quot;为堆叠属性<br>\n绘制图片：设置图表画布宽度<br>\n设置标签属性</p>\n<pre><code class=\"language-python\">import pandas as pd\nfrom pyecharts.charts import Bar,Grid\nfrom pyecharts import options as opts\n#生成数据\ndata = pd.DataFrame(&#123;\n    &quot;Type&quot;: [&quot;person (29%)&quot;, &quot;org (19.5%)&quot;, &quot;location (12.1%)&quot;, &quot;location/city (4.6%)&quot;, &quot;org/spo/team (3.7%)&quot;, &quot;org/company (3.3%)&quot;, \n             &quot;time (3.1%) &quot;, &quot;org/edu/ins (2.7%)&quot;, &quot;building (2.0%)&quot;, &quot;art (1.7%)&quot;],\n    &quot;AAA&quot;: [93,78,77,78,61,56,79,35,37,17],\n    &quot;add&quot;: [&quot;+5&quot;,&quot;+4&quot;,&quot;+9&quot;,&quot;+8&quot;,&quot;+26&quot;,&quot;+15&quot;,&quot;+17&quot;,&quot;+12&quot;,&quot;+21&quot;,&quot;+4&quot;],\n&#125;)\n#绘制图片\nstack_bar = (\n    Bar(init_opts=opts.InitOpts(width=&quot;900px&quot;, height=&quot;500px&quot;))#设置图表画布宽度\n    .add_xaxis(data[&quot;Type&quot;].tolist())\n    .add_yaxis(&quot;AAA&quot;, data[&quot;AAA&quot;].tolist(),bar_min_width=1,bar_max_width=50,color=&quot;#a834a8&quot;, stack=&quot;stack&quot;)\n    .add_yaxis(&quot;BBB&quot;, data[&quot;add&quot;].tolist(),color=&quot;#42b7bd&quot;, stack=&quot;stack&quot;,bar_min_width=11,is_selected=True)\n    #设置标签属性\n    .set_series_opts(\n        label_opts=opts.LabelOpts(position=&quot;inside&quot;, color=&quot;white&quot;, font_size=18,font_style=&quot;normal&quot;,font_weight='normal',font_family='Times New Roman', formatter=&quot;&#123;c&#125;%&quot;))     \n    .set_global_opts(\n        legend_opts=opts.LegendOpts(textstyle_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight='bold')),#设置图例属性\n        #设置横纵坐标属性\n        xaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=&quot;Type&quot;,axislabel_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight=&quot;normal&quot; ,rotate=19),interval=115,boundary_gap=['50%', '80%']),\n        yaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=&quot;Accurary&quot;,axislabel_opts=opts.LabelOpts(font_size=18,font_style=&quot;normal&quot;,font_weight=&quot;normal&quot; ,font_family='Times New Romanrial',formatter=&quot;&#123;value&#125;%&quot;))\n    )\n)\nstack_bar.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152636683.png\" alt=\"image-20210430152636683\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、基本饼图</h1>\n<p>首先生成数据，然后讲数据分成两个数组x_data和y_data。<br>\n饼图的data参数：遍历进行zip封装；<br>\n饼图的标题参数：基本饼图-月度开支；<br>\n设置饼图的位置和格式。</p>\n<pre><code class=\"language-python\">import pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame(&#123;&quot;消费&quot;:[&quot;住宿&quot;,&quot;餐饮&quot;,&quot;交通&quot;,&quot;服装&quot;,&quot;红包&quot;],&quot;数据&quot;:[2580,1300,500,900,1300]&#125;)\nx_data=df[&quot;消费&quot;].tolist()\ny_data=df[&quot;数据&quot;].tolist()\nc = (\n    Pie()\n    .add(&quot;&quot;, [list(z) for z in zip(x_data, y_data)])   # zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;基本饼图-月度开支&quot;))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts(position=&quot;inside&quot;,formatter=&quot;&#123;b&#125;:&#123;d&#125;%&quot;))  # 数据标签设置\n)\n\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152325572.png\" alt=\"image-20210430152325572\"></p>\n<h1>2、旭日图</h1>\n<p>首先生成数据，用opts.SunburstItem()叠加生成内容，设置name和value<br>\n绘制图片,Sunburst方法设置width、height<br>\nseries_name=“”，设置系列名称<br>\ndata_pair=data，设置数据<br>\nradius=[a,b]为半径,a:最小半径，b最大半径<br>\nset_global_opts()方法设置标题<br>\nset_series_opts()方法设置标签<br>\nname直接显示,光标放置在name上可显示value</p>\n<pre><code class=\"language-python\">from pyecharts.charts import Sunburst\nfrom pyecharts import options as opts\n# 生成数据\ndata = [\n    opts.SunburstItem(\n        name=&quot;Grandpa&quot;,\n        children=[\n            opts.SunburstItem(\n                name=&quot;Uncle Leo&quot;,\n                value=15,\n                children=[\n                    opts.SunburstItem(name=&quot;Cousin Jack&quot;, value=2),\n                    opts.SunburstItem(\n                        name=&quot;Cousin Mary&quot;,\n                        value=5,\n                        children=[opts.SunburstItem(name=&quot;Jackson&quot;, value=2)],\n                    ),\n                    opts.SunburstItem(name=&quot;Cousin Ben&quot;, value=4),\n                ],\n            ),\n            opts.SunburstItem(\n                name=&quot;Father&quot;,\n                value=10,\n                children=[\n                    opts.SunburstItem(name=&quot;Me&quot;, value=5),\n                    opts.SunburstItem(name=&quot;Brother Peter&quot;, value=1),\n                ],\n            ),\n        ],\n    ),\n    opts.SunburstItem(\n        name=&quot;Nancy&quot;,\n        children=[\n            opts.SunburstItem(\n                name=&quot;Uncle Nike&quot;,\n                children=[\n                    opts.SunburstItem(name=&quot;Cousin Betty&quot;, value=1),\n                    opts.SunburstItem(name=&quot;Cousin Jenny&quot;, value=2),\n                ],\n            )\n        ],\n    ),\n]\n# 绘制图片\nc = (\n    Sunburst(init_opts=opts.InitOpts(width=&quot;1000px&quot;, height=&quot;600px&quot;))\n    .add(series_name=&quot;&quot;, data_pair=data, radius=[0, &quot;90%&quot;])\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;旭日图-族谱&quot;))\n    .set_series_opts(label_opts=opts.LabelOpts(formatter=&quot;&#123;b&#125;&quot;))\n)\n\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152414489.png\" alt=\"image-20210430152414489\"></p>\n<h1>3.环形图</h1>\n<p>生成数据：pd.DataFrame()<br>\nPie.add():zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]<br>\n设置标题，标签设置</p>\n<pre><code class=\"language-python\">import pyecharts.options as opts\nfrom pyecharts.charts import Pie\nimport pandas as pd\n\n# 生成数据\ndf=pd.DataFrame(&#123;&quot;消费&quot;:[&quot;住宿&quot;,&quot;餐饮&quot;,&quot;交通&quot;,&quot;服装&quot;,&quot;红包&quot;],&quot;数据&quot;:[2580,1300,500,900,1300]&#125;)\nx_data=df[&quot;消费&quot;].tolist()\ny_data=df[&quot;数据&quot;].tolist()\n\nc = (\n    Pie()\n    .add(&quot;&quot;, [list(z) for z in zip(x_data, y_data)],radius = [&quot;50%&quot;,&quot;90%&quot;])   # zip函数两个部分组合在一起list(zip(x,y))-----&gt; [(x,y)]\n    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;环形图-月度开支&quot;))  # 标题\n    .set_series_opts(label_opts=opts.LabelOpts('position=&quot;inside&quot;,formatter=&quot;&#123;b&#125;:&#123;d&#125;%&quot;'))  # 数据标签设置\n)\nc.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152457125.png\" alt=\"image-20210430152457125\"></p>\n<h1>4.华夫饼图</h1>\n<p>plt.figure()设置行数列数颜色</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nfrom pywaffle import Waffle\n\nplt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    columns=10,\n    values=[48, 46, 6]\n)\nplt.show()\n</code></pre>\n<p><img src=\"/img/image-20210430152537381.png\" alt=\"image-20210430152537381\"></p>\n<h1>5.堆叠条形图</h1>\n<p>生成数据:pd.DataFrame(),其中&quot;Type&quot;包括x轴坐标标签和y轴百分比数据，“AAA”、&quot;add&quot;为堆叠属性<br>\n绘制图片：设置图表画布宽度<br>\n设置标签属性</p>\n<pre><code class=\"language-python\">import pandas as pd\nfrom pyecharts.charts import Bar,Grid\nfrom pyecharts import options as opts\n#生成数据\ndata = pd.DataFrame(&#123;\n    &quot;Type&quot;: [&quot;person (29%)&quot;, &quot;org (19.5%)&quot;, &quot;location (12.1%)&quot;, &quot;location/city (4.6%)&quot;, &quot;org/spo/team (3.7%)&quot;, &quot;org/company (3.3%)&quot;, \n             &quot;time (3.1%) &quot;, &quot;org/edu/ins (2.7%)&quot;, &quot;building (2.0%)&quot;, &quot;art (1.7%)&quot;],\n    &quot;AAA&quot;: [93,78,77,78,61,56,79,35,37,17],\n    &quot;add&quot;: [&quot;+5&quot;,&quot;+4&quot;,&quot;+9&quot;,&quot;+8&quot;,&quot;+26&quot;,&quot;+15&quot;,&quot;+17&quot;,&quot;+12&quot;,&quot;+21&quot;,&quot;+4&quot;],\n&#125;)\n#绘制图片\nstack_bar = (\n    Bar(init_opts=opts.InitOpts(width=&quot;900px&quot;, height=&quot;500px&quot;))#设置图表画布宽度\n    .add_xaxis(data[&quot;Type&quot;].tolist())\n    .add_yaxis(&quot;AAA&quot;, data[&quot;AAA&quot;].tolist(),bar_min_width=1,bar_max_width=50,color=&quot;#a834a8&quot;, stack=&quot;stack&quot;)\n    .add_yaxis(&quot;BBB&quot;, data[&quot;add&quot;].tolist(),color=&quot;#42b7bd&quot;, stack=&quot;stack&quot;,bar_min_width=11,is_selected=True)\n    #设置标签属性\n    .set_series_opts(\n        label_opts=opts.LabelOpts(position=&quot;inside&quot;, color=&quot;white&quot;, font_size=18,font_style=&quot;normal&quot;,font_weight='normal',font_family='Times New Roman', formatter=&quot;&#123;c&#125;%&quot;))     \n    .set_global_opts(\n        legend_opts=opts.LegendOpts(textstyle_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight='bold')),#设置图例属性\n        #设置横纵坐标属性\n        xaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=&quot;Type&quot;,axislabel_opts=opts.LabelOpts(font_size=18,font_family='Times New Roman',font_weight=&quot;normal&quot; ,rotate=19),interval=115,boundary_gap=['50%', '80%']),\n        yaxis_opts=opts.AxisOpts(name_textstyle_opts=opts.TextStyleOpts(font_weight='bold',font_size=17,font_family='Times New Roman'),name=&quot;Accurary&quot;,axislabel_opts=opts.LabelOpts(font_size=18,font_style=&quot;normal&quot;,font_weight=&quot;normal&quot; ,font_family='Times New Romanrial',formatter=&quot;&#123;value&#125;%&quot;))\n    )\n)\nstack_bar.render_notebook()\n</code></pre>\n<p><img src=\"/img/image-20210430152636683.png\" alt=\"image-20210430152636683\"></p>\n"},{"title":"Histogram（直方图）&KDE（密度图)","author":"ztq","date":"2021-04-17T10:54:00.000Z","_content":"\n# <center>Histogram（直方图）&KDE（密度图)   </center>\n\n<h2>0 Pandas数据集的导入</h2>\n\n<b3><b>0.1 pandas导入csv/txt文件</b></b3>\n\npd.read_csv()\n\n常用参数：\nfilepath_or_buffer：文件路径（必填，其他参数按需求填写）\n\nsep：指定分隔符，默认逗号','。\n\nheader：指定第几行作为表头。默认为0（即第1行作为表头），若没有表头，需设置header=None，可以是int或list。\n\nnames：指定列的名称，用list表示，默认None。\n\nindex_col：指定行索引，可以是一列或多列，默认None。\n\nusecols：需要读取的列，可以使用列序列也可以使用列名，默认None。\n\nprefix：给列名添加前缀。如prefix=x,会出来X0,X1,....，默认None。\n\nskiprows：需要忽略的行数（从文件开始处算起)，或需要跳过的行号列表（从0开始），默认None。\n\nskipfooter：需要忽略的行数（从最后一行开始算)\n\nnrows：需要读取的行数（从文件头开始算起），默认None。\n\nencoding：编码方式，乱码时使用，默认None。\n\n例1：导入文件house-price-index-full-table.csv中数据：\n\n\n```python\nimport pandas as pd\ndf1=pd.read_csv(\"Data/house-price-index-full-table.csv\")\n#df1=pd.read_csv(\"Data/house-price2.csv\",encoding='gbk')\ndf1.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n      <th>House Price Index (seasonally adjusted)</th>\n      <th>Average price 1-bedroom flats</th>\n      <th>Average price 2-bedroom flats</th>\n      <th>Average price 2-bedroom houses</th>\n      <th>Average price 3-bedroom houses</th>\n      <th>Average price 4-bedroom houses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n      <td>99.4</td>\n      <td>166</td>\n      <td>213</td>\n      <td>265</td>\n      <td>332</td>\n      <td>416</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n      <td>98.3</td>\n      <td>160</td>\n      <td>268</td>\n      <td>268</td>\n      <td>314</td>\n      <td>432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n      <td>101.7</td>\n      <td>160</td>\n      <td>259</td>\n      <td>284</td>\n      <td>332</td>\n      <td>474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n      <td>100.9</td>\n      <td>137</td>\n      <td>242</td>\n      <td>300</td>\n      <td>333</td>\n      <td>459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n      <td>97.2</td>\n      <td>156</td>\n      <td>216</td>\n      <td>285</td>\n      <td>328</td>\n      <td>380</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\ndf2=pd.read_csv(\"Data/house-price2.csv\",usecols=[0,1,2],nrows=5,encoding='gbk')\ndf2\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n例2：导入文件data.txt中数据：\n\n\n```python\ndf3=pd.read_csv(\"Data/data.txt\",sep='\\t',header=None,names=['特征1','特征2','特征3','标签'])\ndf3\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>特征1</th>\n      <th>特征2</th>\n      <th>特征3</th>\n      <th>标签</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40920</td>\n      <td>8.326976</td>\n      <td>0.953952</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14488</td>\n      <td>7.153469</td>\n      <td>1.673904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26052</td>\n      <td>1.441871</td>\n      <td>0.805124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75136</td>\n      <td>13.147394</td>\n      <td>0.428964</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38344</td>\n      <td>1.669788</td>\n      <td>0.134296</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>72993</td>\n      <td>10.141740</td>\n      <td>1.032955</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>35948</td>\n      <td>6.830792</td>\n      <td>1.213192</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>42666</td>\n      <td>13.276369</td>\n      <td>0.543880</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>67497</td>\n      <td>8.631577</td>\n      <td>0.749278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>35483</td>\n      <td>12.273169</td>\n      <td>1.508053</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n<b3><b>0.2 pandas导入excel文件</b></b3>\n\npd.read_excel()\n\n常用参数：\nio：excel文件路径（必填，其他参数按需求填写）\n\nsheet_name：需要导入数据的工作表表名,可以是int\\string\\list，None导入所有工作表数据，默认0。\n\n参数header、names、index_col、usecols、skiprows、nrows、skip_footer、encoding的用法与pd.read_csv相同。比如读取数据时想把第一列设为index，那么只需要简单的\npd.read_csv(\"new_wordvecter.csv\",index_col=[0])\n\n这里index_col可以设为列名\n\n后续更改index可以使用df.index = df.iloc[:,\"column\"].tolist()或df.set_index('column')\n例：读取Canada.xlsx文件中的数据\n\n\n```python\ndf_canada=pd.read_excel(\"Data/Canada.xlsx\",sheet_name='Canada by Citizenship',skiprows=20,skip_footer=2)\ndf_canada.head()\ndf_India_China=df_canada[df_canada['OdName'].isin(['China','India'])]\n```\n\n\n```python\ndf_India_China.columns\n```\n\n\n\n\n    Index([    'Type', 'Coverage',   'OdName',     'AREA', 'AreaName',      'REG',\n            'RegName',      'DEV',  'DevName',       1980,       1981,       1982,\n                 1983,       1984,       1985,       1986,       1987,       1988,\n                 1989,       1990,       1991,       1992,       1993,       1994,\n                 1995,       1996,       1997,       1998,       1999,       2000,\n                 2001,       2002,       2003,       2004,       2005,       2006,\n                 2007,       2008,       2009,       2010,       2011,       2012,\n                 2013],\n          dtype='object')\n\n\n\n\n```python\ndf_India_China\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Coverage</th>\n      <th>OdName</th>\n      <th>AREA</th>\n      <th>AreaName</th>\n      <th>REG</th>\n      <th>RegName</th>\n      <th>DEV</th>\n      <th>DevName</th>\n      <th>1980</th>\n      <th>...</th>\n      <th>2004</th>\n      <th>2005</th>\n      <th>2006</th>\n      <th>2007</th>\n      <th>2008</th>\n      <th>2009</th>\n      <th>2010</th>\n      <th>2011</th>\n      <th>2012</th>\n      <th>2013</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>China</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>906</td>\n      <td>Eastern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>5123</td>\n      <td>...</td>\n      <td>36619</td>\n      <td>42584</td>\n      <td>33518</td>\n      <td>27642</td>\n      <td>30037</td>\n      <td>29622</td>\n      <td>30391</td>\n      <td>28502</td>\n      <td>33024</td>\n      <td>34129</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>India</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>5501</td>\n      <td>Southern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>8880</td>\n      <td>...</td>\n      <td>28235</td>\n      <td>36210</td>\n      <td>33848</td>\n      <td>28742</td>\n      <td>28261</td>\n      <td>29456</td>\n      <td>34235</td>\n      <td>27509</td>\n      <td>30933</td>\n      <td>33087</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 43 columns</p>\n\n</div>\n\n\n\n选取等于某些值的行记录 用 ==\n\ndf.loc[df[‘column_name’] == some_value]\n\n选取某列是否是某一类型的数值 用 isin\n\ndf.loc[df[‘column_name’].isin(some_values)]\n\n\ndf['a']#取a列\n\ndf[['a','b']]#取a、b列\n\n<h2>1 直方图及密度图的画法</h2>\n\n直方图是表示变量频率分布的一种方法，是一个可以快速展示数据概率分布的工具，直观易于理解，并深受数据爱好者的喜爱。大家平时可能见到最多就是 matplotlib，seaborn 等高级封装的库包。<br/><br/>\n\n直方图又称质量分布图，它是表示资料变化情况的一种主要工具。用直方图可以解析出资料的规则性，比较直观地看出产品质量特性的分布状态，对于资料分布状况一目了然，便于判断其总体质量分布情况。直方图表示通过沿数据范围形成分箱(bin)，然后绘制条以显示落入每个分箱的观测次数的数据分布。\n\n核密度图（kde直方图的拟合曲线）可以看作是概率密度图\n\n<b3><b>1.1 matplotlib画直方图及密度图</b></b3>\n\n<b>(1)plt.hist()\n用于画直方图。</b>\n\n参数列表：\nplt.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, normed=None)\n\nx：指定要绘制直方图的数据；输入值，这需要一个数组或者一个序列，不需要长度相同的数组。\nbins：指定直方图条形的个数；\nrange：指定直方图数据的上下界，默认包含绘图数据的最大值和最小值；\ndensity：布尔,可选。如果\"True\"，返回元组的第一个元素将会将计数标准化以形成一个概率密度，也就是说，直方图下的面积（或积分）总和为1。这是通过将计数除以数字的数量来实现的观察乘以箱子的宽度而不是除以总数数量的观察。如果叠加也是“真实”的，那么柱状图被规范化为1。(替代normed)\nweights：该参数可为每一个数据点设置权重；\ncumulative：是否需要计算累计频数或频率；\nbottom：可以为直方图的每个条形添加基准线，默认为0；\nhisttype：指定直方图的类型，默认为bar，除此还有’barstacked’, ‘step’, ‘stepfilled’；\nalign：设置条形边界值的对其方式，默认为mid，除此还有’left’和’right’；\norientation：设置直方图的摆放方向，默认为垂直方向；\nrwidth：设置直方图条形宽度的百分比；\nlog：是否需要对绘图数据进行log变换；\ncolor：设置直方图的填充色；\nlabel：设置直方图的标签，可通过legend展示其图例；\nstacked：当有多个数据时，是否需要将直方图呈堆叠摆放，默认水平摆放；\nnormed：是否将直方图的频数转换成频率；(弃用，被density替代)\nalpha：透明度，浮点数。\n返回值：\nn：直方图的值\nbins：返回各个bin的区间范围\npatches：返回每个bin的信息\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,方差/标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值，是否对数据进行归一化\nn1, bins1, patches1 = plt.hist(x1, bins=50, density=True,color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50, density=True,color='r', alpha=0.2)\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\n# plt.show()\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x21b1a893250>]\n\n\n\n\n![png](/img/output_30_12.png)\n    \n\n\n<b>(2)添加分布曲线</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值\nn1, bins1, patches1 = plt.hist(x1, bins=50,  density=True, color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50,  density=True, color='r', alpha=0.2)\n\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\nplt.show()\n```\n\n\n![png](/img/output_32_02.png)\n    \n\n通俗的说，返回的第一个值是每个直方柱的y值（取决于normed和weights的取值），\n而第二个值是每个直方柱的x值——如果分成10份，则有11个分割位置，因此返回数目为nbins+1。\n<b>(3)多类型直方图</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n#plt.rcParams['font.family']='Times New Roman'\n#plt.rcParams['axes.unicode_minus']=False\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(\"ABC\"))\nplt.show()\n```\n\n\n![png](/img/output_35_02.png)\n    \n\n\n<b>(4)添加说明信息</b>\n\n<b>添加title</b><br/><br/>\n你可以给图示或图添加标题。但是默认不支持中文，需要自己添加中文字体。\n\nwindows的字体文件放在 C:\\Windows\\Fonts，通过fontproperties设置字体，fontsize设置字体大小.\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(\"ABC\"))\n\nplt.rcParams['font.family']='SimHei'\nplt.rcParams['axes.unicode_minus']=False\nplt.rcParams['font.size']='12'\nplt.title(\"多类型直方图\")\n#songTi = matplotlib.font_manager.FontProperties(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc')\n#plt.title(\"多类型直方图\",fontproperties=songTi,fontsize=12)\nplt.show()\n```\n\n\n![png](/img/output_38_02.png)\n    \n\n\n<b>添加文字、网格、轴标签及轴范围</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, color='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')  #(x,y,str,...)r raw string \nplt.xlim(40, 160)\nplt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()\n```\n\n\n![png](/img/output_40_02.png)\n    \n\n\n<b3><b>1.2 pandas画直方图及密度图</b></b3>\n\n<b>（1）pandas的plot方法</b>\n\npandas默认的 plot 方法可以帮助我们快速地绘制各种图形<br/>\nPandas 提供了 plot() 方法可以快速方便地将 Series 和 DataFrame 中的数据进行可视化, 它是 matplotlib.axes.Axes.plot 的封装。代码执行后会生成一张图片，并直接显示在 notebook 上。<br/>\n\n<b>基本使用</b>\nplot 默认为折线图，折线图也是最常用和最基础的可视化图形，足以满足我们日常 80% 的需求：\n\ndf.plot()\ns.plot()\n\n\n我们可以在 plot 后增加调用来使用其他的图形，当然这些图形对数据结构也有自己的要求：\n\ndf.plot.line() # 折线的全写方式\ndf.plot.bar() # 柱状图\ndf.plot.barh() # 横向柱状图 （条形图）\ndf.plot.hist() # 直方图\ndf.plot.box() # 箱形图\ndf.plot.kde() # 核密度估计图\ndf.plot.density() # 同 df.plot.kde()\ndf.plot.area() # 面积图\ndf.plot.pie() # 饼图\ndf.plot.scatter() # 散点图\ndf.plot.hexbin() # 六边形箱体图，或简称六边形图\n<b>使用方法</b>\n\n<b>Series 使用</b>\n\n使用 plot 时 x 轴为索引，y 轴为索引对应的具体值：\n\n\n```python\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(20),\n               index=pd.date_range('1/1/2000', periods=20)\n              )\n\nts.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd7ad0d00>\n\n\n\n\n![png](/img/output_49_12.png)\n    \n\n\n<b>DataFrame 使用</b><br/>\n\nDataFrame 使用 plot 时 x 轴为索引，y 轴为索引对应的多个具体值：\n\n\n```python\ndf = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd6a56610>\n\n\n\n\n![png](/img/output_51_12.png)\n    \n\n\n<b>plot 绘图时常用的一些绘图参数。</b><br/>\n\n图形类型\ndf.plot() 可以通过参数来指定具体图形类型：\n\ndf.plot(kind='pie') # 其他的名称和上文相同\ns.plot(kind='pie')\n直方图kind=\"hist\"\n密度图kind=\"kde\"\n生成或读取数据后，调用pandas的plot方法画图。默认情况下参数 kind=\"line\" 表示图的类型为折线图。<br/>\n\n```python\ndf = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot(kind='hist',title='this is a hist',grid=True)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x21b1ba68460>\n\n\n\n\n![png](/img/output_55_12.png)\n    \n\n\n图的标题：\n\n\n```python\ndf.plot(title='my plot')\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd5f8df10>\n\n\n\n\n![png](/img/output_57_12.png)\n    \n\n\n字体大小\n\n\n```python\n指定轴上的字体大小\n```\n\n\n```python\ndf.plot(fontsize=15)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd601e7c0>\n\n\n\n\n![png](/img/output_60_12.png)\n    \n\n\n线条样式\nstyle 可指定图的线条等样式，可参考可选的值 Matplotlib Line-style：\ndf[:5].plot(style=':') # 虚线\ndf[:5].plot(style='-.') # 虚实相间\ndf[:5].plot(style='--') # 长虚线\ndf[:5].plot(style='-') # 实线（默认）\ndf[:5].plot(style='.') # 点\ndf[:5].plot(style='*-') # 实线，数值为星星\ndf[:5].plot(style='^-') # 实线，数值为三角形\n\n```python\ndf.plot(style=':') # 虚线\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd603bd30>\n\n\n\n\n![png](/img/output_63_12.png)\n    \n\n\n对不同线分别给样式：\n\n\n```python\ndf[:5].plot(style=[':', '--', '.-', '*-'])\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd601a250>\n\n\n\n\n![png](/img/output_65_12.png)\n    \n\n\n背景辅助线\n\n\n```python\ngrid 会给 x 方向和 y 方向增加灰色辅助线：\n```\n\n\n```python\ndf.plot(grid=True)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd6162fd0>\n\n\n\n\n![png](/img/output_68_12.png)\n    \n\n\n图例\n\n\n```python\ndf.plot(legend=False)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd61e5cd0>\n\n\n\n\n![png](/img/output_70_12.png)\n    \n\n\n可以反向排序图例：\n\n\n```python\ndf.plot(legend='reverse')\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd5e02bb0>\n\n\n\n\n![png](/img/output_72_12.png)\n    \n\n\n<b>（2）Pandas画直方图和密度图</b>\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv('Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nTitanic.Age.plot(kind = 'hist', bins = 20, color = 'steelblue', edgecolor = 'black', density = True, label = '直方图')\n# 绘制核密度图\nTitanic.Age.plot(kind = 'kde', color = 'red', label = '核密度图')\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n    <class 'pandas.core.series.Series'>\n\n\n\n\n![png](/img/output_74_12.png)\n    \n\n\n<b3><b>1.3 seaborn画直方图及密度图</b></b3>\n\n尽管上一幅图满足了两种图形的合成，但其表达的是所有乘客的年龄分布，如果按性别分组，研究不同性别下年龄分布的差异，该如何实现？针对这个问题，使用matplotlib模块或pandas模块都会稍微复杂一些，推荐使用seaborn模块中的distplot函数，因为该函数的代码简洁而易懂。关于该函数的语法和参数含义如下：\nsns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None,\n\t         hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None,\n\t         color=None, vertical=False, norm_hist=False, axlabel=None,\n\t         label=None, ax=None)a：指定绘图数据，可以是序列、一维数组或列表。\nbins：指定直方图条形的个数。\nhist：bool类型的参数，是否绘制直方图，默认为True。\nkde：bool类型的参数，是否绘制核密度图，默认为True。\nrug：bool类型的参数，是否绘制须图（如果数据比较密集，该参数比较有用），默认为False。\nfit：指定一个随机分布对象（需调用scipy模块中的随机分布函数），用于绘制随机分布的概率密度曲线。\nhist_kws：以字典形式传递直方图的其他修饰属性，如填充色、边框色、宽度等。\nkde_kws：以字典形式传递核密度图的其他修饰属性，如线的颜色、线的类型等。\nrug_kws：以字典形式传递须图的其他修饰属性，如线的颜色、线的宽度等。\nfit_kws：以字典形式传递概率密度曲线的其他修饰属性，如线条颜色、形状、宽度等。\ncolor：指定图形的颜色，除了随机分布曲线的颜色。\nvertical：bool类型的参数，是否将图形垂直显示，默认为True。（改为False即为horizontal）\nnorm_hist：bool类型的参数，是否将频数更改为频率，默认为False。\naxlabel：用于显示轴标签。 label：指定图形的图例，需结合plt.legend()一起使用。\nax：指定子图的位置。\n从函数的参数可知，通过该函数，可以实现三种图形的合成，分别是直方图（hist参数）、核密度曲线（kde参数）以及指定的理论分布密度曲线（fit参数）。接下来，针对如上介绍的distplot函数，绘制不同性别下乘客的年龄分布图，具体代码如下：\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = {'color':'steelblue'}, label = '男性')\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = {'color':'purple'}, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄直方图')\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = {'color':'red', 'linestyle':'-'},\n             norm_hist = True, label = '男性')\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = {'color':'black', 'linestyle':'--'},\n             norm_hist = True, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄核密度图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_79_02.png)\n    \n\n\n\n\n![png](/img/output_79_12.png)\n    \n\n\n为了避免四个图形混在一起不易发现数据背后的特征，将直方图与核密度图分开绘制。从直方图来看，女性年龄的分布明显比男性矮，说明在各年龄段下，男性乘客要比女性乘客多；再看核密度图，男女性别的年龄分布趋势比较接近，说明各年龄段下的男女乘客人数同步增加或减少。\n\n把两种图合在一起画：\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = {'color':'steelblue'},\n             label = ('男性','直方图'),norm_hist=True)\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = {'color':'purple'},\n             label = ('女性','直方图'),norm_hist=True)\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = {'color':'red', 'linestyle':'-'},\n             norm_hist = True, label = ('男性','核密度图'))\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = {'color':'black', 'linestyle':'--'},\n             norm_hist = True, label = ('女性','核密度图'))\nplt.title('泰坦尼克号男女乘客的年龄分布图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_81_02.png)\n    \n\n\n<b>（1）seaborn直方图</b>\n\nseaborn的kdeplot函数专门用于画核密度估计图.\n\n<b>（2）seaborn密度图</b>\n\nseaborn的kdeplot函数专门用于画核密度估计图.\n\n<h2>2 课堂练习</h2>\n\n<b>（1）直方图</b>\n\n以Titanic数据集为例绘制乘客的年龄直方图\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nplt.hist(x = Titanic.Age, # 指定绘图数据\n         bins = 20, # 指定直方图中条块的个数\n         color = 'steelblue', # 指定直方图的填充色\n         edgecolor = 'black' # 指定直方图的边框色\n         )\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_89_02.png)\n    \n\n\n<h2>3 直方图与条形图的区别</h2>\n\n首先，条形图是用条形的长度表示各类别频数的多少，其宽度（表示类别）则是固定的；\n\n      直方图是用面积表示各组频数的多少，矩形的高度表示每一组的频数或频率，宽度则表示各组的组距，因此其高度与宽度均有意义。\n\n其次，由于分组数据具有连续性，直方图的各矩形通常是连续排列，而条形图则是分开排列。\n\n最后，条形图主要用于展示分类数据，而直方图则主要用于展示数据型数据<br/><br/>\n\n原文链接：https://blog.csdn.net/xjl271314/article/details/80295935\n\n<h2>附录1-本小节用到的以前的知识复习</h2>\n<h3>1)批量注释/去掉注释</h3> \n\nCtrl+/\n\n->常常出现在python函数定义的函数名后面，为函数添加元数据,描述函数的返回类型，从而方便开发人员使用。\ndef add(x, y) -> int:\n  return x+y\n\n<h3>2)中文字体设置</h3>\n\nhttps://blog.csdn.net/dxawdc/article/details/110311549\n\n<h3>3)函数定义</h3>\n\n```python\ns = '杰瑞你好啊'\ndef mylen():\n    n = 0\n    for i in s :\n        n += 1\n    print(n)\nmylen()\n#len1 = mylen()\n#print(len1)\n```\n\n    5\n\n\n<h2>附录2-错误提示及解决办法</h2>\n<h3>1) 8722 missing from current font, matplotlib画图</h3>\n\n负号问题，添加语句plt.rcParams['axes.unicode_minus']=False\n\n更多请参考https://blog.csdn.net/seeker3/article/details/108432781?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs\n\n\n```python\n\n```","source":"_posts/Histogram（直方图）-KDE（密度图.md","raw":"title: Histogram（直方图）&KDE（密度图)\nauthor: ztq\ntags:\n\n  - python\ncategories:\n  - python基础\ndate: 2021-04-17 18:54:00\n\n---\n\n# <center>Histogram（直方图）&KDE（密度图)   </center>\n\n<h2>0 Pandas数据集的导入</h2>\n\n<b3><b>0.1 pandas导入csv/txt文件</b></b3>\n\npd.read_csv()\n\n常用参数：\nfilepath_or_buffer：文件路径（必填，其他参数按需求填写）\n\nsep：指定分隔符，默认逗号','。\n\nheader：指定第几行作为表头。默认为0（即第1行作为表头），若没有表头，需设置header=None，可以是int或list。\n\nnames：指定列的名称，用list表示，默认None。\n\nindex_col：指定行索引，可以是一列或多列，默认None。\n\nusecols：需要读取的列，可以使用列序列也可以使用列名，默认None。\n\nprefix：给列名添加前缀。如prefix=x,会出来X0,X1,....，默认None。\n\nskiprows：需要忽略的行数（从文件开始处算起)，或需要跳过的行号列表（从0开始），默认None。\n\nskipfooter：需要忽略的行数（从最后一行开始算)\n\nnrows：需要读取的行数（从文件头开始算起），默认None。\n\nencoding：编码方式，乱码时使用，默认None。\n\n例1：导入文件house-price-index-full-table.csv中数据：\n\n\n```python\nimport pandas as pd\ndf1=pd.read_csv(\"Data/house-price-index-full-table.csv\")\n#df1=pd.read_csv(\"Data/house-price2.csv\",encoding='gbk')\ndf1.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n      <th>House Price Index (seasonally adjusted)</th>\n      <th>Average price 1-bedroom flats</th>\n      <th>Average price 2-bedroom flats</th>\n      <th>Average price 2-bedroom houses</th>\n      <th>Average price 3-bedroom houses</th>\n      <th>Average price 4-bedroom houses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n      <td>99.4</td>\n      <td>166</td>\n      <td>213</td>\n      <td>265</td>\n      <td>332</td>\n      <td>416</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n      <td>98.3</td>\n      <td>160</td>\n      <td>268</td>\n      <td>268</td>\n      <td>314</td>\n      <td>432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n      <td>101.7</td>\n      <td>160</td>\n      <td>259</td>\n      <td>284</td>\n      <td>332</td>\n      <td>474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n      <td>100.9</td>\n      <td>137</td>\n      <td>242</td>\n      <td>300</td>\n      <td>333</td>\n      <td>459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n      <td>97.2</td>\n      <td>156</td>\n      <td>216</td>\n      <td>285</td>\n      <td>328</td>\n      <td>380</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\ndf2=pd.read_csv(\"Data/house-price2.csv\",usecols=[0,1,2],nrows=5,encoding='gbk')\ndf2\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n例2：导入文件data.txt中数据：\n\n\n```python\ndf3=pd.read_csv(\"Data/data.txt\",sep='\\t',header=None,names=['特征1','特征2','特征3','标签'])\ndf3\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>特征1</th>\n      <th>特征2</th>\n      <th>特征3</th>\n      <th>标签</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40920</td>\n      <td>8.326976</td>\n      <td>0.953952</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14488</td>\n      <td>7.153469</td>\n      <td>1.673904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26052</td>\n      <td>1.441871</td>\n      <td>0.805124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75136</td>\n      <td>13.147394</td>\n      <td>0.428964</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38344</td>\n      <td>1.669788</td>\n      <td>0.134296</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>72993</td>\n      <td>10.141740</td>\n      <td>1.032955</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>35948</td>\n      <td>6.830792</td>\n      <td>1.213192</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>42666</td>\n      <td>13.276369</td>\n      <td>0.543880</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>67497</td>\n      <td>8.631577</td>\n      <td>0.749278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>35483</td>\n      <td>12.273169</td>\n      <td>1.508053</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n<b3><b>0.2 pandas导入excel文件</b></b3>\n\npd.read_excel()\n\n常用参数：\nio：excel文件路径（必填，其他参数按需求填写）\n\nsheet_name：需要导入数据的工作表表名,可以是int\\string\\list，None导入所有工作表数据，默认0。\n\n参数header、names、index_col、usecols、skiprows、nrows、skip_footer、encoding的用法与pd.read_csv相同。比如读取数据时想把第一列设为index，那么只需要简单的\npd.read_csv(\"new_wordvecter.csv\",index_col=[0])\n\n这里index_col可以设为列名\n\n后续更改index可以使用df.index = df.iloc[:,\"column\"].tolist()或df.set_index('column')\n例：读取Canada.xlsx文件中的数据\n\n\n```python\ndf_canada=pd.read_excel(\"Data/Canada.xlsx\",sheet_name='Canada by Citizenship',skiprows=20,skip_footer=2)\ndf_canada.head()\ndf_India_China=df_canada[df_canada['OdName'].isin(['China','India'])]\n```\n\n\n```python\ndf_India_China.columns\n```\n\n\n\n\n    Index([    'Type', 'Coverage',   'OdName',     'AREA', 'AreaName',      'REG',\n            'RegName',      'DEV',  'DevName',       1980,       1981,       1982,\n                 1983,       1984,       1985,       1986,       1987,       1988,\n                 1989,       1990,       1991,       1992,       1993,       1994,\n                 1995,       1996,       1997,       1998,       1999,       2000,\n                 2001,       2002,       2003,       2004,       2005,       2006,\n                 2007,       2008,       2009,       2010,       2011,       2012,\n                 2013],\n          dtype='object')\n\n\n\n\n```python\ndf_India_China\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Coverage</th>\n      <th>OdName</th>\n      <th>AREA</th>\n      <th>AreaName</th>\n      <th>REG</th>\n      <th>RegName</th>\n      <th>DEV</th>\n      <th>DevName</th>\n      <th>1980</th>\n      <th>...</th>\n      <th>2004</th>\n      <th>2005</th>\n      <th>2006</th>\n      <th>2007</th>\n      <th>2008</th>\n      <th>2009</th>\n      <th>2010</th>\n      <th>2011</th>\n      <th>2012</th>\n      <th>2013</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>China</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>906</td>\n      <td>Eastern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>5123</td>\n      <td>...</td>\n      <td>36619</td>\n      <td>42584</td>\n      <td>33518</td>\n      <td>27642</td>\n      <td>30037</td>\n      <td>29622</td>\n      <td>30391</td>\n      <td>28502</td>\n      <td>33024</td>\n      <td>34129</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>India</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>5501</td>\n      <td>Southern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>8880</td>\n      <td>...</td>\n      <td>28235</td>\n      <td>36210</td>\n      <td>33848</td>\n      <td>28742</td>\n      <td>28261</td>\n      <td>29456</td>\n      <td>34235</td>\n      <td>27509</td>\n      <td>30933</td>\n      <td>33087</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 43 columns</p>\n\n</div>\n\n\n\n选取等于某些值的行记录 用 ==\n\ndf.loc[df[‘column_name’] == some_value]\n\n选取某列是否是某一类型的数值 用 isin\n\ndf.loc[df[‘column_name’].isin(some_values)]\n\n\ndf['a']#取a列\n\ndf[['a','b']]#取a、b列\n\n<h2>1 直方图及密度图的画法</h2>\n\n直方图是表示变量频率分布的一种方法，是一个可以快速展示数据概率分布的工具，直观易于理解，并深受数据爱好者的喜爱。大家平时可能见到最多就是 matplotlib，seaborn 等高级封装的库包。<br/><br/>\n\n直方图又称质量分布图，它是表示资料变化情况的一种主要工具。用直方图可以解析出资料的规则性，比较直观地看出产品质量特性的分布状态，对于资料分布状况一目了然，便于判断其总体质量分布情况。直方图表示通过沿数据范围形成分箱(bin)，然后绘制条以显示落入每个分箱的观测次数的数据分布。\n\n核密度图（kde直方图的拟合曲线）可以看作是概率密度图\n\n<b3><b>1.1 matplotlib画直方图及密度图</b></b3>\n\n<b>(1)plt.hist()\n用于画直方图。</b>\n\n参数列表：\nplt.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, normed=None)\n\nx：指定要绘制直方图的数据；输入值，这需要一个数组或者一个序列，不需要长度相同的数组。\nbins：指定直方图条形的个数；\nrange：指定直方图数据的上下界，默认包含绘图数据的最大值和最小值；\ndensity：布尔,可选。如果\"True\"，返回元组的第一个元素将会将计数标准化以形成一个概率密度，也就是说，直方图下的面积（或积分）总和为1。这是通过将计数除以数字的数量来实现的观察乘以箱子的宽度而不是除以总数数量的观察。如果叠加也是“真实”的，那么柱状图被规范化为1。(替代normed)\nweights：该参数可为每一个数据点设置权重；\ncumulative：是否需要计算累计频数或频率；\nbottom：可以为直方图的每个条形添加基准线，默认为0；\nhisttype：指定直方图的类型，默认为bar，除此还有’barstacked’, ‘step’, ‘stepfilled’；\nalign：设置条形边界值的对其方式，默认为mid，除此还有’left’和’right’；\norientation：设置直方图的摆放方向，默认为垂直方向；\nrwidth：设置直方图条形宽度的百分比；\nlog：是否需要对绘图数据进行log变换；\ncolor：设置直方图的填充色；\nlabel：设置直方图的标签，可通过legend展示其图例；\nstacked：当有多个数据时，是否需要将直方图呈堆叠摆放，默认水平摆放；\nnormed：是否将直方图的频数转换成频率；(弃用，被density替代)\nalpha：透明度，浮点数。\n返回值：\nn：直方图的值\nbins：返回各个bin的区间范围\npatches：返回每个bin的信息\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,方差/标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值，是否对数据进行归一化\nn1, bins1, patches1 = plt.hist(x1, bins=50, density=True,color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50, density=True,color='r', alpha=0.2)\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\n# plt.show()\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x21b1a893250>]\n\n\n\n\n![png](/img/output_30_12.png)\n    \n\n\n<b>(2)添加分布曲线</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值\nn1, bins1, patches1 = plt.hist(x1, bins=50,  density=True, color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50,  density=True, color='r', alpha=0.2)\n\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\nplt.show()\n```\n\n\n![png](/img/output_32_02.png)\n    \n\n通俗的说，返回的第一个值是每个直方柱的y值（取决于normed和weights的取值），\n而第二个值是每个直方柱的x值——如果分成10份，则有11个分割位置，因此返回数目为nbins+1。\n<b>(3)多类型直方图</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n#plt.rcParams['font.family']='Times New Roman'\n#plt.rcParams['axes.unicode_minus']=False\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(\"ABC\"))\nplt.show()\n```\n\n\n![png](/img/output_35_02.png)\n    \n\n\n<b>(4)添加说明信息</b>\n\n<b>添加title</b><br/><br/>\n你可以给图示或图添加标题。但是默认不支持中文，需要自己添加中文字体。\n\nwindows的字体文件放在 C:\\Windows\\Fonts，通过fontproperties设置字体，fontsize设置字体大小.\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(\"ABC\"))\n\nplt.rcParams['font.family']='SimHei'\nplt.rcParams['axes.unicode_minus']=False\nplt.rcParams['font.size']='12'\nplt.title(\"多类型直方图\")\n#songTi = matplotlib.font_manager.FontProperties(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc')\n#plt.title(\"多类型直方图\",fontproperties=songTi,fontsize=12)\nplt.show()\n```\n\n\n![png](/img/output_38_02.png)\n    \n\n\n<b>添加文字、网格、轴标签及轴范围</b>\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, color='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')  #(x,y,str,...)r raw string \nplt.xlim(40, 160)\nplt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()\n```\n\n\n![png](/img/output_40_02.png)\n    \n\n\n<b3><b>1.2 pandas画直方图及密度图</b></b3>\n\n<b>（1）pandas的plot方法</b>\n\npandas默认的 plot 方法可以帮助我们快速地绘制各种图形<br/>\nPandas 提供了 plot() 方法可以快速方便地将 Series 和 DataFrame 中的数据进行可视化, 它是 matplotlib.axes.Axes.plot 的封装。代码执行后会生成一张图片，并直接显示在 notebook 上。<br/>\n\n<b>基本使用</b>\nplot 默认为折线图，折线图也是最常用和最基础的可视化图形，足以满足我们日常 80% 的需求：\n\ndf.plot()\ns.plot()\n\n\n我们可以在 plot 后增加调用来使用其他的图形，当然这些图形对数据结构也有自己的要求：\n\ndf.plot.line() # 折线的全写方式\ndf.plot.bar() # 柱状图\ndf.plot.barh() # 横向柱状图 （条形图）\ndf.plot.hist() # 直方图\ndf.plot.box() # 箱形图\ndf.plot.kde() # 核密度估计图\ndf.plot.density() # 同 df.plot.kde()\ndf.plot.area() # 面积图\ndf.plot.pie() # 饼图\ndf.plot.scatter() # 散点图\ndf.plot.hexbin() # 六边形箱体图，或简称六边形图\n<b>使用方法</b>\n\n<b>Series 使用</b>\n\n使用 plot 时 x 轴为索引，y 轴为索引对应的具体值：\n\n\n```python\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(20),\n               index=pd.date_range('1/1/2000', periods=20)\n              )\n\nts.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd7ad0d00>\n\n\n\n\n![png](/img/output_49_12.png)\n    \n\n\n<b>DataFrame 使用</b><br/>\n\nDataFrame 使用 plot 时 x 轴为索引，y 轴为索引对应的多个具体值：\n\n\n```python\ndf = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd6a56610>\n\n\n\n\n![png](/img/output_51_12.png)\n    \n\n\n<b>plot 绘图时常用的一些绘图参数。</b><br/>\n\n图形类型\ndf.plot() 可以通过参数来指定具体图形类型：\n\ndf.plot(kind='pie') # 其他的名称和上文相同\ns.plot(kind='pie')\n直方图kind=\"hist\"\n密度图kind=\"kde\"\n生成或读取数据后，调用pandas的plot方法画图。默认情况下参数 kind=\"line\" 表示图的类型为折线图。<br/>\n\n```python\ndf = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot(kind='hist',title='this is a hist',grid=True)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x21b1ba68460>\n\n\n\n\n![png](/img/output_55_12.png)\n    \n\n\n图的标题：\n\n\n```python\ndf.plot(title='my plot')\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd5f8df10>\n\n\n\n\n![png](/img/output_57_12.png)\n    \n\n\n字体大小\n\n\n```python\n指定轴上的字体大小\n```\n\n\n```python\ndf.plot(fontsize=15)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd601e7c0>\n\n\n\n\n![png](/img/output_60_12.png)\n    \n\n\n线条样式\nstyle 可指定图的线条等样式，可参考可选的值 Matplotlib Line-style：\ndf[:5].plot(style=':') # 虚线\ndf[:5].plot(style='-.') # 虚实相间\ndf[:5].plot(style='--') # 长虚线\ndf[:5].plot(style='-') # 实线（默认）\ndf[:5].plot(style='.') # 点\ndf[:5].plot(style='*-') # 实线，数值为星星\ndf[:5].plot(style='^-') # 实线，数值为三角形\n\n```python\ndf.plot(style=':') # 虚线\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd603bd30>\n\n\n\n\n![png](/img/output_63_12.png)\n    \n\n\n对不同线分别给样式：\n\n\n```python\ndf[:5].plot(style=[':', '--', '.-', '*-'])\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd601a250>\n\n\n\n\n![png](/img/output_65_12.png)\n    \n\n\n背景辅助线\n\n\n```python\ngrid 会给 x 方向和 y 方向增加灰色辅助线：\n```\n\n\n```python\ndf.plot(grid=True)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd6162fd0>\n\n\n\n\n![png](/img/output_68_12.png)\n    \n\n\n图例\n\n\n```python\ndf.plot(legend=False)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd61e5cd0>\n\n\n\n\n![png](/img/output_70_12.png)\n    \n\n\n可以反向排序图例：\n\n\n```python\ndf.plot(legend='reverse')\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fd5e02bb0>\n\n\n\n\n![png](/img/output_72_12.png)\n    \n\n\n<b>（2）Pandas画直方图和密度图</b>\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv('Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nTitanic.Age.plot(kind = 'hist', bins = 20, color = 'steelblue', edgecolor = 'black', density = True, label = '直方图')\n# 绘制核密度图\nTitanic.Age.plot(kind = 'kde', color = 'red', label = '核密度图')\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n    <class 'pandas.core.series.Series'>\n\n\n\n\n![png](/img/output_74_12.png)\n    \n\n\n<b3><b>1.3 seaborn画直方图及密度图</b></b3>\n\n尽管上一幅图满足了两种图形的合成，但其表达的是所有乘客的年龄分布，如果按性别分组，研究不同性别下年龄分布的差异，该如何实现？针对这个问题，使用matplotlib模块或pandas模块都会稍微复杂一些，推荐使用seaborn模块中的distplot函数，因为该函数的代码简洁而易懂。关于该函数的语法和参数含义如下：\nsns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None,\n\t         hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None,\n\t         color=None, vertical=False, norm_hist=False, axlabel=None,\n\t         label=None, ax=None)a：指定绘图数据，可以是序列、一维数组或列表。\nbins：指定直方图条形的个数。\nhist：bool类型的参数，是否绘制直方图，默认为True。\nkde：bool类型的参数，是否绘制核密度图，默认为True。\nrug：bool类型的参数，是否绘制须图（如果数据比较密集，该参数比较有用），默认为False。\nfit：指定一个随机分布对象（需调用scipy模块中的随机分布函数），用于绘制随机分布的概率密度曲线。\nhist_kws：以字典形式传递直方图的其他修饰属性，如填充色、边框色、宽度等。\nkde_kws：以字典形式传递核密度图的其他修饰属性，如线的颜色、线的类型等。\nrug_kws：以字典形式传递须图的其他修饰属性，如线的颜色、线的宽度等。\nfit_kws：以字典形式传递概率密度曲线的其他修饰属性，如线条颜色、形状、宽度等。\ncolor：指定图形的颜色，除了随机分布曲线的颜色。\nvertical：bool类型的参数，是否将图形垂直显示，默认为True。（改为False即为horizontal）\nnorm_hist：bool类型的参数，是否将频数更改为频率，默认为False。\naxlabel：用于显示轴标签。 label：指定图形的图例，需结合plt.legend()一起使用。\nax：指定子图的位置。\n从函数的参数可知，通过该函数，可以实现三种图形的合成，分别是直方图（hist参数）、核密度曲线（kde参数）以及指定的理论分布密度曲线（fit参数）。接下来，针对如上介绍的distplot函数，绘制不同性别下乘客的年龄分布图，具体代码如下：\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = {'color':'steelblue'}, label = '男性')\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = {'color':'purple'}, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄直方图')\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = {'color':'red', 'linestyle':'-'},\n             norm_hist = True, label = '男性')\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = {'color':'black', 'linestyle':'--'},\n             norm_hist = True, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄核密度图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_79_02.png)\n    \n\n\n\n\n![png](/img/output_79_12.png)\n    \n\n\n为了避免四个图形混在一起不易发现数据背后的特征，将直方图与核密度图分开绘制。从直方图来看，女性年龄的分布明显比男性矮，说明在各年龄段下，男性乘客要比女性乘客多；再看核密度图，男女性别的年龄分布趋势比较接近，说明各年龄段下的男女乘客人数同步增加或减少。\n\n把两种图合在一起画：\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = {'color':'steelblue'},\n             label = ('男性','直方图'),norm_hist=True)\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = {'color':'purple'},\n             label = ('女性','直方图'),norm_hist=True)\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = {'color':'red', 'linestyle':'-'},\n             norm_hist = True, label = ('男性','核密度图'))\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = {'color':'black', 'linestyle':'--'},\n             norm_hist = True, label = ('女性','核密度图'))\nplt.title('泰坦尼克号男女乘客的年龄分布图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_81_02.png)\n    \n\n\n<b>（1）seaborn直方图</b>\n\nseaborn的kdeplot函数专门用于画核密度估计图.\n\n<b>（2）seaborn密度图</b>\n\nseaborn的kdeplot函数专门用于画核密度估计图.\n\n<h2>2 课堂练习</h2>\n\n<b>（1）直方图</b>\n\n以Titanic数据集为例绘制乘客的年龄直方图\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nplt.hist(x = Titanic.Age, # 指定绘图数据\n         bins = 20, # 指定直方图中条块的个数\n         color = 'steelblue', # 指定直方图的填充色\n         edgecolor = 'black' # 指定直方图的边框色\n         )\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图形\nplt.show()\n```\n\n\n![png](/img/output_89_02.png)\n    \n\n\n<h2>3 直方图与条形图的区别</h2>\n\n首先，条形图是用条形的长度表示各类别频数的多少，其宽度（表示类别）则是固定的；\n\n      直方图是用面积表示各组频数的多少，矩形的高度表示每一组的频数或频率，宽度则表示各组的组距，因此其高度与宽度均有意义。\n\n其次，由于分组数据具有连续性，直方图的各矩形通常是连续排列，而条形图则是分开排列。\n\n最后，条形图主要用于展示分类数据，而直方图则主要用于展示数据型数据<br/><br/>\n\n原文链接：https://blog.csdn.net/xjl271314/article/details/80295935\n\n<h2>附录1-本小节用到的以前的知识复习</h2>\n<h3>1)批量注释/去掉注释</h3> \n\nCtrl+/\n\n->常常出现在python函数定义的函数名后面，为函数添加元数据,描述函数的返回类型，从而方便开发人员使用。\ndef add(x, y) -> int:\n  return x+y\n\n<h3>2)中文字体设置</h3>\n\nhttps://blog.csdn.net/dxawdc/article/details/110311549\n\n<h3>3)函数定义</h3>\n\n```python\ns = '杰瑞你好啊'\ndef mylen():\n    n = 0\n    for i in s :\n        n += 1\n    print(n)\nmylen()\n#len1 = mylen()\n#print(len1)\n```\n\n    5\n\n\n<h2>附录2-错误提示及解决办法</h2>\n<h3>1) 8722 missing from current font, matplotlib画图</h3>\n\n负号问题，添加语句plt.rcParams['axes.unicode_minus']=False\n\n更多请参考https://blog.csdn.net/seeker3/article/details/108432781?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs\n\n\n```python\n\n```","slug":"Histogram（直方图）-KDE（密度图","published":1,"updated":"2022-04-04T08:32:40.143Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2400f57kt959xqbw35","content":"<h1><center>Histogram（直方图）&amp;KDE（密度图)   </center></h1>\n<h2>0 Pandas数据集的导入</h2>\n<p><b3><b>0.1 pandas导入csv/txt文件</b></b3></p>\n<p>pd.read_csv()</p>\n<p>常用参数：<br>\nfilepath_or_buffer：文件路径（必填，其他参数按需求填写）</p>\n<p>sep：指定分隔符，默认逗号’,'。</p>\n<p>header：指定第几行作为表头。默认为0（即第1行作为表头），若没有表头，需设置header=None，可以是int或list。</p>\n<p>names：指定列的名称，用list表示，默认None。</p>\n<p>index_col：指定行索引，可以是一列或多列，默认None。</p>\n<p>usecols：需要读取的列，可以使用列序列也可以使用列名，默认None。</p>\n<p>prefix：给列名添加前缀。如prefix=x,会出来X0,X1,…，默认None。</p>\n<p>skiprows：需要忽略的行数（从文件开始处算起)，或需要跳过的行号列表（从0开始），默认None。</p>\n<p>skipfooter：需要忽略的行数（从最后一行开始算)</p>\n<p>nrows：需要读取的行数（从文件头开始算起），默认None。</p>\n<p>encoding：编码方式，乱码时使用，默认None。</p>\n<p>例1：导入文件house-price-index-full-table.csv中数据：</p>\n<pre><code class=\"language-python\">import pandas as pd\ndf1=pd.read_csv(&quot;Data/house-price-index-full-table.csv&quot;)\n#df1=pd.read_csv(&quot;Data/house-price2.csv&quot;,encoding='gbk')\ndf1.head()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n      <th>House Price Index (seasonally adjusted)</th>\n      <th>Average price 1-bedroom flats</th>\n      <th>Average price 2-bedroom flats</th>\n      <th>Average price 2-bedroom houses</th>\n      <th>Average price 3-bedroom houses</th>\n      <th>Average price 4-bedroom houses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n      <td>99.4</td>\n      <td>166</td>\n      <td>213</td>\n      <td>265</td>\n      <td>332</td>\n      <td>416</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n      <td>98.3</td>\n      <td>160</td>\n      <td>268</td>\n      <td>268</td>\n      <td>314</td>\n      <td>432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n      <td>101.7</td>\n      <td>160</td>\n      <td>259</td>\n      <td>284</td>\n      <td>332</td>\n      <td>474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n      <td>100.9</td>\n      <td>137</td>\n      <td>242</td>\n      <td>300</td>\n      <td>333</td>\n      <td>459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n      <td>97.2</td>\n      <td>156</td>\n      <td>216</td>\n      <td>285</td>\n      <td>328</td>\n      <td>380</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\">df2=pd.read_csv(&quot;Data/house-price2.csv&quot;,usecols=[0,1,2],nrows=5,encoding='gbk')\ndf2\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>例2：导入文件data.txt中数据：</p>\n<pre><code class=\"language-python\">df3=pd.read_csv(&quot;Data/data.txt&quot;,sep='\\t',header=None,names=['特征1','特征2','特征3','标签'])\ndf3\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>特征1</th>\n      <th>特征2</th>\n      <th>特征3</th>\n      <th>标签</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40920</td>\n      <td>8.326976</td>\n      <td>0.953952</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14488</td>\n      <td>7.153469</td>\n      <td>1.673904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26052</td>\n      <td>1.441871</td>\n      <td>0.805124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75136</td>\n      <td>13.147394</td>\n      <td>0.428964</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38344</td>\n      <td>1.669788</td>\n      <td>0.134296</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>72993</td>\n      <td>10.141740</td>\n      <td>1.032955</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>35948</td>\n      <td>6.830792</td>\n      <td>1.213192</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>42666</td>\n      <td>13.276369</td>\n      <td>0.543880</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>67497</td>\n      <td>8.631577</td>\n      <td>0.749278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>35483</td>\n      <td>12.273169</td>\n      <td>1.508053</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p><b3><b>0.2 pandas导入excel文件</b></b3></p>\n<p>pd.read_excel()</p>\n<p>常用参数：<br>\nio：excel文件路径（必填，其他参数按需求填写）</p>\n<p>sheet_name：需要导入数据的工作表表名,可以是int\\string\\list，None导入所有工作表数据，默认0。</p>\n<p>参数header、names、index_col、usecols、skiprows、nrows、skip_footer、encoding的用法与pd.read_csv相同。比如读取数据时想把第一列设为index，那么只需要简单的<br>\npd.read_csv(“new_wordvecter.csv”,index_col=[0])</p>\n<p>这里index_col可以设为列名</p>\n<p>后续更改index可以使用df.index = df.iloc[:,“column”].tolist()或df.set_index(‘column’)<br>\n例：读取Canada.xlsx文件中的数据</p>\n<pre><code class=\"language-python\">df_canada=pd.read_excel(&quot;Data/Canada.xlsx&quot;,sheet_name='Canada by Citizenship',skiprows=20,skip_footer=2)\ndf_canada.head()\ndf_India_China=df_canada[df_canada['OdName'].isin(['China','India'])]\n</code></pre>\n<pre><code class=\"language-python\">df_India_China.columns\n</code></pre>\n<pre><code>Index([    'Type', 'Coverage',   'OdName',     'AREA', 'AreaName',      'REG',\n        'RegName',      'DEV',  'DevName',       1980,       1981,       1982,\n             1983,       1984,       1985,       1986,       1987,       1988,\n             1989,       1990,       1991,       1992,       1993,       1994,\n             1995,       1996,       1997,       1998,       1999,       2000,\n             2001,       2002,       2003,       2004,       2005,       2006,\n             2007,       2008,       2009,       2010,       2011,       2012,\n             2013],\n      dtype='object')\n</code></pre>\n<pre><code class=\"language-python\">df_India_China\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Coverage</th>\n      <th>OdName</th>\n      <th>AREA</th>\n      <th>AreaName</th>\n      <th>REG</th>\n      <th>RegName</th>\n      <th>DEV</th>\n      <th>DevName</th>\n      <th>1980</th>\n      <th>...</th>\n      <th>2004</th>\n      <th>2005</th>\n      <th>2006</th>\n      <th>2007</th>\n      <th>2008</th>\n      <th>2009</th>\n      <th>2010</th>\n      <th>2011</th>\n      <th>2012</th>\n      <th>2013</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>China</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>906</td>\n      <td>Eastern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>5123</td>\n      <td>...</td>\n      <td>36619</td>\n      <td>42584</td>\n      <td>33518</td>\n      <td>27642</td>\n      <td>30037</td>\n      <td>29622</td>\n      <td>30391</td>\n      <td>28502</td>\n      <td>33024</td>\n      <td>34129</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>India</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>5501</td>\n      <td>Southern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>8880</td>\n      <td>...</td>\n      <td>28235</td>\n      <td>36210</td>\n      <td>33848</td>\n      <td>28742</td>\n      <td>28261</td>\n      <td>29456</td>\n      <td>34235</td>\n      <td>27509</td>\n      <td>30933</td>\n      <td>33087</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 43 columns</p>\n</div>\n<p>选取等于某些值的行记录 用 ==</p>\n<p>df.loc[df[‘column_name’] == some_value]</p>\n<p>选取某列是否是某一类型的数值 用 isin</p>\n<p>df.loc[df[‘column_name’].isin(some_values)]</p>\n<p>df[‘a’]#取a列</p>\n<p>df[[‘a’,‘b’]]#取a、b列</p>\n<h2>1 直方图及密度图的画法</h2>\n<p>直方图是表示变量频率分布的一种方法，是一个可以快速展示数据概率分布的工具，直观易于理解，并深受数据爱好者的喜爱。大家平时可能见到最多就是 matplotlib，seaborn 等高级封装的库包。<br/><br/></p>\n<p>直方图又称质量分布图，它是表示资料变化情况的一种主要工具。用直方图可以解析出资料的规则性，比较直观地看出产品质量特性的分布状态，对于资料分布状况一目了然，便于判断其总体质量分布情况。直方图表示通过沿数据范围形成分箱(bin)，然后绘制条以显示落入每个分箱的观测次数的数据分布。</p>\n<p>核密度图（kde直方图的拟合曲线）可以看作是概率密度图</p>\n<p><b3><b>1.1 matplotlib画直方图及密度图</b></b3></p>\n<p><b>(1)plt.hist()<br>\n用于画直方图。</b></p>\n<p>参数列表：<br>\nplt.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False, bottom=None, histtype=‘bar’, align=‘mid’, orientation=‘vertical’, rwidth=None, log=False, color=None, label=None, stacked=False, normed=None)</p>\n<p>x：指定要绘制直方图的数据；输入值，这需要一个数组或者一个序列，不需要长度相同的数组。<br>\nbins：指定直方图条形的个数；<br>\nrange：指定直方图数据的上下界，默认包含绘图数据的最大值和最小值；<br>\ndensity：布尔,可选。如果&quot;True&quot;，返回元组的第一个元素将会将计数标准化以形成一个概率密度，也就是说，直方图下的面积（或积分）总和为1。这是通过将计数除以数字的数量来实现的观察乘以箱子的宽度而不是除以总数数量的观察。如果叠加也是“真实”的，那么柱状图被规范化为1。(替代normed)<br>\nweights：该参数可为每一个数据点设置权重；<br>\ncumulative：是否需要计算累计频数或频率；<br>\nbottom：可以为直方图的每个条形添加基准线，默认为0；<br>\nhisttype：指定直方图的类型，默认为bar，除此还有’barstacked’, ‘step’, ‘stepfilled’；<br>\nalign：设置条形边界值的对其方式，默认为mid，除此还有’left’和’right’；<br>\norientation：设置直方图的摆放方向，默认为垂直方向；<br>\nrwidth：设置直方图条形宽度的百分比；<br>\nlog：是否需要对绘图数据进行log变换；<br>\ncolor：设置直方图的填充色；<br>\nlabel：设置直方图的标签，可通过legend展示其图例；<br>\nstacked：当有多个数据时，是否需要将直方图呈堆叠摆放，默认水平摆放；<br>\nnormed：是否将直方图的频数转换成频率；(弃用，被density替代)<br>\nalpha：透明度，浮点数。<br>\n返回值：<br>\nn：直方图的值<br>\nbins：返回各个bin的区间范围<br>\npatches：返回每个bin的信息</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,方差/标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值，是否对数据进行归一化\nn1, bins1, patches1 = plt.hist(x1, bins=50, density=True,color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50, density=True,color='r', alpha=0.2)\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\n# plt.show()\n</code></pre>\n<pre><code>[&lt;matplotlib.lines.Line2D at 0x21b1a893250&gt;]\n</code></pre>\n<p><img src=\"/img/output_30_12.png\" alt=\"png\"></p>\n<p><b>(2)添加分布曲线</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值\nn1, bins1, patches1 = plt.hist(x1, bins=50,  density=True, color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50,  density=True, color='r', alpha=0.2)\n\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_32_02.png\" alt=\"png\"></p>\n<p>通俗的说，返回的第一个值是每个直方柱的y值（取决于normed和weights的取值），<br>\n而第二个值是每个直方柱的x值——如果分成10份，则有11个分割位置，因此返回数目为nbins+1。<br>\n<b>(3)多类型直方图</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n#plt.rcParams['font.family']='Times New Roman'\n#plt.rcParams['axes.unicode_minus']=False\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(&quot;ABC&quot;))\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_35_02.png\" alt=\"png\"></p>\n<p><b>(4)添加说明信息</b></p>\n<p><b>添加title</b><br/><br/><br>\n你可以给图示或图添加标题。但是默认不支持中文，需要自己添加中文字体。</p>\n<p>windows的字体文件放在 C:\\Windows\\Fonts，通过fontproperties设置字体，fontsize设置字体大小.</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(&quot;ABC&quot;))\n\nplt.rcParams['font.family']='SimHei'\nplt.rcParams['axes.unicode_minus']=False\nplt.rcParams['font.size']='12'\nplt.title(&quot;多类型直方图&quot;)\n#songTi = matplotlib.font_manager.FontProperties(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc')\n#plt.title(&quot;多类型直方图&quot;,fontproperties=songTi,fontsize=12)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_38_02.png\" alt=\"png\"></p>\n<p><b>添加文字、网格、轴标签及轴范围</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, color='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')  #(x,y,str,...)r raw string \nplt.xlim(40, 160)\nplt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_02.png\" alt=\"png\"></p>\n<p><b3><b>1.2 pandas画直方图及密度图</b></b3></p>\n<p><b>（1）pandas的plot方法</b></p>\n<p>pandas默认的 plot 方法可以帮助我们快速地绘制各种图形<br/><br>\nPandas 提供了 plot() 方法可以快速方便地将 Series 和 DataFrame 中的数据进行可视化, 它是 matplotlib.axes.Axes.plot 的封装。代码执行后会生成一张图片，并直接显示在 notebook 上。<br/></p>\n<p><b>基本使用</b><br>\nplot 默认为折线图，折线图也是最常用和最基础的可视化图形，足以满足我们日常 80% 的需求：</p>\n<p>df.plot()<br>\ns.plot()</p>\n<p>我们可以在 plot 后增加调用来使用其他的图形，当然这些图形对数据结构也有自己的要求：</p>\n<p>df.plot.line() # 折线的全写方式<br>\ndf.plot.bar() # 柱状图<br>\ndf.plot.barh() # 横向柱状图 （条形图）<br>\ndf.plot.hist() # 直方图<br>\ndf.plot.box() # 箱形图<br>\ndf.plot.kde() # 核密度估计图<br>\ndf.plot.density() # 同 df.plot.kde()<br>\ndf.plot.area() # 面积图<br>\ndf.plot.pie() # 饼图<br>\ndf.plot.scatter() # 散点图<br>\ndf.plot.hexbin() # 六边形箱体图，或简称六边形图<br>\n<b>使用方法</b></p>\n<p><b>Series 使用</b></p>\n<p>使用 plot 时 x 轴为索引，y 轴为索引对应的具体值：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(20),\n               index=pd.date_range('1/1/2000', periods=20)\n              )\n\nts.plot()\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd7ad0d00&gt;\n</code></pre>\n<p><img src=\"/img/output_49_12.png\" alt=\"png\"></p>\n<p><b>DataFrame 使用</b><br/></p>\n<p>DataFrame 使用 plot 时 x 轴为索引，y 轴为索引对应的多个具体值：</p>\n<pre><code class=\"language-python\">df = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot()\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd6a56610&gt;\n</code></pre>\n<p><img src=\"/img/output_51_12.png\" alt=\"png\"></p>\n<p><b>plot 绘图时常用的一些绘图参数。</b><br/></p>\n<p>图形类型<br>\ndf.plot() 可以通过参数来指定具体图形类型：</p>\n<p>df.plot(kind=‘pie’) # 其他的名称和上文相同<br>\ns.plot(kind=‘pie’)<br>\n直方图kind=“hist”<br>\n密度图kind=“kde”<br>\n生成或读取数据后，调用pandas的plot方法画图。默认情况下参数 kind=“line” 表示图的类型为折线图。<br/></p>\n<pre><code class=\"language-python\">df = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot(kind='hist',title='this is a hist',grid=True)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x21b1ba68460&gt;\n</code></pre>\n<p><img src=\"/img/output_55_12.png\" alt=\"png\"></p>\n<p>图的标题：</p>\n<pre><code class=\"language-python\">df.plot(title='my plot')\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd5f8df10&gt;\n</code></pre>\n<p><img src=\"/img/output_57_12.png\" alt=\"png\"></p>\n<p>字体大小</p>\n<pre><code class=\"language-python\">指定轴上的字体大小\n</code></pre>\n<pre><code class=\"language-python\">df.plot(fontsize=15)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd601e7c0&gt;\n</code></pre>\n<p><img src=\"/img/output_60_12.png\" alt=\"png\"></p>\n<p>线条样式<br>\nstyle 可指定图的线条等样式，可参考可选的值 Matplotlib Line-style：<br>\ndf[:5].plot(style=‘:’) # 虚线<br>\ndf[:5].plot(style=‘-.’) # 虚实相间<br>\ndf[:5].plot(style=‘–’) # 长虚线<br>\ndf[:5].plot(style=‘-’) # 实线（默认）<br>\ndf[:5].plot(style=‘.’) # 点<br>\ndf[:5].plot(style=‘*-’) # 实线，数值为星星<br>\ndf[:5].plot(style=‘^-’) # 实线，数值为三角形</p>\n<pre><code class=\"language-python\">df.plot(style=':') # 虚线\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd603bd30&gt;\n</code></pre>\n<p><img src=\"/img/output_63_12.png\" alt=\"png\"></p>\n<p>对不同线分别给样式：</p>\n<pre><code class=\"language-python\">df[:5].plot(style=[':', '--', '.-', '*-'])\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd601a250&gt;\n</code></pre>\n<p><img src=\"/img/output_65_12.png\" alt=\"png\"></p>\n<p>背景辅助线</p>\n<pre><code class=\"language-python\">grid 会给 x 方向和 y 方向增加灰色辅助线：\n</code></pre>\n<pre><code class=\"language-python\">df.plot(grid=True)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd6162fd0&gt;\n</code></pre>\n<p><img src=\"/img/output_68_12.png\" alt=\"png\"></p>\n<p>图例</p>\n<pre><code class=\"language-python\">df.plot(legend=False)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd61e5cd0&gt;\n</code></pre>\n<p><img src=\"/img/output_70_12.png\" alt=\"png\"></p>\n<p>可以反向排序图例：</p>\n<pre><code class=\"language-python\">df.plot(legend='reverse')\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd5e02bb0&gt;\n</code></pre>\n<p><img src=\"/img/output_72_12.png\" alt=\"png\"></p>\n<p><b>（2）Pandas画直方图和密度图</b></p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv('Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nTitanic.Age.plot(kind = 'hist', bins = 20, color = 'steelblue', edgecolor = 'black', density = True, label = '直方图')\n# 绘制核密度图\nTitanic.Age.plot(kind = 'kde', color = 'red', label = '核密度图')\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n<p><img src=\"/img/output_74_12.png\" alt=\"png\"></p>\n<p><b3><b>1.3 seaborn画直方图及密度图</b></b3></p>\n<p>尽管上一幅图满足了两种图形的合成，但其表达的是所有乘客的年龄分布，如果按性别分组，研究不同性别下年龄分布的差异，该如何实现？针对这个问题，使用matplotlib模块或pandas模块都会稍微复杂一些，推荐使用seaborn模块中的distplot函数，因为该函数的代码简洁而易懂。关于该函数的语法和参数含义如下：<br>\nsns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None,<br>\nhist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None,<br>\ncolor=None, vertical=False, norm_hist=False, axlabel=None,<br>\nlabel=None, ax=None)a：指定绘图数据，可以是序列、一维数组或列表。<br>\nbins：指定直方图条形的个数。<br>\nhist：bool类型的参数，是否绘制直方图，默认为True。<br>\nkde：bool类型的参数，是否绘制核密度图，默认为True。<br>\nrug：bool类型的参数，是否绘制须图（如果数据比较密集，该参数比较有用），默认为False。<br>\nfit：指定一个随机分布对象（需调用scipy模块中的随机分布函数），用于绘制随机分布的概率密度曲线。<br>\nhist_kws：以字典形式传递直方图的其他修饰属性，如填充色、边框色、宽度等。<br>\nkde_kws：以字典形式传递核密度图的其他修饰属性，如线的颜色、线的类型等。<br>\nrug_kws：以字典形式传递须图的其他修饰属性，如线的颜色、线的宽度等。<br>\nfit_kws：以字典形式传递概率密度曲线的其他修饰属性，如线条颜色、形状、宽度等。<br>\ncolor：指定图形的颜色，除了随机分布曲线的颜色。<br>\nvertical：bool类型的参数，是否将图形垂直显示，默认为True。（改为False即为horizontal）<br>\nnorm_hist：bool类型的参数，是否将频数更改为频率，默认为False。<br>\naxlabel：用于显示轴标签。 label：指定图形的图例，需结合plt.legend()一起使用。<br>\nax：指定子图的位置。<br>\n从函数的参数可知，通过该函数，可以实现三种图形的合成，分别是直方图（hist参数）、核密度曲线（kde参数）以及指定的理论分布密度曲线（fit参数）。接下来，针对如上介绍的distplot函数，绘制不同性别下乘客的年龄分布图，具体代码如下：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = &#123;'color':'steelblue'&#125;, label = '男性')\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = &#123;'color':'purple'&#125;, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄直方图')\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = &#123;'color':'red', 'linestyle':'-'&#125;,\n             norm_hist = True, label = '男性')\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = &#123;'color':'black', 'linestyle':'--'&#125;,\n             norm_hist = True, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄核密度图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_79_02.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_79_12.png\" alt=\"png\"></p>\n<p>为了避免四个图形混在一起不易发现数据背后的特征，将直方图与核密度图分开绘制。从直方图来看，女性年龄的分布明显比男性矮，说明在各年龄段下，男性乘客要比女性乘客多；再看核密度图，男女性别的年龄分布趋势比较接近，说明各年龄段下的男女乘客人数同步增加或减少。</p>\n<p>把两种图合在一起画：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = &#123;'color':'steelblue'&#125;,\n             label = ('男性','直方图'),norm_hist=True)\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = &#123;'color':'purple'&#125;,\n             label = ('女性','直方图'),norm_hist=True)\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = &#123;'color':'red', 'linestyle':'-'&#125;,\n             norm_hist = True, label = ('男性','核密度图'))\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = &#123;'color':'black', 'linestyle':'--'&#125;,\n             norm_hist = True, label = ('女性','核密度图'))\nplt.title('泰坦尼克号男女乘客的年龄分布图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_81_02.png\" alt=\"png\"></p>\n<p><b>（1）seaborn直方图</b></p>\n<p>seaborn的kdeplot函数专门用于画核密度估计图.</p>\n<p><b>（2）seaborn密度图</b></p>\n<p>seaborn的kdeplot函数专门用于画核密度估计图.</p>\n<h2>2 课堂练习</h2>\n<p><b>（1）直方图</b></p>\n<p>以Titanic数据集为例绘制乘客的年龄直方图</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nplt.hist(x = Titanic.Age, # 指定绘图数据\n         bins = 20, # 指定直方图中条块的个数\n         color = 'steelblue', # 指定直方图的填充色\n         edgecolor = 'black' # 指定直方图的边框色\n         )\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_89_02.png\" alt=\"png\"></p>\n<h2>3 直方图与条形图的区别</h2>\n<p>首先，条形图是用条形的长度表示各类别频数的多少，其宽度（表示类别）则是固定的；</p>\n<pre><code>  直方图是用面积表示各组频数的多少，矩形的高度表示每一组的频数或频率，宽度则表示各组的组距，因此其高度与宽度均有意义。\n</code></pre>\n<p>其次，由于分组数据具有连续性，直方图的各矩形通常是连续排列，而条形图则是分开排列。</p>\n<p>最后，条形图主要用于展示分类数据，而直方图则主要用于展示数据型数据<br/><br/></p>\n<p>原文链接：<a href=\"https://blog.csdn.net/xjl271314/article/details/80295935\">https://blog.csdn.net/xjl271314/article/details/80295935</a></p>\n<h2>附录1-本小节用到的以前的知识复习</h2>\n<h3>1)批量注释/去掉注释</h3> \n<p>Ctrl+/</p>\n<p>-&gt;常常出现在python函数定义的函数名后面，为函数添加元数据,描述函数的返回类型，从而方便开发人员使用。<br>\ndef add(x, y) -&gt; int:<br>\nreturn x+y</p>\n<h3>2)中文字体设置</h3>\n<p><a href=\"https://blog.csdn.net/dxawdc/article/details/110311549\">https://blog.csdn.net/dxawdc/article/details/110311549</a></p>\n<h3>3)函数定义</h3>\n<pre><code class=\"language-python\">s = '杰瑞你好啊'\ndef mylen():\n    n = 0\n    for i in s :\n        n += 1\n    print(n)\nmylen()\n#len1 = mylen()\n#print(len1)\n</code></pre>\n<pre><code>5\n</code></pre>\n<h2>附录2-错误提示及解决办法</h2>\n<h3>1) 8722 missing from current font, matplotlib画图</h3>\n<p>负号问题，添加语句plt.rcParams[‘axes.unicode_minus’]=False</p>\n<p>更多请参考https://blog.csdn.net/seeker3/article/details/108432781?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs</p>\n<pre><code class=\"language-python\">\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1><center>Histogram（直方图）&amp;KDE（密度图)   </center></h1>\n<h2>0 Pandas数据集的导入</h2>\n<p><b3><b>0.1 pandas导入csv/txt文件</b></b3></p>\n<p>pd.read_csv()</p>\n<p>常用参数：<br>\nfilepath_or_buffer：文件路径（必填，其他参数按需求填写）</p>\n<p>sep：指定分隔符，默认逗号’,'。</p>\n<p>header：指定第几行作为表头。默认为0（即第1行作为表头），若没有表头，需设置header=None，可以是int或list。</p>\n<p>names：指定列的名称，用list表示，默认None。</p>\n<p>index_col：指定行索引，可以是一列或多列，默认None。</p>\n<p>usecols：需要读取的列，可以使用列序列也可以使用列名，默认None。</p>\n<p>prefix：给列名添加前缀。如prefix=x,会出来X0,X1,…，默认None。</p>\n<p>skiprows：需要忽略的行数（从文件开始处算起)，或需要跳过的行号列表（从0开始），默认None。</p>\n<p>skipfooter：需要忽略的行数（从最后一行开始算)</p>\n<p>nrows：需要读取的行数（从文件头开始算起），默认None。</p>\n<p>encoding：编码方式，乱码时使用，默认None。</p>\n<p>例1：导入文件house-price-index-full-table.csv中数据：</p>\n<pre><code class=\"language-python\">import pandas as pd\ndf1=pd.read_csv(&quot;Data/house-price-index-full-table.csv&quot;)\n#df1=pd.read_csv(&quot;Data/house-price2.csv&quot;,encoding='gbk')\ndf1.head()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n      <th>House Price Index (seasonally adjusted)</th>\n      <th>Average price 1-bedroom flats</th>\n      <th>Average price 2-bedroom flats</th>\n      <th>Average price 2-bedroom houses</th>\n      <th>Average price 3-bedroom houses</th>\n      <th>Average price 4-bedroom houses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n      <td>99.4</td>\n      <td>166</td>\n      <td>213</td>\n      <td>265</td>\n      <td>332</td>\n      <td>416</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n      <td>98.3</td>\n      <td>160</td>\n      <td>268</td>\n      <td>268</td>\n      <td>314</td>\n      <td>432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n      <td>101.7</td>\n      <td>160</td>\n      <td>259</td>\n      <td>284</td>\n      <td>332</td>\n      <td>474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n      <td>100.9</td>\n      <td>137</td>\n      <td>242</td>\n      <td>300</td>\n      <td>333</td>\n      <td>459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n      <td>97.2</td>\n      <td>156</td>\n      <td>216</td>\n      <td>285</td>\n      <td>328</td>\n      <td>380</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\">df2=pd.read_csv(&quot;Data/house-price2.csv&quot;,usecols=[0,1,2],nrows=5,encoding='gbk')\ndf2\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Quarter</th>\n      <th>Quarter Start Date</th>\n      <th>House Price Index (non seasonally adjusted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q1 2002</td>\n      <td>01/01/2002</td>\n      <td>96.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q2 2002</td>\n      <td>01/04/2002</td>\n      <td>98.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q3 2002</td>\n      <td>01/07/2002</td>\n      <td>103.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q4 2002</td>\n      <td>01/10/2002</td>\n      <td>101.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q1 2003</td>\n      <td>01/01/2003</td>\n      <td>95.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>例2：导入文件data.txt中数据：</p>\n<pre><code class=\"language-python\">df3=pd.read_csv(&quot;Data/data.txt&quot;,sep='\\t',header=None,names=['特征1','特征2','特征3','标签'])\ndf3\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>特征1</th>\n      <th>特征2</th>\n      <th>特征3</th>\n      <th>标签</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40920</td>\n      <td>8.326976</td>\n      <td>0.953952</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14488</td>\n      <td>7.153469</td>\n      <td>1.673904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26052</td>\n      <td>1.441871</td>\n      <td>0.805124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75136</td>\n      <td>13.147394</td>\n      <td>0.428964</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38344</td>\n      <td>1.669788</td>\n      <td>0.134296</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>72993</td>\n      <td>10.141740</td>\n      <td>1.032955</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>35948</td>\n      <td>6.830792</td>\n      <td>1.213192</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>42666</td>\n      <td>13.276369</td>\n      <td>0.543880</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>67497</td>\n      <td>8.631577</td>\n      <td>0.749278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>35483</td>\n      <td>12.273169</td>\n      <td>1.508053</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p><b3><b>0.2 pandas导入excel文件</b></b3></p>\n<p>pd.read_excel()</p>\n<p>常用参数：<br>\nio：excel文件路径（必填，其他参数按需求填写）</p>\n<p>sheet_name：需要导入数据的工作表表名,可以是int\\string\\list，None导入所有工作表数据，默认0。</p>\n<p>参数header、names、index_col、usecols、skiprows、nrows、skip_footer、encoding的用法与pd.read_csv相同。比如读取数据时想把第一列设为index，那么只需要简单的<br>\npd.read_csv(“new_wordvecter.csv”,index_col=[0])</p>\n<p>这里index_col可以设为列名</p>\n<p>后续更改index可以使用df.index = df.iloc[:,“column”].tolist()或df.set_index(‘column’)<br>\n例：读取Canada.xlsx文件中的数据</p>\n<pre><code class=\"language-python\">df_canada=pd.read_excel(&quot;Data/Canada.xlsx&quot;,sheet_name='Canada by Citizenship',skiprows=20,skip_footer=2)\ndf_canada.head()\ndf_India_China=df_canada[df_canada['OdName'].isin(['China','India'])]\n</code></pre>\n<pre><code class=\"language-python\">df_India_China.columns\n</code></pre>\n<pre><code>Index([    'Type', 'Coverage',   'OdName',     'AREA', 'AreaName',      'REG',\n        'RegName',      'DEV',  'DevName',       1980,       1981,       1982,\n             1983,       1984,       1985,       1986,       1987,       1988,\n             1989,       1990,       1991,       1992,       1993,       1994,\n             1995,       1996,       1997,       1998,       1999,       2000,\n             2001,       2002,       2003,       2004,       2005,       2006,\n             2007,       2008,       2009,       2010,       2011,       2012,\n             2013],\n      dtype='object')\n</code></pre>\n<pre><code class=\"language-python\">df_India_China\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Coverage</th>\n      <th>OdName</th>\n      <th>AREA</th>\n      <th>AreaName</th>\n      <th>REG</th>\n      <th>RegName</th>\n      <th>DEV</th>\n      <th>DevName</th>\n      <th>1980</th>\n      <th>...</th>\n      <th>2004</th>\n      <th>2005</th>\n      <th>2006</th>\n      <th>2007</th>\n      <th>2008</th>\n      <th>2009</th>\n      <th>2010</th>\n      <th>2011</th>\n      <th>2012</th>\n      <th>2013</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>China</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>906</td>\n      <td>Eastern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>5123</td>\n      <td>...</td>\n      <td>36619</td>\n      <td>42584</td>\n      <td>33518</td>\n      <td>27642</td>\n      <td>30037</td>\n      <td>29622</td>\n      <td>30391</td>\n      <td>28502</td>\n      <td>33024</td>\n      <td>34129</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Immigrants</td>\n      <td>Foreigners</td>\n      <td>India</td>\n      <td>935</td>\n      <td>Asia</td>\n      <td>5501</td>\n      <td>Southern Asia</td>\n      <td>902</td>\n      <td>Developing regions</td>\n      <td>8880</td>\n      <td>...</td>\n      <td>28235</td>\n      <td>36210</td>\n      <td>33848</td>\n      <td>28742</td>\n      <td>28261</td>\n      <td>29456</td>\n      <td>34235</td>\n      <td>27509</td>\n      <td>30933</td>\n      <td>33087</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 43 columns</p>\n</div>\n<p>选取等于某些值的行记录 用 ==</p>\n<p>df.loc[df[‘column_name’] == some_value]</p>\n<p>选取某列是否是某一类型的数值 用 isin</p>\n<p>df.loc[df[‘column_name’].isin(some_values)]</p>\n<p>df[‘a’]#取a列</p>\n<p>df[[‘a’,‘b’]]#取a、b列</p>\n<h2>1 直方图及密度图的画法</h2>\n<p>直方图是表示变量频率分布的一种方法，是一个可以快速展示数据概率分布的工具，直观易于理解，并深受数据爱好者的喜爱。大家平时可能见到最多就是 matplotlib，seaborn 等高级封装的库包。<br/><br/></p>\n<p>直方图又称质量分布图，它是表示资料变化情况的一种主要工具。用直方图可以解析出资料的规则性，比较直观地看出产品质量特性的分布状态，对于资料分布状况一目了然，便于判断其总体质量分布情况。直方图表示通过沿数据范围形成分箱(bin)，然后绘制条以显示落入每个分箱的观测次数的数据分布。</p>\n<p>核密度图（kde直方图的拟合曲线）可以看作是概率密度图</p>\n<p><b3><b>1.1 matplotlib画直方图及密度图</b></b3></p>\n<p><b>(1)plt.hist()<br>\n用于画直方图。</b></p>\n<p>参数列表：<br>\nplt.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False, bottom=None, histtype=‘bar’, align=‘mid’, orientation=‘vertical’, rwidth=None, log=False, color=None, label=None, stacked=False, normed=None)</p>\n<p>x：指定要绘制直方图的数据；输入值，这需要一个数组或者一个序列，不需要长度相同的数组。<br>\nbins：指定直方图条形的个数；<br>\nrange：指定直方图数据的上下界，默认包含绘图数据的最大值和最小值；<br>\ndensity：布尔,可选。如果&quot;True&quot;，返回元组的第一个元素将会将计数标准化以形成一个概率密度，也就是说，直方图下的面积（或积分）总和为1。这是通过将计数除以数字的数量来实现的观察乘以箱子的宽度而不是除以总数数量的观察。如果叠加也是“真实”的，那么柱状图被规范化为1。(替代normed)<br>\nweights：该参数可为每一个数据点设置权重；<br>\ncumulative：是否需要计算累计频数或频率；<br>\nbottom：可以为直方图的每个条形添加基准线，默认为0；<br>\nhisttype：指定直方图的类型，默认为bar，除此还有’barstacked’, ‘step’, ‘stepfilled’；<br>\nalign：设置条形边界值的对其方式，默认为mid，除此还有’left’和’right’；<br>\norientation：设置直方图的摆放方向，默认为垂直方向；<br>\nrwidth：设置直方图条形宽度的百分比；<br>\nlog：是否需要对绘图数据进行log变换；<br>\ncolor：设置直方图的填充色；<br>\nlabel：设置直方图的标签，可通过legend展示其图例；<br>\nstacked：当有多个数据时，是否需要将直方图呈堆叠摆放，默认水平摆放；<br>\nnormed：是否将直方图的频数转换成频率；(弃用，被density替代)<br>\nalpha：透明度，浮点数。<br>\n返回值：<br>\nn：直方图的值<br>\nbins：返回各个bin的区间范围<br>\npatches：返回每个bin的信息</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,方差/标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值，是否对数据进行归一化\nn1, bins1, patches1 = plt.hist(x1, bins=50, density=True,color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50, density=True,color='r', alpha=0.2)\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\n# plt.show()\n</code></pre>\n<pre><code>[&lt;matplotlib.lines.Line2D at 0x21b1a893250&gt;]\n</code></pre>\n<p><img src=\"/img/output_30_12.png\" alt=\"png\"></p>\n<p><b>(2)添加分布曲线</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19260801)\n\nmu1, sigma1 = 100, 15\nmu2, sigma2 = 80, 15\nx1 =  np.random.normal(mu1,sigma1,10000) # (均值,标准差,个数)\nx2 =  np.random.normal(mu2,sigma2,10000) \n\n# the histogram of the data\n# 50：将数据分成50组\n# color：颜色；alpha：透明度\n# density：是密度而不是具体数值\nn1, bins1, patches1 = plt.hist(x1, bins=50,  density=True, color='g', alpha=1)\nn2, bins2, patches2 = plt.hist(x2, bins=50,  density=True, color='r', alpha=0.2)\n\n\nplt.plot(bins1[:-1],n1,'--')\nplt.plot(bins2[:-1],n2,'--')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_32_02.png\" alt=\"png\"></p>\n<p>通俗的说，返回的第一个值是每个直方柱的y值（取决于normed和weights的取值），<br>\n而第二个值是每个直方柱的x值——如果分成10份，则有11个分割位置，因此返回数目为nbins+1。<br>\n<b>(3)多类型直方图</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n#plt.rcParams['font.family']='Times New Roman'\n#plt.rcParams['axes.unicode_minus']=False\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(&quot;ABC&quot;))\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_35_02.png\" alt=\"png\"></p>\n<p><b>(4)添加说明信息</b></p>\n<p><b>添加title</b><br/><br/><br>\n你可以给图示或图添加标题。但是默认不支持中文，需要自己添加中文字体。</p>\n<p>windows的字体文件放在 C:\\Windows\\Fonts，通过fontproperties设置字体，fontsize设置字体大小.</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)      \n\n# 生成3组值，每组的个数可以不一样\nx1,x2,x3 = [np.random.randn(n) for n in [10000, 5000, 2000]]\n\nplt.figure(figsize=(8,5))\n# 在 ax.hist 函数中先指定图例 label 名称\nplt.hist([x1, x2, x3], bins=10, density=True, histtype='bar')\n\n# 通过 ax.legend 函数来添加图例\nplt.legend(list(&quot;ABC&quot;))\n\nplt.rcParams['font.family']='SimHei'\nplt.rcParams['axes.unicode_minus']=False\nplt.rcParams['font.size']='12'\nplt.title(&quot;多类型直方图&quot;)\n#songTi = matplotlib.font_manager.FontProperties(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc')\n#plt.title(&quot;多类型直方图&quot;,fontproperties=songTi,fontsize=12)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_38_02.png\" alt=\"png\"></p>\n<p><b>添加文字、网格、轴标签及轴范围</b></p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, color='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')  #(x,y,str,...)r raw string \nplt.xlim(40, 160)\nplt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_02.png\" alt=\"png\"></p>\n<p><b3><b>1.2 pandas画直方图及密度图</b></b3></p>\n<p><b>（1）pandas的plot方法</b></p>\n<p>pandas默认的 plot 方法可以帮助我们快速地绘制各种图形<br/><br>\nPandas 提供了 plot() 方法可以快速方便地将 Series 和 DataFrame 中的数据进行可视化, 它是 matplotlib.axes.Axes.plot 的封装。代码执行后会生成一张图片，并直接显示在 notebook 上。<br/></p>\n<p><b>基本使用</b><br>\nplot 默认为折线图，折线图也是最常用和最基础的可视化图形，足以满足我们日常 80% 的需求：</p>\n<p>df.plot()<br>\ns.plot()</p>\n<p>我们可以在 plot 后增加调用来使用其他的图形，当然这些图形对数据结构也有自己的要求：</p>\n<p>df.plot.line() # 折线的全写方式<br>\ndf.plot.bar() # 柱状图<br>\ndf.plot.barh() # 横向柱状图 （条形图）<br>\ndf.plot.hist() # 直方图<br>\ndf.plot.box() # 箱形图<br>\ndf.plot.kde() # 核密度估计图<br>\ndf.plot.density() # 同 df.plot.kde()<br>\ndf.plot.area() # 面积图<br>\ndf.plot.pie() # 饼图<br>\ndf.plot.scatter() # 散点图<br>\ndf.plot.hexbin() # 六边形箱体图，或简称六边形图<br>\n<b>使用方法</b></p>\n<p><b>Series 使用</b></p>\n<p>使用 plot 时 x 轴为索引，y 轴为索引对应的具体值：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(20),\n               index=pd.date_range('1/1/2000', periods=20)\n              )\n\nts.plot()\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd7ad0d00&gt;\n</code></pre>\n<p><img src=\"/img/output_49_12.png\" alt=\"png\"></p>\n<p><b>DataFrame 使用</b><br/></p>\n<p>DataFrame 使用 plot 时 x 轴为索引，y 轴为索引对应的多个具体值：</p>\n<pre><code class=\"language-python\">df = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot()\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd6a56610&gt;\n</code></pre>\n<p><img src=\"/img/output_51_12.png\" alt=\"png\"></p>\n<p><b>plot 绘图时常用的一些绘图参数。</b><br/></p>\n<p>图形类型<br>\ndf.plot() 可以通过参数来指定具体图形类型：</p>\n<p>df.plot(kind=‘pie’) # 其他的名称和上文相同<br>\ns.plot(kind=‘pie’)<br>\n直方图kind=“hist”<br>\n密度图kind=“kde”<br>\n生成或读取数据后，调用pandas的plot方法画图。默认情况下参数 kind=“line” 表示图的类型为折线图。<br/></p>\n<pre><code class=\"language-python\">df = pd.DataFrame(np.random.randn(6, 4),\n                  index=pd.date_range('1/1/2000', periods=6),\n                  columns=list('ABCD'))\ndf.plot(kind='hist',title='this is a hist',grid=True)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x21b1ba68460&gt;\n</code></pre>\n<p><img src=\"/img/output_55_12.png\" alt=\"png\"></p>\n<p>图的标题：</p>\n<pre><code class=\"language-python\">df.plot(title='my plot')\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd5f8df10&gt;\n</code></pre>\n<p><img src=\"/img/output_57_12.png\" alt=\"png\"></p>\n<p>字体大小</p>\n<pre><code class=\"language-python\">指定轴上的字体大小\n</code></pre>\n<pre><code class=\"language-python\">df.plot(fontsize=15)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd601e7c0&gt;\n</code></pre>\n<p><img src=\"/img/output_60_12.png\" alt=\"png\"></p>\n<p>线条样式<br>\nstyle 可指定图的线条等样式，可参考可选的值 Matplotlib Line-style：<br>\ndf[:5].plot(style=‘:’) # 虚线<br>\ndf[:5].plot(style=‘-.’) # 虚实相间<br>\ndf[:5].plot(style=‘–’) # 长虚线<br>\ndf[:5].plot(style=‘-’) # 实线（默认）<br>\ndf[:5].plot(style=‘.’) # 点<br>\ndf[:5].plot(style=‘*-’) # 实线，数值为星星<br>\ndf[:5].plot(style=‘^-’) # 实线，数值为三角形</p>\n<pre><code class=\"language-python\">df.plot(style=':') # 虚线\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd603bd30&gt;\n</code></pre>\n<p><img src=\"/img/output_63_12.png\" alt=\"png\"></p>\n<p>对不同线分别给样式：</p>\n<pre><code class=\"language-python\">df[:5].plot(style=[':', '--', '.-', '*-'])\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd601a250&gt;\n</code></pre>\n<p><img src=\"/img/output_65_12.png\" alt=\"png\"></p>\n<p>背景辅助线</p>\n<pre><code class=\"language-python\">grid 会给 x 方向和 y 方向增加灰色辅助线：\n</code></pre>\n<pre><code class=\"language-python\">df.plot(grid=True)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd6162fd0&gt;\n</code></pre>\n<p><img src=\"/img/output_68_12.png\" alt=\"png\"></p>\n<p>图例</p>\n<pre><code class=\"language-python\">df.plot(legend=False)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd61e5cd0&gt;\n</code></pre>\n<p><img src=\"/img/output_70_12.png\" alt=\"png\"></p>\n<p>可以反向排序图例：</p>\n<pre><code class=\"language-python\">df.plot(legend='reverse')\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14fd5e02bb0&gt;\n</code></pre>\n<p><img src=\"/img/output_72_12.png\" alt=\"png\"></p>\n<p><b>（2）Pandas画直方图和密度图</b></p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv('Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nTitanic.Age.plot(kind = 'hist', bins = 20, color = 'steelblue', edgecolor = 'black', density = True, label = '直方图')\n# 绘制核密度图\nTitanic.Age.plot(kind = 'kde', color = 'red', label = '核密度图')\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n<p><img src=\"/img/output_74_12.png\" alt=\"png\"></p>\n<p><b3><b>1.3 seaborn画直方图及密度图</b></b3></p>\n<p>尽管上一幅图满足了两种图形的合成，但其表达的是所有乘客的年龄分布，如果按性别分组，研究不同性别下年龄分布的差异，该如何实现？针对这个问题，使用matplotlib模块或pandas模块都会稍微复杂一些，推荐使用seaborn模块中的distplot函数，因为该函数的代码简洁而易懂。关于该函数的语法和参数含义如下：<br>\nsns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None,<br>\nhist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None,<br>\ncolor=None, vertical=False, norm_hist=False, axlabel=None,<br>\nlabel=None, ax=None)a：指定绘图数据，可以是序列、一维数组或列表。<br>\nbins：指定直方图条形的个数。<br>\nhist：bool类型的参数，是否绘制直方图，默认为True。<br>\nkde：bool类型的参数，是否绘制核密度图，默认为True。<br>\nrug：bool类型的参数，是否绘制须图（如果数据比较密集，该参数比较有用），默认为False。<br>\nfit：指定一个随机分布对象（需调用scipy模块中的随机分布函数），用于绘制随机分布的概率密度曲线。<br>\nhist_kws：以字典形式传递直方图的其他修饰属性，如填充色、边框色、宽度等。<br>\nkde_kws：以字典形式传递核密度图的其他修饰属性，如线的颜色、线的类型等。<br>\nrug_kws：以字典形式传递须图的其他修饰属性，如线的颜色、线的宽度等。<br>\nfit_kws：以字典形式传递概率密度曲线的其他修饰属性，如线条颜色、形状、宽度等。<br>\ncolor：指定图形的颜色，除了随机分布曲线的颜色。<br>\nvertical：bool类型的参数，是否将图形垂直显示，默认为True。（改为False即为horizontal）<br>\nnorm_hist：bool类型的参数，是否将频数更改为频率，默认为False。<br>\naxlabel：用于显示轴标签。 label：指定图形的图例，需结合plt.legend()一起使用。<br>\nax：指定子图的位置。<br>\n从函数的参数可知，通过该函数，可以实现三种图形的合成，分别是直方图（hist参数）、核密度曲线（kde参数）以及指定的理论分布密度曲线（fit参数）。接下来，针对如上介绍的distplot函数，绘制不同性别下乘客的年龄分布图，具体代码如下：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = &#123;'color':'steelblue'&#125;, label = '男性')\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = &#123;'color':'purple'&#125;, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄直方图')\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = &#123;'color':'red', 'linestyle':'-'&#125;,\n             norm_hist = True, label = '男性')\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = &#123;'color':'black', 'linestyle':'--'&#125;,\n             norm_hist = True, label = '女性')\nplt.title('泰坦尼克号男女乘客的年龄核密度图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_79_02.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_79_12.png\" alt=\"png\"></p>\n<p>为了避免四个图形混在一起不易发现数据背后的特征，将直方图与核密度图分开绘制。从直方图来看，女性年龄的分布明显比男性矮，说明在各年龄段下，男性乘客要比女性乘客多；再看核密度图，男女性别的年龄分布趋势比较接近，说明各年龄段下的男女乘客人数同步增加或减少。</p>\n<p>把两种图合在一起画：</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 取出男性年龄\nAge_Male = Titanic.Age[Titanic.Sex == 0]\n# 取出女性年龄\nAge_Female = Titanic.Age[Titanic.Sex ==1]\n\n# seaborn模块绘制分组的直方图和核密度图\n# 绘制男女乘客年龄的直方图\nsns.distplot(Age_Male, bins = 20, kde = False, hist_kws = &#123;'color':'steelblue'&#125;,\n             label = ('男性','直方图'),norm_hist=True)\n# 绘制女性年龄的直方图\nsns.distplot(Age_Female, bins = 20, kde = False, hist_kws = &#123;'color':'purple'&#125;,\n             label = ('女性','直方图'),norm_hist=True)\n\n# 绘制男女乘客年龄的核密度图\nsns.distplot(Age_Male, hist = False, kde_kws = &#123;'color':'red', 'linestyle':'-'&#125;,\n             norm_hist = True, label = ('男性','核密度图'))\n# 绘制女性年龄的核密度图\nsns.distplot(Age_Female, hist = False, kde_kws = &#123;'color':'black', 'linestyle':'--'&#125;,\n             norm_hist = True, label = ('女性','核密度图'))\nplt.title('泰坦尼克号男女乘客的年龄分布图')\nplt.xlabel('年龄')\nplt.ylabel('核密度值')\n# 显示图例\nplt.legend()\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_81_02.png\" alt=\"png\"></p>\n<p><b>（1）seaborn直方图</b></p>\n<p>seaborn的kdeplot函数专门用于画核密度估计图.</p>\n<p><b>（2）seaborn密度图</b></p>\n<p>seaborn的kdeplot函数专门用于画核密度估计图.</p>\n<h2>2 课堂练习</h2>\n<p><b>（1）直方图</b></p>\n<p>以Titanic数据集为例绘制乘客的年龄直方图</p>\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n# 读入数据\nTitanic = pd.read_csv(r'Data\\Titanic.csv')\n# 检查年龄是否有缺失\nany(Titanic.Age.isnull())\n# 不妨删除含有缺失年龄的观察\nTitanic.dropna(subset=['Age'], inplace=True)\n#设置绘图风格\nplt.style.use('ggplot')\n#处理中文乱码\nplt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n#坐标轴负号的处理\nplt.rcParams['axes.unicode_minus']=False\n# 绘制直方图\nplt.hist(x = Titanic.Age, # 指定绘图数据\n         bins = 20, # 指定直方图中条块的个数\n         color = 'steelblue', # 指定直方图的填充色\n         edgecolor = 'black' # 指定直方图的边框色\n         )\n# 添加x轴和y轴标签\nplt.xlabel('年龄')\nplt.ylabel('频数')\n# 添加标题\nplt.title('泰坦尼克号乘客年龄分布')\n# 显示图形\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_89_02.png\" alt=\"png\"></p>\n<h2>3 直方图与条形图的区别</h2>\n<p>首先，条形图是用条形的长度表示各类别频数的多少，其宽度（表示类别）则是固定的；</p>\n<pre><code>  直方图是用面积表示各组频数的多少，矩形的高度表示每一组的频数或频率，宽度则表示各组的组距，因此其高度与宽度均有意义。\n</code></pre>\n<p>其次，由于分组数据具有连续性，直方图的各矩形通常是连续排列，而条形图则是分开排列。</p>\n<p>最后，条形图主要用于展示分类数据，而直方图则主要用于展示数据型数据<br/><br/></p>\n<p>原文链接：<a href=\"https://blog.csdn.net/xjl271314/article/details/80295935\">https://blog.csdn.net/xjl271314/article/details/80295935</a></p>\n<h2>附录1-本小节用到的以前的知识复习</h2>\n<h3>1)批量注释/去掉注释</h3> \n<p>Ctrl+/</p>\n<p>-&gt;常常出现在python函数定义的函数名后面，为函数添加元数据,描述函数的返回类型，从而方便开发人员使用。<br>\ndef add(x, y) -&gt; int:<br>\nreturn x+y</p>\n<h3>2)中文字体设置</h3>\n<p><a href=\"https://blog.csdn.net/dxawdc/article/details/110311549\">https://blog.csdn.net/dxawdc/article/details/110311549</a></p>\n<h3>3)函数定义</h3>\n<pre><code class=\"language-python\">s = '杰瑞你好啊'\ndef mylen():\n    n = 0\n    for i in s :\n        n += 1\n    print(n)\nmylen()\n#len1 = mylen()\n#print(len1)\n</code></pre>\n<pre><code>5\n</code></pre>\n<h2>附录2-错误提示及解决办法</h2>\n<h3>1) 8722 missing from current font, matplotlib画图</h3>\n<p>负号问题，添加语句plt.rcParams[‘axes.unicode_minus’]=False</p>\n<p>更多请参考https://blog.csdn.net/seeker3/article/details/108432781?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs</p>\n<pre><code class=\"language-python\">\n</code></pre>\n"},{"title":"Redis数据结构与对象（一）-简单动态字符串","author":"ztq","date":"2022-01-22T04:34:00.000Z","_content":"\n# 数据结构与对象\n\n## 简单动态字符串\n\n### SDS的定义\n\n每个 `sds.h/sdshdr` 结构表示一个 SDS 值：\n\n```java\nstruct sdshdr {\n    // 记录 buf 数组中已使用字节的数量\n    // 等于 SDS 所保存字符串的长度\n    int len;\n    // 记录 buf 数组中未使用字节的数量\n    int free;\n    // 字节数组，用于保存字符串\n    char buf[];\n};\n```\n\n图 2-1 展示了一个 SDS 示例：\n\n- `free` 属性的值为 `0` ，表示这个 SDS 没有分配任何未使用空间。\n- `len` 属性的值为 `5` ，表示这个 SDS 保存了一个五字节长的字符串。\n- `buf` 属性是一个 `char` 类型的数组，数组的前五个字节分别保存了 `'R'` 、 `'e'` 、 `'d'` 、 `'i'` 、 `'s'` 五个字符，而最后一个字节则保存了空字符 `'\\0'` 。\n\n![img](/img/ffd661fb15bf746863a6e57d7aca04c4.png)\n\nSDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 `1` 字节空间不计算在 SDS 的 `len` 属性里面，并且为空字符分配额外的 `1` 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。\n\n遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。\n\n举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 `s` ，那么我们可以直接使用 `stdio.h/printf` 函数，通过执行以下语句：\n\n```java\nprintf(\"%s\", s->buf);\n```\n\n来打印出 SDS 保存的字符串值 `\"Redis\"` ，而无须为 SDS 编写专门的打印函数。\n\n 图2-2 展示了另一个 SDS 示例:\n\n- 这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 `\"Redis\"` 。\n- 这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 `buf` 数组分配了五字节未使用空间，所以它的 `free` 属性的值为 `5`（图中使用五个空格来表示五字节的未使用空间）。\n\n![img](/img/e13f637bd584b1563442aeaece8cf1e4.png)\n\n接下来的一节将详细地说明未使用空间在 SDS 中的作用。\n\n### SDS 与 C 字符串的区别\n\n根据传统，C 语言使用长度为 `N+1` 的字符数组来表示长度为 `N` 的字符串，并且字符数组的最后一个元素总是空字符 `'\\0'` 。\n\n比如说，图 2-3 就展示了一个值为 `\"Redis\"` 的 C 字符串：\n\n![img](/img/2cfc91e2dc6104321827e210ccfc595c.png)\n\nC 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，本节接下来的内容将详细对比 C 字符串和 SDS 之间的区别，并说明 SDS 比 C 字符串更适用于 Redis 的原因。\n\n#### 常数复杂度获取字符串长度\n\n因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。\n\n举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程。\n\n![img](/img/ce0bdff067bf19a083c5d26b1266bb44.png)\n\n![img](/img/edf97c87de27aae8adafce7f255f0870.png)\n\n![img](/img/f8f4bde6cb9aeac56366a2f3853f24f3.png)\n\n![img](/img/a7d8693f0dccfde8ecd0919cff94ff88.png)\n\n![img](/img/879bfdfc7cab85a54bed5a33275c6dc0.png)\n\n![img](/img/d133e6d9b982b320594ea62168dbe462.png)\n\n和 C 字符串不同，因为 SDS 在 `len` 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。\n\n举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 `len` 属性，就可以立即知道 SDS 的长度为 `5` 字节：\n\n![img](/img/1112dfc7b11a214351a67103cd58a5c8.png)\n\n又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 `len` 属性，就可以立即知道 SDS 的长度为 `11` 字节。\n\n![img](/img/7fdd593ac3e140d5cd2e9328b51110c9.png)\n\n设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。\n\n通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。\n\n比如说，因为字符串键在底层使用 SDS 来实现，所以即使我们对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 。\n\n#### 杜绝缓冲区溢出\n\n除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。\n\n举个例子，`<string.h>/strcat` 函数可以将 `src` 字符串中的内容拼接到 `dest` 字符串的末尾：\n\n```java\nchar *strcat(char *dest, const char *src);\n```\n\n因为 C 字符串不记录自身的长度，所以 `strcat` 假定用户在执行这个函数时，已经为 `dest` 分配了足够多的内存，可以容纳 `src` 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。\n\n举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 `s1` 和 `s2` ，其中 `s1` 保存了字符串 `\"Redis\"` ，而 `s2` 则保存了字符串 `\"MongoDB\"` ，如图 2-7 所示。\n\n![img](/img/9b9139fd46f7419279ba6346b090ddc7.png)\n\n如果一个程序员决定通过执行：\n\n```java\nstrcat(s1, \" Cluster\");\n```\n\n将 `s1` 的内容修改为 `\"Redis Cluster\"` ，但粗心的他却忘了在执行 `strcat` 之前为 `s1` 分配足够的空间，那么在 `strcat` 函数执行之后，`s1` 的数据将溢出到 `s2` 所在的空间中，导致 `s2` 保存的内容被意外地修改，如图 2-8 所示。\n\n![img](/img/79c1c0a4746f50074f0e44491d3cb8b3.png)\n\n与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。\n\n举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 `sdscat` 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，`sdscat` 会先检查给定 SDS 的空间是否足够，如果不够的话，`sdscat` 就会先扩展 SDS 的空间，然后才执行拼接操作。\n\n比如说，如果我们执行：\n\n```java\nsdscat(s, \" Cluster\");\n```\n\n其中 SDS 值 `s` 如图 2-9 所示，那么 `sdscat` 将在执行拼接操作之前检查 `s` 的长度是否足够，在发现 `s` 目前的空间不足以拼接 `\" Cluster\"` 之后，`sdscat` 就会先扩展 `s` 的空间，然后才执行拼接 `\" Cluster\"` 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。\n\n![img](/img/a4ef9088f117a8766ac65c4a1a630a34.png)\n\n![img](/img/57c9d55222f65769c0ab0844bc5b6432.png)\n\n注意图 2-10 所示的 SDS ：`sdscat` 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 `13` 字节的未使用空间，并且拼接之后的字符串也正好是 `13` 字节长，这种现象既不是 bug 也不是巧合，它和 SDS 的空间分配策略有关，接下来的小节将对这一策略进行说明。\n\n#### 减少修改字符串时带来的内存重分配次数\n\n正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 `N` 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 `N+1` 个字符长的数组（额外的一个字符空间用于保存空字符）。\n\n因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作：\n\n- 如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。\n- 如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。\n\n举个例子，如果我们持有一个值为 `\"Redis\"` 的 C 字符串 `s` ，那么为了将 `s` 的值改为 `\"Redis Cluster\"` ，在执行：\n\n```java\nstrcat(s, \" Cluster\");\n```\n\n之前，我们需要先使用内存重分配操作，扩展 `s` 的空间。\n\n之后，如果我们又打算将 `s` 的值从 `\"Redis Cluster\"` 改为 `\"Redis Cluster Tutorial\"` ，那么在执行：\n\n```java\nstrcat(s, \" Tutorial\");\n```\n\n之前，我们需要再次使用内存重分配扩展 `s` 的空间，诸如此类。\n\n因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作：\n\n- 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。\n- 但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。\n\n为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，`buf` 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 `free` 属性记录。\n\n通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略。\n\n##### 空间预分配\n\n空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。\n\n其中，额外分配的未使用空间数量由以下公式决定：\n\n- 如果对 SDS 进行修改之后，SDS 的长度（也即是 `len` 属性的值）将小于 `1 MB` ，那么程序分配和 `len` 属性同样大小的未使用空间，这时 SDS `len` 属性的值将和 `free` 属性的值相同。举个例子，如果进行修改之后，SDS 的 `len` 将变成 `13` 字节，那么程序也会分配 `13` 字节的未使用空间，SDS 的 `buf` 数组的实际长度将变成 `13 + 13 + 1 = 27` 字节（额外的一字节用于保存空字符）。\n- 如果对 SDS 进行修改之后，SDS 的长度将大于等于 `1 MB` ，那么程序会分配 `1 MB` 的未使用空间。举个例子，如果进行修改之后，SDS 的 `len` 将变成 `30 MB` ，那么程序会分配 `1 MB` 的未使用空间，SDS 的 `buf` 数组的实际长度将为 `30 MB + 1 MB + 1 byte` 。\n\n通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。\n\n举个例子，对于图 2-11 所示的 SDS 值 `s` 来说，如果我们执行：\n\n```java\nsdscat(s, \" Cluster\");\n```\n\n那么 `sdscat` 将执行一次内存重分配操作，将 SDS 的长度修改为 `13` 字节，并将 SDS 的未使用空间同样修改为 `13` 字节，如图 2-12 所示。\n\n![img](/img/d6f2bab64eddad3ee5d092672e8fac50.png)\n\n![img](/img/41d2985c3545a456341af1e4f5b8231a.png)\n\n如果这时，我们再次对 `s` 执行：\n\n```java\nsdscat(s, \" Tutorial\");\n```\n\n那么这次 `sdscat` 将不需要执行内存重分配：因为未使用空间里面的 `13` 字节足以保存 `9` 字节的 `\" Tutorial\"` ，执行 `sdscat` 之后的 SDS 如图 2-13 所示。\n\n![img](/img/b5a900fe460481976d0ae062ab18dd61.png)\n\n在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。\n\n通过这种预分配策略，SDS 将连续增长 `N` 次字符串所需的内存重分配次数从必定 `N` 次降低为最多 `N` 次。\n\n##### 惰性空间释放\n\n惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 `free` 属性将这些字节的数量记录起来，并等待将来使用。\n\n举个例子，`sdstrim` 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。\n\n比如对于图 2-14 所示的 SDS 值 `s` 来说，执行：\n\n```java\nsdstrim(s, \"XY\");   // 移除 SDS 字符串中的所有 'X' 和 'Y'\n```\n\n会将 SDS 修改成图 2-15 所示的样子。\n\n![img](/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png)\n\n![img](/img/f785ade6f77521bce18fc9f8825d4d9b.png)\n\n注意执行 `sdstrim` 之后的 SDS 并没有释放多出来的 `8` 字节空间，而是将这 `8` 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。\n\n举个例子，如果现在对 `s` 执行：\n\n```java\nsdscat(s, \" Redis\");\n```\n\n那么完成这次 `sdscat` 操作将不需要执行内存重分配：因为 SDS 里面预留的 `8` 字节空间已经足以拼接 `6` 个字节长的 `\" Redis\"` ，如图 2-16 所示。\n\n![img](/img/2b97031c92882db452d126852b74cfb2.png)\n\n通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。\n\n与此同时，SDS 也提供了相应的 API ，让我们可以在有需要时，真正地释放 SDS 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。\n\n#### 二进制安全\n\nC 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。\n\n举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 `\"Redis\"` ，而忽略之后的 `\"Cluster\"` 。\n\n![img](/img/9c9e497f4dc73587aa9d620a4149101c.png)\n\n虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的（binary-safe）：所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 `buf` 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。\n\n这也是我们将 SDS 的 `buf` 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。\n\n比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 `len` 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示。\n\n![img](/img/157e6450653a0fc457342b5794658fc9.png)\n\n通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据。\n\n##### 兼容部分 C 字符串函数\n\n虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 `buf` 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 `<string.h>` 库定义的函数。\n\n![img](/img/d1a8d5d473881fffebc5baccd896258d.png)\n\n举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 `sds` ，那么我们就可以重用 `<string.h>/strcasecmp` 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串：\n\n```\nstrcasecmp(sds->buf, \"hello world\");\n```\n\n这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。\n\n与此类似，我们还可以将一个保存文本数据的 SDS 作为 `strcat` 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面：\n\n```\nstrcat(c_string, sds->buf);\n```\n\n这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。\n\n通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 `<string.h>` 函数库，从而避免了不必要的代码重复。\n\n### 总结\n\n表 2-1 对 C 字符串和 SDS 之间的区别进行了总结。\n\n------\n\n表 2-1 C 字符串和 SDS 之间的区别\n\n![image-20220122144100590](/img/image-20220122144100590.png)\n\n","source":"_posts/Redis设计与实现.md","raw":"title: Redis数据结构与对象（一）-简单动态字符串\nauthor: ztq\ntags:\n\n  - redis\ncategories:\n  - 数据库\ndate: 2022-01-22 12:34:00\n\n---\n\n# 数据结构与对象\n\n## 简单动态字符串\n\n### SDS的定义\n\n每个 `sds.h/sdshdr` 结构表示一个 SDS 值：\n\n```java\nstruct sdshdr {\n    // 记录 buf 数组中已使用字节的数量\n    // 等于 SDS 所保存字符串的长度\n    int len;\n    // 记录 buf 数组中未使用字节的数量\n    int free;\n    // 字节数组，用于保存字符串\n    char buf[];\n};\n```\n\n图 2-1 展示了一个 SDS 示例：\n\n- `free` 属性的值为 `0` ，表示这个 SDS 没有分配任何未使用空间。\n- `len` 属性的值为 `5` ，表示这个 SDS 保存了一个五字节长的字符串。\n- `buf` 属性是一个 `char` 类型的数组，数组的前五个字节分别保存了 `'R'` 、 `'e'` 、 `'d'` 、 `'i'` 、 `'s'` 五个字符，而最后一个字节则保存了空字符 `'\\0'` 。\n\n![img](/img/ffd661fb15bf746863a6e57d7aca04c4.png)\n\nSDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 `1` 字节空间不计算在 SDS 的 `len` 属性里面，并且为空字符分配额外的 `1` 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。\n\n遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。\n\n举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 `s` ，那么我们可以直接使用 `stdio.h/printf` 函数，通过执行以下语句：\n\n```java\nprintf(\"%s\", s->buf);\n```\n\n来打印出 SDS 保存的字符串值 `\"Redis\"` ，而无须为 SDS 编写专门的打印函数。\n\n 图2-2 展示了另一个 SDS 示例:\n\n- 这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 `\"Redis\"` 。\n- 这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 `buf` 数组分配了五字节未使用空间，所以它的 `free` 属性的值为 `5`（图中使用五个空格来表示五字节的未使用空间）。\n\n![img](/img/e13f637bd584b1563442aeaece8cf1e4.png)\n\n接下来的一节将详细地说明未使用空间在 SDS 中的作用。\n\n### SDS 与 C 字符串的区别\n\n根据传统，C 语言使用长度为 `N+1` 的字符数组来表示长度为 `N` 的字符串，并且字符数组的最后一个元素总是空字符 `'\\0'` 。\n\n比如说，图 2-3 就展示了一个值为 `\"Redis\"` 的 C 字符串：\n\n![img](/img/2cfc91e2dc6104321827e210ccfc595c.png)\n\nC 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，本节接下来的内容将详细对比 C 字符串和 SDS 之间的区别，并说明 SDS 比 C 字符串更适用于 Redis 的原因。\n\n#### 常数复杂度获取字符串长度\n\n因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。\n\n举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程。\n\n![img](/img/ce0bdff067bf19a083c5d26b1266bb44.png)\n\n![img](/img/edf97c87de27aae8adafce7f255f0870.png)\n\n![img](/img/f8f4bde6cb9aeac56366a2f3853f24f3.png)\n\n![img](/img/a7d8693f0dccfde8ecd0919cff94ff88.png)\n\n![img](/img/879bfdfc7cab85a54bed5a33275c6dc0.png)\n\n![img](/img/d133e6d9b982b320594ea62168dbe462.png)\n\n和 C 字符串不同，因为 SDS 在 `len` 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。\n\n举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 `len` 属性，就可以立即知道 SDS 的长度为 `5` 字节：\n\n![img](/img/1112dfc7b11a214351a67103cd58a5c8.png)\n\n又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 `len` 属性，就可以立即知道 SDS 的长度为 `11` 字节。\n\n![img](/img/7fdd593ac3e140d5cd2e9328b51110c9.png)\n\n设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。\n\n通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。\n\n比如说，因为字符串键在底层使用 SDS 来实现，所以即使我们对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 。\n\n#### 杜绝缓冲区溢出\n\n除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。\n\n举个例子，`<string.h>/strcat` 函数可以将 `src` 字符串中的内容拼接到 `dest` 字符串的末尾：\n\n```java\nchar *strcat(char *dest, const char *src);\n```\n\n因为 C 字符串不记录自身的长度，所以 `strcat` 假定用户在执行这个函数时，已经为 `dest` 分配了足够多的内存，可以容纳 `src` 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。\n\n举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 `s1` 和 `s2` ，其中 `s1` 保存了字符串 `\"Redis\"` ，而 `s2` 则保存了字符串 `\"MongoDB\"` ，如图 2-7 所示。\n\n![img](/img/9b9139fd46f7419279ba6346b090ddc7.png)\n\n如果一个程序员决定通过执行：\n\n```java\nstrcat(s1, \" Cluster\");\n```\n\n将 `s1` 的内容修改为 `\"Redis Cluster\"` ，但粗心的他却忘了在执行 `strcat` 之前为 `s1` 分配足够的空间，那么在 `strcat` 函数执行之后，`s1` 的数据将溢出到 `s2` 所在的空间中，导致 `s2` 保存的内容被意外地修改，如图 2-8 所示。\n\n![img](/img/79c1c0a4746f50074f0e44491d3cb8b3.png)\n\n与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。\n\n举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 `sdscat` 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，`sdscat` 会先检查给定 SDS 的空间是否足够，如果不够的话，`sdscat` 就会先扩展 SDS 的空间，然后才执行拼接操作。\n\n比如说，如果我们执行：\n\n```java\nsdscat(s, \" Cluster\");\n```\n\n其中 SDS 值 `s` 如图 2-9 所示，那么 `sdscat` 将在执行拼接操作之前检查 `s` 的长度是否足够，在发现 `s` 目前的空间不足以拼接 `\" Cluster\"` 之后，`sdscat` 就会先扩展 `s` 的空间，然后才执行拼接 `\" Cluster\"` 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。\n\n![img](/img/a4ef9088f117a8766ac65c4a1a630a34.png)\n\n![img](/img/57c9d55222f65769c0ab0844bc5b6432.png)\n\n注意图 2-10 所示的 SDS ：`sdscat` 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 `13` 字节的未使用空间，并且拼接之后的字符串也正好是 `13` 字节长，这种现象既不是 bug 也不是巧合，它和 SDS 的空间分配策略有关，接下来的小节将对这一策略进行说明。\n\n#### 减少修改字符串时带来的内存重分配次数\n\n正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 `N` 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 `N+1` 个字符长的数组（额外的一个字符空间用于保存空字符）。\n\n因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作：\n\n- 如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。\n- 如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。\n\n举个例子，如果我们持有一个值为 `\"Redis\"` 的 C 字符串 `s` ，那么为了将 `s` 的值改为 `\"Redis Cluster\"` ，在执行：\n\n```java\nstrcat(s, \" Cluster\");\n```\n\n之前，我们需要先使用内存重分配操作，扩展 `s` 的空间。\n\n之后，如果我们又打算将 `s` 的值从 `\"Redis Cluster\"` 改为 `\"Redis Cluster Tutorial\"` ，那么在执行：\n\n```java\nstrcat(s, \" Tutorial\");\n```\n\n之前，我们需要再次使用内存重分配扩展 `s` 的空间，诸如此类。\n\n因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作：\n\n- 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。\n- 但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。\n\n为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，`buf` 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 `free` 属性记录。\n\n通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略。\n\n##### 空间预分配\n\n空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。\n\n其中，额外分配的未使用空间数量由以下公式决定：\n\n- 如果对 SDS 进行修改之后，SDS 的长度（也即是 `len` 属性的值）将小于 `1 MB` ，那么程序分配和 `len` 属性同样大小的未使用空间，这时 SDS `len` 属性的值将和 `free` 属性的值相同。举个例子，如果进行修改之后，SDS 的 `len` 将变成 `13` 字节，那么程序也会分配 `13` 字节的未使用空间，SDS 的 `buf` 数组的实际长度将变成 `13 + 13 + 1 = 27` 字节（额外的一字节用于保存空字符）。\n- 如果对 SDS 进行修改之后，SDS 的长度将大于等于 `1 MB` ，那么程序会分配 `1 MB` 的未使用空间。举个例子，如果进行修改之后，SDS 的 `len` 将变成 `30 MB` ，那么程序会分配 `1 MB` 的未使用空间，SDS 的 `buf` 数组的实际长度将为 `30 MB + 1 MB + 1 byte` 。\n\n通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。\n\n举个例子，对于图 2-11 所示的 SDS 值 `s` 来说，如果我们执行：\n\n```java\nsdscat(s, \" Cluster\");\n```\n\n那么 `sdscat` 将执行一次内存重分配操作，将 SDS 的长度修改为 `13` 字节，并将 SDS 的未使用空间同样修改为 `13` 字节，如图 2-12 所示。\n\n![img](/img/d6f2bab64eddad3ee5d092672e8fac50.png)\n\n![img](/img/41d2985c3545a456341af1e4f5b8231a.png)\n\n如果这时，我们再次对 `s` 执行：\n\n```java\nsdscat(s, \" Tutorial\");\n```\n\n那么这次 `sdscat` 将不需要执行内存重分配：因为未使用空间里面的 `13` 字节足以保存 `9` 字节的 `\" Tutorial\"` ，执行 `sdscat` 之后的 SDS 如图 2-13 所示。\n\n![img](/img/b5a900fe460481976d0ae062ab18dd61.png)\n\n在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。\n\n通过这种预分配策略，SDS 将连续增长 `N` 次字符串所需的内存重分配次数从必定 `N` 次降低为最多 `N` 次。\n\n##### 惰性空间释放\n\n惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 `free` 属性将这些字节的数量记录起来，并等待将来使用。\n\n举个例子，`sdstrim` 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。\n\n比如对于图 2-14 所示的 SDS 值 `s` 来说，执行：\n\n```java\nsdstrim(s, \"XY\");   // 移除 SDS 字符串中的所有 'X' 和 'Y'\n```\n\n会将 SDS 修改成图 2-15 所示的样子。\n\n![img](/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png)\n\n![img](/img/f785ade6f77521bce18fc9f8825d4d9b.png)\n\n注意执行 `sdstrim` 之后的 SDS 并没有释放多出来的 `8` 字节空间，而是将这 `8` 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。\n\n举个例子，如果现在对 `s` 执行：\n\n```java\nsdscat(s, \" Redis\");\n```\n\n那么完成这次 `sdscat` 操作将不需要执行内存重分配：因为 SDS 里面预留的 `8` 字节空间已经足以拼接 `6` 个字节长的 `\" Redis\"` ，如图 2-16 所示。\n\n![img](/img/2b97031c92882db452d126852b74cfb2.png)\n\n通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。\n\n与此同时，SDS 也提供了相应的 API ，让我们可以在有需要时，真正地释放 SDS 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。\n\n#### 二进制安全\n\nC 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。\n\n举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 `\"Redis\"` ，而忽略之后的 `\"Cluster\"` 。\n\n![img](/img/9c9e497f4dc73587aa9d620a4149101c.png)\n\n虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的（binary-safe）：所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 `buf` 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。\n\n这也是我们将 SDS 的 `buf` 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。\n\n比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 `len` 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示。\n\n![img](/img/157e6450653a0fc457342b5794658fc9.png)\n\n通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据。\n\n##### 兼容部分 C 字符串函数\n\n虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 `buf` 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 `<string.h>` 库定义的函数。\n\n![img](/img/d1a8d5d473881fffebc5baccd896258d.png)\n\n举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 `sds` ，那么我们就可以重用 `<string.h>/strcasecmp` 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串：\n\n```\nstrcasecmp(sds->buf, \"hello world\");\n```\n\n这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。\n\n与此类似，我们还可以将一个保存文本数据的 SDS 作为 `strcat` 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面：\n\n```\nstrcat(c_string, sds->buf);\n```\n\n这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。\n\n通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 `<string.h>` 函数库，从而避免了不必要的代码重复。\n\n### 总结\n\n表 2-1 对 C 字符串和 SDS 之间的区别进行了总结。\n\n------\n\n表 2-1 C 字符串和 SDS 之间的区别\n\n![image-20220122144100590](/img/image-20220122144100590.png)\n\n","slug":"Redis设计与实现","published":1,"updated":"2022-04-04T08:32:40.152Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2400f77kt9fjhvbrmd","content":"<h1>数据结构与对象</h1>\n<h2 id=\"简单动态字符串\">简单动态字符串</h2>\n<h3 id=\"SDS的定义\">SDS的定义</h3>\n<p>每个 <code>sds.h/sdshdr</code> 结构表示一个 SDS 值：</p>\n<pre><code class=\"language-java\">struct sdshdr &#123;\n    // 记录 buf 数组中已使用字节的数量\n    // 等于 SDS 所保存字符串的长度\n    int len;\n    // 记录 buf 数组中未使用字节的数量\n    int free;\n    // 字节数组，用于保存字符串\n    char buf[];\n&#125;;\n</code></pre>\n<p>图 2-1 展示了一个 SDS 示例：</p>\n<ul>\n<li><code>free</code> 属性的值为 <code>0</code> ，表示这个 SDS 没有分配任何未使用空间。</li>\n<li><code>len</code> 属性的值为 <code>5</code> ，表示这个 SDS 保存了一个五字节长的字符串。</li>\n<li><code>buf</code> 属性是一个 <code>char</code> 类型的数组，数组的前五个字节分别保存了 <code>'R'</code> 、 <code>'e'</code> 、 <code>'d'</code> 、 <code>'i'</code> 、 <code>'s'</code> 五个字符，而最后一个字节则保存了空字符 <code>'\\0'</code> 。</li>\n</ul>\n<p><img src=\"/img/ffd661fb15bf746863a6e57d7aca04c4.png\" alt=\"img\"></p>\n<p>SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 <code>1</code> 字节空间不计算在 SDS 的 <code>len</code> 属性里面，并且为空字符分配额外的 <code>1</code> 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。</p>\n<p>遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。</p>\n<p>举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 <code>s</code> ，那么我们可以直接使用 <code>stdio.h/printf</code> 函数，通过执行以下语句：</p>\n<pre><code class=\"language-java\">printf(&quot;%s&quot;, s-&gt;buf);\n</code></pre>\n<p>来打印出 SDS 保存的字符串值 <code>&quot;Redis&quot;</code> ，而无须为 SDS 编写专门的打印函数。</p>\n<p>图2-2 展示了另一个 SDS 示例:</p>\n<ul>\n<li>这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 <code>&quot;Redis&quot;</code> 。</li>\n<li>这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 <code>buf</code> 数组分配了五字节未使用空间，所以它的 <code>free</code> 属性的值为 <code>5</code>（图中使用五个空格来表示五字节的未使用空间）。</li>\n</ul>\n<p><img src=\"/img/e13f637bd584b1563442aeaece8cf1e4.png\" alt=\"img\"></p>\n<p>接下来的一节将详细地说明未使用空间在 SDS 中的作用。</p>\n<h3 id=\"SDS-与-C-字符串的区别\">SDS 与 C 字符串的区别</h3>\n<p>根据传统，C 语言使用长度为 <code>N+1</code> 的字符数组来表示长度为 <code>N</code> 的字符串，并且字符数组的最后一个元素总是空字符 <code>'\\0'</code> 。</p>\n<p>比如说，图 2-3 就展示了一个值为 <code>&quot;Redis&quot;</code> 的 C 字符串：</p>\n<p><img src=\"/img/2cfc91e2dc6104321827e210ccfc595c.png\" alt=\"img\"></p>\n<p>C 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，本节接下来的内容将详细对比 C 字符串和 SDS 之间的区别，并说明 SDS 比 C 字符串更适用于 Redis 的原因。</p>\n<h4 id=\"常数复杂度获取字符串长度\">常数复杂度获取字符串长度</h4>\n<p>因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。</p>\n<p>举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程。</p>\n<p><img src=\"/img/ce0bdff067bf19a083c5d26b1266bb44.png\" alt=\"img\"></p>\n<p><img src=\"/img/edf97c87de27aae8adafce7f255f0870.png\" alt=\"img\"></p>\n<p><img src=\"/img/f8f4bde6cb9aeac56366a2f3853f24f3.png\" alt=\"img\"></p>\n<p><img src=\"/img/a7d8693f0dccfde8ecd0919cff94ff88.png\" alt=\"img\"></p>\n<p><img src=\"/img/879bfdfc7cab85a54bed5a33275c6dc0.png\" alt=\"img\"></p>\n<p><img src=\"/img/d133e6d9b982b320594ea62168dbe462.png\" alt=\"img\"></p>\n<p>和 C 字符串不同，因为 SDS 在 <code>len</code> 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。</p>\n<p>举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 <code>len</code> 属性，就可以立即知道 SDS 的长度为 <code>5</code> 字节：</p>\n<p><img src=\"/img/1112dfc7b11a214351a67103cd58a5c8.png\" alt=\"img\"></p>\n<p>又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 <code>len</code> 属性，就可以立即知道 SDS 的长度为 <code>11</code> 字节。</p>\n<p><img src=\"/img/7fdd593ac3e140d5cd2e9328b51110c9.png\" alt=\"img\"></p>\n<p>设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。</p>\n<p>通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。</p>\n<p>比如说，因为字符串键在底层使用 SDS 来实现，所以即使我们对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 。</p>\n<h4 id=\"杜绝缓冲区溢出\">杜绝缓冲区溢出</h4>\n<p>除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。</p>\n<p>举个例子，<code>&lt;string.h&gt;/strcat</code> 函数可以将 <code>src</code> 字符串中的内容拼接到 <code>dest</code> 字符串的末尾：</p>\n<pre><code class=\"language-java\">char *strcat(char *dest, const char *src);\n</code></pre>\n<p>因为 C 字符串不记录自身的长度，所以 <code>strcat</code> 假定用户在执行这个函数时，已经为 <code>dest</code> 分配了足够多的内存，可以容纳 <code>src</code> 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。</p>\n<p>举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 <code>s1</code> 和 <code>s2</code> ，其中 <code>s1</code> 保存了字符串 <code>&quot;Redis&quot;</code> ，而 <code>s2</code> 则保存了字符串 <code>&quot;MongoDB&quot;</code> ，如图 2-7 所示。</p>\n<p><img src=\"/img/9b9139fd46f7419279ba6346b090ddc7.png\" alt=\"img\"></p>\n<p>如果一个程序员决定通过执行：</p>\n<pre><code class=\"language-java\">strcat(s1, &quot; Cluster&quot;);\n</code></pre>\n<p>将 <code>s1</code> 的内容修改为 <code>&quot;Redis Cluster&quot;</code> ，但粗心的他却忘了在执行 <code>strcat</code> 之前为 <code>s1</code> 分配足够的空间，那么在 <code>strcat</code> 函数执行之后，<code>s1</code> 的数据将溢出到 <code>s2</code> 所在的空间中，导致 <code>s2</code> 保存的内容被意外地修改，如图 2-8 所示。</p>\n<p><img src=\"/img/79c1c0a4746f50074f0e44491d3cb8b3.png\" alt=\"img\"></p>\n<p>与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。</p>\n<p>举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 <code>sdscat</code> 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，<code>sdscat</code> 会先检查给定 SDS 的空间是否足够，如果不够的话，<code>sdscat</code> 就会先扩展 SDS 的空间，然后才执行拼接操作。</p>\n<p>比如说，如果我们执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>其中 SDS 值 <code>s</code> 如图 2-9 所示，那么 <code>sdscat</code> 将在执行拼接操作之前检查 <code>s</code> 的长度是否足够，在发现 <code>s</code> 目前的空间不足以拼接 <code>&quot; Cluster&quot;</code> 之后，<code>sdscat</code> 就会先扩展 <code>s</code> 的空间，然后才执行拼接 <code>&quot; Cluster&quot;</code> 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。</p>\n<p><img src=\"/img/a4ef9088f117a8766ac65c4a1a630a34.png\" alt=\"img\"></p>\n<p><img src=\"/img/57c9d55222f65769c0ab0844bc5b6432.png\" alt=\"img\"></p>\n<p>注意图 2-10 所示的 SDS ：<code>sdscat</code> 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 <code>13</code> 字节的未使用空间，并且拼接之后的字符串也正好是 <code>13</code> 字节长，这种现象既不是 bug 也不是巧合，它和 SDS 的空间分配策略有关，接下来的小节将对这一策略进行说明。</p>\n<h4 id=\"减少修改字符串时带来的内存重分配次数\">减少修改字符串时带来的内存重分配次数</h4>\n<p>正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 <code>N</code> 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 <code>N+1</code> 个字符长的数组（额外的一个字符空间用于保存空字符）。</p>\n<p>因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作：</p>\n<ul>\n<li>如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。</li>\n<li>如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。</li>\n</ul>\n<p>举个例子，如果我们持有一个值为 <code>&quot;Redis&quot;</code> 的 C 字符串 <code>s</code> ，那么为了将 <code>s</code> 的值改为 <code>&quot;Redis Cluster&quot;</code> ，在执行：</p>\n<pre><code class=\"language-java\">strcat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>之前，我们需要先使用内存重分配操作，扩展 <code>s</code> 的空间。</p>\n<p>之后，如果我们又打算将 <code>s</code> 的值从 <code>&quot;Redis Cluster&quot;</code> 改为 <code>&quot;Redis Cluster Tutorial&quot;</code> ，那么在执行：</p>\n<pre><code class=\"language-java\">strcat(s, &quot; Tutorial&quot;);\n</code></pre>\n<p>之前，我们需要再次使用内存重分配扩展 <code>s</code> 的空间，诸如此类。</p>\n<p>因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作：</p>\n<ul>\n<li>在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。</li>\n<li>但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。</li>\n</ul>\n<p>为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，<code>buf</code> 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 <code>free</code> 属性记录。</p>\n<p>通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略。</p>\n<h5 id=\"空间预分配\">空间预分配</h5>\n<p>空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。</p>\n<p>其中，额外分配的未使用空间数量由以下公式决定：</p>\n<ul>\n<li>如果对 SDS 进行修改之后，SDS 的长度（也即是 <code>len</code> 属性的值）将小于 <code>1 MB</code> ，那么程序分配和 <code>len</code> 属性同样大小的未使用空间，这时 SDS <code>len</code> 属性的值将和 <code>free</code> 属性的值相同。举个例子，如果进行修改之后，SDS 的 <code>len</code> 将变成 <code>13</code> 字节，那么程序也会分配 <code>13</code> 字节的未使用空间，SDS 的 <code>buf</code> 数组的实际长度将变成 <code>13 + 13 + 1 = 27</code> 字节（额外的一字节用于保存空字符）。</li>\n<li>如果对 SDS 进行修改之后，SDS 的长度将大于等于 <code>1 MB</code> ，那么程序会分配 <code>1 MB</code> 的未使用空间。举个例子，如果进行修改之后，SDS 的 <code>len</code> 将变成 <code>30 MB</code> ，那么程序会分配 <code>1 MB</code> 的未使用空间，SDS 的 <code>buf</code> 数组的实际长度将为 <code>30 MB + 1 MB + 1 byte</code> 。</li>\n</ul>\n<p>通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。</p>\n<p>举个例子，对于图 2-11 所示的 SDS 值 <code>s</code> 来说，如果我们执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>那么 <code>sdscat</code> 将执行一次内存重分配操作，将 SDS 的长度修改为 <code>13</code> 字节，并将 SDS 的未使用空间同样修改为 <code>13</code> 字节，如图 2-12 所示。</p>\n<p><img src=\"/img/d6f2bab64eddad3ee5d092672e8fac50.png\" alt=\"img\"></p>\n<p><img src=\"/img/41d2985c3545a456341af1e4f5b8231a.png\" alt=\"img\"></p>\n<p>如果这时，我们再次对 <code>s</code> 执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Tutorial&quot;);\n</code></pre>\n<p>那么这次 <code>sdscat</code> 将不需要执行内存重分配：因为未使用空间里面的 <code>13</code> 字节足以保存 <code>9</code> 字节的 <code>&quot; Tutorial&quot;</code> ，执行 <code>sdscat</code> 之后的 SDS 如图 2-13 所示。</p>\n<p><img src=\"/img/b5a900fe460481976d0ae062ab18dd61.png\" alt=\"img\"></p>\n<p>在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。</p>\n<p>通过这种预分配策略，SDS 将连续增长 <code>N</code> 次字符串所需的内存重分配次数从必定 <code>N</code> 次降低为最多 <code>N</code> 次。</p>\n<h5 id=\"惰性空间释放\">惰性空间释放</h5>\n<p>惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 <code>free</code> 属性将这些字节的数量记录起来，并等待将来使用。</p>\n<p>举个例子，<code>sdstrim</code> 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。</p>\n<p>比如对于图 2-14 所示的 SDS 值 <code>s</code> 来说，执行：</p>\n<pre><code class=\"language-java\">sdstrim(s, &quot;XY&quot;);   // 移除 SDS 字符串中的所有 'X' 和 'Y'\n</code></pre>\n<p>会将 SDS 修改成图 2-15 所示的样子。</p>\n<p><img src=\"/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png\" alt=\"img\"></p>\n<p><img src=\"/img/f785ade6f77521bce18fc9f8825d4d9b.png\" alt=\"img\"></p>\n<p>注意执行 <code>sdstrim</code> 之后的 SDS 并没有释放多出来的 <code>8</code> 字节空间，而是将这 <code>8</code> 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。</p>\n<p>举个例子，如果现在对 <code>s</code> 执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Redis&quot;);\n</code></pre>\n<p>那么完成这次 <code>sdscat</code> 操作将不需要执行内存重分配：因为 SDS 里面预留的 <code>8</code> 字节空间已经足以拼接 <code>6</code> 个字节长的 <code>&quot; Redis&quot;</code> ，如图 2-16 所示。</p>\n<p><img src=\"/img/2b97031c92882db452d126852b74cfb2.png\" alt=\"img\"></p>\n<p>通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。</p>\n<p>与此同时，SDS 也提供了相应的 API ，让我们可以在有需要时，真正地释放 SDS 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。</p>\n<h4 id=\"二进制安全\">二进制安全</h4>\n<p>C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。</p>\n<p>举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 <code>&quot;Redis&quot;</code> ，而忽略之后的 <code>&quot;Cluster&quot;</code> 。</p>\n<p><img src=\"/img/9c9e497f4dc73587aa9d620a4149101c.png\" alt=\"img\"></p>\n<p>虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的（binary-safe）：所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 <code>buf</code> 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。</p>\n<p>这也是我们将 SDS 的 <code>buf</code> 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。</p>\n<p>比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 <code>len</code> 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示。</p>\n<p><img src=\"/img/157e6450653a0fc457342b5794658fc9.png\" alt=\"img\"></p>\n<p>通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据。</p>\n<h5 id=\"兼容部分-C-字符串函数\">兼容部分 C 字符串函数</h5>\n<p>虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 <code>buf</code> 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 <code>&lt;string.h&gt;</code> 库定义的函数。</p>\n<p><img src=\"/img/d1a8d5d473881fffebc5baccd896258d.png\" alt=\"img\"></p>\n<p>举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 <code>sds</code> ，那么我们就可以重用 <code>&lt;string.h&gt;/strcasecmp</code> 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串：</p>\n<pre><code>strcasecmp(sds-&gt;buf, &quot;hello world&quot;);\n</code></pre>\n<p>这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。</p>\n<p>与此类似，我们还可以将一个保存文本数据的 SDS 作为 <code>strcat</code> 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面：</p>\n<pre><code>strcat(c_string, sds-&gt;buf);\n</code></pre>\n<p>这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。</p>\n<p>通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 <code>&lt;string.h&gt;</code> 函数库，从而避免了不必要的代码重复。</p>\n<h3 id=\"总结\">总结</h3>\n<p>表 2-1 对 C 字符串和 SDS 之间的区别进行了总结。</p>\n<hr>\n<p>表 2-1 C 字符串和 SDS 之间的区别</p>\n<p><img src=\"/img/image-20220122144100590.png\" alt=\"image-20220122144100590\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>数据结构与对象</h1>\n<h2 id=\"简单动态字符串\">简单动态字符串</h2>\n<h3 id=\"SDS的定义\">SDS的定义</h3>\n<p>每个 <code>sds.h/sdshdr</code> 结构表示一个 SDS 值：</p>\n<pre><code class=\"language-java\">struct sdshdr &#123;\n    // 记录 buf 数组中已使用字节的数量\n    // 等于 SDS 所保存字符串的长度\n    int len;\n    // 记录 buf 数组中未使用字节的数量\n    int free;\n    // 字节数组，用于保存字符串\n    char buf[];\n&#125;;\n</code></pre>\n<p>图 2-1 展示了一个 SDS 示例：</p>\n<ul>\n<li><code>free</code> 属性的值为 <code>0</code> ，表示这个 SDS 没有分配任何未使用空间。</li>\n<li><code>len</code> 属性的值为 <code>5</code> ，表示这个 SDS 保存了一个五字节长的字符串。</li>\n<li><code>buf</code> 属性是一个 <code>char</code> 类型的数组，数组的前五个字节分别保存了 <code>'R'</code> 、 <code>'e'</code> 、 <code>'d'</code> 、 <code>'i'</code> 、 <code>'s'</code> 五个字符，而最后一个字节则保存了空字符 <code>'\\0'</code> 。</li>\n</ul>\n<p><img src=\"/img/ffd661fb15bf746863a6e57d7aca04c4.png\" alt=\"img\"></p>\n<p>SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 <code>1</code> 字节空间不计算在 SDS 的 <code>len</code> 属性里面，并且为空字符分配额外的 <code>1</code> 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。</p>\n<p>遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。</p>\n<p>举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 <code>s</code> ，那么我们可以直接使用 <code>stdio.h/printf</code> 函数，通过执行以下语句：</p>\n<pre><code class=\"language-java\">printf(&quot;%s&quot;, s-&gt;buf);\n</code></pre>\n<p>来打印出 SDS 保存的字符串值 <code>&quot;Redis&quot;</code> ，而无须为 SDS 编写专门的打印函数。</p>\n<p>图2-2 展示了另一个 SDS 示例:</p>\n<ul>\n<li>这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 <code>&quot;Redis&quot;</code> 。</li>\n<li>这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 <code>buf</code> 数组分配了五字节未使用空间，所以它的 <code>free</code> 属性的值为 <code>5</code>（图中使用五个空格来表示五字节的未使用空间）。</li>\n</ul>\n<p><img src=\"/img/e13f637bd584b1563442aeaece8cf1e4.png\" alt=\"img\"></p>\n<p>接下来的一节将详细地说明未使用空间在 SDS 中的作用。</p>\n<h3 id=\"SDS-与-C-字符串的区别\">SDS 与 C 字符串的区别</h3>\n<p>根据传统，C 语言使用长度为 <code>N+1</code> 的字符数组来表示长度为 <code>N</code> 的字符串，并且字符数组的最后一个元素总是空字符 <code>'\\0'</code> 。</p>\n<p>比如说，图 2-3 就展示了一个值为 <code>&quot;Redis&quot;</code> 的 C 字符串：</p>\n<p><img src=\"/img/2cfc91e2dc6104321827e210ccfc595c.png\" alt=\"img\"></p>\n<p>C 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，本节接下来的内容将详细对比 C 字符串和 SDS 之间的区别，并说明 SDS 比 C 字符串更适用于 Redis 的原因。</p>\n<h4 id=\"常数复杂度获取字符串长度\">常数复杂度获取字符串长度</h4>\n<p>因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。</p>\n<p>举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程。</p>\n<p><img src=\"/img/ce0bdff067bf19a083c5d26b1266bb44.png\" alt=\"img\"></p>\n<p><img src=\"/img/edf97c87de27aae8adafce7f255f0870.png\" alt=\"img\"></p>\n<p><img src=\"/img/f8f4bde6cb9aeac56366a2f3853f24f3.png\" alt=\"img\"></p>\n<p><img src=\"/img/a7d8693f0dccfde8ecd0919cff94ff88.png\" alt=\"img\"></p>\n<p><img src=\"/img/879bfdfc7cab85a54bed5a33275c6dc0.png\" alt=\"img\"></p>\n<p><img src=\"/img/d133e6d9b982b320594ea62168dbe462.png\" alt=\"img\"></p>\n<p>和 C 字符串不同，因为 SDS 在 <code>len</code> 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。</p>\n<p>举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 <code>len</code> 属性，就可以立即知道 SDS 的长度为 <code>5</code> 字节：</p>\n<p><img src=\"/img/1112dfc7b11a214351a67103cd58a5c8.png\" alt=\"img\"></p>\n<p>又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 <code>len</code> 属性，就可以立即知道 SDS 的长度为 <code>11</code> 字节。</p>\n<p><img src=\"/img/7fdd593ac3e140d5cd2e9328b51110c9.png\" alt=\"img\"></p>\n<p>设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。</p>\n<p>通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。</p>\n<p>比如说，因为字符串键在底层使用 SDS 来实现，所以即使我们对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 。</p>\n<h4 id=\"杜绝缓冲区溢出\">杜绝缓冲区溢出</h4>\n<p>除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。</p>\n<p>举个例子，<code>&lt;string.h&gt;/strcat</code> 函数可以将 <code>src</code> 字符串中的内容拼接到 <code>dest</code> 字符串的末尾：</p>\n<pre><code class=\"language-java\">char *strcat(char *dest, const char *src);\n</code></pre>\n<p>因为 C 字符串不记录自身的长度，所以 <code>strcat</code> 假定用户在执行这个函数时，已经为 <code>dest</code> 分配了足够多的内存，可以容纳 <code>src</code> 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。</p>\n<p>举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 <code>s1</code> 和 <code>s2</code> ，其中 <code>s1</code> 保存了字符串 <code>&quot;Redis&quot;</code> ，而 <code>s2</code> 则保存了字符串 <code>&quot;MongoDB&quot;</code> ，如图 2-7 所示。</p>\n<p><img src=\"/img/9b9139fd46f7419279ba6346b090ddc7.png\" alt=\"img\"></p>\n<p>如果一个程序员决定通过执行：</p>\n<pre><code class=\"language-java\">strcat(s1, &quot; Cluster&quot;);\n</code></pre>\n<p>将 <code>s1</code> 的内容修改为 <code>&quot;Redis Cluster&quot;</code> ，但粗心的他却忘了在执行 <code>strcat</code> 之前为 <code>s1</code> 分配足够的空间，那么在 <code>strcat</code> 函数执行之后，<code>s1</code> 的数据将溢出到 <code>s2</code> 所在的空间中，导致 <code>s2</code> 保存的内容被意外地修改，如图 2-8 所示。</p>\n<p><img src=\"/img/79c1c0a4746f50074f0e44491d3cb8b3.png\" alt=\"img\"></p>\n<p>与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。</p>\n<p>举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 <code>sdscat</code> 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，<code>sdscat</code> 会先检查给定 SDS 的空间是否足够，如果不够的话，<code>sdscat</code> 就会先扩展 SDS 的空间，然后才执行拼接操作。</p>\n<p>比如说，如果我们执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>其中 SDS 值 <code>s</code> 如图 2-9 所示，那么 <code>sdscat</code> 将在执行拼接操作之前检查 <code>s</code> 的长度是否足够，在发现 <code>s</code> 目前的空间不足以拼接 <code>&quot; Cluster&quot;</code> 之后，<code>sdscat</code> 就会先扩展 <code>s</code> 的空间，然后才执行拼接 <code>&quot; Cluster&quot;</code> 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。</p>\n<p><img src=\"/img/a4ef9088f117a8766ac65c4a1a630a34.png\" alt=\"img\"></p>\n<p><img src=\"/img/57c9d55222f65769c0ab0844bc5b6432.png\" alt=\"img\"></p>\n<p>注意图 2-10 所示的 SDS ：<code>sdscat</code> 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 <code>13</code> 字节的未使用空间，并且拼接之后的字符串也正好是 <code>13</code> 字节长，这种现象既不是 bug 也不是巧合，它和 SDS 的空间分配策略有关，接下来的小节将对这一策略进行说明。</p>\n<h4 id=\"减少修改字符串时带来的内存重分配次数\">减少修改字符串时带来的内存重分配次数</h4>\n<p>正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 <code>N</code> 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 <code>N+1</code> 个字符长的数组（额外的一个字符空间用于保存空字符）。</p>\n<p>因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作：</p>\n<ul>\n<li>如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。</li>\n<li>如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。</li>\n</ul>\n<p>举个例子，如果我们持有一个值为 <code>&quot;Redis&quot;</code> 的 C 字符串 <code>s</code> ，那么为了将 <code>s</code> 的值改为 <code>&quot;Redis Cluster&quot;</code> ，在执行：</p>\n<pre><code class=\"language-java\">strcat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>之前，我们需要先使用内存重分配操作，扩展 <code>s</code> 的空间。</p>\n<p>之后，如果我们又打算将 <code>s</code> 的值从 <code>&quot;Redis Cluster&quot;</code> 改为 <code>&quot;Redis Cluster Tutorial&quot;</code> ，那么在执行：</p>\n<pre><code class=\"language-java\">strcat(s, &quot; Tutorial&quot;);\n</code></pre>\n<p>之前，我们需要再次使用内存重分配扩展 <code>s</code> 的空间，诸如此类。</p>\n<p>因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作：</p>\n<ul>\n<li>在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。</li>\n<li>但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。</li>\n</ul>\n<p>为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，<code>buf</code> 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 <code>free</code> 属性记录。</p>\n<p>通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略。</p>\n<h5 id=\"空间预分配\">空间预分配</h5>\n<p>空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。</p>\n<p>其中，额外分配的未使用空间数量由以下公式决定：</p>\n<ul>\n<li>如果对 SDS 进行修改之后，SDS 的长度（也即是 <code>len</code> 属性的值）将小于 <code>1 MB</code> ，那么程序分配和 <code>len</code> 属性同样大小的未使用空间，这时 SDS <code>len</code> 属性的值将和 <code>free</code> 属性的值相同。举个例子，如果进行修改之后，SDS 的 <code>len</code> 将变成 <code>13</code> 字节，那么程序也会分配 <code>13</code> 字节的未使用空间，SDS 的 <code>buf</code> 数组的实际长度将变成 <code>13 + 13 + 1 = 27</code> 字节（额外的一字节用于保存空字符）。</li>\n<li>如果对 SDS 进行修改之后，SDS 的长度将大于等于 <code>1 MB</code> ，那么程序会分配 <code>1 MB</code> 的未使用空间。举个例子，如果进行修改之后，SDS 的 <code>len</code> 将变成 <code>30 MB</code> ，那么程序会分配 <code>1 MB</code> 的未使用空间，SDS 的 <code>buf</code> 数组的实际长度将为 <code>30 MB + 1 MB + 1 byte</code> 。</li>\n</ul>\n<p>通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。</p>\n<p>举个例子，对于图 2-11 所示的 SDS 值 <code>s</code> 来说，如果我们执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Cluster&quot;);\n</code></pre>\n<p>那么 <code>sdscat</code> 将执行一次内存重分配操作，将 SDS 的长度修改为 <code>13</code> 字节，并将 SDS 的未使用空间同样修改为 <code>13</code> 字节，如图 2-12 所示。</p>\n<p><img src=\"/img/d6f2bab64eddad3ee5d092672e8fac50.png\" alt=\"img\"></p>\n<p><img src=\"/img/41d2985c3545a456341af1e4f5b8231a.png\" alt=\"img\"></p>\n<p>如果这时，我们再次对 <code>s</code> 执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Tutorial&quot;);\n</code></pre>\n<p>那么这次 <code>sdscat</code> 将不需要执行内存重分配：因为未使用空间里面的 <code>13</code> 字节足以保存 <code>9</code> 字节的 <code>&quot; Tutorial&quot;</code> ，执行 <code>sdscat</code> 之后的 SDS 如图 2-13 所示。</p>\n<p><img src=\"/img/b5a900fe460481976d0ae062ab18dd61.png\" alt=\"img\"></p>\n<p>在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。</p>\n<p>通过这种预分配策略，SDS 将连续增长 <code>N</code> 次字符串所需的内存重分配次数从必定 <code>N</code> 次降低为最多 <code>N</code> 次。</p>\n<h5 id=\"惰性空间释放\">惰性空间释放</h5>\n<p>惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 <code>free</code> 属性将这些字节的数量记录起来，并等待将来使用。</p>\n<p>举个例子，<code>sdstrim</code> 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。</p>\n<p>比如对于图 2-14 所示的 SDS 值 <code>s</code> 来说，执行：</p>\n<pre><code class=\"language-java\">sdstrim(s, &quot;XY&quot;);   // 移除 SDS 字符串中的所有 'X' 和 'Y'\n</code></pre>\n<p>会将 SDS 修改成图 2-15 所示的样子。</p>\n<p><img src=\"/img/a9ef70a7189d7eeaba7c7b93d1d93e7b.png\" alt=\"img\"></p>\n<p><img src=\"/img/f785ade6f77521bce18fc9f8825d4d9b.png\" alt=\"img\"></p>\n<p>注意执行 <code>sdstrim</code> 之后的 SDS 并没有释放多出来的 <code>8</code> 字节空间，而是将这 <code>8</code> 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。</p>\n<p>举个例子，如果现在对 <code>s</code> 执行：</p>\n<pre><code class=\"language-java\">sdscat(s, &quot; Redis&quot;);\n</code></pre>\n<p>那么完成这次 <code>sdscat</code> 操作将不需要执行内存重分配：因为 SDS 里面预留的 <code>8</code> 字节空间已经足以拼接 <code>6</code> 个字节长的 <code>&quot; Redis&quot;</code> ，如图 2-16 所示。</p>\n<p><img src=\"/img/2b97031c92882db452d126852b74cfb2.png\" alt=\"img\"></p>\n<p>通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。</p>\n<p>与此同时，SDS 也提供了相应的 API ，让我们可以在有需要时，真正地释放 SDS 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。</p>\n<h4 id=\"二进制安全\">二进制安全</h4>\n<p>C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。</p>\n<p>举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 <code>&quot;Redis&quot;</code> ，而忽略之后的 <code>&quot;Cluster&quot;</code> 。</p>\n<p><img src=\"/img/9c9e497f4dc73587aa9d620a4149101c.png\" alt=\"img\"></p>\n<p>虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的（binary-safe）：所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 <code>buf</code> 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。</p>\n<p>这也是我们将 SDS 的 <code>buf</code> 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。</p>\n<p>比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 <code>len</code> 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示。</p>\n<p><img src=\"/img/157e6450653a0fc457342b5794658fc9.png\" alt=\"img\"></p>\n<p>通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据。</p>\n<h5 id=\"兼容部分-C-字符串函数\">兼容部分 C 字符串函数</h5>\n<p>虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 <code>buf</code> 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 <code>&lt;string.h&gt;</code> 库定义的函数。</p>\n<p><img src=\"/img/d1a8d5d473881fffebc5baccd896258d.png\" alt=\"img\"></p>\n<p>举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 <code>sds</code> ，那么我们就可以重用 <code>&lt;string.h&gt;/strcasecmp</code> 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串：</p>\n<pre><code>strcasecmp(sds-&gt;buf, &quot;hello world&quot;);\n</code></pre>\n<p>这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。</p>\n<p>与此类似，我们还可以将一个保存文本数据的 SDS 作为 <code>strcat</code> 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面：</p>\n<pre><code>strcat(c_string, sds-&gt;buf);\n</code></pre>\n<p>这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。</p>\n<p>通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 <code>&lt;string.h&gt;</code> 函数库，从而避免了不必要的代码重复。</p>\n<h3 id=\"总结\">总结</h3>\n<p>表 2-1 对 C 字符串和 SDS 之间的区别进行了总结。</p>\n<hr>\n<p>表 2-1 C 字符串和 SDS 之间的区别</p>\n<p><img src=\"/img/image-20220122144100590.png\" alt=\"image-20220122144100590\"></p>\n"},{"title":"python可视化基础","author":"ztq","date":"2021-04-17T10:45:00.000Z","_content":"\n# <center>Python可视化基础</center>\n\n# 1 常用的Python可视化库\n\n* <b>Matplotlib</b>\n  是一个最基础的Python可视化库，作图风格接近MATLAB，所以称为matplotlib。一般都是从matplotlib上手Python数据可视化，然后开始做纵向与横向拓展。\n* <b>Pandas</b>: \n  easy to use interface, built on Matplotlib\n* <b>Seaborn</b>: \n  high-level interface, great default styles\n* <b>ggplot</b>:\n  based on R’s ggplot2, uses Grammar of Graphics\n* <b>Plotly</b>: \n  创建交互动态图\n* <b>Pyecharts</b>\n  网页中的动态图\n\n# 2 Jupyter Notebook\n\nJupyter这个名字是它要服务的三种语言的缩写：Julia，PYThon和R，这个名字与“木星（jupiter）”谐音。 \n\n * <h3> Jupyter Notebook打开方式</h3>\n\n&emsp;&emsp;Anaconda prompt 输入Jupyter notebook\n\n&emsp;&emsp;通过Anaconda Navigator打开\n\n * <h3>更改Jupyter Notebook的默认路径</h3>\n\n&emsp;（1）找到配置文件的位置\njupyter notebook --generate-config\n若不是首次执行，会出现是否覆盖的提示，选择No<br/>\n\n&emsp;（2）打开配置文件，进下如下设置\nc.NotebookApp.notebook_dir = '换成自己的目录'，保存\n我最后用的格式是E:\\\\zxc\\\\dir这种形式<br/>\n\n&emsp;（3）关闭重启jupyter  notebook。 \n\n# 3 matplotlib\n\n  <h3> 3.1 Matplotlib简介</h3>\n\n&emsp;Matplotlib是 Python 2D-绘图领域使用最广泛的套件，可以简易地将数据图形化，并且提供多样化的输出格式。<br/>\n&emsp;matplotlib有两个接口，一个是状态机层的接口，通过<font color=red>pyplot</font>模块来进行管理；<br/>\n&emsp;一个是面向对象的接口，通过<font color=red>pylab</font>模块将所有的功能函数全部导入其单独的命名空间内。\n\n在线文档https://matplotlib.org/<br/>\n\n <h3> 3.2 Matplotlib安装</h3>\n\n（1） 使用pip安装\n\n&emsp;1）Win+R输入cmd进入到CMD窗口下，执行python -m pip install -U pip setuptools进行升级。\n\n&emsp;2）输入python -m pip install matplotlib进行自动的安装，系统会自动下载安装包\n\n（2）使用conda安装\nconda install matplotlib\n\n<h3> 3.3 Matplotlib图表结构</h3>\n\nMatplotlib基本图表结构<br/>\n包括坐标轴（X轴、Y轴）、坐标轴标签（axisLabel）、\n坐标轴刻度（tick）、坐标轴刻度标签（tick label）、绘图区（axes）、画布（figure）。\n\n![avatar](C:/Users/zheng/Desktop/aaa/structure.png)\n\n在绘图结构中，figure创建窗口，subplot创建子图。所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。\n\nFigure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。\n\nSubplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像\n\n\n<h3> 3.4 Matplotlib基本应用</h3>\n\n<b> 导入matplotlib</b>\n\n\n```python\nimport matplotlib as mpl\nprint(\"matplotlib version\",mpl.__version__)\n```\n\n    matplotlib version 3.2.2\n\n\n\n```python\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n\n<b>（1）画一个简单的图形</b>\n\n1)简单的使用\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-1,1,50)#从(-1,1)均匀取50个点\ny = 2 * x\n\nplt.plot(x,y)\nplt.show()\n```\n\n\n![png](/img/output_30_0111.png)\n    \n\n\n2）这里我们继续通过画出一个正弦曲线图来讲解下基本用法。<br/>\n通过 np.linspace 方式生成 x，它包含了 50 个元素的数组，这 50 个元素均匀的分布在 [0, 2pi] 的区间上。然后通过 np.sin(x) 生成 y。\n\n\n```python\nx = np.linspace(0, 2 * np.pi, 50)\ny = np.sin(x)\n```\n\n有了 x 和 y 数据之后，我们通过 plt.plot(x, y) 来画出图形，并通过 plt.show() 来显示\n\n\n```python\nplt.plot(x, y)\nplt.show()\n```\n\n\n![png](/img/output_34_0111.png)\n    \n\n\n<center><b>Plot的基本用法</b></center>\n\n折线图和散点图 \nplot(x轴数据，y轴数据，展现形式)，参数是可以叠加的\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\n```python\nplt.plot(x,y,'b:.')\nplt.show()\n```\n\n\n![png](/img/output_37_0111.png)\n    \n\n\n<center><b>线条相关属性标记设置</b></center>\n\n常用属性设置\n颜色参数：\n\t\t\tc-cyan--青色\n\t\t\tr-red--红色\n\t\t\tm-magente--品红\n\t\t\tg-green--绿色\n\t\t\tb-blue--蓝色\n\t\t\ty-yellow--黄色\n\t\t\tk-black--黑色\n\t\t\tw-white--白色\n线条样式：\n\t\t\t-  实线\n\t\t\t-- 虚线\n\t\t\t-. -.式\n\t\t\t:  细小虚线\n            o  circle \n点的样式：\n\t\t\ts--方形\n\t\t\th--六角形\n\t\t\tH--六角形\n\t\t\t*--星形\n\t\t\t+--加号\n\t\t\tx--x形\n\t\t\td--菱形\n\t\t\tD--菱形\n\t\t\tp--五角星\n            . 点标记\n            , 像素标记\n            o 圆标记\n\n 更多请参见在线文档https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot\n<b>（2）在一张图里绘制多图形</b>\n\n有时候，可能需要在一个图纸里绘制多个图形，这里我们同时绘制了 (x, y), (x, y * 2)两个图形\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n```\n\n\n![png](/img/output_42_0111.png)\n    \n\n\n绘制出图形之后，我们可以自己调整更多的样式，比如颜色、点、线。\n\n\n```python\nplt.plot(x, y, 'y*-')\nplt.plot(x, y * 2, 'm--')\nplt.show()\n```\n\n\n![png](/img/output_44_0111.png)\n    \n\n\n<b>（3）设置Figure</b>\n\n  你可以认为Matplotlib绘制的图形都在一个默认的 figure 中，当然了，你可以自己创建 figure，好处就是可以控制更多的参数，常见的就是控制图形的大小，这里创建一个 figure，设置大小为 (6, 3)\n\n\n```python\nplt.figure(figsize=(6, 3))\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n```\n\n\n![png](/img/output_47_0111.png)\n    \n\n\n<b>（4）设置标题</b>\n\n直接通过 plt.title 即可设置图形标题\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.title(\"sin(x) & 2sin(x)\")\nplt.show()\n```\n\n\n![png](/img/output_50_0111.png)\n    \n\n\n<b>（5）设置坐标轴</b>\n\n我们来看下如何设置坐标轴的范围以及名称。\n\n坐标轴范围设置\nplt.axis([xmin,xmax,ymin,ymax])<br/>\n也可以通过xlim(xmin,xmax)，ylim(xmin,xmax)方法设置坐标轴范围\n\n通过 xlabel 和 ylabel 来设置轴的名称。\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.xlim((0, np.pi + 1))\nplt.ylim((-3, 3))\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n```\n\n\n![png](/img/output_55_0111.png)\n    \n\n\n等价与下面的方式\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.axis([0, np.pi + 1,-3,3])\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n```\n\n\n![png](/img/output_57_0111.png)\n    \n\n\n此外，我们也可以通过 xticks 和 yticks 来设置轴的刻度\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.xticks((0, np.pi * 0.5, np.pi, np.pi * 1.5, np.pi * 2))\nplt.show()\n```\n\n\n![png](/img/output_59_0111.png)\n    \n\n\n<center><b>xticks yticks</b></center>\n\nplt.xticks()/plt.yticks()设置轴记号,人为设置坐标轴的刻度显示的值\n\n<b>例1：xticks的用法</b>\nxticks()函数原型：\n\nxticks(ticks, [labels], **kwargs)\n\n参数说明：\nticks：数组类型，用于设置X轴刻度间隔\n[labels]：数组类型，用于设置每个间隔的显示标签\n**kwargs：用于设置标签字体倾斜度和颜色等外观属性。（注：python里的双星号代表这个位置接收任意多个关键字参数，可参考：python学习：python的星号（*）和双星号（**）用法）\n不使用xticks时\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.show()\n```\n\n\n![png](/img/output_65_0111.png)\n    \n\n\n使用xticks后\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.xticks(x2, calendar.month_name[1:13],color='blue',rotation=60) \n#参数x空值X轴的间隔，第二个参数控制每个间隔显示的文本，后面两个参数控制标签的颜色和旋转角度\nplt.show()\n```\n\n\n![png](/img/output_67_0111.png)\n    \n\n\n例2参考：http://blog.sina.com.cn/s/blog_14478a3250102y73b.html\n\n<b>（6）设置 label 和 legend</b>\n\n设置 label 和 legend 的目的就是为了区分出每个数据对应的图形名称。\n\n\n```python\nplt.plot(x, y, label=\"sin(x)\")\nplt.plot(x, y * 2, label=\"2sin(x)\")\n# plt.legend()\nplt.legend(loc='best')\nplt.show()\n```\n\n\n![png](/img/output_71_0111.png)\n ![png](/img/loc.png)\n\n<b>（7）添加注释</b>\n\n有时候我们需要对特定的点进行标注，我们可以使用 plt.annotate 函数来实现。\n\n这里我们要标注的点是 (x0, y0) = (π, 0)。\n\n我们也可以使用 plt.text 函数来添加注释。\n\n\n```python\nplt.plot(x, y)\n\nx0 = np.pi\ny0 = 0\n\n# 画出标注点\nplt.scatter(x0, y0, s=50)\n\nplt.annotate('sin(np.pi)=%s' % y0, xy=(np.pi, 0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n\nplt.text(0.5, -0.25, \"sin(np.pi) = 0\", fontdict={'size': 16, 'color': 'r'})\n\nplt.show()\n```\n\n\n![png](/img/output_75_0111.png)\n    \n\n\n对于 annotate 函数的参数，做一个简单解释：<br/>\n• \n'sin(np.pi)=%s' % y0 代表标注的内容，可以通过字符串 %s 将 y0 的值传入字符串；\n\n• \n参数 xycoords='data' 是说基于数据的值来选位置;\n\n• \nxytext=(+30, -30) 和 textcoords='offset points' 表示对于标注位置的描述 和 xy 偏差值，即标注位置是 xy 位置向右移动 30，向下移动30；\n\n• \narrowprops 是对图中箭头类型和箭头弧度的设置，需要用 dict 形式传入。\n\n\n\n<b>（8）使用子图</b>\n\n有时候我们需要将多张子图展示在一起，可以使用 subplot() 实现。即在调用 plot() 函数之前需要先调用 subplot() 函数。该函数的第一个参数代表子图的总行数，第二个参数代表子图的总列数，第三个参数代表活跃区域。\n\n\n```python\nax1 = plt.subplot(2, 2, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 2, 2, sharey=ax1) # 与 ax1 共享y轴\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 2, 3)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 2, 4, sharey=ax3) # 与 ax3 共享y轴\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n```\n\n\n![png](/img/output_79_0111.png)\n    \n\n\n上面的 subplot(2, 2, x) 表示将图像窗口分为 2 行 2 列。x 表示当前子图所在的活跃区。\n\n可以看到，上面的每个子图的大小都是一样的。有时候我们需要不同大小的子图。比如将上面第一张子图完全放置在第一行，其他的子图都放在第二行。\n\n\n```python\nax1 = plt.subplot(2, 1, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 3, 4)\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 3, 5, sharey=ax2)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 3, 6, sharey=ax2)\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n```\n\n\n![png](/img/output_81_0111.png)\n    \n\n\nplt.subplot(2, 1, 1) 将图像窗口分为了 2 行 1 列, 当前活跃区为 1。\n\n使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 当前活跃区为 4。\n\n解释下为什么活跃区为 4，因为上一步中使用 plt.subplot(2, 1, 1) 将整个图像窗口分为 2 行 1 列, 第1个小图占用了第1个位置, 也就是整个第1行. 这一步中使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 于是整个图像窗口的第1行就变成了3列, 也就是成了3个位置, 于是第2行的第1个位置是整个图像窗口的第4个位置。\n\n\n<b>（9）中文乱码解决</b>\n\nMatplotlib 有个让人恼火的问题是，默认情况下，Matplotlib 中文会乱码。\n\n\n```python\nx = ['北京', '上海', '深圳', '广州']\ny = [60000, 58000, 50000, 52000]\nplt.plot(x, y)\nplt.show()\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21271 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 20140 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 19978 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28023 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28145 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 22323 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24191 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24030 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21271 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 20140 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 19978 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28023 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28145 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 22323 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24191 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24030 missing from current font.\n      font.set_text(s, 0, flags=flags)\n\n\n\n\n![png](/img/output_85_1111.png)\n    \n\n\n可以看到，上面所有的中文都乱码了，显示成方框了，如何解决呢？<br/>\n只需要配置下后台字体即可\n\n\n```python\nplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\nplt.plot(x, y)\nplt.show()\n```\n\n\n![png](/img/output_87_0111.png)\n    \n\n\n<h3>3.5数据读取与导入</h3>\n\nhttp://cc.nohup.cc/article/207/#menu_index_1以折线图为例讲解\n\n# 总结\n\n本节课我们了解了Python的画图包，matplotlib的基本用法及数据读取的方法。<br/>\n请同学们课下结合实例进行练习\n\n# 附:MarkDown基本使用\n\n1）空格 \n&nbsp;半角的不换行的空格（推荐使用）\n&emsp;全角空格\n&ensp;半角空格\n2）删除：连按两次D即可删除当前行\n\n3)换行：使用&lt;br\\\\&gt;实现\n\n4)显示行号：在命令模式中的代码单元格按 L 打开数字。\n\n5)颜色的使用 font\n\n6) Markdown表格\n\n| 表头   | 表头   |\n| ------ | ------ |\n| 单元格 | 单元格 |\n| 单元格 | 单元格 |\n\n表格效果如下：\n\n| 表头   | 表头   |\n| ------ | ------ |\n| 单元格 | 单元格 |\n| 单元格 | 单元格 |\n\n\n\n<h3>其他</h3>\n\n1)生新启动内核\n\n2）生成html格式\n\n\n```python\n!jupyter nbconvert --to html Matplotlib.ipynb\n```\n\n    [NbConvertApp] Converting notebook Matplotlib.ipynb to html\n    [NbConvertApp] Writing 642412 bytes to Matplotlib.html\n\n\n\n```python\n\n```","source":"_posts/python可视化基础.md","raw":"title: python可视化基础\nauthor: ztq\ntags:\n  - python\ncategories:\n  - python基础\ndate: 2021-04-17 18:45:00\n\n---\n\n# <center>Python可视化基础</center>\n\n# 1 常用的Python可视化库\n\n* <b>Matplotlib</b>\n  是一个最基础的Python可视化库，作图风格接近MATLAB，所以称为matplotlib。一般都是从matplotlib上手Python数据可视化，然后开始做纵向与横向拓展。\n* <b>Pandas</b>: \n  easy to use interface, built on Matplotlib\n* <b>Seaborn</b>: \n  high-level interface, great default styles\n* <b>ggplot</b>:\n  based on R’s ggplot2, uses Grammar of Graphics\n* <b>Plotly</b>: \n  创建交互动态图\n* <b>Pyecharts</b>\n  网页中的动态图\n\n# 2 Jupyter Notebook\n\nJupyter这个名字是它要服务的三种语言的缩写：Julia，PYThon和R，这个名字与“木星（jupiter）”谐音。 \n\n * <h3> Jupyter Notebook打开方式</h3>\n\n&emsp;&emsp;Anaconda prompt 输入Jupyter notebook\n\n&emsp;&emsp;通过Anaconda Navigator打开\n\n * <h3>更改Jupyter Notebook的默认路径</h3>\n\n&emsp;（1）找到配置文件的位置\njupyter notebook --generate-config\n若不是首次执行，会出现是否覆盖的提示，选择No<br/>\n\n&emsp;（2）打开配置文件，进下如下设置\nc.NotebookApp.notebook_dir = '换成自己的目录'，保存\n我最后用的格式是E:\\\\zxc\\\\dir这种形式<br/>\n\n&emsp;（3）关闭重启jupyter  notebook。 \n\n# 3 matplotlib\n\n  <h3> 3.1 Matplotlib简介</h3>\n\n&emsp;Matplotlib是 Python 2D-绘图领域使用最广泛的套件，可以简易地将数据图形化，并且提供多样化的输出格式。<br/>\n&emsp;matplotlib有两个接口，一个是状态机层的接口，通过<font color=red>pyplot</font>模块来进行管理；<br/>\n&emsp;一个是面向对象的接口，通过<font color=red>pylab</font>模块将所有的功能函数全部导入其单独的命名空间内。\n\n在线文档https://matplotlib.org/<br/>\n\n <h3> 3.2 Matplotlib安装</h3>\n\n（1） 使用pip安装\n\n&emsp;1）Win+R输入cmd进入到CMD窗口下，执行python -m pip install -U pip setuptools进行升级。\n\n&emsp;2）输入python -m pip install matplotlib进行自动的安装，系统会自动下载安装包\n\n（2）使用conda安装\nconda install matplotlib\n\n<h3> 3.3 Matplotlib图表结构</h3>\n\nMatplotlib基本图表结构<br/>\n包括坐标轴（X轴、Y轴）、坐标轴标签（axisLabel）、\n坐标轴刻度（tick）、坐标轴刻度标签（tick label）、绘图区（axes）、画布（figure）。\n\n![avatar](C:/Users/zheng/Desktop/aaa/structure.png)\n\n在绘图结构中，figure创建窗口，subplot创建子图。所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。\n\nFigure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。\n\nSubplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像\n\n\n<h3> 3.4 Matplotlib基本应用</h3>\n\n<b> 导入matplotlib</b>\n\n\n```python\nimport matplotlib as mpl\nprint(\"matplotlib version\",mpl.__version__)\n```\n\n    matplotlib version 3.2.2\n\n\n\n```python\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n\n<b>（1）画一个简单的图形</b>\n\n1)简单的使用\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-1,1,50)#从(-1,1)均匀取50个点\ny = 2 * x\n\nplt.plot(x,y)\nplt.show()\n```\n\n\n![png](/img/output_30_0111.png)\n    \n\n\n2）这里我们继续通过画出一个正弦曲线图来讲解下基本用法。<br/>\n通过 np.linspace 方式生成 x，它包含了 50 个元素的数组，这 50 个元素均匀的分布在 [0, 2pi] 的区间上。然后通过 np.sin(x) 生成 y。\n\n\n```python\nx = np.linspace(0, 2 * np.pi, 50)\ny = np.sin(x)\n```\n\n有了 x 和 y 数据之后，我们通过 plt.plot(x, y) 来画出图形，并通过 plt.show() 来显示\n\n\n```python\nplt.plot(x, y)\nplt.show()\n```\n\n\n![png](/img/output_34_0111.png)\n    \n\n\n<center><b>Plot的基本用法</b></center>\n\n折线图和散点图 \nplot(x轴数据，y轴数据，展现形式)，参数是可以叠加的\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\n```python\nplt.plot(x,y,'b:.')\nplt.show()\n```\n\n\n![png](/img/output_37_0111.png)\n    \n\n\n<center><b>线条相关属性标记设置</b></center>\n\n常用属性设置\n颜色参数：\n\t\t\tc-cyan--青色\n\t\t\tr-red--红色\n\t\t\tm-magente--品红\n\t\t\tg-green--绿色\n\t\t\tb-blue--蓝色\n\t\t\ty-yellow--黄色\n\t\t\tk-black--黑色\n\t\t\tw-white--白色\n线条样式：\n\t\t\t-  实线\n\t\t\t-- 虚线\n\t\t\t-. -.式\n\t\t\t:  细小虚线\n            o  circle \n点的样式：\n\t\t\ts--方形\n\t\t\th--六角形\n\t\t\tH--六角形\n\t\t\t*--星形\n\t\t\t+--加号\n\t\t\tx--x形\n\t\t\td--菱形\n\t\t\tD--菱形\n\t\t\tp--五角星\n            . 点标记\n            , 像素标记\n            o 圆标记\n\n 更多请参见在线文档https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot\n<b>（2）在一张图里绘制多图形</b>\n\n有时候，可能需要在一个图纸里绘制多个图形，这里我们同时绘制了 (x, y), (x, y * 2)两个图形\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n```\n\n\n![png](/img/output_42_0111.png)\n    \n\n\n绘制出图形之后，我们可以自己调整更多的样式，比如颜色、点、线。\n\n\n```python\nplt.plot(x, y, 'y*-')\nplt.plot(x, y * 2, 'm--')\nplt.show()\n```\n\n\n![png](/img/output_44_0111.png)\n    \n\n\n<b>（3）设置Figure</b>\n\n  你可以认为Matplotlib绘制的图形都在一个默认的 figure 中，当然了，你可以自己创建 figure，好处就是可以控制更多的参数，常见的就是控制图形的大小，这里创建一个 figure，设置大小为 (6, 3)\n\n\n```python\nplt.figure(figsize=(6, 3))\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n```\n\n\n![png](/img/output_47_0111.png)\n    \n\n\n<b>（4）设置标题</b>\n\n直接通过 plt.title 即可设置图形标题\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.title(\"sin(x) & 2sin(x)\")\nplt.show()\n```\n\n\n![png](/img/output_50_0111.png)\n    \n\n\n<b>（5）设置坐标轴</b>\n\n我们来看下如何设置坐标轴的范围以及名称。\n\n坐标轴范围设置\nplt.axis([xmin,xmax,ymin,ymax])<br/>\n也可以通过xlim(xmin,xmax)，ylim(xmin,xmax)方法设置坐标轴范围\n\n通过 xlabel 和 ylabel 来设置轴的名称。\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.xlim((0, np.pi + 1))\nplt.ylim((-3, 3))\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n```\n\n\n![png](/img/output_55_0111.png)\n    \n\n\n等价与下面的方式\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.axis([0, np.pi + 1,-3,3])\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n```\n\n\n![png](/img/output_57_0111.png)\n    \n\n\n此外，我们也可以通过 xticks 和 yticks 来设置轴的刻度\n\n\n```python\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.xticks((0, np.pi * 0.5, np.pi, np.pi * 1.5, np.pi * 2))\nplt.show()\n```\n\n\n![png](/img/output_59_0111.png)\n    \n\n\n<center><b>xticks yticks</b></center>\n\nplt.xticks()/plt.yticks()设置轴记号,人为设置坐标轴的刻度显示的值\n\n<b>例1：xticks的用法</b>\nxticks()函数原型：\n\nxticks(ticks, [labels], **kwargs)\n\n参数说明：\nticks：数组类型，用于设置X轴刻度间隔\n[labels]：数组类型，用于设置每个间隔的显示标签\n**kwargs：用于设置标签字体倾斜度和颜色等外观属性。（注：python里的双星号代表这个位置接收任意多个关键字参数，可参考：python学习：python的星号（*）和双星号（**）用法）\n不使用xticks时\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.show()\n```\n\n\n![png](/img/output_65_0111.png)\n    \n\n\n使用xticks后\n\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.xticks(x2, calendar.month_name[1:13],color='blue',rotation=60) \n#参数x空值X轴的间隔，第二个参数控制每个间隔显示的文本，后面两个参数控制标签的颜色和旋转角度\nplt.show()\n```\n\n\n![png](/img/output_67_0111.png)\n    \n\n\n例2参考：http://blog.sina.com.cn/s/blog_14478a3250102y73b.html\n\n<b>（6）设置 label 和 legend</b>\n\n设置 label 和 legend 的目的就是为了区分出每个数据对应的图形名称。\n\n\n```python\nplt.plot(x, y, label=\"sin(x)\")\nplt.plot(x, y * 2, label=\"2sin(x)\")\n# plt.legend()\nplt.legend(loc='best')\nplt.show()\n```\n\n\n![png](/img/output_71_0111.png)\n ![png](/img/loc.png)\n\n<b>（7）添加注释</b>\n\n有时候我们需要对特定的点进行标注，我们可以使用 plt.annotate 函数来实现。\n\n这里我们要标注的点是 (x0, y0) = (π, 0)。\n\n我们也可以使用 plt.text 函数来添加注释。\n\n\n```python\nplt.plot(x, y)\n\nx0 = np.pi\ny0 = 0\n\n# 画出标注点\nplt.scatter(x0, y0, s=50)\n\nplt.annotate('sin(np.pi)=%s' % y0, xy=(np.pi, 0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n\nplt.text(0.5, -0.25, \"sin(np.pi) = 0\", fontdict={'size': 16, 'color': 'r'})\n\nplt.show()\n```\n\n\n![png](/img/output_75_0111.png)\n    \n\n\n对于 annotate 函数的参数，做一个简单解释：<br/>\n• \n'sin(np.pi)=%s' % y0 代表标注的内容，可以通过字符串 %s 将 y0 的值传入字符串；\n\n• \n参数 xycoords='data' 是说基于数据的值来选位置;\n\n• \nxytext=(+30, -30) 和 textcoords='offset points' 表示对于标注位置的描述 和 xy 偏差值，即标注位置是 xy 位置向右移动 30，向下移动30；\n\n• \narrowprops 是对图中箭头类型和箭头弧度的设置，需要用 dict 形式传入。\n\n\n\n<b>（8）使用子图</b>\n\n有时候我们需要将多张子图展示在一起，可以使用 subplot() 实现。即在调用 plot() 函数之前需要先调用 subplot() 函数。该函数的第一个参数代表子图的总行数，第二个参数代表子图的总列数，第三个参数代表活跃区域。\n\n\n```python\nax1 = plt.subplot(2, 2, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 2, 2, sharey=ax1) # 与 ax1 共享y轴\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 2, 3)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 2, 4, sharey=ax3) # 与 ax3 共享y轴\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n```\n\n\n![png](/img/output_79_0111.png)\n    \n\n\n上面的 subplot(2, 2, x) 表示将图像窗口分为 2 行 2 列。x 表示当前子图所在的活跃区。\n\n可以看到，上面的每个子图的大小都是一样的。有时候我们需要不同大小的子图。比如将上面第一张子图完全放置在第一行，其他的子图都放在第二行。\n\n\n```python\nax1 = plt.subplot(2, 1, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 3, 4)\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 3, 5, sharey=ax2)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 3, 6, sharey=ax2)\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n```\n\n\n![png](/img/output_81_0111.png)\n    \n\n\nplt.subplot(2, 1, 1) 将图像窗口分为了 2 行 1 列, 当前活跃区为 1。\n\n使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 当前活跃区为 4。\n\n解释下为什么活跃区为 4，因为上一步中使用 plt.subplot(2, 1, 1) 将整个图像窗口分为 2 行 1 列, 第1个小图占用了第1个位置, 也就是整个第1行. 这一步中使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 于是整个图像窗口的第1行就变成了3列, 也就是成了3个位置, 于是第2行的第1个位置是整个图像窗口的第4个位置。\n\n\n<b>（9）中文乱码解决</b>\n\nMatplotlib 有个让人恼火的问题是，默认情况下，Matplotlib 中文会乱码。\n\n\n```python\nx = ['北京', '上海', '深圳', '广州']\ny = [60000, 58000, 50000, 52000]\nplt.plot(x, y)\nplt.show()\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21271 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 20140 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 19978 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28023 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28145 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 22323 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24191 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24030 missing from current font.\n      font.set_text(s, 0.0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21271 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 20140 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 19978 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28023 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28145 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 22323 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24191 missing from current font.\n      font.set_text(s, 0, flags=flags)\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24030 missing from current font.\n      font.set_text(s, 0, flags=flags)\n\n\n\n\n![png](/img/output_85_1111.png)\n    \n\n\n可以看到，上面所有的中文都乱码了，显示成方框了，如何解决呢？<br/>\n只需要配置下后台字体即可\n\n\n```python\nplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\nplt.plot(x, y)\nplt.show()\n```\n\n\n![png](/img/output_87_0111.png)\n    \n\n\n<h3>3.5数据读取与导入</h3>\n\nhttp://cc.nohup.cc/article/207/#menu_index_1以折线图为例讲解\n\n# 总结\n\n本节课我们了解了Python的画图包，matplotlib的基本用法及数据读取的方法。<br/>\n请同学们课下结合实例进行练习\n\n# 附:MarkDown基本使用\n\n1）空格 \n&nbsp;半角的不换行的空格（推荐使用）\n&emsp;全角空格\n&ensp;半角空格\n2）删除：连按两次D即可删除当前行\n\n3)换行：使用&lt;br\\\\&gt;实现\n\n4)显示行号：在命令模式中的代码单元格按 L 打开数字。\n\n5)颜色的使用 font\n\n6) Markdown表格\n\n| 表头   | 表头   |\n| ------ | ------ |\n| 单元格 | 单元格 |\n| 单元格 | 单元格 |\n\n表格效果如下：\n\n| 表头   | 表头   |\n| ------ | ------ |\n| 单元格 | 单元格 |\n| 单元格 | 单元格 |\n\n\n\n<h3>其他</h3>\n\n1)生新启动内核\n\n2）生成html格式\n\n\n```python\n!jupyter nbconvert --to html Matplotlib.ipynb\n```\n\n    [NbConvertApp] Converting notebook Matplotlib.ipynb to html\n    [NbConvertApp] Writing 642412 bytes to Matplotlib.html\n\n\n\n```python\n\n```","slug":"python可视化基础","published":1,"updated":"2022-04-04T08:32:40.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2500f97kt94qug0h5q","content":"<h1><center>Python可视化基础</center></h1>\n<h1>1 常用的Python可视化库</h1>\n<ul>\n<li><b>Matplotlib</b><br>\n是一个最基础的Python可视化库，作图风格接近MATLAB，所以称为matplotlib。一般都是从matplotlib上手Python数据可视化，然后开始做纵向与横向拓展。</li>\n<li><b>Pandas</b>: <br>\neasy to use interface, built on Matplotlib</li>\n<li><b>Seaborn</b>: <br>\nhigh-level interface, great default styles</li>\n<li><b>ggplot</b>:<br>\nbased on R’s ggplot2, uses Grammar of Graphics</li>\n<li><b>Plotly</b>:<br>\n创建交互动态图</li>\n<li><b>Pyecharts</b><br>\n网页中的动态图</li>\n</ul>\n<h1>2 Jupyter Notebook</h1>\n<p>Jupyter这个名字是它要服务的三种语言的缩写：Julia，PYThon和R，这个名字与“木星（jupiter）”谐音。</p>\n<ul>\n<li>\n<h3> Jupyter Notebook打开方式</h3>\n</li>\n</ul>\n<p>  Anaconda prompt 输入Jupyter notebook</p>\n<p>  通过Anaconda Navigator打开</p>\n<ul>\n<li>\n<h3>更改Jupyter Notebook的默认路径</h3>\n</li>\n</ul>\n<p> （1）找到配置文件的位置<br>\njupyter notebook --generate-config<br>\n若不是首次执行，会出现是否覆盖的提示，选择No<br/></p>\n<p> （2）打开配置文件，进下如下设置<br>\nc.NotebookApp.notebook_dir = ‘换成自己的目录’，保存<br>\n我最后用的格式是E:\\zxc\\dir这种形式<br/></p>\n<p> （3）关闭重启jupyter  notebook。</p>\n<h1>3 matplotlib</h1>\n  <h3> 3.1 Matplotlib简介</h3>\n<p> Matplotlib是 Python 2D-绘图领域使用最广泛的套件，可以简易地将数据图形化，并且提供多样化的输出格式。<br/><br>\n matplotlib有两个接口，一个是状态机层的接口，通过<font color=red>pyplot</font>模块来进行管理；<br/><br>\n 一个是面向对象的接口，通过<font color=red>pylab</font>模块将所有的功能函数全部导入其单独的命名空间内。</p>\n<p>在线文档https://matplotlib.org/<br/></p>\n <h3> 3.2 Matplotlib安装</h3>\n<p>（1） 使用pip安装</p>\n<p> 1）Win+R输入cmd进入到CMD窗口下，执行python -m pip install -U pip setuptools进行升级。</p>\n<p> 2）输入python -m pip install matplotlib进行自动的安装，系统会自动下载安装包</p>\n<p>（2）使用conda安装<br>\nconda install matplotlib</p>\n<h3> 3.3 Matplotlib图表结构</h3>\n<p>Matplotlib基本图表结构<br/><br>\n包括坐标轴（X轴、Y轴）、坐标轴标签（axisLabel）、<br>\n坐标轴刻度（tick）、坐标轴刻度标签（tick label）、绘图区（axes）、画布（figure）。</p>\n<p><img src=\"C:/Users/zheng/Desktop/aaa/structure.png\" alt=\"avatar\"></p>\n<p>在绘图结构中，figure创建窗口，subplot创建子图。所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。</p>\n<p>Figure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。</p>\n<p>Subplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像</p>\n<h3> 3.4 Matplotlib基本应用</h3>\n<p><b> 导入matplotlib</b></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nprint(&quot;matplotlib version&quot;,mpl.__version__)\n</code></pre>\n<pre><code>matplotlib version 3.2.2\n</code></pre>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n</code></pre>\n<p><b>（1）画一个简单的图形</b></p>\n<p>1)简单的使用</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-1,1,50)#从(-1,1)均匀取50个点\ny = 2 * x\n\nplt.plot(x,y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_30_0111.png\" alt=\"png\"></p>\n<p>2）这里我们继续通过画出一个正弦曲线图来讲解下基本用法。<br/><br>\n通过 np.linspace 方式生成 x，它包含了 50 个元素的数组，这 50 个元素均匀的分布在 [0, 2pi] 的区间上。然后通过 np.sin(x) 生成 y。</p>\n<pre><code class=\"language-python\">x = np.linspace(0, 2 * np.pi, 50)\ny = np.sin(x)\n</code></pre>\n<p>有了 x 和 y 数据之后，我们通过 plt.plot(x, y) 来画出图形，并通过 plt.show() 来显示</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_34_0111.png\" alt=\"png\"></p>\n<center><b>Plot的基本用法</b></center>\n<p>折线图和散点图<br>\nplot(x轴数据，y轴数据，展现形式)，参数是可以叠加的</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>plot(x, y)        # plot x and y using default line style and color<br>\nplot(x, y, ‘bo’)  # plot x and y using blue circle markers<br>\nplot(y)           # plot y using x as index array 0…N-1<br>\nplot(y, ‘r+’)     # ditto, but with red plusses</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<pre><code class=\"language-python\">plt.plot(x,y,'b:.')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_37_0111.png\" alt=\"png\"></p>\n<center><b>线条相关属性标记设置</b></center>\n<p>常用属性设置<br>\n颜色参数：<br>\nc-cyan–青色<br>\nr-red–红色<br>\nm-magente–品红<br>\ng-green–绿色<br>\nb-blue–蓝色<br>\ny-yellow–黄色<br>\nk-black–黑色<br>\nw-white–白色<br>\n线条样式：<br>\n-  实线<br>\n– 虚线<br>\n-. -.式<br>\n:  细小虚线<br>\no  circle<br>\n点的样式：<br>\ns–方形<br>\nh–六角形<br>\nH–六角形<br>\n*–星形<br>\n±-加号<br>\nx–x形<br>\nd–菱形<br>\nD–菱形<br>\np–五角星<br>\n. 点标记<br>\n, 像素标记<br>\no 圆标记</p>\n<p>更多请参见在线文档https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot<br>\n<b>（2）在一张图里绘制多图形</b></p>\n<p>有时候，可能需要在一个图纸里绘制多个图形，这里我们同时绘制了 (x, y), (x, y * 2)两个图形</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0111.png\" alt=\"png\"></p>\n<p>绘制出图形之后，我们可以自己调整更多的样式，比如颜色、点、线。</p>\n<pre><code class=\"language-python\">plt.plot(x, y, 'y*-')\nplt.plot(x, y * 2, 'm--')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0111.png\" alt=\"png\"></p>\n<p><b>（3）设置Figure</b></p>\n<p>你可以认为Matplotlib绘制的图形都在一个默认的 figure 中，当然了，你可以自己创建 figure，好处就是可以控制更多的参数，常见的就是控制图形的大小，这里创建一个 figure，设置大小为 (6, 3)</p>\n<pre><code class=\"language-python\">plt.figure(figsize=(6, 3))\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_47_0111.png\" alt=\"png\"></p>\n<p><b>（4）设置标题</b></p>\n<p>直接通过 plt.title 即可设置图形标题</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.title(&quot;sin(x) &amp; 2sin(x)&quot;)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_50_0111.png\" alt=\"png\"></p>\n<p><b>（5）设置坐标轴</b></p>\n<p>我们来看下如何设置坐标轴的范围以及名称。</p>\n<p>坐标轴范围设置<br>\nplt.axis([xmin,xmax,ymin,ymax])<br/><br>\n也可以通过xlim(xmin,xmax)，ylim(xmin,xmax)方法设置坐标轴范围</p>\n<p>通过 xlabel 和 ylabel 来设置轴的名称。</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.xlim((0, np.pi + 1))\nplt.ylim((-3, 3))\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_55_0111.png\" alt=\"png\"></p>\n<p>等价与下面的方式</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.axis([0, np.pi + 1,-3,3])\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_57_0111.png\" alt=\"png\"></p>\n<p>此外，我们也可以通过 xticks 和 yticks 来设置轴的刻度</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.xticks((0, np.pi * 0.5, np.pi, np.pi * 1.5, np.pi * 2))\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_59_0111.png\" alt=\"png\"></p>\n<center><b>xticks yticks</b></center>\n<p>plt.xticks()/plt.yticks()设置轴记号,人为设置坐标轴的刻度显示的值</p>\n<p><b>例1：xticks的用法</b><br>\nxticks()函数原型：</p>\n<p>xticks(ticks, [labels], **kwargs)</p>\n<p>参数说明：<br>\nticks：数组类型，用于设置X轴刻度间隔<br>\n[labels]：数组类型，用于设置每个间隔的显示标签<br>\n<strong>kwargs：用于设置标签字体倾斜度和颜色等外观属性。（注：python里的双星号代表这个位置接收任意多个关键字参数，可参考：python学习：python的星号（*）和双星号（</strong>）用法）<br>\n不使用xticks时</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_65_0111.png\" alt=\"png\"></p>\n<p>使用xticks后</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.xticks(x2, calendar.month_name[1:13],color='blue',rotation=60) \n#参数x空值X轴的间隔，第二个参数控制每个间隔显示的文本，后面两个参数控制标签的颜色和旋转角度\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_67_0111.png\" alt=\"png\"></p>\n<p>例2参考：<a href=\"http://blog.sina.com.cn/s/blog_14478a3250102y73b.html\">http://blog.sina.com.cn/s/blog_14478a3250102y73b.html</a></p>\n<p><b>（6）设置 label 和 legend</b></p>\n<p>设置 label 和 legend 的目的就是为了区分出每个数据对应的图形名称。</p>\n<pre><code class=\"language-python\">plt.plot(x, y, label=&quot;sin(x)&quot;)\nplt.plot(x, y * 2, label=&quot;2sin(x)&quot;)\n# plt.legend()\nplt.legend(loc='best')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_71_0111.png\" alt=\"png\"><br>\n<img src=\"/img/loc.png\" alt=\"png\"></p>\n<p><b>（7）添加注释</b></p>\n<p>有时候我们需要对特定的点进行标注，我们可以使用 plt.annotate 函数来实现。</p>\n<p>这里我们要标注的点是 (x0, y0) = (π, 0)。</p>\n<p>我们也可以使用 plt.text 函数来添加注释。</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\n\nx0 = np.pi\ny0 = 0\n\n# 画出标注点\nplt.scatter(x0, y0, s=50)\n\nplt.annotate('sin(np.pi)=%s' % y0, xy=(np.pi, 0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='-&gt;', connectionstyle=&quot;arc3,rad=.2&quot;))\n\nplt.text(0.5, -0.25, &quot;sin(np.pi) = 0&quot;, fontdict=&#123;'size': 16, 'color': 'r'&#125;)\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_75_0111.png\" alt=\"png\"></p>\n<p>对于 annotate 函数的参数，做一个简单解释：<br/><br>\n•<br>\n‘sin(np.pi)=%s’ % y0 代表标注的内容，可以通过字符串 %s 将 y0 的值传入字符串；</p>\n<p>•<br>\n参数 xycoords=‘data’ 是说基于数据的值来选位置;</p>\n<p>•<br>\nxytext=(+30, -30) 和 textcoords=‘offset points’ 表示对于标注位置的描述 和 xy 偏差值，即标注位置是 xy 位置向右移动 30，向下移动30；</p>\n<p>•<br>\narrowprops 是对图中箭头类型和箭头弧度的设置，需要用 dict 形式传入。</p>\n<p><b>（8）使用子图</b></p>\n<p>有时候我们需要将多张子图展示在一起，可以使用 subplot() 实现。即在调用 plot() 函数之前需要先调用 subplot() 函数。该函数的第一个参数代表子图的总行数，第二个参数代表子图的总列数，第三个参数代表活跃区域。</p>\n<pre><code class=\"language-python\">ax1 = plt.subplot(2, 2, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 2, 2, sharey=ax1) # 与 ax1 共享y轴\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 2, 3)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 2, 4, sharey=ax3) # 与 ax3 共享y轴\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_79_0111.png\" alt=\"png\"></p>\n<p>上面的 subplot(2, 2, x) 表示将图像窗口分为 2 行 2 列。x 表示当前子图所在的活跃区。</p>\n<p>可以看到，上面的每个子图的大小都是一样的。有时候我们需要不同大小的子图。比如将上面第一张子图完全放置在第一行，其他的子图都放在第二行。</p>\n<pre><code class=\"language-python\">ax1 = plt.subplot(2, 1, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 3, 4)\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 3, 5, sharey=ax2)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 3, 6, sharey=ax2)\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_81_0111.png\" alt=\"png\"></p>\n<p>plt.subplot(2, 1, 1) 将图像窗口分为了 2 行 1 列, 当前活跃区为 1。</p>\n<p>使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 当前活跃区为 4。</p>\n<p>解释下为什么活跃区为 4，因为上一步中使用 plt.subplot(2, 1, 1) 将整个图像窗口分为 2 行 1 列, 第1个小图占用了第1个位置, 也就是整个第1行. 这一步中使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 于是整个图像窗口的第1行就变成了3列, 也就是成了3个位置, 于是第2行的第1个位置是整个图像窗口的第4个位置。</p>\n<p><b>（9）中文乱码解决</b></p>\n<p>Matplotlib 有个让人恼火的问题是，默认情况下，Matplotlib 中文会乱码。</p>\n<pre><code class=\"language-python\">x = ['北京', '上海', '深圳', '广州']\ny = [60000, 58000, 50000, 52000]\nplt.plot(x, y)\nplt.show()\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21271 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 20140 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 19978 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28023 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28145 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 22323 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24191 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24030 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21271 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 20140 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 19978 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28023 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28145 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 22323 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24191 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24030 missing from current font.\n  font.set_text(s, 0, flags=flags)\n</code></pre>\n<p><img src=\"/img/output_85_1111.png\" alt=\"png\"></p>\n<p>可以看到，上面所有的中文都乱码了，显示成方框了，如何解决呢？<br/><br>\n只需要配置下后台字体即可</p>\n<pre><code class=\"language-python\">plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\nplt.plot(x, y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_87_0111.png\" alt=\"png\"></p>\n<h3>3.5数据读取与导入</h3>\n<p><a href=\"http://cc.nohup.cc/article/207/#menu_index_1%E4%BB%A5%E6%8A%98%E7%BA%BF%E5%9B%BE%E4%B8%BA%E4%BE%8B%E8%AE%B2%E8%A7%A3\">http://cc.nohup.cc/article/207/#menu_index_1以折线图为例讲解</a></p>\n<h1>总结</h1>\n<p>本节课我们了解了Python的画图包，matplotlib的基本用法及数据读取的方法。<br/><br>\n请同学们课下结合实例进行练习</p>\n<h1>附:MarkDown基本使用</h1>\n<p>1）空格<br>\n 半角的不换行的空格（推荐使用）<br>\n 全角空格<br>\n 半角空格<br>\n2）删除：连按两次D即可删除当前行</p>\n<p>3)换行：使用&lt;br\\&gt;实现</p>\n<p>4)显示行号：在命令模式中的代码单元格按 L 打开数字。</p>\n<p>5)颜色的使用 font</p>\n<ol start=\"6\">\n<li>Markdown表格</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>表头</th>\n<th>表头</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n</tbody>\n</table>\n<p>表格效果如下：</p>\n<table>\n<thead>\n<tr>\n<th>表头</th>\n<th>表头</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n</tbody>\n</table>\n<h3>其他</h3>\n<p>1)生新启动内核</p>\n<p>2）生成html格式</p>\n<pre><code class=\"language-python\">!jupyter nbconvert --to html Matplotlib.ipynb\n</code></pre>\n<pre><code>[NbConvertApp] Converting notebook Matplotlib.ipynb to html\n[NbConvertApp] Writing 642412 bytes to Matplotlib.html\n</code></pre>\n<pre><code class=\"language-python\">\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1><center>Python可视化基础</center></h1>\n<h1>1 常用的Python可视化库</h1>\n<ul>\n<li><b>Matplotlib</b><br>\n是一个最基础的Python可视化库，作图风格接近MATLAB，所以称为matplotlib。一般都是从matplotlib上手Python数据可视化，然后开始做纵向与横向拓展。</li>\n<li><b>Pandas</b>: <br>\neasy to use interface, built on Matplotlib</li>\n<li><b>Seaborn</b>: <br>\nhigh-level interface, great default styles</li>\n<li><b>ggplot</b>:<br>\nbased on R’s ggplot2, uses Grammar of Graphics</li>\n<li><b>Plotly</b>:<br>\n创建交互动态图</li>\n<li><b>Pyecharts</b><br>\n网页中的动态图</li>\n</ul>\n<h1>2 Jupyter Notebook</h1>\n<p>Jupyter这个名字是它要服务的三种语言的缩写：Julia，PYThon和R，这个名字与“木星（jupiter）”谐音。</p>\n<ul>\n<li>\n<h3> Jupyter Notebook打开方式</h3>\n</li>\n</ul>\n<p>  Anaconda prompt 输入Jupyter notebook</p>\n<p>  通过Anaconda Navigator打开</p>\n<ul>\n<li>\n<h3>更改Jupyter Notebook的默认路径</h3>\n</li>\n</ul>\n<p> （1）找到配置文件的位置<br>\njupyter notebook --generate-config<br>\n若不是首次执行，会出现是否覆盖的提示，选择No<br/></p>\n<p> （2）打开配置文件，进下如下设置<br>\nc.NotebookApp.notebook_dir = ‘换成自己的目录’，保存<br>\n我最后用的格式是E:\\zxc\\dir这种形式<br/></p>\n<p> （3）关闭重启jupyter  notebook。</p>\n<h1>3 matplotlib</h1>\n  <h3> 3.1 Matplotlib简介</h3>\n<p> Matplotlib是 Python 2D-绘图领域使用最广泛的套件，可以简易地将数据图形化，并且提供多样化的输出格式。<br/><br>\n matplotlib有两个接口，一个是状态机层的接口，通过<font color=red>pyplot</font>模块来进行管理；<br/><br>\n 一个是面向对象的接口，通过<font color=red>pylab</font>模块将所有的功能函数全部导入其单独的命名空间内。</p>\n<p>在线文档https://matplotlib.org/<br/></p>\n <h3> 3.2 Matplotlib安装</h3>\n<p>（1） 使用pip安装</p>\n<p> 1）Win+R输入cmd进入到CMD窗口下，执行python -m pip install -U pip setuptools进行升级。</p>\n<p> 2）输入python -m pip install matplotlib进行自动的安装，系统会自动下载安装包</p>\n<p>（2）使用conda安装<br>\nconda install matplotlib</p>\n<h3> 3.3 Matplotlib图表结构</h3>\n<p>Matplotlib基本图表结构<br/><br>\n包括坐标轴（X轴、Y轴）、坐标轴标签（axisLabel）、<br>\n坐标轴刻度（tick）、坐标轴刻度标签（tick label）、绘图区（axes）、画布（figure）。</p>\n<p><img src=\"C:/Users/zheng/Desktop/aaa/structure.png\" alt=\"avatar\"></p>\n<p>在绘图结构中，figure创建窗口，subplot创建子图。所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。</p>\n<p>Figure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。</p>\n<p>Subplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像</p>\n<h3> 3.4 Matplotlib基本应用</h3>\n<p><b> 导入matplotlib</b></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nprint(&quot;matplotlib version&quot;,mpl.__version__)\n</code></pre>\n<pre><code>matplotlib version 3.2.2\n</code></pre>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n</code></pre>\n<p><b>（1）画一个简单的图形</b></p>\n<p>1)简单的使用</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-1,1,50)#从(-1,1)均匀取50个点\ny = 2 * x\n\nplt.plot(x,y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_30_0111.png\" alt=\"png\"></p>\n<p>2）这里我们继续通过画出一个正弦曲线图来讲解下基本用法。<br/><br>\n通过 np.linspace 方式生成 x，它包含了 50 个元素的数组，这 50 个元素均匀的分布在 [0, 2pi] 的区间上。然后通过 np.sin(x) 生成 y。</p>\n<pre><code class=\"language-python\">x = np.linspace(0, 2 * np.pi, 50)\ny = np.sin(x)\n</code></pre>\n<p>有了 x 和 y 数据之后，我们通过 plt.plot(x, y) 来画出图形，并通过 plt.show() 来显示</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_34_0111.png\" alt=\"png\"></p>\n<center><b>Plot的基本用法</b></center>\n<p>折线图和散点图<br>\nplot(x轴数据，y轴数据，展现形式)，参数是可以叠加的</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>plot(x, y)        # plot x and y using default line style and color<br>\nplot(x, y, ‘bo’)  # plot x and y using blue circle markers<br>\nplot(y)           # plot y using x as index array 0…N-1<br>\nplot(y, ‘r+’)     # ditto, but with red plusses</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<pre><code class=\"language-python\">plt.plot(x,y,'b:.')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_37_0111.png\" alt=\"png\"></p>\n<center><b>线条相关属性标记设置</b></center>\n<p>常用属性设置<br>\n颜色参数：<br>\nc-cyan–青色<br>\nr-red–红色<br>\nm-magente–品红<br>\ng-green–绿色<br>\nb-blue–蓝色<br>\ny-yellow–黄色<br>\nk-black–黑色<br>\nw-white–白色<br>\n线条样式：<br>\n-  实线<br>\n– 虚线<br>\n-. -.式<br>\n:  细小虚线<br>\no  circle<br>\n点的样式：<br>\ns–方形<br>\nh–六角形<br>\nH–六角形<br>\n*–星形<br>\n±-加号<br>\nx–x形<br>\nd–菱形<br>\nD–菱形<br>\np–五角星<br>\n. 点标记<br>\n, 像素标记<br>\no 圆标记</p>\n<p>更多请参见在线文档https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot<br>\n<b>（2）在一张图里绘制多图形</b></p>\n<p>有时候，可能需要在一个图纸里绘制多个图形，这里我们同时绘制了 (x, y), (x, y * 2)两个图形</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0111.png\" alt=\"png\"></p>\n<p>绘制出图形之后，我们可以自己调整更多的样式，比如颜色、点、线。</p>\n<pre><code class=\"language-python\">plt.plot(x, y, 'y*-')\nplt.plot(x, y * 2, 'm--')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0111.png\" alt=\"png\"></p>\n<p><b>（3）设置Figure</b></p>\n<p>你可以认为Matplotlib绘制的图形都在一个默认的 figure 中，当然了，你可以自己创建 figure，好处就是可以控制更多的参数，常见的就是控制图形的大小，这里创建一个 figure，设置大小为 (6, 3)</p>\n<pre><code class=\"language-python\">plt.figure(figsize=(6, 3))\nplt.plot(x, y)\nplt.plot(x, y * 2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_47_0111.png\" alt=\"png\"></p>\n<p><b>（4）设置标题</b></p>\n<p>直接通过 plt.title 即可设置图形标题</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.title(&quot;sin(x) &amp; 2sin(x)&quot;)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_50_0111.png\" alt=\"png\"></p>\n<p><b>（5）设置坐标轴</b></p>\n<p>我们来看下如何设置坐标轴的范围以及名称。</p>\n<p>坐标轴范围设置<br>\nplt.axis([xmin,xmax,ymin,ymax])<br/><br>\n也可以通过xlim(xmin,xmax)，ylim(xmin,xmax)方法设置坐标轴范围</p>\n<p>通过 xlabel 和 ylabel 来设置轴的名称。</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.xlim((0, np.pi + 1))\nplt.ylim((-3, 3))\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_55_0111.png\" alt=\"png\"></p>\n<p>等价与下面的方式</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\n\nplt.axis([0, np.pi + 1,-3,3])\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_57_0111.png\" alt=\"png\"></p>\n<p>此外，我们也可以通过 xticks 和 yticks 来设置轴的刻度</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\nplt.plot(x, y * 2)\nplt.xticks((0, np.pi * 0.5, np.pi, np.pi * 1.5, np.pi * 2))\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_59_0111.png\" alt=\"png\"></p>\n<center><b>xticks yticks</b></center>\n<p>plt.xticks()/plt.yticks()设置轴记号,人为设置坐标轴的刻度显示的值</p>\n<p><b>例1：xticks的用法</b><br>\nxticks()函数原型：</p>\n<p>xticks(ticks, [labels], **kwargs)</p>\n<p>参数说明：<br>\nticks：数组类型，用于设置X轴刻度间隔<br>\n[labels]：数组类型，用于设置每个间隔的显示标签<br>\n<strong>kwargs：用于设置标签字体倾斜度和颜色等外观属性。（注：python里的双星号代表这个位置接收任意多个关键字参数，可参考：python学习：python的星号（*）和双星号（</strong>）用法）<br>\n不使用xticks时</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_65_0111.png\" alt=\"png\"></p>\n<p>使用xticks后</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nimport calendar\nx2 = range(1,13,1)\ny2 = range(1,13,1)\nplt.plot(x2,y2)\nplt.xticks(x2, calendar.month_name[1:13],color='blue',rotation=60) \n#参数x空值X轴的间隔，第二个参数控制每个间隔显示的文本，后面两个参数控制标签的颜色和旋转角度\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_67_0111.png\" alt=\"png\"></p>\n<p>例2参考：<a href=\"http://blog.sina.com.cn/s/blog_14478a3250102y73b.html\">http://blog.sina.com.cn/s/blog_14478a3250102y73b.html</a></p>\n<p><b>（6）设置 label 和 legend</b></p>\n<p>设置 label 和 legend 的目的就是为了区分出每个数据对应的图形名称。</p>\n<pre><code class=\"language-python\">plt.plot(x, y, label=&quot;sin(x)&quot;)\nplt.plot(x, y * 2, label=&quot;2sin(x)&quot;)\n# plt.legend()\nplt.legend(loc='best')\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_71_0111.png\" alt=\"png\"><br>\n<img src=\"/img/loc.png\" alt=\"png\"></p>\n<p><b>（7）添加注释</b></p>\n<p>有时候我们需要对特定的点进行标注，我们可以使用 plt.annotate 函数来实现。</p>\n<p>这里我们要标注的点是 (x0, y0) = (π, 0)。</p>\n<p>我们也可以使用 plt.text 函数来添加注释。</p>\n<pre><code class=\"language-python\">plt.plot(x, y)\n\nx0 = np.pi\ny0 = 0\n\n# 画出标注点\nplt.scatter(x0, y0, s=50)\n\nplt.annotate('sin(np.pi)=%s' % y0, xy=(np.pi, 0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='-&gt;', connectionstyle=&quot;arc3,rad=.2&quot;))\n\nplt.text(0.5, -0.25, &quot;sin(np.pi) = 0&quot;, fontdict=&#123;'size': 16, 'color': 'r'&#125;)\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_75_0111.png\" alt=\"png\"></p>\n<p>对于 annotate 函数的参数，做一个简单解释：<br/><br>\n•<br>\n‘sin(np.pi)=%s’ % y0 代表标注的内容，可以通过字符串 %s 将 y0 的值传入字符串；</p>\n<p>•<br>\n参数 xycoords=‘data’ 是说基于数据的值来选位置;</p>\n<p>•<br>\nxytext=(+30, -30) 和 textcoords=‘offset points’ 表示对于标注位置的描述 和 xy 偏差值，即标注位置是 xy 位置向右移动 30，向下移动30；</p>\n<p>•<br>\narrowprops 是对图中箭头类型和箭头弧度的设置，需要用 dict 形式传入。</p>\n<p><b>（8）使用子图</b></p>\n<p>有时候我们需要将多张子图展示在一起，可以使用 subplot() 实现。即在调用 plot() 函数之前需要先调用 subplot() 函数。该函数的第一个参数代表子图的总行数，第二个参数代表子图的总列数，第三个参数代表活跃区域。</p>\n<pre><code class=\"language-python\">ax1 = plt.subplot(2, 2, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 2, 2, sharey=ax1) # 与 ax1 共享y轴\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 2, 3)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 2, 4, sharey=ax3) # 与 ax3 共享y轴\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_79_0111.png\" alt=\"png\"></p>\n<p>上面的 subplot(2, 2, x) 表示将图像窗口分为 2 行 2 列。x 表示当前子图所在的活跃区。</p>\n<p>可以看到，上面的每个子图的大小都是一样的。有时候我们需要不同大小的子图。比如将上面第一张子图完全放置在第一行，其他的子图都放在第二行。</p>\n<pre><code class=\"language-python\">ax1 = plt.subplot(2, 1, 1) # （行，列，活跃区）\nplt.plot(x, np.sin(x), 'r')\n\nax2 = plt.subplot(2, 3, 4)\nplt.plot(x, 2 * np.sin(x), 'g')\n\nax3 = plt.subplot(2, 3, 5, sharey=ax2)\nplt.plot(x, np.cos(x), 'b')\n\nax4 = plt.subplot(2, 3, 6, sharey=ax2)\nplt.plot(x, 2 * np.cos(x), 'y')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_81_0111.png\" alt=\"png\"></p>\n<p>plt.subplot(2, 1, 1) 将图像窗口分为了 2 行 1 列, 当前活跃区为 1。</p>\n<p>使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 当前活跃区为 4。</p>\n<p>解释下为什么活跃区为 4，因为上一步中使用 plt.subplot(2, 1, 1) 将整个图像窗口分为 2 行 1 列, 第1个小图占用了第1个位置, 也就是整个第1行. 这一步中使用 plt.subplot(2, 3, 4) 将整个图像窗口分为 2 行 3 列, 于是整个图像窗口的第1行就变成了3列, 也就是成了3个位置, 于是第2行的第1个位置是整个图像窗口的第4个位置。</p>\n<p><b>（9）中文乱码解决</b></p>\n<p>Matplotlib 有个让人恼火的问题是，默认情况下，Matplotlib 中文会乱码。</p>\n<pre><code class=\"language-python\">x = ['北京', '上海', '深圳', '广州']\ny = [60000, 58000, 50000, 52000]\nplt.plot(x, y)\nplt.show()\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 21271 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 20140 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 19978 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28023 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 28145 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 22323 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24191 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 24030 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 21271 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 20140 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 19978 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28023 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 28145 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 22323 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24191 missing from current font.\n  font.set_text(s, 0, flags=flags)\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 24030 missing from current font.\n  font.set_text(s, 0, flags=flags)\n</code></pre>\n<p><img src=\"/img/output_85_1111.png\" alt=\"png\"></p>\n<p>可以看到，上面所有的中文都乱码了，显示成方框了，如何解决呢？<br/><br>\n只需要配置下后台字体即可</p>\n<pre><code class=\"language-python\">plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\nplt.plot(x, y)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_87_0111.png\" alt=\"png\"></p>\n<h3>3.5数据读取与导入</h3>\n<p><a href=\"http://cc.nohup.cc/article/207/#menu_index_1%E4%BB%A5%E6%8A%98%E7%BA%BF%E5%9B%BE%E4%B8%BA%E4%BE%8B%E8%AE%B2%E8%A7%A3\">http://cc.nohup.cc/article/207/#menu_index_1以折线图为例讲解</a></p>\n<h1>总结</h1>\n<p>本节课我们了解了Python的画图包，matplotlib的基本用法及数据读取的方法。<br/><br>\n请同学们课下结合实例进行练习</p>\n<h1>附:MarkDown基本使用</h1>\n<p>1）空格<br>\n 半角的不换行的空格（推荐使用）<br>\n 全角空格<br>\n 半角空格<br>\n2）删除：连按两次D即可删除当前行</p>\n<p>3)换行：使用&lt;br\\&gt;实现</p>\n<p>4)显示行号：在命令模式中的代码单元格按 L 打开数字。</p>\n<p>5)颜色的使用 font</p>\n<ol start=\"6\">\n<li>Markdown表格</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>表头</th>\n<th>表头</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n</tbody>\n</table>\n<p>表格效果如下：</p>\n<table>\n<thead>\n<tr>\n<th>表头</th>\n<th>表头</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n<tr>\n<td>单元格</td>\n<td>单元格</td>\n</tr>\n</tbody>\n</table>\n<h3>其他</h3>\n<p>1)生新启动内核</p>\n<p>2）生成html格式</p>\n<pre><code class=\"language-python\">!jupyter nbconvert --to html Matplotlib.ipynb\n</code></pre>\n<pre><code>[NbConvertApp] Converting notebook Matplotlib.ipynb to html\n[NbConvertApp] Writing 642412 bytes to Matplotlib.html\n</code></pre>\n<pre><code class=\"language-python\">\n</code></pre>\n"},{"title":"分布式锁","author":"ztq","date":"2021-04-18T05:00:00.000Z","_content":"\n# 1、什么是分布式锁？\n\n​\t\t当多个进程在同一个系统中，用分布式锁控制多个进程对资源的访问。传统的单体应用单机部署情况下，可以使用java并发处理相关的API进行互斥控制。分布式系统后由于多线程，多进程分布在不同机器上，使单机部署情况下的并发控制锁策略失效，为了解决跨JVM互斥机制来控制共享资源的访问，这就是分布式锁的来源；分布式锁应用场景大都是高并发、大流量场景。\n\n# 2、分布式锁实现\n\n## （1）、redis分布式锁的实现\n\n1. 加锁机制：根据hash节点选择一个客户端执行lua脚本\n\n2. 锁互斥机制：再来一个客户端执行同样的lua脚本会提示已经存在锁，然后进入循环一直尝试加锁\n\n3. 可重入机制\n\n4. watch dog自动延期机制\n\n5. 释放锁机制\n\n![1](/img/redis分布式锁实现原理.jpg)\n\n测试用例\n单机\n\n```java\nprivate RedissonClient getClient(){\n        Config config = new Config();\n        config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\");//.setPassword(\"\");//.setConnectionMinimumIdleSize(10).setConnectionPoolSize(10);//.setConnectionPoolSize();//172.16.10.164\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient;\n    }\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n    @Test\n    public void test() throws Exception {\n        int[] count = {0};\n        for (int i = 0; i < 10; i++) {\n            RedissonClient client = getClient();\n            final RedisLock redisLock = new RedisLock(client,\"lock_key\");\n            executorService.submit(() -> {\n                try {\n                    redisLock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        redisLock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    }\n```\n\nRedLock\n\n```java\npublic static RLock create (String url, String key){\n        Config config = new Config();\n        config.useSingleServer().setAddress(url);\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient.getLock(key);\n    }\n\n    RedissonRedLock redissonRedLock = new RedissonRedLock(\n            create(\"redis://redis://127.0.0.1:6379\",\"lock_key1\"),\n            create(\"redis://redis://127.0.0.1:6380\",\"lock_key2\"),\n            create(\"redis://redis://127.0.0.1:6381\",\"lock_key3\"));\n    RedisRedLock redLock = new RedisRedLock(redissonRedLock);\n\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n\n   @Test\n    public void test() throws Exception {\n        int[] count = {0};\n        for (int i = 0; i < 2; i++) {\n            executorService.submit(() -> {\n                try {\n                    redLock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        redLock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    }\n```\n\n利用StringRedisTemplate，分布式锁工具类\n\n```java\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\nimport java.util.concurrent.TimeUnit;\n\n@Component\npublic class RedisLock {\n\n    private static final Logger logger = LoggerFactory.getLogger(RedisLock.class);\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    /**\n     * 加锁\n     *\n     * @param key   - 唯一标志\n     * @param value 跟唯一标志对应的随机值\n     * @return\n     */\n    public boolean lock(String key, String value) {\n        //设置30秒的锁\n        return stringRedisTemplate.opsForValue().setIfAbsent(key, value, 30, TimeUnit.SECONDS);//对应setnx命令\n    }\n\n\n    /**\n     * 解锁\n     *\n     * @param key\n     * @param value\n     */\n    public void unlock(String key, String value) {\n        try {\n            String currentValue = stringRedisTemplate.opsForValue().get(key);\n            if (!StringUtils.isEmpty(currentValue) && currentValue.equals(value)) {\n                stringRedisTemplate.opsForValue().getOperations().delete(key);//删除key\n            }\n        } catch (Exception e) {\n            logger.error(\"[Redis分布式锁] 解锁出现异常\", e);\n        }\n    }\n\n}\n```\n\n```java\nString uuid = UUID.randomUUID().toString();\nboolean lock = redisLock.lock(\"lock_\" + id, uuid);\nif (lock) {\n    // xxxxxx\n}\nredisLock.unlock(\"lock_\" + id, uuid);\n```\n\n\n\n## （2）、基于ETCD实现分布式锁分析\n\nETCD分布式锁的实现\n1. Lease机制：租约机制（TTL，Time To Live），Etcd 可以为存储的 key-value 对设置租约，当租约到期，key-value 将失效删除；同时也支持续约，通过客户端可以在租约到期之前续约，以避免 key-value 对过期失效。Lease 机制可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。\n\n2. Revision机制：每个 key 带有一个 Revision 号，每进行一次事务加一，它是全局唯一的，通过 Revision 的大小就可以知道进行写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” ，实现公平锁。\n\n3. Prefix机制：即前缀机制。例如，一个名为 /etcdlock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=\"/etcdlock/UUID1\"，key2=\"/etcdlock/UUID2\"，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。写操作都会成功，但返回的 Revision 不一样，那么，如何判断谁获得了锁呢？通过前缀 /etcdlock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁。\n\n4. Watch机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个范围（前缀机制），当被 Watch 的 key 或范围发生变化，客户端将收到通知；在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件则说明 pre-key 已经释放，自己已经持有锁。\n\n![基于ETCD实现分布式锁分析](/img/基于ETCD实现分布式锁分析.png)\n\n基于ETCD分布式锁\n\n**步骤1：**建立连接\n\n客户端连接 Etcd，以 /etcd/lock 为前缀创建全局唯一的 key，假设第一个客户端对应的 key=\"/etcd/lock/UUID1\"，第二个为 key=\"/etcd/lock/UUID2\"；客户端分别为自己的 key 创建租约 - Lease，租约的长度根据业务耗时确定；\n\n**步骤2：**创建定时任务作为租约的“心跳”\n\n当一个客户端持有锁期间，其它客户端只能等待，为了避免等待期间租约失效，客户端需创建一个定时任务作为“心跳”进行续约。此外，如果持有锁期间客户端崩溃，心跳停止，key 将因租约到期而被删除，从而锁释放，避免死锁。\n\n**步骤3：**客户端将自己全局唯一的 key 写入 Etcd\n\n执行 put 操作，将步骤 1 中创建的 key 绑定租约写入 Etcd，根据 Etcd 的 Revision 机制，假设两个客户端 put 操作返回的 Revision 分别为 1、2，客户端需记录 Revision 用以接下来判断自己是否获得锁\n\n**步骤 4：**客户端判断是否获得锁\n\n客户端以前缀 /etcd/lock/ 读取 keyValue 列表，判断自己 key 的 Revision 是否为当前列表中最小的，如果是则认为获得锁；否则监听列表中前一个 Revision 比自己小的 key 的删除事件，一旦监听到删除事件或者因租约失效而删除的事件，则自己获得锁。\n\n**步骤 5：**执行业务\n\n获得锁后，操作共享资源，执行业务代码\n\n**步骤 6：**释放锁\n\n完成业务流程后，删除对应的key释放锁\n\n例子：\n\n```java\npublic class EtcdDistributeLock extends AbstractLock{\n\n    private Client client;\n    private Lock lockClient;\n    private Lease leaseClient;\n    private String lockKey;\n    private String lockPath;\n    /** 锁的次数 */\n    private AtomicInteger lockCount;\n    /** 租约有效期,防止客户端崩溃，可在租约到期后自动释放锁；另一方面，正常执行过程中，会自动进行续租,单位 ns */\n    private Long leaseTTL;\n    /** 续约锁租期的定时任务，初次启动延迟，单位默认为 s,默认为1s，可根据业务定制设置*/\n    private Long initialDelay = 0L;\n    /** 定时任务线程池类 */\n    ScheduledExecutorService service = null;\n    /** 保存线程与锁对象的映射，锁对象包含重入次数，重入次数的最大限制为Int的最大值 */\n    private final ConcurrentMap<Thread, LockData> threadData = Maps.newConcurrentMap();\n\n    public EtcdDistributeLock(){}\n\n    public EtcdDistributeLock(Client client, String lockKey, long leaseTTL,TimeUnit unit){\n        this.client = client;\n        lockClient = client.getLockClient();\n        leaseClient = client.getLeaseClient();\n        this.lockKey = lockKey;\n        // 转纳秒\n        this.leaseTTL = unit.toNanos(leaseTTL);\n        service = Executors.newSingleThreadScheduledExecutor();\n    }\n\n\n    @Override\n    public void lock() {\n        // 检查重入性\n        Thread currentThread = Thread.currentThread();\n        LockData oldLockData = threadData.get(currentThread);\n        if (oldLockData != null && oldLockData.isLockSuccess()) {\n            // re-entering\n            int lockCount = oldLockData.lockCount.incrementAndGet();\n            if(lockCount < 0 ){\n                throw new Error(\"超出可重入次数限制\");\n            }\n            return;\n        }\n\n        // 记录租约 ID\n        Long leaseId = 0L;\n        try{\n            leaseId = leaseClient.grant(TimeUnit.NANOSECONDS.toSeconds(leaseTTL)).get().getID();\n            // 续租心跳周期\n            long period = leaseTTL - leaseTTL / 5;\n            // 启动定时任务续约\n            service.scheduleAtFixedRate(new EtcdDistributeLock.KeepAliveRunnable(leaseClient, leaseId),\n                    initialDelay,period,TimeUnit.NANOSECONDS);\n            LockResponse lockResponse = lockClient.lock(ByteSequence.from(lockKey.getBytes()), leaseId).get();\n            if(lockResponse != null){\n                lockPath = lockResponse.getKey().toString(Charset.forName(\"utf-8\"));\n                log.info(\"获取锁成功,锁路径:{},线程:{}\",lockPath,currentThread.getName());\n            }\n        }catch (InterruptedException | ExecutionException e){\n            log.error(\"获取锁失败\",e);\n            return;\n        }\n        // 获取锁成功，锁对象设置\n        LockData newLockData = new LockData(currentThread, lockKey);\n        newLockData.setLeaseId(leaseId);\n        newLockData.setService(service);\n        threadData.put(currentThread, newLockData);\n        newLockData.setLockSuccess(true);\n    }\n\n    @Override\n    public void lockInterruptibly() throws InterruptedException {\n        super.lockInterruptibly();\n    }\n\n    @Override\n    public boolean tryLock() {\n        return super.tryLock();\n    }\n\n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {\n        return super.tryLock(time,unit);\n    }\n\n\n    @Override\n    public void unlock() {\n        Thread currentThread = Thread.currentThread();\n        LockData lockData = threadData.get(currentThread);\n        if (lockData == null){\n            throw new IllegalMonitorStateException(\"You do not own the lock: \" + lockKey);\n        }\n        int newLockCount = lockData.lockCount.decrementAndGet();\n        if ( newLockCount > 0 ) {\n            return;\n        }\n        if ( newLockCount < 0 ) {\n            throw new IllegalMonitorStateException(\"Lock count has gone negative for lock: \" + lockKey);\n        }\n        try {\n            // 释放锁\n            if(lockPath != null){\n                lockClient.unlock(ByteSequence.from(lockPath.getBytes())).get();\n            }\n            if(lockData != null){\n                // 关闭定时任务\n                lockData.getService().shutdown();\n                // 删除租约\n                if (lockData.getLeaseId() != 0L) {\n                    leaseClient.revoke(lockData.getLeaseId());\n                }\n            }\n        } catch (InterruptedException | ExecutionException e) {\n            log.error(\"解锁失败\",e);\n        }finally {\n            // 移除当前线程资源\n            threadData.remove(currentThread);\n        }\n    }\n\n\n    @Override\n    public Condition newCondition() {\n        return super.newCondition();\n    }\n\n    /**\n     * 心跳续约线程类\n     */\n    public static class KeepAliveRunnable implements Runnable {\n        private Lease leaseClient;\n        private long leaseId;\n\n        public KeepAliveRunnable(Lease leaseClient, long leaseId) {\n            this.leaseClient = leaseClient;\n            this.leaseId = leaseId;\n        }\n\n        @Override\n        public void run() {\n            // 对该leaseid进行一次续约\n            leaseClient.keepAliveOnce(leaseId);\n        }\n    }\n```\n\n```java\npublic class EtcdLockTest {\n    private Client client;\n    private String key = \"/etcd/lock\";\n    private static final String server = \"http://xxxx:xxxx\";\n    private ExecutorService executorService = Executors.newFixedThreadPool(10000);\n\n    @Before\n    public void before() throws Exception {\n        initEtcdClient();\n    }\n\n    private void initEtcdClient(){\n       client = Client.builder().endpoints(server).build();\n    }\n\n    @Test\n    public void testEtcdDistributeLock() throws InterruptedException {\n        int[] count = {0};\n        for (int i = 0; i < 100; i++) {\n            executorService.submit(() -> {\n                final EtcdDistributeLock lock = new EtcdDistributeLock(client, key,20,TimeUnit.SECONDS);\n                try {\n                    lock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        lock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.err.println(\"执行结果: \" + count[0]);\n    }\n}\n```\n\n（3）、基于Zookeeper分布式锁\n\n实现原理\n1. 启动客户端，确认链接到了服务器\n\n2. 多个客户端并发的在特定路径下创建临时性顺序节点\n\n3. 客户端判断自己的创建的顺序节点是否是最小的，如果是最小的，则获取锁成功\n\n4. 第三步若判定失败，则采用zk的watch机制监听自己的前一个顺序节点，等待前一个节点的删除（放锁）事件，再开始第三步判定\n\n![基于Zookeeper分布式锁](/img/基于Zookeeper分布式锁.png)\n\nzookeeper作为高性能分布式协调框架，可以把其看做一个文件系统，其中有节点的概念，并且分为4种：1.持久性节点2.持久性顺序节点3.临时性节点4.临时性顺序节点。\n\n分布式锁的实现主要思路就是：监控其他客户端的状态，来判断自己是否可以获得锁。\n\n采用临时性顺序节点的原因：\n\n1. zk服务器维护了客户端的会话有效性，当会话失效的时候，其会话所创建的临时性节点都会被删除，通过这一特点，可以通过watch临时节点来监控其他客户端的情况，方便自己做出相应动作。\n\n2. 因为zk对写操作是顺序性的，所以并发创建的顺序节点会有一个唯一确定的序号，当前锁是公平锁的一种实现，所以依靠这种顺序性可以很好的解释—节点序列小的获取到锁并且可以采用watch自己的前一个节点来避免惊群现象（这样watch事件的传播是线性的）。\n\n例子：\n\n```java\npublic class ZKLock extends AbstractLock {\n\n    /**\n     *     1.Connect to zk\n     */\n    private CuratorFramework client;\n\n    private InterProcessLock lock ;\n\n\n    public  ZKLock(String zkAddress,String lockPath) {\n        // 1.Connect to zk\n        client = CuratorFrameworkFactory.newClient(\n                zkAddress,\n                new RetryNTimes(5, 5000)\n        );\n        client.start();\n        if(client.getState() == CuratorFrameworkState.STARTED){\n            log.info(\"zk client start successfully!\");\n            log.info(\"zkAddress:{},lockPath:{}\",zkAddress,lockPath);\n        }else{\n            throw new RuntimeException(\"客户端启动失败。。。\");\n        }\n        this.lock = defaultLock(lockPath);\n    }\n\n    private InterProcessLock defaultLock(String lockPath ){\n       return  new InterProcessMutex(client, lockPath);\n    }\n    \n    @Override\n    public void lock() {\n        try {\n            this.lock.acquire();\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    @Override\n    public boolean tryLock() {\n        boolean flag ;\n        try {\n            flag=this.lock.acquire(0,TimeUnit.SECONDS);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return flag;\n    }\n    \n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {\n        boolean flag ;\n        try {\n            flag=this.lock.acquire(time,unit);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return flag;\n    }\n    \n    @Override\n    public void unlock() {\n        try {\n            this.lock.release();\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n}\n```\n\n```java\n private ExecutorService executorService = Executors.newCachedThreadPool();\n\n    @Test\n    public void testLock() throws Exception{\n        ZKLock zkLock = new ZKLock(\"xxxx:xxxx\",\"/lockPath\");\n        int[] num = {0};\n        long start = System.currentTimeMillis();\n        for(int i=0;i<200;i++){\n            executorService.submit(()->{\n                try {\n                    zkLock.lock();\n                    num[0]++;\n                } catch (Exception e){\n                    throw new RuntimeException(e);\n                } finally {\n                    zkLock.unlock();\n                }\n            });\n\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        log.info(\"耗时:{}\",System.currentTimeMillis()-start);\n        System.out.println(num[0]);\n    }\n```\n\n# 3、总结\n\n1. redis的分布式锁中redisson一般为单实例，当单实例不可用时，会阻塞业务流程。主从方式、主从数据异步，会存在锁失效的问题。RedLock一般要求至少3台以上的redis主从实例，维护成本相对来说比较高。\n\n2. ZK锁具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。但是因为需要频繁的创建和删除节点，性能上不如Redis方式。\n\n3. ETCD分布式锁的实现原理与zk锁类似，但是ETCD分布式锁更加可靠强大。其Lease功能保证分布式锁的安全性；watch功能支持监听某个固定的key，也支持watch一个范围的key（前缀机制）；revision功能可通过 Revision 的大小就可以知道进行写操作的顺序。可以避免 “羊群效应” （也称 “惊群效应”），实现公平锁。前缀机制与watch功能配合使用解决了死锁问题。总之ETCD的灵感来源于Zookeeper,但实现的时候做了很多的改进，如：高负载下的稳定读写、数据模型的多版本并发控制、稳定的watch功能，通知订阅者监听值得变化、可以容忍脑裂现场的发生、客户端的协议使用gRPC协议,支持go、c++、java等。","source":"_posts/分布式锁.md","raw":"title: 分布式锁\nauthor: ztq\ntags:\n\n  - 分布式锁\ncategories:\n  - 分布式\ndate: 2021-04-18 13:00:00\n\n---\n\n# 1、什么是分布式锁？\n\n​\t\t当多个进程在同一个系统中，用分布式锁控制多个进程对资源的访问。传统的单体应用单机部署情况下，可以使用java并发处理相关的API进行互斥控制。分布式系统后由于多线程，多进程分布在不同机器上，使单机部署情况下的并发控制锁策略失效，为了解决跨JVM互斥机制来控制共享资源的访问，这就是分布式锁的来源；分布式锁应用场景大都是高并发、大流量场景。\n\n# 2、分布式锁实现\n\n## （1）、redis分布式锁的实现\n\n1. 加锁机制：根据hash节点选择一个客户端执行lua脚本\n\n2. 锁互斥机制：再来一个客户端执行同样的lua脚本会提示已经存在锁，然后进入循环一直尝试加锁\n\n3. 可重入机制\n\n4. watch dog自动延期机制\n\n5. 释放锁机制\n\n![1](/img/redis分布式锁实现原理.jpg)\n\n测试用例\n单机\n\n```java\nprivate RedissonClient getClient(){\n        Config config = new Config();\n        config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\");//.setPassword(\"\");//.setConnectionMinimumIdleSize(10).setConnectionPoolSize(10);//.setConnectionPoolSize();//172.16.10.164\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient;\n    }\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n    @Test\n    public void test() throws Exception {\n        int[] count = {0};\n        for (int i = 0; i < 10; i++) {\n            RedissonClient client = getClient();\n            final RedisLock redisLock = new RedisLock(client,\"lock_key\");\n            executorService.submit(() -> {\n                try {\n                    redisLock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        redisLock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    }\n```\n\nRedLock\n\n```java\npublic static RLock create (String url, String key){\n        Config config = new Config();\n        config.useSingleServer().setAddress(url);\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient.getLock(key);\n    }\n\n    RedissonRedLock redissonRedLock = new RedissonRedLock(\n            create(\"redis://redis://127.0.0.1:6379\",\"lock_key1\"),\n            create(\"redis://redis://127.0.0.1:6380\",\"lock_key2\"),\n            create(\"redis://redis://127.0.0.1:6381\",\"lock_key3\"));\n    RedisRedLock redLock = new RedisRedLock(redissonRedLock);\n\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n\n   @Test\n    public void test() throws Exception {\n        int[] count = {0};\n        for (int i = 0; i < 2; i++) {\n            executorService.submit(() -> {\n                try {\n                    redLock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        redLock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    }\n```\n\n利用StringRedisTemplate，分布式锁工具类\n\n```java\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\nimport java.util.concurrent.TimeUnit;\n\n@Component\npublic class RedisLock {\n\n    private static final Logger logger = LoggerFactory.getLogger(RedisLock.class);\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    /**\n     * 加锁\n     *\n     * @param key   - 唯一标志\n     * @param value 跟唯一标志对应的随机值\n     * @return\n     */\n    public boolean lock(String key, String value) {\n        //设置30秒的锁\n        return stringRedisTemplate.opsForValue().setIfAbsent(key, value, 30, TimeUnit.SECONDS);//对应setnx命令\n    }\n\n\n    /**\n     * 解锁\n     *\n     * @param key\n     * @param value\n     */\n    public void unlock(String key, String value) {\n        try {\n            String currentValue = stringRedisTemplate.opsForValue().get(key);\n            if (!StringUtils.isEmpty(currentValue) && currentValue.equals(value)) {\n                stringRedisTemplate.opsForValue().getOperations().delete(key);//删除key\n            }\n        } catch (Exception e) {\n            logger.error(\"[Redis分布式锁] 解锁出现异常\", e);\n        }\n    }\n\n}\n```\n\n```java\nString uuid = UUID.randomUUID().toString();\nboolean lock = redisLock.lock(\"lock_\" + id, uuid);\nif (lock) {\n    // xxxxxx\n}\nredisLock.unlock(\"lock_\" + id, uuid);\n```\n\n\n\n## （2）、基于ETCD实现分布式锁分析\n\nETCD分布式锁的实现\n1. Lease机制：租约机制（TTL，Time To Live），Etcd 可以为存储的 key-value 对设置租约，当租约到期，key-value 将失效删除；同时也支持续约，通过客户端可以在租约到期之前续约，以避免 key-value 对过期失效。Lease 机制可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。\n\n2. Revision机制：每个 key 带有一个 Revision 号，每进行一次事务加一，它是全局唯一的，通过 Revision 的大小就可以知道进行写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” ，实现公平锁。\n\n3. Prefix机制：即前缀机制。例如，一个名为 /etcdlock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=\"/etcdlock/UUID1\"，key2=\"/etcdlock/UUID2\"，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。写操作都会成功，但返回的 Revision 不一样，那么，如何判断谁获得了锁呢？通过前缀 /etcdlock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁。\n\n4. Watch机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个范围（前缀机制），当被 Watch 的 key 或范围发生变化，客户端将收到通知；在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件则说明 pre-key 已经释放，自己已经持有锁。\n\n![基于ETCD实现分布式锁分析](/img/基于ETCD实现分布式锁分析.png)\n\n基于ETCD分布式锁\n\n**步骤1：**建立连接\n\n客户端连接 Etcd，以 /etcd/lock 为前缀创建全局唯一的 key，假设第一个客户端对应的 key=\"/etcd/lock/UUID1\"，第二个为 key=\"/etcd/lock/UUID2\"；客户端分别为自己的 key 创建租约 - Lease，租约的长度根据业务耗时确定；\n\n**步骤2：**创建定时任务作为租约的“心跳”\n\n当一个客户端持有锁期间，其它客户端只能等待，为了避免等待期间租约失效，客户端需创建一个定时任务作为“心跳”进行续约。此外，如果持有锁期间客户端崩溃，心跳停止，key 将因租约到期而被删除，从而锁释放，避免死锁。\n\n**步骤3：**客户端将自己全局唯一的 key 写入 Etcd\n\n执行 put 操作，将步骤 1 中创建的 key 绑定租约写入 Etcd，根据 Etcd 的 Revision 机制，假设两个客户端 put 操作返回的 Revision 分别为 1、2，客户端需记录 Revision 用以接下来判断自己是否获得锁\n\n**步骤 4：**客户端判断是否获得锁\n\n客户端以前缀 /etcd/lock/ 读取 keyValue 列表，判断自己 key 的 Revision 是否为当前列表中最小的，如果是则认为获得锁；否则监听列表中前一个 Revision 比自己小的 key 的删除事件，一旦监听到删除事件或者因租约失效而删除的事件，则自己获得锁。\n\n**步骤 5：**执行业务\n\n获得锁后，操作共享资源，执行业务代码\n\n**步骤 6：**释放锁\n\n完成业务流程后，删除对应的key释放锁\n\n例子：\n\n```java\npublic class EtcdDistributeLock extends AbstractLock{\n\n    private Client client;\n    private Lock lockClient;\n    private Lease leaseClient;\n    private String lockKey;\n    private String lockPath;\n    /** 锁的次数 */\n    private AtomicInteger lockCount;\n    /** 租约有效期,防止客户端崩溃，可在租约到期后自动释放锁；另一方面，正常执行过程中，会自动进行续租,单位 ns */\n    private Long leaseTTL;\n    /** 续约锁租期的定时任务，初次启动延迟，单位默认为 s,默认为1s，可根据业务定制设置*/\n    private Long initialDelay = 0L;\n    /** 定时任务线程池类 */\n    ScheduledExecutorService service = null;\n    /** 保存线程与锁对象的映射，锁对象包含重入次数，重入次数的最大限制为Int的最大值 */\n    private final ConcurrentMap<Thread, LockData> threadData = Maps.newConcurrentMap();\n\n    public EtcdDistributeLock(){}\n\n    public EtcdDistributeLock(Client client, String lockKey, long leaseTTL,TimeUnit unit){\n        this.client = client;\n        lockClient = client.getLockClient();\n        leaseClient = client.getLeaseClient();\n        this.lockKey = lockKey;\n        // 转纳秒\n        this.leaseTTL = unit.toNanos(leaseTTL);\n        service = Executors.newSingleThreadScheduledExecutor();\n    }\n\n\n    @Override\n    public void lock() {\n        // 检查重入性\n        Thread currentThread = Thread.currentThread();\n        LockData oldLockData = threadData.get(currentThread);\n        if (oldLockData != null && oldLockData.isLockSuccess()) {\n            // re-entering\n            int lockCount = oldLockData.lockCount.incrementAndGet();\n            if(lockCount < 0 ){\n                throw new Error(\"超出可重入次数限制\");\n            }\n            return;\n        }\n\n        // 记录租约 ID\n        Long leaseId = 0L;\n        try{\n            leaseId = leaseClient.grant(TimeUnit.NANOSECONDS.toSeconds(leaseTTL)).get().getID();\n            // 续租心跳周期\n            long period = leaseTTL - leaseTTL / 5;\n            // 启动定时任务续约\n            service.scheduleAtFixedRate(new EtcdDistributeLock.KeepAliveRunnable(leaseClient, leaseId),\n                    initialDelay,period,TimeUnit.NANOSECONDS);\n            LockResponse lockResponse = lockClient.lock(ByteSequence.from(lockKey.getBytes()), leaseId).get();\n            if(lockResponse != null){\n                lockPath = lockResponse.getKey().toString(Charset.forName(\"utf-8\"));\n                log.info(\"获取锁成功,锁路径:{},线程:{}\",lockPath,currentThread.getName());\n            }\n        }catch (InterruptedException | ExecutionException e){\n            log.error(\"获取锁失败\",e);\n            return;\n        }\n        // 获取锁成功，锁对象设置\n        LockData newLockData = new LockData(currentThread, lockKey);\n        newLockData.setLeaseId(leaseId);\n        newLockData.setService(service);\n        threadData.put(currentThread, newLockData);\n        newLockData.setLockSuccess(true);\n    }\n\n    @Override\n    public void lockInterruptibly() throws InterruptedException {\n        super.lockInterruptibly();\n    }\n\n    @Override\n    public boolean tryLock() {\n        return super.tryLock();\n    }\n\n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {\n        return super.tryLock(time,unit);\n    }\n\n\n    @Override\n    public void unlock() {\n        Thread currentThread = Thread.currentThread();\n        LockData lockData = threadData.get(currentThread);\n        if (lockData == null){\n            throw new IllegalMonitorStateException(\"You do not own the lock: \" + lockKey);\n        }\n        int newLockCount = lockData.lockCount.decrementAndGet();\n        if ( newLockCount > 0 ) {\n            return;\n        }\n        if ( newLockCount < 0 ) {\n            throw new IllegalMonitorStateException(\"Lock count has gone negative for lock: \" + lockKey);\n        }\n        try {\n            // 释放锁\n            if(lockPath != null){\n                lockClient.unlock(ByteSequence.from(lockPath.getBytes())).get();\n            }\n            if(lockData != null){\n                // 关闭定时任务\n                lockData.getService().shutdown();\n                // 删除租约\n                if (lockData.getLeaseId() != 0L) {\n                    leaseClient.revoke(lockData.getLeaseId());\n                }\n            }\n        } catch (InterruptedException | ExecutionException e) {\n            log.error(\"解锁失败\",e);\n        }finally {\n            // 移除当前线程资源\n            threadData.remove(currentThread);\n        }\n    }\n\n\n    @Override\n    public Condition newCondition() {\n        return super.newCondition();\n    }\n\n    /**\n     * 心跳续约线程类\n     */\n    public static class KeepAliveRunnable implements Runnable {\n        private Lease leaseClient;\n        private long leaseId;\n\n        public KeepAliveRunnable(Lease leaseClient, long leaseId) {\n            this.leaseClient = leaseClient;\n            this.leaseId = leaseId;\n        }\n\n        @Override\n        public void run() {\n            // 对该leaseid进行一次续约\n            leaseClient.keepAliveOnce(leaseId);\n        }\n    }\n```\n\n```java\npublic class EtcdLockTest {\n    private Client client;\n    private String key = \"/etcd/lock\";\n    private static final String server = \"http://xxxx:xxxx\";\n    private ExecutorService executorService = Executors.newFixedThreadPool(10000);\n\n    @Before\n    public void before() throws Exception {\n        initEtcdClient();\n    }\n\n    private void initEtcdClient(){\n       client = Client.builder().endpoints(server).build();\n    }\n\n    @Test\n    public void testEtcdDistributeLock() throws InterruptedException {\n        int[] count = {0};\n        for (int i = 0; i < 100; i++) {\n            executorService.submit(() -> {\n                final EtcdDistributeLock lock = new EtcdDistributeLock(client, key,20,TimeUnit.SECONDS);\n                try {\n                    lock.lock();\n                    count[0]++;\n                } catch (Exception e) {\n                    e.printStackTrace();\n                } finally {\n                    try {\n                        lock.unlock();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            });\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.err.println(\"执行结果: \" + count[0]);\n    }\n}\n```\n\n（3）、基于Zookeeper分布式锁\n\n实现原理\n1. 启动客户端，确认链接到了服务器\n\n2. 多个客户端并发的在特定路径下创建临时性顺序节点\n\n3. 客户端判断自己的创建的顺序节点是否是最小的，如果是最小的，则获取锁成功\n\n4. 第三步若判定失败，则采用zk的watch机制监听自己的前一个顺序节点，等待前一个节点的删除（放锁）事件，再开始第三步判定\n\n![基于Zookeeper分布式锁](/img/基于Zookeeper分布式锁.png)\n\nzookeeper作为高性能分布式协调框架，可以把其看做一个文件系统，其中有节点的概念，并且分为4种：1.持久性节点2.持久性顺序节点3.临时性节点4.临时性顺序节点。\n\n分布式锁的实现主要思路就是：监控其他客户端的状态，来判断自己是否可以获得锁。\n\n采用临时性顺序节点的原因：\n\n1. zk服务器维护了客户端的会话有效性，当会话失效的时候，其会话所创建的临时性节点都会被删除，通过这一特点，可以通过watch临时节点来监控其他客户端的情况，方便自己做出相应动作。\n\n2. 因为zk对写操作是顺序性的，所以并发创建的顺序节点会有一个唯一确定的序号，当前锁是公平锁的一种实现，所以依靠这种顺序性可以很好的解释—节点序列小的获取到锁并且可以采用watch自己的前一个节点来避免惊群现象（这样watch事件的传播是线性的）。\n\n例子：\n\n```java\npublic class ZKLock extends AbstractLock {\n\n    /**\n     *     1.Connect to zk\n     */\n    private CuratorFramework client;\n\n    private InterProcessLock lock ;\n\n\n    public  ZKLock(String zkAddress,String lockPath) {\n        // 1.Connect to zk\n        client = CuratorFrameworkFactory.newClient(\n                zkAddress,\n                new RetryNTimes(5, 5000)\n        );\n        client.start();\n        if(client.getState() == CuratorFrameworkState.STARTED){\n            log.info(\"zk client start successfully!\");\n            log.info(\"zkAddress:{},lockPath:{}\",zkAddress,lockPath);\n        }else{\n            throw new RuntimeException(\"客户端启动失败。。。\");\n        }\n        this.lock = defaultLock(lockPath);\n    }\n\n    private InterProcessLock defaultLock(String lockPath ){\n       return  new InterProcessMutex(client, lockPath);\n    }\n    \n    @Override\n    public void lock() {\n        try {\n            this.lock.acquire();\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    @Override\n    public boolean tryLock() {\n        boolean flag ;\n        try {\n            flag=this.lock.acquire(0,TimeUnit.SECONDS);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return flag;\n    }\n    \n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {\n        boolean flag ;\n        try {\n            flag=this.lock.acquire(time,unit);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return flag;\n    }\n    \n    @Override\n    public void unlock() {\n        try {\n            this.lock.release();\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n}\n```\n\n```java\n private ExecutorService executorService = Executors.newCachedThreadPool();\n\n    @Test\n    public void testLock() throws Exception{\n        ZKLock zkLock = new ZKLock(\"xxxx:xxxx\",\"/lockPath\");\n        int[] num = {0};\n        long start = System.currentTimeMillis();\n        for(int i=0;i<200;i++){\n            executorService.submit(()->{\n                try {\n                    zkLock.lock();\n                    num[0]++;\n                } catch (Exception e){\n                    throw new RuntimeException(e);\n                } finally {\n                    zkLock.unlock();\n                }\n            });\n\n        }\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        log.info(\"耗时:{}\",System.currentTimeMillis()-start);\n        System.out.println(num[0]);\n    }\n```\n\n# 3、总结\n\n1. redis的分布式锁中redisson一般为单实例，当单实例不可用时，会阻塞业务流程。主从方式、主从数据异步，会存在锁失效的问题。RedLock一般要求至少3台以上的redis主从实例，维护成本相对来说比较高。\n\n2. ZK锁具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。但是因为需要频繁的创建和删除节点，性能上不如Redis方式。\n\n3. ETCD分布式锁的实现原理与zk锁类似，但是ETCD分布式锁更加可靠强大。其Lease功能保证分布式锁的安全性；watch功能支持监听某个固定的key，也支持watch一个范围的key（前缀机制）；revision功能可通过 Revision 的大小就可以知道进行写操作的顺序。可以避免 “羊群效应” （也称 “惊群效应”），实现公平锁。前缀机制与watch功能配合使用解决了死锁问题。总之ETCD的灵感来源于Zookeeper,但实现的时候做了很多的改进，如：高负载下的稳定读写、数据模型的多版本并发控制、稳定的watch功能，通知订阅者监听值得变化、可以容忍脑裂现场的发生、客户端的协议使用gRPC协议,支持go、c++、java等。","slug":"分布式锁","published":1,"updated":"2022-04-04T08:32:40.168Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2600fc7kt921oyepz3","content":"<h1>1、什么是分布式锁？</h1>\n<p>​\t\t当多个进程在同一个系统中，用分布式锁控制多个进程对资源的访问。传统的单体应用单机部署情况下，可以使用java并发处理相关的API进行互斥控制。分布式系统后由于多线程，多进程分布在不同机器上，使单机部署情况下的并发控制锁策略失效，为了解决跨JVM互斥机制来控制共享资源的访问，这就是分布式锁的来源；分布式锁应用场景大都是高并发、大流量场景。</p>\n<h1>2、分布式锁实现</h1>\n<h2 id=\"（1）、redis分布式锁的实现\">（1）、redis分布式锁的实现</h2>\n<ol>\n<li>\n<p>加锁机制：根据hash节点选择一个客户端执行lua脚本</p>\n</li>\n<li>\n<p>锁互斥机制：再来一个客户端执行同样的lua脚本会提示已经存在锁，然后进入循环一直尝试加锁</p>\n</li>\n<li>\n<p>可重入机制</p>\n</li>\n<li>\n<p>watch dog自动延期机制</p>\n</li>\n<li>\n<p>释放锁机制</p>\n</li>\n</ol>\n<p><img src=\"/img/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.jpg\" alt=\"1\"></p>\n<p>测试用例<br>\n单机</p>\n<pre><code class=\"language-java\">private RedissonClient getClient()&#123;\n        Config config = new Config();\n        config.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;);//.setPassword(&quot;&quot;);//.setConnectionMinimumIdleSize(10).setConnectionPoolSize(10);//.setConnectionPoolSize();//172.16.10.164\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient;\n    &#125;\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n    @Test\n    public void test() throws Exception &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 10; i++) &#123;\n            RedissonClient client = getClient();\n            final RedisLock redisLock = new RedisLock(client,&quot;lock_key&quot;);\n            executorService.submit(() -&gt; &#123;\n                try &#123;\n                    redisLock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        redisLock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    &#125;\n</code></pre>\n<p>RedLock</p>\n<pre><code class=\"language-java\">public static RLock create (String url, String key)&#123;\n        Config config = new Config();\n        config.useSingleServer().setAddress(url);\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient.getLock(key);\n    &#125;\n\n    RedissonRedLock redissonRedLock = new RedissonRedLock(\n            create(&quot;redis://redis://127.0.0.1:6379&quot;,&quot;lock_key1&quot;),\n            create(&quot;redis://redis://127.0.0.1:6380&quot;,&quot;lock_key2&quot;),\n            create(&quot;redis://redis://127.0.0.1:6381&quot;,&quot;lock_key3&quot;));\n    RedisRedLock redLock = new RedisRedLock(redissonRedLock);\n\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n\n   @Test\n    public void test() throws Exception &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 2; i++) &#123;\n            executorService.submit(() -&gt; &#123;\n                try &#123;\n                    redLock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        redLock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    &#125;\n</code></pre>\n<p>利用StringRedisTemplate，分布式锁工具类</p>\n<pre><code class=\"language-java\">import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\nimport java.util.concurrent.TimeUnit;\n\n@Component\npublic class RedisLock &#123;\n\n    private static final Logger logger = LoggerFactory.getLogger(RedisLock.class);\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    /**\n     * 加锁\n     *\n     * @param key   - 唯一标志\n     * @param value 跟唯一标志对应的随机值\n     * @return\n     */\n    public boolean lock(String key, String value) &#123;\n        //设置30秒的锁\n        return stringRedisTemplate.opsForValue().setIfAbsent(key, value, 30, TimeUnit.SECONDS);//对应setnx命令\n    &#125;\n\n\n    /**\n     * 解锁\n     *\n     * @param key\n     * @param value\n     */\n    public void unlock(String key, String value) &#123;\n        try &#123;\n            String currentValue = stringRedisTemplate.opsForValue().get(key);\n            if (!StringUtils.isEmpty(currentValue) &amp;&amp; currentValue.equals(value)) &#123;\n                stringRedisTemplate.opsForValue().getOperations().delete(key);//删除key\n            &#125;\n        &#125; catch (Exception e) &#123;\n            logger.error(&quot;[Redis分布式锁] 解锁出现异常&quot;, e);\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">String uuid = UUID.randomUUID().toString();\nboolean lock = redisLock.lock(&quot;lock_&quot; + id, uuid);\nif (lock) &#123;\n    // xxxxxx\n&#125;\nredisLock.unlock(&quot;lock_&quot; + id, uuid);\n</code></pre>\n<h2 id=\"（2）、基于ETCD实现分布式锁分析\">（2）、基于ETCD实现分布式锁分析</h2>\n<p>ETCD分布式锁的实现</p>\n<ol>\n<li>\n<p>Lease机制：租约机制（TTL，Time To Live），Etcd 可以为存储的 key-value 对设置租约，当租约到期，key-value 将失效删除；同时也支持续约，通过客户端可以在租约到期之前续约，以避免 key-value 对过期失效。Lease 机制可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。</p>\n</li>\n<li>\n<p>Revision机制：每个 key 带有一个 Revision 号，每进行一次事务加一，它是全局唯一的，通过 Revision 的大小就可以知道进行写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” ，实现公平锁。</p>\n</li>\n<li>\n<p>Prefix机制：即前缀机制。例如，一个名为 /etcdlock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=“/etcdlock/UUID1”，key2=“/etcdlock/UUID2”，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。写操作都会成功，但返回的 Revision 不一样，那么，如何判断谁获得了锁呢？通过前缀 /etcdlock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁。</p>\n</li>\n<li>\n<p>Watch机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个范围（前缀机制），当被 Watch 的 key 或范围发生变化，客户端将收到通知；在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件则说明 pre-key 已经释放，自己已经持有锁。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E5%9F%BA%E4%BA%8EETCD%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%88%86%E6%9E%90.png\" alt=\"基于ETCD实现分布式锁分析\"></p>\n<p>基于ETCD分布式锁</p>\n<p>**步骤1：**建立连接</p>\n<p>客户端连接 Etcd，以 /etcd/lock 为前缀创建全局唯一的 key，假设第一个客户端对应的 key=“/etcd/lock/UUID1”，第二个为 key=“/etcd/lock/UUID2”；客户端分别为自己的 key 创建租约 - Lease，租约的长度根据业务耗时确定；</p>\n<p>**步骤2：**创建定时任务作为租约的“心跳”</p>\n<p>当一个客户端持有锁期间，其它客户端只能等待，为了避免等待期间租约失效，客户端需创建一个定时任务作为“心跳”进行续约。此外，如果持有锁期间客户端崩溃，心跳停止，key 将因租约到期而被删除，从而锁释放，避免死锁。</p>\n<p>**步骤3：**客户端将自己全局唯一的 key 写入 Etcd</p>\n<p>执行 put 操作，将步骤 1 中创建的 key 绑定租约写入 Etcd，根据 Etcd 的 Revision 机制，假设两个客户端 put 操作返回的 Revision 分别为 1、2，客户端需记录 Revision 用以接下来判断自己是否获得锁</p>\n<p>**步骤 4：**客户端判断是否获得锁</p>\n<p>客户端以前缀 /etcd/lock/ 读取 keyValue 列表，判断自己 key 的 Revision 是否为当前列表中最小的，如果是则认为获得锁；否则监听列表中前一个 Revision 比自己小的 key 的删除事件，一旦监听到删除事件或者因租约失效而删除的事件，则自己获得锁。</p>\n<p>**步骤 5：**执行业务</p>\n<p>获得锁后，操作共享资源，执行业务代码</p>\n<p>**步骤 6：**释放锁</p>\n<p>完成业务流程后，删除对应的key释放锁</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">public class EtcdDistributeLock extends AbstractLock&#123;\n\n    private Client client;\n    private Lock lockClient;\n    private Lease leaseClient;\n    private String lockKey;\n    private String lockPath;\n    /** 锁的次数 */\n    private AtomicInteger lockCount;\n    /** 租约有效期,防止客户端崩溃，可在租约到期后自动释放锁；另一方面，正常执行过程中，会自动进行续租,单位 ns */\n    private Long leaseTTL;\n    /** 续约锁租期的定时任务，初次启动延迟，单位默认为 s,默认为1s，可根据业务定制设置*/\n    private Long initialDelay = 0L;\n    /** 定时任务线程池类 */\n    ScheduledExecutorService service = null;\n    /** 保存线程与锁对象的映射，锁对象包含重入次数，重入次数的最大限制为Int的最大值 */\n    private final ConcurrentMap&lt;Thread, LockData&gt; threadData = Maps.newConcurrentMap();\n\n    public EtcdDistributeLock()&#123;&#125;\n\n    public EtcdDistributeLock(Client client, String lockKey, long leaseTTL,TimeUnit unit)&#123;\n        this.client = client;\n        lockClient = client.getLockClient();\n        leaseClient = client.getLeaseClient();\n        this.lockKey = lockKey;\n        // 转纳秒\n        this.leaseTTL = unit.toNanos(leaseTTL);\n        service = Executors.newSingleThreadScheduledExecutor();\n    &#125;\n\n\n    @Override\n    public void lock() &#123;\n        // 检查重入性\n        Thread currentThread = Thread.currentThread();\n        LockData oldLockData = threadData.get(currentThread);\n        if (oldLockData != null &amp;&amp; oldLockData.isLockSuccess()) &#123;\n            // re-entering\n            int lockCount = oldLockData.lockCount.incrementAndGet();\n            if(lockCount &lt; 0 )&#123;\n                throw new Error(&quot;超出可重入次数限制&quot;);\n            &#125;\n            return;\n        &#125;\n\n        // 记录租约 ID\n        Long leaseId = 0L;\n        try&#123;\n            leaseId = leaseClient.grant(TimeUnit.NANOSECONDS.toSeconds(leaseTTL)).get().getID();\n            // 续租心跳周期\n            long period = leaseTTL - leaseTTL / 5;\n            // 启动定时任务续约\n            service.scheduleAtFixedRate(new EtcdDistributeLock.KeepAliveRunnable(leaseClient, leaseId),\n                    initialDelay,period,TimeUnit.NANOSECONDS);\n            LockResponse lockResponse = lockClient.lock(ByteSequence.from(lockKey.getBytes()), leaseId).get();\n            if(lockResponse != null)&#123;\n                lockPath = lockResponse.getKey().toString(Charset.forName(&quot;utf-8&quot;));\n                log.info(&quot;获取锁成功,锁路径:&#123;&#125;,线程:&#123;&#125;&quot;,lockPath,currentThread.getName());\n            &#125;\n        &#125;catch (InterruptedException | ExecutionException e)&#123;\n            log.error(&quot;获取锁失败&quot;,e);\n            return;\n        &#125;\n        // 获取锁成功，锁对象设置\n        LockData newLockData = new LockData(currentThread, lockKey);\n        newLockData.setLeaseId(leaseId);\n        newLockData.setService(service);\n        threadData.put(currentThread, newLockData);\n        newLockData.setLockSuccess(true);\n    &#125;\n\n    @Override\n    public void lockInterruptibly() throws InterruptedException &#123;\n        super.lockInterruptibly();\n    &#125;\n\n    @Override\n    public boolean tryLock() &#123;\n        return super.tryLock();\n    &#125;\n\n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123;\n        return super.tryLock(time,unit);\n    &#125;\n\n\n    @Override\n    public void unlock() &#123;\n        Thread currentThread = Thread.currentThread();\n        LockData lockData = threadData.get(currentThread);\n        if (lockData == null)&#123;\n            throw new IllegalMonitorStateException(&quot;You do not own the lock: &quot; + lockKey);\n        &#125;\n        int newLockCount = lockData.lockCount.decrementAndGet();\n        if ( newLockCount &gt; 0 ) &#123;\n            return;\n        &#125;\n        if ( newLockCount &lt; 0 ) &#123;\n            throw new IllegalMonitorStateException(&quot;Lock count has gone negative for lock: &quot; + lockKey);\n        &#125;\n        try &#123;\n            // 释放锁\n            if(lockPath != null)&#123;\n                lockClient.unlock(ByteSequence.from(lockPath.getBytes())).get();\n            &#125;\n            if(lockData != null)&#123;\n                // 关闭定时任务\n                lockData.getService().shutdown();\n                // 删除租约\n                if (lockData.getLeaseId() != 0L) &#123;\n                    leaseClient.revoke(lockData.getLeaseId());\n                &#125;\n            &#125;\n        &#125; catch (InterruptedException | ExecutionException e) &#123;\n            log.error(&quot;解锁失败&quot;,e);\n        &#125;finally &#123;\n            // 移除当前线程资源\n            threadData.remove(currentThread);\n        &#125;\n    &#125;\n\n\n    @Override\n    public Condition newCondition() &#123;\n        return super.newCondition();\n    &#125;\n\n    /**\n     * 心跳续约线程类\n     */\n    public static class KeepAliveRunnable implements Runnable &#123;\n        private Lease leaseClient;\n        private long leaseId;\n\n        public KeepAliveRunnable(Lease leaseClient, long leaseId) &#123;\n            this.leaseClient = leaseClient;\n            this.leaseId = leaseId;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            // 对该leaseid进行一次续约\n            leaseClient.keepAliveOnce(leaseId);\n        &#125;\n    &#125;\n</code></pre>\n<pre><code class=\"language-java\">public class EtcdLockTest &#123;\n    private Client client;\n    private String key = &quot;/etcd/lock&quot;;\n    private static final String server = &quot;http://xxxx:xxxx&quot;;\n    private ExecutorService executorService = Executors.newFixedThreadPool(10000);\n\n    @Before\n    public void before() throws Exception &#123;\n        initEtcdClient();\n    &#125;\n\n    private void initEtcdClient()&#123;\n       client = Client.builder().endpoints(server).build();\n    &#125;\n\n    @Test\n    public void testEtcdDistributeLock() throws InterruptedException &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 100; i++) &#123;\n            executorService.submit(() -&gt; &#123;\n                final EtcdDistributeLock lock = new EtcdDistributeLock(client, key,20,TimeUnit.SECONDS);\n                try &#123;\n                    lock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        lock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.err.println(&quot;执行结果: &quot; + count[0]);\n    &#125;\n&#125;\n</code></pre>\n<p>（3）、基于Zookeeper分布式锁</p>\n<p>实现原理</p>\n<ol>\n<li>\n<p>启动客户端，确认链接到了服务器</p>\n</li>\n<li>\n<p>多个客户端并发的在特定路径下创建临时性顺序节点</p>\n</li>\n<li>\n<p>客户端判断自己的创建的顺序节点是否是最小的，如果是最小的，则获取锁成功</p>\n</li>\n<li>\n<p>第三步若判定失败，则采用zk的watch机制监听自己的前一个顺序节点，等待前一个节点的删除（放锁）事件，再开始第三步判定</p>\n</li>\n</ol>\n<p><img src=\"/img/%E5%9F%BA%E4%BA%8EZookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.png\" alt=\"基于Zookeeper分布式锁\"></p>\n<p>zookeeper作为高性能分布式协调框架，可以把其看做一个文件系统，其中有节点的概念，并且分为4种：1.持久性节点2.持久性顺序节点3.临时性节点4.临时性顺序节点。</p>\n<p>分布式锁的实现主要思路就是：监控其他客户端的状态，来判断自己是否可以获得锁。</p>\n<p>采用临时性顺序节点的原因：</p>\n<ol>\n<li>\n<p>zk服务器维护了客户端的会话有效性，当会话失效的时候，其会话所创建的临时性节点都会被删除，通过这一特点，可以通过watch临时节点来监控其他客户端的情况，方便自己做出相应动作。</p>\n</li>\n<li>\n<p>因为zk对写操作是顺序性的，所以并发创建的顺序节点会有一个唯一确定的序号，当前锁是公平锁的一种实现，所以依靠这种顺序性可以很好的解释—节点序列小的获取到锁并且可以采用watch自己的前一个节点来避免惊群现象（这样watch事件的传播是线性的）。</p>\n</li>\n</ol>\n<p>例子：</p>\n<pre><code class=\"language-java\">public class ZKLock extends AbstractLock &#123;\n\n    /**\n     *     1.Connect to zk\n     */\n    private CuratorFramework client;\n\n    private InterProcessLock lock ;\n\n\n    public  ZKLock(String zkAddress,String lockPath) &#123;\n        // 1.Connect to zk\n        client = CuratorFrameworkFactory.newClient(\n                zkAddress,\n                new RetryNTimes(5, 5000)\n        );\n        client.start();\n        if(client.getState() == CuratorFrameworkState.STARTED)&#123;\n            log.info(&quot;zk client start successfully!&quot;);\n            log.info(&quot;zkAddress:&#123;&#125;,lockPath:&#123;&#125;&quot;,zkAddress,lockPath);\n        &#125;else&#123;\n            throw new RuntimeException(&quot;客户端启动失败。。。&quot;);\n        &#125;\n        this.lock = defaultLock(lockPath);\n    &#125;\n\n    private InterProcessLock defaultLock(String lockPath )&#123;\n       return  new InterProcessMutex(client, lockPath);\n    &#125;\n    \n    @Override\n    public void lock() &#123;\n        try &#123;\n            this.lock.acquire();\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n    @Override\n    public boolean tryLock() &#123;\n        boolean flag ;\n        try &#123;\n            flag=this.lock.acquire(0,TimeUnit.SECONDS);\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n        return flag;\n    &#125;\n    \n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123;\n        boolean flag ;\n        try &#123;\n            flag=this.lock.acquire(time,unit);\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n        return flag;\n    &#125;\n    \n    @Override\n    public void unlock() &#123;\n        try &#123;\n            this.lock.release();\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<pre><code class=\"language-java\"> private ExecutorService executorService = Executors.newCachedThreadPool();\n\n    @Test\n    public void testLock() throws Exception&#123;\n        ZKLock zkLock = new ZKLock(&quot;xxxx:xxxx&quot;,&quot;/lockPath&quot;);\n        int[] num = &#123;0&#125;;\n        long start = System.currentTimeMillis();\n        for(int i=0;i&lt;200;i++)&#123;\n            executorService.submit(()-&gt;&#123;\n                try &#123;\n                    zkLock.lock();\n                    num[0]++;\n                &#125; catch (Exception e)&#123;\n                    throw new RuntimeException(e);\n                &#125; finally &#123;\n                    zkLock.unlock();\n                &#125;\n            &#125;);\n\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        log.info(&quot;耗时:&#123;&#125;&quot;,System.currentTimeMillis()-start);\n        System.out.println(num[0]);\n    &#125;\n</code></pre>\n<h1>3、总结</h1>\n<ol>\n<li>\n<p>redis的分布式锁中redisson一般为单实例，当单实例不可用时，会阻塞业务流程。主从方式、主从数据异步，会存在锁失效的问题。RedLock一般要求至少3台以上的redis主从实例，维护成本相对来说比较高。</p>\n</li>\n<li>\n<p>ZK锁具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。但是因为需要频繁的创建和删除节点，性能上不如Redis方式。</p>\n</li>\n<li>\n<p>ETCD分布式锁的实现原理与zk锁类似，但是ETCD分布式锁更加可靠强大。其Lease功能保证分布式锁的安全性；watch功能支持监听某个固定的key，也支持watch一个范围的key（前缀机制）；revision功能可通过 Revision 的大小就可以知道进行写操作的顺序。可以避免 “羊群效应” （也称 “惊群效应”），实现公平锁。前缀机制与watch功能配合使用解决了死锁问题。总之ETCD的灵感来源于Zookeeper,但实现的时候做了很多的改进，如：高负载下的稳定读写、数据模型的多版本并发控制、稳定的watch功能，通知订阅者监听值得变化、可以容忍脑裂现场的发生、客户端的协议使用gRPC协议,支持go、c++、java等。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、什么是分布式锁？</h1>\n<p>​\t\t当多个进程在同一个系统中，用分布式锁控制多个进程对资源的访问。传统的单体应用单机部署情况下，可以使用java并发处理相关的API进行互斥控制。分布式系统后由于多线程，多进程分布在不同机器上，使单机部署情况下的并发控制锁策略失效，为了解决跨JVM互斥机制来控制共享资源的访问，这就是分布式锁的来源；分布式锁应用场景大都是高并发、大流量场景。</p>\n<h1>2、分布式锁实现</h1>\n<h2 id=\"（1）、redis分布式锁的实现\">（1）、redis分布式锁的实现</h2>\n<ol>\n<li>\n<p>加锁机制：根据hash节点选择一个客户端执行lua脚本</p>\n</li>\n<li>\n<p>锁互斥机制：再来一个客户端执行同样的lua脚本会提示已经存在锁，然后进入循环一直尝试加锁</p>\n</li>\n<li>\n<p>可重入机制</p>\n</li>\n<li>\n<p>watch dog自动延期机制</p>\n</li>\n<li>\n<p>释放锁机制</p>\n</li>\n</ol>\n<p><img src=\"/img/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.jpg\" alt=\"1\"></p>\n<p>测试用例<br>\n单机</p>\n<pre><code class=\"language-java\">private RedissonClient getClient()&#123;\n        Config config = new Config();\n        config.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;);//.setPassword(&quot;&quot;);//.setConnectionMinimumIdleSize(10).setConnectionPoolSize(10);//.setConnectionPoolSize();//172.16.10.164\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient;\n    &#125;\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n    @Test\n    public void test() throws Exception &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 10; i++) &#123;\n            RedissonClient client = getClient();\n            final RedisLock redisLock = new RedisLock(client,&quot;lock_key&quot;);\n            executorService.submit(() -&gt; &#123;\n                try &#123;\n                    redisLock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        redisLock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    &#125;\n</code></pre>\n<p>RedLock</p>\n<pre><code class=\"language-java\">public static RLock create (String url, String key)&#123;\n        Config config = new Config();\n        config.useSingleServer().setAddress(url);\n        RedissonClient redissonClient = Redisson.create(config);\n        return redissonClient.getLock(key);\n    &#125;\n\n    RedissonRedLock redissonRedLock = new RedissonRedLock(\n            create(&quot;redis://redis://127.0.0.1:6379&quot;,&quot;lock_key1&quot;),\n            create(&quot;redis://redis://127.0.0.1:6380&quot;,&quot;lock_key2&quot;),\n            create(&quot;redis://redis://127.0.0.1:6381&quot;,&quot;lock_key3&quot;));\n    RedisRedLock redLock = new RedisRedLock(redissonRedLock);\n\n    private ExecutorService executorService = Executors.newCachedThreadPool();\n\n   @Test\n    public void test() throws Exception &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 2; i++) &#123;\n            executorService.submit(() -&gt; &#123;\n                try &#123;\n                    redLock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        redLock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.out.println(count[0]);\n    &#125;\n</code></pre>\n<p>利用StringRedisTemplate，分布式锁工具类</p>\n<pre><code class=\"language-java\">import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\nimport java.util.concurrent.TimeUnit;\n\n@Component\npublic class RedisLock &#123;\n\n    private static final Logger logger = LoggerFactory.getLogger(RedisLock.class);\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    /**\n     * 加锁\n     *\n     * @param key   - 唯一标志\n     * @param value 跟唯一标志对应的随机值\n     * @return\n     */\n    public boolean lock(String key, String value) &#123;\n        //设置30秒的锁\n        return stringRedisTemplate.opsForValue().setIfAbsent(key, value, 30, TimeUnit.SECONDS);//对应setnx命令\n    &#125;\n\n\n    /**\n     * 解锁\n     *\n     * @param key\n     * @param value\n     */\n    public void unlock(String key, String value) &#123;\n        try &#123;\n            String currentValue = stringRedisTemplate.opsForValue().get(key);\n            if (!StringUtils.isEmpty(currentValue) &amp;&amp; currentValue.equals(value)) &#123;\n                stringRedisTemplate.opsForValue().getOperations().delete(key);//删除key\n            &#125;\n        &#125; catch (Exception e) &#123;\n            logger.error(&quot;[Redis分布式锁] 解锁出现异常&quot;, e);\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">String uuid = UUID.randomUUID().toString();\nboolean lock = redisLock.lock(&quot;lock_&quot; + id, uuid);\nif (lock) &#123;\n    // xxxxxx\n&#125;\nredisLock.unlock(&quot;lock_&quot; + id, uuid);\n</code></pre>\n<h2 id=\"（2）、基于ETCD实现分布式锁分析\">（2）、基于ETCD实现分布式锁分析</h2>\n<p>ETCD分布式锁的实现</p>\n<ol>\n<li>\n<p>Lease机制：租约机制（TTL，Time To Live），Etcd 可以为存储的 key-value 对设置租约，当租约到期，key-value 将失效删除；同时也支持续约，通过客户端可以在租约到期之前续约，以避免 key-value 对过期失效。Lease 机制可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。</p>\n</li>\n<li>\n<p>Revision机制：每个 key 带有一个 Revision 号，每进行一次事务加一，它是全局唯一的，通过 Revision 的大小就可以知道进行写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” ，实现公平锁。</p>\n</li>\n<li>\n<p>Prefix机制：即前缀机制。例如，一个名为 /etcdlock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=“/etcdlock/UUID1”，key2=“/etcdlock/UUID2”，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。写操作都会成功，但返回的 Revision 不一样，那么，如何判断谁获得了锁呢？通过前缀 /etcdlock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁。</p>\n</li>\n<li>\n<p>Watch机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个范围（前缀机制），当被 Watch 的 key 或范围发生变化，客户端将收到通知；在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件则说明 pre-key 已经释放，自己已经持有锁。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E5%9F%BA%E4%BA%8EETCD%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%88%86%E6%9E%90.png\" alt=\"基于ETCD实现分布式锁分析\"></p>\n<p>基于ETCD分布式锁</p>\n<p>**步骤1：**建立连接</p>\n<p>客户端连接 Etcd，以 /etcd/lock 为前缀创建全局唯一的 key，假设第一个客户端对应的 key=“/etcd/lock/UUID1”，第二个为 key=“/etcd/lock/UUID2”；客户端分别为自己的 key 创建租约 - Lease，租约的长度根据业务耗时确定；</p>\n<p>**步骤2：**创建定时任务作为租约的“心跳”</p>\n<p>当一个客户端持有锁期间，其它客户端只能等待，为了避免等待期间租约失效，客户端需创建一个定时任务作为“心跳”进行续约。此外，如果持有锁期间客户端崩溃，心跳停止，key 将因租约到期而被删除，从而锁释放，避免死锁。</p>\n<p>**步骤3：**客户端将自己全局唯一的 key 写入 Etcd</p>\n<p>执行 put 操作，将步骤 1 中创建的 key 绑定租约写入 Etcd，根据 Etcd 的 Revision 机制，假设两个客户端 put 操作返回的 Revision 分别为 1、2，客户端需记录 Revision 用以接下来判断自己是否获得锁</p>\n<p>**步骤 4：**客户端判断是否获得锁</p>\n<p>客户端以前缀 /etcd/lock/ 读取 keyValue 列表，判断自己 key 的 Revision 是否为当前列表中最小的，如果是则认为获得锁；否则监听列表中前一个 Revision 比自己小的 key 的删除事件，一旦监听到删除事件或者因租约失效而删除的事件，则自己获得锁。</p>\n<p>**步骤 5：**执行业务</p>\n<p>获得锁后，操作共享资源，执行业务代码</p>\n<p>**步骤 6：**释放锁</p>\n<p>完成业务流程后，删除对应的key释放锁</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">public class EtcdDistributeLock extends AbstractLock&#123;\n\n    private Client client;\n    private Lock lockClient;\n    private Lease leaseClient;\n    private String lockKey;\n    private String lockPath;\n    /** 锁的次数 */\n    private AtomicInteger lockCount;\n    /** 租约有效期,防止客户端崩溃，可在租约到期后自动释放锁；另一方面，正常执行过程中，会自动进行续租,单位 ns */\n    private Long leaseTTL;\n    /** 续约锁租期的定时任务，初次启动延迟，单位默认为 s,默认为1s，可根据业务定制设置*/\n    private Long initialDelay = 0L;\n    /** 定时任务线程池类 */\n    ScheduledExecutorService service = null;\n    /** 保存线程与锁对象的映射，锁对象包含重入次数，重入次数的最大限制为Int的最大值 */\n    private final ConcurrentMap&lt;Thread, LockData&gt; threadData = Maps.newConcurrentMap();\n\n    public EtcdDistributeLock()&#123;&#125;\n\n    public EtcdDistributeLock(Client client, String lockKey, long leaseTTL,TimeUnit unit)&#123;\n        this.client = client;\n        lockClient = client.getLockClient();\n        leaseClient = client.getLeaseClient();\n        this.lockKey = lockKey;\n        // 转纳秒\n        this.leaseTTL = unit.toNanos(leaseTTL);\n        service = Executors.newSingleThreadScheduledExecutor();\n    &#125;\n\n\n    @Override\n    public void lock() &#123;\n        // 检查重入性\n        Thread currentThread = Thread.currentThread();\n        LockData oldLockData = threadData.get(currentThread);\n        if (oldLockData != null &amp;&amp; oldLockData.isLockSuccess()) &#123;\n            // re-entering\n            int lockCount = oldLockData.lockCount.incrementAndGet();\n            if(lockCount &lt; 0 )&#123;\n                throw new Error(&quot;超出可重入次数限制&quot;);\n            &#125;\n            return;\n        &#125;\n\n        // 记录租约 ID\n        Long leaseId = 0L;\n        try&#123;\n            leaseId = leaseClient.grant(TimeUnit.NANOSECONDS.toSeconds(leaseTTL)).get().getID();\n            // 续租心跳周期\n            long period = leaseTTL - leaseTTL / 5;\n            // 启动定时任务续约\n            service.scheduleAtFixedRate(new EtcdDistributeLock.KeepAliveRunnable(leaseClient, leaseId),\n                    initialDelay,period,TimeUnit.NANOSECONDS);\n            LockResponse lockResponse = lockClient.lock(ByteSequence.from(lockKey.getBytes()), leaseId).get();\n            if(lockResponse != null)&#123;\n                lockPath = lockResponse.getKey().toString(Charset.forName(&quot;utf-8&quot;));\n                log.info(&quot;获取锁成功,锁路径:&#123;&#125;,线程:&#123;&#125;&quot;,lockPath,currentThread.getName());\n            &#125;\n        &#125;catch (InterruptedException | ExecutionException e)&#123;\n            log.error(&quot;获取锁失败&quot;,e);\n            return;\n        &#125;\n        // 获取锁成功，锁对象设置\n        LockData newLockData = new LockData(currentThread, lockKey);\n        newLockData.setLeaseId(leaseId);\n        newLockData.setService(service);\n        threadData.put(currentThread, newLockData);\n        newLockData.setLockSuccess(true);\n    &#125;\n\n    @Override\n    public void lockInterruptibly() throws InterruptedException &#123;\n        super.lockInterruptibly();\n    &#125;\n\n    @Override\n    public boolean tryLock() &#123;\n        return super.tryLock();\n    &#125;\n\n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123;\n        return super.tryLock(time,unit);\n    &#125;\n\n\n    @Override\n    public void unlock() &#123;\n        Thread currentThread = Thread.currentThread();\n        LockData lockData = threadData.get(currentThread);\n        if (lockData == null)&#123;\n            throw new IllegalMonitorStateException(&quot;You do not own the lock: &quot; + lockKey);\n        &#125;\n        int newLockCount = lockData.lockCount.decrementAndGet();\n        if ( newLockCount &gt; 0 ) &#123;\n            return;\n        &#125;\n        if ( newLockCount &lt; 0 ) &#123;\n            throw new IllegalMonitorStateException(&quot;Lock count has gone negative for lock: &quot; + lockKey);\n        &#125;\n        try &#123;\n            // 释放锁\n            if(lockPath != null)&#123;\n                lockClient.unlock(ByteSequence.from(lockPath.getBytes())).get();\n            &#125;\n            if(lockData != null)&#123;\n                // 关闭定时任务\n                lockData.getService().shutdown();\n                // 删除租约\n                if (lockData.getLeaseId() != 0L) &#123;\n                    leaseClient.revoke(lockData.getLeaseId());\n                &#125;\n            &#125;\n        &#125; catch (InterruptedException | ExecutionException e) &#123;\n            log.error(&quot;解锁失败&quot;,e);\n        &#125;finally &#123;\n            // 移除当前线程资源\n            threadData.remove(currentThread);\n        &#125;\n    &#125;\n\n\n    @Override\n    public Condition newCondition() &#123;\n        return super.newCondition();\n    &#125;\n\n    /**\n     * 心跳续约线程类\n     */\n    public static class KeepAliveRunnable implements Runnable &#123;\n        private Lease leaseClient;\n        private long leaseId;\n\n        public KeepAliveRunnable(Lease leaseClient, long leaseId) &#123;\n            this.leaseClient = leaseClient;\n            this.leaseId = leaseId;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            // 对该leaseid进行一次续约\n            leaseClient.keepAliveOnce(leaseId);\n        &#125;\n    &#125;\n</code></pre>\n<pre><code class=\"language-java\">public class EtcdLockTest &#123;\n    private Client client;\n    private String key = &quot;/etcd/lock&quot;;\n    private static final String server = &quot;http://xxxx:xxxx&quot;;\n    private ExecutorService executorService = Executors.newFixedThreadPool(10000);\n\n    @Before\n    public void before() throws Exception &#123;\n        initEtcdClient();\n    &#125;\n\n    private void initEtcdClient()&#123;\n       client = Client.builder().endpoints(server).build();\n    &#125;\n\n    @Test\n    public void testEtcdDistributeLock() throws InterruptedException &#123;\n        int[] count = &#123;0&#125;;\n        for (int i = 0; i &lt; 100; i++) &#123;\n            executorService.submit(() -&gt; &#123;\n                final EtcdDistributeLock lock = new EtcdDistributeLock(client, key,20,TimeUnit.SECONDS);\n                try &#123;\n                    lock.lock();\n                    count[0]++;\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125; finally &#123;\n                    try &#123;\n                        lock.unlock();\n                    &#125; catch (Exception e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        System.err.println(&quot;执行结果: &quot; + count[0]);\n    &#125;\n&#125;\n</code></pre>\n<p>（3）、基于Zookeeper分布式锁</p>\n<p>实现原理</p>\n<ol>\n<li>\n<p>启动客户端，确认链接到了服务器</p>\n</li>\n<li>\n<p>多个客户端并发的在特定路径下创建临时性顺序节点</p>\n</li>\n<li>\n<p>客户端判断自己的创建的顺序节点是否是最小的，如果是最小的，则获取锁成功</p>\n</li>\n<li>\n<p>第三步若判定失败，则采用zk的watch机制监听自己的前一个顺序节点，等待前一个节点的删除（放锁）事件，再开始第三步判定</p>\n</li>\n</ol>\n<p><img src=\"/img/%E5%9F%BA%E4%BA%8EZookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.png\" alt=\"基于Zookeeper分布式锁\"></p>\n<p>zookeeper作为高性能分布式协调框架，可以把其看做一个文件系统，其中有节点的概念，并且分为4种：1.持久性节点2.持久性顺序节点3.临时性节点4.临时性顺序节点。</p>\n<p>分布式锁的实现主要思路就是：监控其他客户端的状态，来判断自己是否可以获得锁。</p>\n<p>采用临时性顺序节点的原因：</p>\n<ol>\n<li>\n<p>zk服务器维护了客户端的会话有效性，当会话失效的时候，其会话所创建的临时性节点都会被删除，通过这一特点，可以通过watch临时节点来监控其他客户端的情况，方便自己做出相应动作。</p>\n</li>\n<li>\n<p>因为zk对写操作是顺序性的，所以并发创建的顺序节点会有一个唯一确定的序号，当前锁是公平锁的一种实现，所以依靠这种顺序性可以很好的解释—节点序列小的获取到锁并且可以采用watch自己的前一个节点来避免惊群现象（这样watch事件的传播是线性的）。</p>\n</li>\n</ol>\n<p>例子：</p>\n<pre><code class=\"language-java\">public class ZKLock extends AbstractLock &#123;\n\n    /**\n     *     1.Connect to zk\n     */\n    private CuratorFramework client;\n\n    private InterProcessLock lock ;\n\n\n    public  ZKLock(String zkAddress,String lockPath) &#123;\n        // 1.Connect to zk\n        client = CuratorFrameworkFactory.newClient(\n                zkAddress,\n                new RetryNTimes(5, 5000)\n        );\n        client.start();\n        if(client.getState() == CuratorFrameworkState.STARTED)&#123;\n            log.info(&quot;zk client start successfully!&quot;);\n            log.info(&quot;zkAddress:&#123;&#125;,lockPath:&#123;&#125;&quot;,zkAddress,lockPath);\n        &#125;else&#123;\n            throw new RuntimeException(&quot;客户端启动失败。。。&quot;);\n        &#125;\n        this.lock = defaultLock(lockPath);\n    &#125;\n\n    private InterProcessLock defaultLock(String lockPath )&#123;\n       return  new InterProcessMutex(client, lockPath);\n    &#125;\n    \n    @Override\n    public void lock() &#123;\n        try &#123;\n            this.lock.acquire();\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n    @Override\n    public boolean tryLock() &#123;\n        boolean flag ;\n        try &#123;\n            flag=this.lock.acquire(0,TimeUnit.SECONDS);\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n        return flag;\n    &#125;\n    \n    @Override\n    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123;\n        boolean flag ;\n        try &#123;\n            flag=this.lock.acquire(time,unit);\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n        return flag;\n    &#125;\n    \n    @Override\n    public void unlock() &#123;\n        try &#123;\n            this.lock.release();\n        &#125; catch (Exception e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<pre><code class=\"language-java\"> private ExecutorService executorService = Executors.newCachedThreadPool();\n\n    @Test\n    public void testLock() throws Exception&#123;\n        ZKLock zkLock = new ZKLock(&quot;xxxx:xxxx&quot;,&quot;/lockPath&quot;);\n        int[] num = &#123;0&#125;;\n        long start = System.currentTimeMillis();\n        for(int i=0;i&lt;200;i++)&#123;\n            executorService.submit(()-&gt;&#123;\n                try &#123;\n                    zkLock.lock();\n                    num[0]++;\n                &#125; catch (Exception e)&#123;\n                    throw new RuntimeException(e);\n                &#125; finally &#123;\n                    zkLock.unlock();\n                &#125;\n            &#125;);\n\n        &#125;\n        executorService.shutdown();\n        executorService.awaitTermination(1, TimeUnit.HOURS);\n        log.info(&quot;耗时:&#123;&#125;&quot;,System.currentTimeMillis()-start);\n        System.out.println(num[0]);\n    &#125;\n</code></pre>\n<h1>3、总结</h1>\n<ol>\n<li>\n<p>redis的分布式锁中redisson一般为单实例，当单实例不可用时，会阻塞业务流程。主从方式、主从数据异步，会存在锁失效的问题。RedLock一般要求至少3台以上的redis主从实例，维护成本相对来说比较高。</p>\n</li>\n<li>\n<p>ZK锁具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。但是因为需要频繁的创建和删除节点，性能上不如Redis方式。</p>\n</li>\n<li>\n<p>ETCD分布式锁的实现原理与zk锁类似，但是ETCD分布式锁更加可靠强大。其Lease功能保证分布式锁的安全性；watch功能支持监听某个固定的key，也支持watch一个范围的key（前缀机制）；revision功能可通过 Revision 的大小就可以知道进行写操作的顺序。可以避免 “羊群效应” （也称 “惊群效应”），实现公平锁。前缀机制与watch功能配合使用解决了死锁问题。总之ETCD的灵感来源于Zookeeper,但实现的时候做了很多的改进，如：高负载下的稳定读写、数据模型的多版本并发控制、稳定的watch功能，通知订阅者监听值得变化、可以容忍脑裂现场的发生、客户端的协议使用gRPC协议,支持go、c++、java等。</p>\n</li>\n</ol>\n"},{"title":"散点图","author":"ztq","date":"2021-04-17T10:18:00.000Z","_content":"\n# <center> 散点图</center>\n\n# 一、什么是散点图\n\n散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。\n\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n\n\n\n# 二、散点图的好处\n\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n\n\n\n## 三、matplotlib的散点图\n\n<b>1、plot画散点图</b>\n\n在折线图的基础上设置linestyle='none'\n\n\n```python\n # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame({'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) })\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n```\n\n\n![png](/img/output_8_0.png)\n    \n\n\n<b>2,使用scatter画散点图</b>\n\n简单的一个点的散点图\n\n\n```python\nimport matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n```\n\n\n![png](/img/output_11_0.png)\n    \n\n\n添加标题，给轴加上标签,并确保所有文本都大到能够看清： \n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.title(\"Square Numbers\", fontsize=24)\n\nplt.scatter(2, 4, s=200,color=\"r\") \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(\"Square of Value\", fontsize=14) \n\nplt.xlabel(\"x value\")\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n```\n\n\n![png](/img/output_13_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_14_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_15_0.png)\n    \n\n\n绘制一系列点,x_values,y_values为list\n\n\n```python\nimport matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_17_0.png)\n    \n\n\n自动生成x,y值\n\n\n```python\nx_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n```\n\n\n![png](/img/output_19_0.png)\n    \n\n\n\n```python\nplt.scatter(x_values, y_values, edgecolor='none', s=40) \n```\n\n\n\n\n    <matplotlib.collections.PathCollection at 0x2b4aa653280>\n\n\n\n\n![png](/img/output_20_1.png)\n    \n\n\n<strong>保存到文件</strong>\n\n\n```python\nimport matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(\"Square Numbers\", fontsize=24) \nplt.xlabel(\"Value\", fontsize=14) \nplt.ylabel(\"Square of Value\", fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n```\n\n\n![png](/img/output_22_0.png)\n    \n\n\n\n```python\nimport matplotlib as mpl\nhelp(mpl.markers)\n```\n\n    Help on module matplotlib.markers in matplotlib:\n    \n    NAME\n        matplotlib.markers\n    \n    DESCRIPTION\n        This module contains functions to handle markers.  Used by both the\n        marker functionality of `~matplotlib.axes.Axes.plot` and\n        `~matplotlib.axes.Axes.scatter`.\n        \n        All possible markers are defined here:\n        \n        ============================== ====== =========================================\n        marker                         symbol description\n        ============================== ====== =========================================\n        ``\".\"``                        |m00|  point\n        ``\",\"``                        |m01|  pixel\n        ``\"o\"``                        |m02|  circle\n        ``\"v\"``                        |m03|  triangle_down\n        ``\"^\"``                        |m04|  triangle_up\n        ``\"<\"``                        |m05|  triangle_left\n        ``\">\"``                        |m06|  triangle_right\n        ``\"1\"``                        |m07|  tri_down\n        ``\"2\"``                        |m08|  tri_up\n        ``\"3\"``                        |m09|  tri_left\n        ``\"4\"``                        |m10|  tri_right\n        ``\"8\"``                        |m11|  octagon\n        ``\"s\"``                        |m12|  square\n        ``\"p\"``                        |m13|  pentagon\n        ``\"P\"``                        |m23|  plus (filled)\n        ``\"*\"``                        |m14|  star\n        ``\"h\"``                        |m15|  hexagon1\n        ``\"H\"``                        |m16|  hexagon2\n        ``\"+\"``                        |m17|  plus\n        ``\"x\"``                        |m18|  x\n        ``\"X\"``                        |m24|  x (filled)\n        ``\"D\"``                        |m19|  diamond\n        ``\"d\"``                        |m20|  thin_diamond\n        ``\"|\"``                        |m21|  vline\n        ``\"_\"``                        |m22|  hline\n        ``0`` (``TICKLEFT``)           |m25|  tickleft\n        ``1`` (``TICKRIGHT``)          |m26|  tickright\n        ``2`` (``TICKUP``)             |m27|  tickup\n        ``3`` (``TICKDOWN``)           |m28|  tickdown\n        ``4`` (``CARETLEFT``)          |m29|  caretleft\n        ``5`` (``CARETRIGHT``)         |m30|  caretright\n        ``6`` (``CARETUP``)            |m31|  caretup\n        ``7`` (``CARETDOWN``)          |m32|  caretdown\n        ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n        ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n        ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n        ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n        ``\"None\"``, ``\" \"`` or  ``\"\"``        nothing\n        ``'$...$'``                    |m37|  Render the string using mathtext.\n                                              E.g ``\"$f$\"`` for marker showing the\n                                              letter ``f``.\n        ``verts``                             A list of (x, y) pairs used for Path\n                                              vertices. The center of the marker is\n                                              located at (0, 0) and the size is\n                                              normalized, such that the created path\n                                              is encapsulated inside the unit cell.\n        path                                  A `~matplotlib.path.Path` instance.\n        ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                              rotated by ``angle``.\n        ============================== ====== =========================================\n        \n        ``None`` is the default which means 'nothing', however this table is\n        referred to from other docs for the valid inputs from marker inputs and in\n        those cases ``None`` still means 'default'.\n        \n        Note that special symbols can be defined via the\n        :doc:`STIX math font </tutorials/text/mathtext>`,\n        e.g. ``\"$\\u266B$\"``. For an overview over the STIX font symbols refer to the\n        `STIX font table <http://www.stixfonts.org/allGlyphs.html>`_.\n        Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n        \n        Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n        equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n        Hence the following are equivalent::\n        \n            plt.plot([1, 2, 3], marker=11)\n            plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n        \n        Examples showing the use of markers:\n        \n        * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n        * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n        * :doc:`/gallery/shapes_and_collections/marker_path`\n\n\n​        \n\n        .. |m00| image:: /_static/markers/m00.png\n        .. |m01| image:: /_static/markers/m01.png\n        .. |m02| image:: /_static/markers/m02.png\n        .. |m03| image:: /_static/markers/m03.png\n        .. |m04| image:: /_static/markers/m04.png\n        .. |m05| image:: /_static/markers/m05.png\n        .. |m06| image:: /_static/markers/m06.png\n        .. |m07| image:: /_static/markers/m07.png\n        .. |m08| image:: /_static/markers/m08.png\n        .. |m09| image:: /_static/markers/m09.png\n        .. |m10| image:: /_static/markers/m10.png\n        .. |m11| image:: /_static/markers/m11.png\n        .. |m12| image:: /_static/markers/m12.png\n        .. |m13| image:: /_static/markers/m13.png\n        .. |m14| image:: /_static/markers/m14.png\n        .. |m15| image:: /_static/markers/m15.png\n        .. |m16| image:: /_static/markers/m16.png\n        .. |m17| image:: /_static/markers/m17.png\n        .. |m18| image:: /_static/markers/m18.png\n        .. |m19| image:: /_static/markers/m19.png\n        .. |m20| image:: /_static/markers/m20.png\n        .. |m21| image:: /_static/markers/m21.png\n        .. |m22| image:: /_static/markers/m22.png\n        .. |m23| image:: /_static/markers/m23.png\n        .. |m24| image:: /_static/markers/m24.png\n        .. |m25| image:: /_static/markers/m25.png\n        .. |m26| image:: /_static/markers/m26.png\n        .. |m27| image:: /_static/markers/m27.png\n        .. |m28| image:: /_static/markers/m28.png\n        .. |m29| image:: /_static/markers/m29.png\n        .. |m30| image:: /_static/markers/m30.png\n        .. |m31| image:: /_static/markers/m31.png\n        .. |m32| image:: /_static/markers/m32.png\n        .. |m33| image:: /_static/markers/m33.png\n        .. |m34| image:: /_static/markers/m34.png\n        .. |m35| image:: /_static/markers/m35.png\n        .. |m36| image:: /_static/markers/m36.png\n        .. |m37| image:: /_static/markers/m37.png\n    \n    CLASSES\n        builtins.object\n            MarkerStyle\n        \n        class MarkerStyle(builtins.object)\n         |  MarkerStyle(marker=None, fillstyle=None)\n         |  \n         |  Methods defined here:\n         |  \n         |  __bool__(self)\n         |  \n         |  __init__(self, marker=None, fillstyle=None)\n         |      Attributes\n         |      ----------\n         |      markers : list of known marks\n         |      \n         |      fillstyles : list of known fillstyles\n         |      \n         |      filled_markers : list of known filled markers.\n         |      \n         |      Parameters\n         |      ----------\n         |      marker : str or array-like, optional, default: None\n         |          See the descriptions of possible markers in the module docstring.\n         |      \n         |      fillstyle : str, optional, default: 'full'\n         |          'full', 'left\", 'right', 'bottom', 'top', 'none'\n         |  \n         |  get_alt_path(self)\n         |  \n         |  get_alt_transform(self)\n         |  \n         |  get_capstyle(self)\n         |  \n         |  get_fillstyle(self)\n         |  \n         |  get_joinstyle(self)\n         |  \n         |  get_marker(self)\n         |  \n         |  get_path(self)\n         |  \n         |  get_snap_threshold(self)\n         |  \n         |  get_transform(self)\n         |  \n         |  is_filled(self)\n         |  \n         |  set_fillstyle(self, fillstyle)\n         |      Sets fillstyle\n         |      \n         |      Parameters\n         |      ----------\n         |      fillstyle : string amongst known fillstyles\n         |  \n         |  set_marker(self, marker)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data descriptors defined here:\n         |  \n         |  __dict__\n         |      dictionary for instance variables (if defined)\n         |  \n         |  __weakref__\n         |      list of weak references to the object (if defined)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data and other attributes defined here:\n         |  \n         |  filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H...\n         |  \n         |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n         |  \n         |  markers = {'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n    \n    DATA\n        CARETDOWN = 7\n        CARETDOWNBASE = 11\n        CARETLEFT = 4\n        CARETLEFTBASE = 8\n        CARETRIGHT = 5\n        CARETRIGHTBASE = 9\n        CARETUP = 6\n        CARETUPBASE = 10\n        TICKDOWN = 3\n        TICKLEFT = 0\n        TICKRIGHT = 1\n        TICKUP = 2\n        rcParams = RcParams({'_internal.classic_mode': False,\n             ...nor.widt...\n    \n    FILE\n        c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n\n\n​    \n​    \n\n## 四、seaborn画散点图\n\n<b>1、scatterplot函数</b>\n\n(1)格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.scatterplot)\n\n```\n\n    Help on function scatterplot in module seaborn.relational:\n    \n    scatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n        Draw a scatter plot with possibility of several semantic groupings.\n        \n        The relationship between ``x`` and ``y`` can be shown for different subsets\n        of the data using the ``hue``, ``size``, and ``style`` parameters. These\n        parameters control what visual semantics are used to identify the different\n        subsets. It is possible to show up to three dimensions independently by\n        using all three semantic types, but this style of plot can be hard to\n        interpret and is often ineffective. Using redundant semantics (i.e. both\n        ``hue`` and ``style`` for the same variable) can be helpful for making\n        graphics more accessible.\n        \n        See the :ref:`tutorial <relational_tutorial>` for more information.\n        \n        The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n        semantic, if present, depends on whether the variable is inferred to\n        represent \"numeric\" or \"categorical\" data. In particular, numeric variables\n        are represented with a sequential colormap by default, and the legend\n        entries show regular \"ticks\" with values that may or may not exist in the\n        data. This behavior can be controlled through various parameters, as\n        described and illustrated below.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        hue : vector or key in ``data``\n            Grouping variable that will produce points with different colors.\n            Can be either categorical or numeric, although color mapping will\n            behave differently in latter case.\n        size : vector or key in ``data``\n            Grouping variable that will produce points with different sizes.\n            Can be either categorical or numeric, although size mapping will\n            behave differently in latter case.\n        style : vector or key in ``data``\n            Grouping variable that will produce points with different markers.\n            Can have a numeric dtype but will always be treated as categorical.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        sizes : list, dict, or tuple\n            An object that determines how sizes are chosen when ``size`` is used.\n            It can always be a list of size values or a dict mapping levels of the\n            ``size`` variable to sizes. When ``size``  is numeric, it can also be\n            a tuple specifying the minimum and maximum size to use such that other\n            values are normalized within this range.\n        size_order : list\n            Specified order for appearance of the ``size`` variable levels,\n            otherwise they are determined from the data. Not relevant when the\n            ``size`` variable is numeric.\n        size_norm : tuple or Normalize object\n            Normalization in data units for scaling plot objects when the\n            ``size`` variable is numeric.\n        markers : boolean, list, or dictionary\n            Object determining how to draw the markers for different levels of the\n            ``style`` variable. Setting to ``True`` will use default markers, or\n            you can pass a list of markers or a dictionary mapping levels of the\n            ``style`` variable to markers. Setting to ``False`` will draw\n            marker-less lines.  Markers are specified as in matplotlib.\n        style_order : list\n            Specified order for appearance of the ``style`` variable levels\n            otherwise they are determined from the data. Not relevant when the\n            ``style`` variable is numeric.\n        {x,y}_bins : lists or arrays or functions\n            *Currently non-functional.*\n        units : vector or key in ``data``\n            Grouping variable identifying sampling units. When used, a separate\n            line will be drawn for each unit with appropriate semantics, but no\n            legend entry will be added. Useful for showing distribution of\n            experimental replicates when exact identities are not needed.\n            *Currently non-functional.*\n        estimator : name of pandas method or callable or None\n            Method for aggregating across multiple observations of the ``y``\n            variable at the same ``x`` level. If ``None``, all observations will\n            be drawn.\n            *Currently non-functional.*\n        ci : int or \"sd\" or None\n            Size of the confidence interval to draw when aggregating with an\n            estimator. \"sd\" means to draw the standard deviation of the data.\n            Setting to ``None`` will skip bootstrapping.\n            *Currently non-functional.*\n        n_boot : int\n            Number of bootstraps to use for computing the confidence interval.\n            *Currently non-functional.*\n        alpha : float\n            Proportional opacity of the points.\n        {x,y}_jitter : booleans or floats\n            *Currently non-functional.*\n        legend : \"auto\", \"brief\", \"full\", or False\n            How to draw the legend. If \"brief\", numeric ``hue`` and ``size``\n            variables will be represented with a sample of evenly spaced values.\n            If \"full\", every group will get an entry in the legend. If \"auto\",\n            choose between brief or full representation based on number of levels.\n            If ``False``, no legend data is added and no legend is drawn.\n        ax : :class:`matplotlib.axes.Axes`\n            Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n            internally.\n        kwargs : key, value mappings\n            Other keyword arguments are passed down to\n            :meth:`matplotlib.axes.Axes.scatter`.\n        \n        Returns\n        -------\n        :class:`matplotlib.axes.Axes`\n            The matplotlib axes containing the plot.\n        \n        See Also\n        --------\n        lineplot : Plot data using lines.\n        stripplot : Plot a categorical scatter with jitter.\n        swarmplot : Plot a categorical scatter with non-overlapping points.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/scatterplot.rst\n\n\n​    \n\n（2）简单例子\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(\"tips\")\n\"\"\"\n案例1：散点图\n\"\"\"\nsns.scatterplot( x=\"total_bill\", y=\"tip\",data=tips)\nplt.show()\n```\n\n\n![png](/img/output_29_0.png)\n    \n\n\n<strong>分组</strong>\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(\"iris\")\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=\"brief\")\n#legend的取值为brief,full和False\n```\n\n\n![png](/img/output_31_0.png)\n    \n\n\n<b>2、regplot函数</b>\n\nregplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n#help(sns.regplot)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490>\n\n\n\n\n![png](/img/output_34_1.png)\n    \n\n如不显示拟合线，可将fit_reg设置为false\n\nregplot完整用法请参阅\nhttps://seaborn.pydata.org/generated/seaborn.regplot.html\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520>\n\n\n\n\n![png](/img/output_36_1.png)\n    \n\n\n<b>3、lmplot函数画散点图</b>\n\nlmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=\"species\"。\n\n(1)implot 简单散点图\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_40_0.png)\n    \n\n\n（2）每组使用不同的标记（markers属性）\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, markers=[\"o\", \"x\", \"2\"])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_42_0.png)\n    \n\n\n（3）使用不同的调色板(palette)\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=\"Set2\")\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_44_0.png)\n    \n\n\n（4）控制每组的颜色\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=\"#9b59b6\", virginica=\"#3498db\", versicolor=\"#95a5a6\"))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_46_0.png)\n    \n\n\n# <center> 气泡图</center>\n\n散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。\n\n假设某个农产品的产量与温度和降雨量的关系如下表所示。<br>\n\n\n```python\nimport pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf_product\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    \n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n作出产量与温度的散点图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_52_0.png)\n    \n\n\n若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\nrainfall=df_product[u\"降雨量\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_54_0.png)\n    \n\n\n### 课堂练习：使用seaborn画气泡图\n\n自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```","source":"_posts/散点图.md","raw":"title: 散点图\nauthor: ztq\ntags:\n  - python\ncategories:\n  - python基础\ndate: 2021-04-17 18:18:00\n---\n\n# <center> 散点图</center>\n\n# 一、什么是散点图\n\n散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。\n\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n\n\n\n# 二、散点图的好处\n\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n\n\n\n## 三、matplotlib的散点图\n\n<b>1、plot画散点图</b>\n\n在折线图的基础上设置linestyle='none'\n\n\n```python\n # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame({'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) })\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n```\n\n\n![png](/img/output_8_0.png)\n    \n\n\n<b>2,使用scatter画散点图</b>\n\n简单的一个点的散点图\n\n\n```python\nimport matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n```\n\n\n![png](/img/output_11_0.png)\n    \n\n\n添加标题，给轴加上标签,并确保所有文本都大到能够看清： \n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.title(\"Square Numbers\", fontsize=24)\n\nplt.scatter(2, 4, s=200,color=\"r\") \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(\"Square of Value\", fontsize=14) \n\nplt.xlabel(\"x value\")\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n```\n\n\n![png](/img/output_13_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_14_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_15_0.png)\n    \n\n\n绘制一系列点,x_values,y_values为list\n\n\n```python\nimport matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_17_0.png)\n    \n\n\n自动生成x,y值\n\n\n```python\nx_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n```\n\n\n![png](/img/output_19_0.png)\n    \n\n\n\n```python\nplt.scatter(x_values, y_values, edgecolor='none', s=40) \n```\n\n\n\n\n    <matplotlib.collections.PathCollection at 0x2b4aa653280>\n\n\n\n\n![png](/img/output_20_1.png)\n    \n\n\n<strong>保存到文件</strong>\n\n\n```python\nimport matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(\"Square Numbers\", fontsize=24) \nplt.xlabel(\"Value\", fontsize=14) \nplt.ylabel(\"Square of Value\", fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n```\n\n\n![png](/img/output_22_0.png)\n    \n\n\n\n```python\nimport matplotlib as mpl\nhelp(mpl.markers)\n```\n\n    Help on module matplotlib.markers in matplotlib:\n    \n    NAME\n        matplotlib.markers\n    \n    DESCRIPTION\n        This module contains functions to handle markers.  Used by both the\n        marker functionality of `~matplotlib.axes.Axes.plot` and\n        `~matplotlib.axes.Axes.scatter`.\n        \n        All possible markers are defined here:\n        \n        ============================== ====== =========================================\n        marker                         symbol description\n        ============================== ====== =========================================\n        ``\".\"``                        |m00|  point\n        ``\",\"``                        |m01|  pixel\n        ``\"o\"``                        |m02|  circle\n        ``\"v\"``                        |m03|  triangle_down\n        ``\"^\"``                        |m04|  triangle_up\n        ``\"<\"``                        |m05|  triangle_left\n        ``\">\"``                        |m06|  triangle_right\n        ``\"1\"``                        |m07|  tri_down\n        ``\"2\"``                        |m08|  tri_up\n        ``\"3\"``                        |m09|  tri_left\n        ``\"4\"``                        |m10|  tri_right\n        ``\"8\"``                        |m11|  octagon\n        ``\"s\"``                        |m12|  square\n        ``\"p\"``                        |m13|  pentagon\n        ``\"P\"``                        |m23|  plus (filled)\n        ``\"*\"``                        |m14|  star\n        ``\"h\"``                        |m15|  hexagon1\n        ``\"H\"``                        |m16|  hexagon2\n        ``\"+\"``                        |m17|  plus\n        ``\"x\"``                        |m18|  x\n        ``\"X\"``                        |m24|  x (filled)\n        ``\"D\"``                        |m19|  diamond\n        ``\"d\"``                        |m20|  thin_diamond\n        ``\"|\"``                        |m21|  vline\n        ``\"_\"``                        |m22|  hline\n        ``0`` (``TICKLEFT``)           |m25|  tickleft\n        ``1`` (``TICKRIGHT``)          |m26|  tickright\n        ``2`` (``TICKUP``)             |m27|  tickup\n        ``3`` (``TICKDOWN``)           |m28|  tickdown\n        ``4`` (``CARETLEFT``)          |m29|  caretleft\n        ``5`` (``CARETRIGHT``)         |m30|  caretright\n        ``6`` (``CARETUP``)            |m31|  caretup\n        ``7`` (``CARETDOWN``)          |m32|  caretdown\n        ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n        ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n        ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n        ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n        ``\"None\"``, ``\" \"`` or  ``\"\"``        nothing\n        ``'$...$'``                    |m37|  Render the string using mathtext.\n                                              E.g ``\"$f$\"`` for marker showing the\n                                              letter ``f``.\n        ``verts``                             A list of (x, y) pairs used for Path\n                                              vertices. The center of the marker is\n                                              located at (0, 0) and the size is\n                                              normalized, such that the created path\n                                              is encapsulated inside the unit cell.\n        path                                  A `~matplotlib.path.Path` instance.\n        ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                              rotated by ``angle``.\n        ============================== ====== =========================================\n        \n        ``None`` is the default which means 'nothing', however this table is\n        referred to from other docs for the valid inputs from marker inputs and in\n        those cases ``None`` still means 'default'.\n        \n        Note that special symbols can be defined via the\n        :doc:`STIX math font </tutorials/text/mathtext>`,\n        e.g. ``\"$\\u266B$\"``. For an overview over the STIX font symbols refer to the\n        `STIX font table <http://www.stixfonts.org/allGlyphs.html>`_.\n        Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n        \n        Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n        equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n        Hence the following are equivalent::\n        \n            plt.plot([1, 2, 3], marker=11)\n            plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n        \n        Examples showing the use of markers:\n        \n        * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n        * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n        * :doc:`/gallery/shapes_and_collections/marker_path`\n\n\n​        \n\n        .. |m00| image:: /_static/markers/m00.png\n        .. |m01| image:: /_static/markers/m01.png\n        .. |m02| image:: /_static/markers/m02.png\n        .. |m03| image:: /_static/markers/m03.png\n        .. |m04| image:: /_static/markers/m04.png\n        .. |m05| image:: /_static/markers/m05.png\n        .. |m06| image:: /_static/markers/m06.png\n        .. |m07| image:: /_static/markers/m07.png\n        .. |m08| image:: /_static/markers/m08.png\n        .. |m09| image:: /_static/markers/m09.png\n        .. |m10| image:: /_static/markers/m10.png\n        .. |m11| image:: /_static/markers/m11.png\n        .. |m12| image:: /_static/markers/m12.png\n        .. |m13| image:: /_static/markers/m13.png\n        .. |m14| image:: /_static/markers/m14.png\n        .. |m15| image:: /_static/markers/m15.png\n        .. |m16| image:: /_static/markers/m16.png\n        .. |m17| image:: /_static/markers/m17.png\n        .. |m18| image:: /_static/markers/m18.png\n        .. |m19| image:: /_static/markers/m19.png\n        .. |m20| image:: /_static/markers/m20.png\n        .. |m21| image:: /_static/markers/m21.png\n        .. |m22| image:: /_static/markers/m22.png\n        .. |m23| image:: /_static/markers/m23.png\n        .. |m24| image:: /_static/markers/m24.png\n        .. |m25| image:: /_static/markers/m25.png\n        .. |m26| image:: /_static/markers/m26.png\n        .. |m27| image:: /_static/markers/m27.png\n        .. |m28| image:: /_static/markers/m28.png\n        .. |m29| image:: /_static/markers/m29.png\n        .. |m30| image:: /_static/markers/m30.png\n        .. |m31| image:: /_static/markers/m31.png\n        .. |m32| image:: /_static/markers/m32.png\n        .. |m33| image:: /_static/markers/m33.png\n        .. |m34| image:: /_static/markers/m34.png\n        .. |m35| image:: /_static/markers/m35.png\n        .. |m36| image:: /_static/markers/m36.png\n        .. |m37| image:: /_static/markers/m37.png\n    \n    CLASSES\n        builtins.object\n            MarkerStyle\n        \n        class MarkerStyle(builtins.object)\n         |  MarkerStyle(marker=None, fillstyle=None)\n         |  \n         |  Methods defined here:\n         |  \n         |  __bool__(self)\n         |  \n         |  __init__(self, marker=None, fillstyle=None)\n         |      Attributes\n         |      ----------\n         |      markers : list of known marks\n         |      \n         |      fillstyles : list of known fillstyles\n         |      \n         |      filled_markers : list of known filled markers.\n         |      \n         |      Parameters\n         |      ----------\n         |      marker : str or array-like, optional, default: None\n         |          See the descriptions of possible markers in the module docstring.\n         |      \n         |      fillstyle : str, optional, default: 'full'\n         |          'full', 'left\", 'right', 'bottom', 'top', 'none'\n         |  \n         |  get_alt_path(self)\n         |  \n         |  get_alt_transform(self)\n         |  \n         |  get_capstyle(self)\n         |  \n         |  get_fillstyle(self)\n         |  \n         |  get_joinstyle(self)\n         |  \n         |  get_marker(self)\n         |  \n         |  get_path(self)\n         |  \n         |  get_snap_threshold(self)\n         |  \n         |  get_transform(self)\n         |  \n         |  is_filled(self)\n         |  \n         |  set_fillstyle(self, fillstyle)\n         |      Sets fillstyle\n         |      \n         |      Parameters\n         |      ----------\n         |      fillstyle : string amongst known fillstyles\n         |  \n         |  set_marker(self, marker)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data descriptors defined here:\n         |  \n         |  __dict__\n         |      dictionary for instance variables (if defined)\n         |  \n         |  __weakref__\n         |      list of weak references to the object (if defined)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data and other attributes defined here:\n         |  \n         |  filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H...\n         |  \n         |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n         |  \n         |  markers = {'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n    \n    DATA\n        CARETDOWN = 7\n        CARETDOWNBASE = 11\n        CARETLEFT = 4\n        CARETLEFTBASE = 8\n        CARETRIGHT = 5\n        CARETRIGHTBASE = 9\n        CARETUP = 6\n        CARETUPBASE = 10\n        TICKDOWN = 3\n        TICKLEFT = 0\n        TICKRIGHT = 1\n        TICKUP = 2\n        rcParams = RcParams({'_internal.classic_mode': False,\n             ...nor.widt...\n    \n    FILE\n        c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n\n\n​    \n​    \n\n## 四、seaborn画散点图\n\n<b>1、scatterplot函数</b>\n\n(1)格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.scatterplot)\n\n```\n\n    Help on function scatterplot in module seaborn.relational:\n    \n    scatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n        Draw a scatter plot with possibility of several semantic groupings.\n        \n        The relationship between ``x`` and ``y`` can be shown for different subsets\n        of the data using the ``hue``, ``size``, and ``style`` parameters. These\n        parameters control what visual semantics are used to identify the different\n        subsets. It is possible to show up to three dimensions independently by\n        using all three semantic types, but this style of plot can be hard to\n        interpret and is often ineffective. Using redundant semantics (i.e. both\n        ``hue`` and ``style`` for the same variable) can be helpful for making\n        graphics more accessible.\n        \n        See the :ref:`tutorial <relational_tutorial>` for more information.\n        \n        The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n        semantic, if present, depends on whether the variable is inferred to\n        represent \"numeric\" or \"categorical\" data. In particular, numeric variables\n        are represented with a sequential colormap by default, and the legend\n        entries show regular \"ticks\" with values that may or may not exist in the\n        data. This behavior can be controlled through various parameters, as\n        described and illustrated below.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        hue : vector or key in ``data``\n            Grouping variable that will produce points with different colors.\n            Can be either categorical or numeric, although color mapping will\n            behave differently in latter case.\n        size : vector or key in ``data``\n            Grouping variable that will produce points with different sizes.\n            Can be either categorical or numeric, although size mapping will\n            behave differently in latter case.\n        style : vector or key in ``data``\n            Grouping variable that will produce points with different markers.\n            Can have a numeric dtype but will always be treated as categorical.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        sizes : list, dict, or tuple\n            An object that determines how sizes are chosen when ``size`` is used.\n            It can always be a list of size values or a dict mapping levels of the\n            ``size`` variable to sizes. When ``size``  is numeric, it can also be\n            a tuple specifying the minimum and maximum size to use such that other\n            values are normalized within this range.\n        size_order : list\n            Specified order for appearance of the ``size`` variable levels,\n            otherwise they are determined from the data. Not relevant when the\n            ``size`` variable is numeric.\n        size_norm : tuple or Normalize object\n            Normalization in data units for scaling plot objects when the\n            ``size`` variable is numeric.\n        markers : boolean, list, or dictionary\n            Object determining how to draw the markers for different levels of the\n            ``style`` variable. Setting to ``True`` will use default markers, or\n            you can pass a list of markers or a dictionary mapping levels of the\n            ``style`` variable to markers. Setting to ``False`` will draw\n            marker-less lines.  Markers are specified as in matplotlib.\n        style_order : list\n            Specified order for appearance of the ``style`` variable levels\n            otherwise they are determined from the data. Not relevant when the\n            ``style`` variable is numeric.\n        {x,y}_bins : lists or arrays or functions\n            *Currently non-functional.*\n        units : vector or key in ``data``\n            Grouping variable identifying sampling units. When used, a separate\n            line will be drawn for each unit with appropriate semantics, but no\n            legend entry will be added. Useful for showing distribution of\n            experimental replicates when exact identities are not needed.\n            *Currently non-functional.*\n        estimator : name of pandas method or callable or None\n            Method for aggregating across multiple observations of the ``y``\n            variable at the same ``x`` level. If ``None``, all observations will\n            be drawn.\n            *Currently non-functional.*\n        ci : int or \"sd\" or None\n            Size of the confidence interval to draw when aggregating with an\n            estimator. \"sd\" means to draw the standard deviation of the data.\n            Setting to ``None`` will skip bootstrapping.\n            *Currently non-functional.*\n        n_boot : int\n            Number of bootstraps to use for computing the confidence interval.\n            *Currently non-functional.*\n        alpha : float\n            Proportional opacity of the points.\n        {x,y}_jitter : booleans or floats\n            *Currently non-functional.*\n        legend : \"auto\", \"brief\", \"full\", or False\n            How to draw the legend. If \"brief\", numeric ``hue`` and ``size``\n            variables will be represented with a sample of evenly spaced values.\n            If \"full\", every group will get an entry in the legend. If \"auto\",\n            choose between brief or full representation based on number of levels.\n            If ``False``, no legend data is added and no legend is drawn.\n        ax : :class:`matplotlib.axes.Axes`\n            Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n            internally.\n        kwargs : key, value mappings\n            Other keyword arguments are passed down to\n            :meth:`matplotlib.axes.Axes.scatter`.\n        \n        Returns\n        -------\n        :class:`matplotlib.axes.Axes`\n            The matplotlib axes containing the plot.\n        \n        See Also\n        --------\n        lineplot : Plot data using lines.\n        stripplot : Plot a categorical scatter with jitter.\n        swarmplot : Plot a categorical scatter with non-overlapping points.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/scatterplot.rst\n\n\n​    \n\n（2）简单例子\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(\"tips\")\n\"\"\"\n案例1：散点图\n\"\"\"\nsns.scatterplot( x=\"total_bill\", y=\"tip\",data=tips)\nplt.show()\n```\n\n\n![png](/img/output_29_0.png)\n    \n\n\n<strong>分组</strong>\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(\"iris\")\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=\"brief\")\n#legend的取值为brief,full和False\n```\n\n\n![png](/img/output_31_0.png)\n    \n\n\n<b>2、regplot函数</b>\n\nregplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n#help(sns.regplot)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490>\n\n\n\n\n![png](/img/output_34_1.png)\n    \n\n如不显示拟合线，可将fit_reg设置为false\n\nregplot完整用法请参阅\nhttps://seaborn.pydata.org/generated/seaborn.regplot.html\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520>\n\n\n\n\n![png](/img/output_36_1.png)\n    \n\n\n<b>3、lmplot函数画散点图</b>\n\nlmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=\"species\"。\n\n(1)implot 简单散点图\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_40_0.png)\n    \n\n\n（2）每组使用不同的标记（markers属性）\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, markers=[\"o\", \"x\", \"2\"])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_42_0.png)\n    \n\n\n（3）使用不同的调色板(palette)\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=\"Set2\")\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_44_0.png)\n    \n\n\n（4）控制每组的颜色\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=\"#9b59b6\", virginica=\"#3498db\", versicolor=\"#95a5a6\"))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_46_0.png)\n    \n\n\n# <center> 气泡图</center>\n\n散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。\n\n假设某个农产品的产量与温度和降雨量的关系如下表所示。<br>\n\n\n```python\nimport pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf_product\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    \n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n作出产量与温度的散点图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_52_0.png)\n    \n\n\n若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\nrainfall=df_product[u\"降雨量\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_54_0.png)\n    \n\n\n### 课堂练习：使用seaborn画气泡图\n\n自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```","slug":"散点图","published":1,"updated":"2022-04-04T08:32:40.176Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2600ff7kt96p3l77is","content":"<h1><center> 散点图</center></h1>\n<h1>一、什么是散点图</h1>\n<p>散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。</p>\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n<h1>二、散点图的好处</h1>\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n<h2 id=\"三、matplotlib的散点图\">三、matplotlib的散点图</h2>\n<p><b>1、plot画散点图</b></p>\n<p>在折线图的基础上设置linestyle=‘none’</p>\n<pre><code class=\"language-python\"> # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame(&#123;'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) &#125;)\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_8_0.png\" alt=\"png\"></p>\n<p><b>2,使用scatter画散点图</b></p>\n<p>简单的一个点的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n</code></pre>\n<p><img src=\"/img/output_11_0.png\" alt=\"png\"></p>\n<p>添加标题，给轴加上标签,并确保所有文本都大到能够看清：</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24)\n\nplt.scatter(2, 4, s=200,color=&quot;r&quot;) \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\nplt.xlabel(&quot;x value&quot;)\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_14_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_15_0.png\" alt=\"png\"></p>\n<p>绘制一系列点,x_values,y_values为list</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_17_0.png\" alt=\"png\"></p>\n<p>自动生成x,y值</p>\n<pre><code class=\"language-python\">x_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_19_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">plt.scatter(x_values, y_values, edgecolor='none', s=40) \n</code></pre>\n<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b4aa653280&gt;\n</code></pre>\n<p><img src=\"/img/output_20_1.png\" alt=\"png\"></p>\n<p><strong>保存到文件</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(&quot;Square Numbers&quot;, fontsize=24) \nplt.xlabel(&quot;Value&quot;, fontsize=14) \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n</code></pre>\n<p><img src=\"/img/output_22_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nhelp(mpl.markers)\n</code></pre>\n<pre><code>Help on module matplotlib.markers in matplotlib:\n\nNAME\n    matplotlib.markers\n\nDESCRIPTION\n    This module contains functions to handle markers.  Used by both the\n    marker functionality of `~matplotlib.axes.Axes.plot` and\n    `~matplotlib.axes.Axes.scatter`.\n    \n    All possible markers are defined here:\n    \n    ============================== ====== =========================================\n    marker                         symbol description\n    ============================== ====== =========================================\n    ``&quot;.&quot;``                        |m00|  point\n    ``&quot;,&quot;``                        |m01|  pixel\n    ``&quot;o&quot;``                        |m02|  circle\n    ``&quot;v&quot;``                        |m03|  triangle_down\n    ``&quot;^&quot;``                        |m04|  triangle_up\n    ``&quot;&lt;&quot;``                        |m05|  triangle_left\n    ``&quot;&gt;&quot;``                        |m06|  triangle_right\n    ``&quot;1&quot;``                        |m07|  tri_down\n    ``&quot;2&quot;``                        |m08|  tri_up\n    ``&quot;3&quot;``                        |m09|  tri_left\n    ``&quot;4&quot;``                        |m10|  tri_right\n    ``&quot;8&quot;``                        |m11|  octagon\n    ``&quot;s&quot;``                        |m12|  square\n    ``&quot;p&quot;``                        |m13|  pentagon\n    ``&quot;P&quot;``                        |m23|  plus (filled)\n    ``&quot;*&quot;``                        |m14|  star\n    ``&quot;h&quot;``                        |m15|  hexagon1\n    ``&quot;H&quot;``                        |m16|  hexagon2\n    ``&quot;+&quot;``                        |m17|  plus\n    ``&quot;x&quot;``                        |m18|  x\n    ``&quot;X&quot;``                        |m24|  x (filled)\n    ``&quot;D&quot;``                        |m19|  diamond\n    ``&quot;d&quot;``                        |m20|  thin_diamond\n    ``&quot;|&quot;``                        |m21|  vline\n    ``&quot;_&quot;``                        |m22|  hline\n    ``0`` (``TICKLEFT``)           |m25|  tickleft\n    ``1`` (``TICKRIGHT``)          |m26|  tickright\n    ``2`` (``TICKUP``)             |m27|  tickup\n    ``3`` (``TICKDOWN``)           |m28|  tickdown\n    ``4`` (``CARETLEFT``)          |m29|  caretleft\n    ``5`` (``CARETRIGHT``)         |m30|  caretright\n    ``6`` (``CARETUP``)            |m31|  caretup\n    ``7`` (``CARETDOWN``)          |m32|  caretdown\n    ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n    ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n    ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n    ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n    ``&quot;None&quot;``, ``&quot; &quot;`` or  ``&quot;&quot;``        nothing\n    ``'$...$'``                    |m37|  Render the string using mathtext.\n                                          E.g ``&quot;$f$&quot;`` for marker showing the\n                                          letter ``f``.\n    ``verts``                             A list of (x, y) pairs used for Path\n                                          vertices. The center of the marker is\n                                          located at (0, 0) and the size is\n                                          normalized, such that the created path\n                                          is encapsulated inside the unit cell.\n    path                                  A `~matplotlib.path.Path` instance.\n    ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                          rotated by ``angle``.\n    ============================== ====== =========================================\n    \n    ``None`` is the default which means 'nothing', however this table is\n    referred to from other docs for the valid inputs from marker inputs and in\n    those cases ``None`` still means 'default'.\n    \n    Note that special symbols can be defined via the\n    :doc:`STIX math font &lt;/tutorials/text/mathtext&gt;`,\n    e.g. ``&quot;$\\u266B$&quot;``. For an overview over the STIX font symbols refer to the\n    `STIX font table &lt;http://www.stixfonts.org/allGlyphs.html&gt;`_.\n    Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n    \n    Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n    equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n    Hence the following are equivalent::\n    \n        plt.plot([1, 2, 3], marker=11)\n        plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n    \n    Examples showing the use of markers:\n    \n    * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n    * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n    * :doc:`/gallery/shapes_and_collections/marker_path`\n</code></pre>\n<p>​</p>\n<pre><code>    .. |m00| image:: /_static/markers/m00.png\n    .. |m01| image:: /_static/markers/m01.png\n    .. |m02| image:: /_static/markers/m02.png\n    .. |m03| image:: /_static/markers/m03.png\n    .. |m04| image:: /_static/markers/m04.png\n    .. |m05| image:: /_static/markers/m05.png\n    .. |m06| image:: /_static/markers/m06.png\n    .. |m07| image:: /_static/markers/m07.png\n    .. |m08| image:: /_static/markers/m08.png\n    .. |m09| image:: /_static/markers/m09.png\n    .. |m10| image:: /_static/markers/m10.png\n    .. |m11| image:: /_static/markers/m11.png\n    .. |m12| image:: /_static/markers/m12.png\n    .. |m13| image:: /_static/markers/m13.png\n    .. |m14| image:: /_static/markers/m14.png\n    .. |m15| image:: /_static/markers/m15.png\n    .. |m16| image:: /_static/markers/m16.png\n    .. |m17| image:: /_static/markers/m17.png\n    .. |m18| image:: /_static/markers/m18.png\n    .. |m19| image:: /_static/markers/m19.png\n    .. |m20| image:: /_static/markers/m20.png\n    .. |m21| image:: /_static/markers/m21.png\n    .. |m22| image:: /_static/markers/m22.png\n    .. |m23| image:: /_static/markers/m23.png\n    .. |m24| image:: /_static/markers/m24.png\n    .. |m25| image:: /_static/markers/m25.png\n    .. |m26| image:: /_static/markers/m26.png\n    .. |m27| image:: /_static/markers/m27.png\n    .. |m28| image:: /_static/markers/m28.png\n    .. |m29| image:: /_static/markers/m29.png\n    .. |m30| image:: /_static/markers/m30.png\n    .. |m31| image:: /_static/markers/m31.png\n    .. |m32| image:: /_static/markers/m32.png\n    .. |m33| image:: /_static/markers/m33.png\n    .. |m34| image:: /_static/markers/m34.png\n    .. |m35| image:: /_static/markers/m35.png\n    .. |m36| image:: /_static/markers/m36.png\n    .. |m37| image:: /_static/markers/m37.png\n\nCLASSES\n    builtins.object\n        MarkerStyle\n    \n    class MarkerStyle(builtins.object)\n     |  MarkerStyle(marker=None, fillstyle=None)\n     |  \n     |  Methods defined here:\n     |  \n     |  __bool__(self)\n     |  \n     |  __init__(self, marker=None, fillstyle=None)\n     |      Attributes\n     |      ----------\n     |      markers : list of known marks\n     |      \n     |      fillstyles : list of known fillstyles\n     |      \n     |      filled_markers : list of known filled markers.\n     |      \n     |      Parameters\n     |      ----------\n     |      marker : str or array-like, optional, default: None\n     |          See the descriptions of possible markers in the module docstring.\n     |      \n     |      fillstyle : str, optional, default: 'full'\n     |          'full', 'left&quot;, 'right', 'bottom', 'top', 'none'\n     |  \n     |  get_alt_path(self)\n     |  \n     |  get_alt_transform(self)\n     |  \n     |  get_capstyle(self)\n     |  \n     |  get_fillstyle(self)\n     |  \n     |  get_joinstyle(self)\n     |  \n     |  get_marker(self)\n     |  \n     |  get_path(self)\n     |  \n     |  get_snap_threshold(self)\n     |  \n     |  get_transform(self)\n     |  \n     |  is_filled(self)\n     |  \n     |  set_fillstyle(self, fillstyle)\n     |      Sets fillstyle\n     |      \n     |      Parameters\n     |      ----------\n     |      fillstyle : string amongst known fillstyles\n     |  \n     |  set_marker(self, marker)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  filled_markers = ('o', 'v', '^', '&lt;', '&gt;', '8', 's', 'p', '*', 'h', 'H...\n     |  \n     |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n     |  \n     |  markers = &#123;'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n\nDATA\n    CARETDOWN = 7\n    CARETDOWNBASE = 11\n    CARETLEFT = 4\n    CARETLEFTBASE = 8\n    CARETRIGHT = 5\n    CARETRIGHTBASE = 9\n    CARETUP = 6\n    CARETUPBASE = 10\n    TICKDOWN = 3\n    TICKLEFT = 0\n    TICKRIGHT = 1\n    TICKUP = 2\n    rcParams = RcParams(&#123;'_internal.classic_mode': False,\n         ...nor.widt...\n\nFILE\n    c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n</code></pre>\n<p>​<br>\n​</p>\n<h2 id=\"四、seaborn画散点图\">四、seaborn画散点图</h2>\n<p><b>1、scatterplot函数</b></p>\n<p>(1)格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.scatterplot)\n\n</code></pre>\n<pre><code>Help on function scatterplot in module seaborn.relational:\n\nscatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n    Draw a scatter plot with possibility of several semantic groupings.\n    \n    The relationship between ``x`` and ``y`` can be shown for different subsets\n    of the data using the ``hue``, ``size``, and ``style`` parameters. These\n    parameters control what visual semantics are used to identify the different\n    subsets. It is possible to show up to three dimensions independently by\n    using all three semantic types, but this style of plot can be hard to\n    interpret and is often ineffective. Using redundant semantics (i.e. both\n    ``hue`` and ``style`` for the same variable) can be helpful for making\n    graphics more accessible.\n    \n    See the :ref:`tutorial &lt;relational_tutorial&gt;` for more information.\n    \n    The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n    semantic, if present, depends on whether the variable is inferred to\n    represent &quot;numeric&quot; or &quot;categorical&quot; data. In particular, numeric variables\n    are represented with a sequential colormap by default, and the legend\n    entries show regular &quot;ticks&quot; with values that may or may not exist in the\n    data. This behavior can be controlled through various parameters, as\n    described and illustrated below.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    hue : vector or key in ``data``\n        Grouping variable that will produce points with different colors.\n        Can be either categorical or numeric, although color mapping will\n        behave differently in latter case.\n    size : vector or key in ``data``\n        Grouping variable that will produce points with different sizes.\n        Can be either categorical or numeric, although size mapping will\n        behave differently in latter case.\n    style : vector or key in ``data``\n        Grouping variable that will produce points with different markers.\n        Can have a numeric dtype but will always be treated as categorical.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    sizes : list, dict, or tuple\n        An object that determines how sizes are chosen when ``size`` is used.\n        It can always be a list of size values or a dict mapping levels of the\n        ``size`` variable to sizes. When ``size``  is numeric, it can also be\n        a tuple specifying the minimum and maximum size to use such that other\n        values are normalized within this range.\n    size_order : list\n        Specified order for appearance of the ``size`` variable levels,\n        otherwise they are determined from the data. Not relevant when the\n        ``size`` variable is numeric.\n    size_norm : tuple or Normalize object\n        Normalization in data units for scaling plot objects when the\n        ``size`` variable is numeric.\n    markers : boolean, list, or dictionary\n        Object determining how to draw the markers for different levels of the\n        ``style`` variable. Setting to ``True`` will use default markers, or\n        you can pass a list of markers or a dictionary mapping levels of the\n        ``style`` variable to markers. Setting to ``False`` will draw\n        marker-less lines.  Markers are specified as in matplotlib.\n    style_order : list\n        Specified order for appearance of the ``style`` variable levels\n        otherwise they are determined from the data. Not relevant when the\n        ``style`` variable is numeric.\n    &#123;x,y&#125;_bins : lists or arrays or functions\n        *Currently non-functional.*\n    units : vector or key in ``data``\n        Grouping variable identifying sampling units. When used, a separate\n        line will be drawn for each unit with appropriate semantics, but no\n        legend entry will be added. Useful for showing distribution of\n        experimental replicates when exact identities are not needed.\n        *Currently non-functional.*\n    estimator : name of pandas method or callable or None\n        Method for aggregating across multiple observations of the ``y``\n        variable at the same ``x`` level. If ``None``, all observations will\n        be drawn.\n        *Currently non-functional.*\n    ci : int or &quot;sd&quot; or None\n        Size of the confidence interval to draw when aggregating with an\n        estimator. &quot;sd&quot; means to draw the standard deviation of the data.\n        Setting to ``None`` will skip bootstrapping.\n        *Currently non-functional.*\n    n_boot : int\n        Number of bootstraps to use for computing the confidence interval.\n        *Currently non-functional.*\n    alpha : float\n        Proportional opacity of the points.\n    &#123;x,y&#125;_jitter : booleans or floats\n        *Currently non-functional.*\n    legend : &quot;auto&quot;, &quot;brief&quot;, &quot;full&quot;, or False\n        How to draw the legend. If &quot;brief&quot;, numeric ``hue`` and ``size``\n        variables will be represented with a sample of evenly spaced values.\n        If &quot;full&quot;, every group will get an entry in the legend. If &quot;auto&quot;,\n        choose between brief or full representation based on number of levels.\n        If ``False``, no legend data is added and no legend is drawn.\n    ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n        internally.\n    kwargs : key, value mappings\n        Other keyword arguments are passed down to\n        :meth:`matplotlib.axes.Axes.scatter`.\n    \n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n        The matplotlib axes containing the plot.\n    \n    See Also\n    --------\n    lineplot : Plot data using lines.\n    stripplot : Plot a categorical scatter with jitter.\n    swarmplot : Plot a categorical scatter with non-overlapping points.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/scatterplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）简单例子</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(&quot;tips&quot;)\n&quot;&quot;&quot;\n案例1：散点图\n&quot;&quot;&quot;\nsns.scatterplot( x=&quot;total_bill&quot;, y=&quot;tip&quot;,data=tips)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_29_0.png\" alt=\"png\"></p>\n<p><strong>分组</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(&quot;iris&quot;)\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=&quot;brief&quot;)\n#legend的取值为brief,full和False\n</code></pre>\n<p><img src=\"/img/output_31_0.png\" alt=\"png\"></p>\n<p><b>2、regplot函数</b></p>\n<p>regplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n#help(sns.regplot)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490&gt;\n</code></pre>\n<p><img src=\"/img/output_34_1.png\" alt=\"png\"></p>\n<p>如不显示拟合线，可将fit_reg设置为false</p>\n<p>regplot完整用法请参阅<br>\n<a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\">https://seaborn.pydata.org/generated/seaborn.regplot.html</a></p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520&gt;\n</code></pre>\n<p><img src=\"/img/output_36_1.png\" alt=\"png\"></p>\n<p><b>3、lmplot函数画散点图</b></p>\n<p>lmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=“species”。</p>\n<p>(1)implot 简单散点图</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_0.png\" alt=\"png\"></p>\n<p>（2）每组使用不同的标记（markers属性）</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, markers=[&quot;o&quot;, &quot;x&quot;, &quot;2&quot;])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0.png\" alt=\"png\"></p>\n<p>（3）使用不同的调色板(palette)</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=&quot;Set2&quot;)\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0.png\" alt=\"png\"></p>\n<p>（4）控制每组的颜色</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=&quot;#9b59b6&quot;, virginica=&quot;#3498db&quot;, versicolor=&quot;#95a5a6&quot;))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_46_0.png\" alt=\"png\"></p>\n<h1><center> 气泡图</center></h1>\n<p>散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。</p>\n<p>假设某个农产品的产量与温度和降雨量的关系如下表所示。<br></p>\n<pre><code class=\"language-python\">import pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf_product\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>作出产量与温度的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_52_0.png\" alt=\"png\"></p>\n<p>若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/><br>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\nrainfall=df_product[u&quot;降雨量&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_54_0.png\" alt=\"png\"></p>\n<h3 id=\"课堂练习：使用seaborn画气泡图\">课堂练习：使用seaborn画气泡图</h3>\n<p>自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1><center> 散点图</center></h1>\n<h1>一、什么是散点图</h1>\n<p>散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。</p>\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n<h1>二、散点图的好处</h1>\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n<h2 id=\"三、matplotlib的散点图\">三、matplotlib的散点图</h2>\n<p><b>1、plot画散点图</b></p>\n<p>在折线图的基础上设置linestyle=‘none’</p>\n<pre><code class=\"language-python\"> # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame(&#123;'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) &#125;)\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_8_0.png\" alt=\"png\"></p>\n<p><b>2,使用scatter画散点图</b></p>\n<p>简单的一个点的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n</code></pre>\n<p><img src=\"/img/output_11_0.png\" alt=\"png\"></p>\n<p>添加标题，给轴加上标签,并确保所有文本都大到能够看清：</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24)\n\nplt.scatter(2, 4, s=200,color=&quot;r&quot;) \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\nplt.xlabel(&quot;x value&quot;)\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_14_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_15_0.png\" alt=\"png\"></p>\n<p>绘制一系列点,x_values,y_values为list</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_17_0.png\" alt=\"png\"></p>\n<p>自动生成x,y值</p>\n<pre><code class=\"language-python\">x_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_19_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">plt.scatter(x_values, y_values, edgecolor='none', s=40) \n</code></pre>\n<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b4aa653280&gt;\n</code></pre>\n<p><img src=\"/img/output_20_1.png\" alt=\"png\"></p>\n<p><strong>保存到文件</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(&quot;Square Numbers&quot;, fontsize=24) \nplt.xlabel(&quot;Value&quot;, fontsize=14) \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n</code></pre>\n<p><img src=\"/img/output_22_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nhelp(mpl.markers)\n</code></pre>\n<pre><code>Help on module matplotlib.markers in matplotlib:\n\nNAME\n    matplotlib.markers\n\nDESCRIPTION\n    This module contains functions to handle markers.  Used by both the\n    marker functionality of `~matplotlib.axes.Axes.plot` and\n    `~matplotlib.axes.Axes.scatter`.\n    \n    All possible markers are defined here:\n    \n    ============================== ====== =========================================\n    marker                         symbol description\n    ============================== ====== =========================================\n    ``&quot;.&quot;``                        |m00|  point\n    ``&quot;,&quot;``                        |m01|  pixel\n    ``&quot;o&quot;``                        |m02|  circle\n    ``&quot;v&quot;``                        |m03|  triangle_down\n    ``&quot;^&quot;``                        |m04|  triangle_up\n    ``&quot;&lt;&quot;``                        |m05|  triangle_left\n    ``&quot;&gt;&quot;``                        |m06|  triangle_right\n    ``&quot;1&quot;``                        |m07|  tri_down\n    ``&quot;2&quot;``                        |m08|  tri_up\n    ``&quot;3&quot;``                        |m09|  tri_left\n    ``&quot;4&quot;``                        |m10|  tri_right\n    ``&quot;8&quot;``                        |m11|  octagon\n    ``&quot;s&quot;``                        |m12|  square\n    ``&quot;p&quot;``                        |m13|  pentagon\n    ``&quot;P&quot;``                        |m23|  plus (filled)\n    ``&quot;*&quot;``                        |m14|  star\n    ``&quot;h&quot;``                        |m15|  hexagon1\n    ``&quot;H&quot;``                        |m16|  hexagon2\n    ``&quot;+&quot;``                        |m17|  plus\n    ``&quot;x&quot;``                        |m18|  x\n    ``&quot;X&quot;``                        |m24|  x (filled)\n    ``&quot;D&quot;``                        |m19|  diamond\n    ``&quot;d&quot;``                        |m20|  thin_diamond\n    ``&quot;|&quot;``                        |m21|  vline\n    ``&quot;_&quot;``                        |m22|  hline\n    ``0`` (``TICKLEFT``)           |m25|  tickleft\n    ``1`` (``TICKRIGHT``)          |m26|  tickright\n    ``2`` (``TICKUP``)             |m27|  tickup\n    ``3`` (``TICKDOWN``)           |m28|  tickdown\n    ``4`` (``CARETLEFT``)          |m29|  caretleft\n    ``5`` (``CARETRIGHT``)         |m30|  caretright\n    ``6`` (``CARETUP``)            |m31|  caretup\n    ``7`` (``CARETDOWN``)          |m32|  caretdown\n    ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n    ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n    ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n    ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n    ``&quot;None&quot;``, ``&quot; &quot;`` or  ``&quot;&quot;``        nothing\n    ``'$...$'``                    |m37|  Render the string using mathtext.\n                                          E.g ``&quot;$f$&quot;`` for marker showing the\n                                          letter ``f``.\n    ``verts``                             A list of (x, y) pairs used for Path\n                                          vertices. The center of the marker is\n                                          located at (0, 0) and the size is\n                                          normalized, such that the created path\n                                          is encapsulated inside the unit cell.\n    path                                  A `~matplotlib.path.Path` instance.\n    ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                          rotated by ``angle``.\n    ============================== ====== =========================================\n    \n    ``None`` is the default which means 'nothing', however this table is\n    referred to from other docs for the valid inputs from marker inputs and in\n    those cases ``None`` still means 'default'.\n    \n    Note that special symbols can be defined via the\n    :doc:`STIX math font &lt;/tutorials/text/mathtext&gt;`,\n    e.g. ``&quot;$\\u266B$&quot;``. For an overview over the STIX font symbols refer to the\n    `STIX font table &lt;http://www.stixfonts.org/allGlyphs.html&gt;`_.\n    Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n    \n    Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n    equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n    Hence the following are equivalent::\n    \n        plt.plot([1, 2, 3], marker=11)\n        plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n    \n    Examples showing the use of markers:\n    \n    * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n    * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n    * :doc:`/gallery/shapes_and_collections/marker_path`\n</code></pre>\n<p>​</p>\n<pre><code>    .. |m00| image:: /_static/markers/m00.png\n    .. |m01| image:: /_static/markers/m01.png\n    .. |m02| image:: /_static/markers/m02.png\n    .. |m03| image:: /_static/markers/m03.png\n    .. |m04| image:: /_static/markers/m04.png\n    .. |m05| image:: /_static/markers/m05.png\n    .. |m06| image:: /_static/markers/m06.png\n    .. |m07| image:: /_static/markers/m07.png\n    .. |m08| image:: /_static/markers/m08.png\n    .. |m09| image:: /_static/markers/m09.png\n    .. |m10| image:: /_static/markers/m10.png\n    .. |m11| image:: /_static/markers/m11.png\n    .. |m12| image:: /_static/markers/m12.png\n    .. |m13| image:: /_static/markers/m13.png\n    .. |m14| image:: /_static/markers/m14.png\n    .. |m15| image:: /_static/markers/m15.png\n    .. |m16| image:: /_static/markers/m16.png\n    .. |m17| image:: /_static/markers/m17.png\n    .. |m18| image:: /_static/markers/m18.png\n    .. |m19| image:: /_static/markers/m19.png\n    .. |m20| image:: /_static/markers/m20.png\n    .. |m21| image:: /_static/markers/m21.png\n    .. |m22| image:: /_static/markers/m22.png\n    .. |m23| image:: /_static/markers/m23.png\n    .. |m24| image:: /_static/markers/m24.png\n    .. |m25| image:: /_static/markers/m25.png\n    .. |m26| image:: /_static/markers/m26.png\n    .. |m27| image:: /_static/markers/m27.png\n    .. |m28| image:: /_static/markers/m28.png\n    .. |m29| image:: /_static/markers/m29.png\n    .. |m30| image:: /_static/markers/m30.png\n    .. |m31| image:: /_static/markers/m31.png\n    .. |m32| image:: /_static/markers/m32.png\n    .. |m33| image:: /_static/markers/m33.png\n    .. |m34| image:: /_static/markers/m34.png\n    .. |m35| image:: /_static/markers/m35.png\n    .. |m36| image:: /_static/markers/m36.png\n    .. |m37| image:: /_static/markers/m37.png\n\nCLASSES\n    builtins.object\n        MarkerStyle\n    \n    class MarkerStyle(builtins.object)\n     |  MarkerStyle(marker=None, fillstyle=None)\n     |  \n     |  Methods defined here:\n     |  \n     |  __bool__(self)\n     |  \n     |  __init__(self, marker=None, fillstyle=None)\n     |      Attributes\n     |      ----------\n     |      markers : list of known marks\n     |      \n     |      fillstyles : list of known fillstyles\n     |      \n     |      filled_markers : list of known filled markers.\n     |      \n     |      Parameters\n     |      ----------\n     |      marker : str or array-like, optional, default: None\n     |          See the descriptions of possible markers in the module docstring.\n     |      \n     |      fillstyle : str, optional, default: 'full'\n     |          'full', 'left&quot;, 'right', 'bottom', 'top', 'none'\n     |  \n     |  get_alt_path(self)\n     |  \n     |  get_alt_transform(self)\n     |  \n     |  get_capstyle(self)\n     |  \n     |  get_fillstyle(self)\n     |  \n     |  get_joinstyle(self)\n     |  \n     |  get_marker(self)\n     |  \n     |  get_path(self)\n     |  \n     |  get_snap_threshold(self)\n     |  \n     |  get_transform(self)\n     |  \n     |  is_filled(self)\n     |  \n     |  set_fillstyle(self, fillstyle)\n     |      Sets fillstyle\n     |      \n     |      Parameters\n     |      ----------\n     |      fillstyle : string amongst known fillstyles\n     |  \n     |  set_marker(self, marker)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  filled_markers = ('o', 'v', '^', '&lt;', '&gt;', '8', 's', 'p', '*', 'h', 'H...\n     |  \n     |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n     |  \n     |  markers = &#123;'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n\nDATA\n    CARETDOWN = 7\n    CARETDOWNBASE = 11\n    CARETLEFT = 4\n    CARETLEFTBASE = 8\n    CARETRIGHT = 5\n    CARETRIGHTBASE = 9\n    CARETUP = 6\n    CARETUPBASE = 10\n    TICKDOWN = 3\n    TICKLEFT = 0\n    TICKRIGHT = 1\n    TICKUP = 2\n    rcParams = RcParams(&#123;'_internal.classic_mode': False,\n         ...nor.widt...\n\nFILE\n    c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n</code></pre>\n<p>​<br>\n​</p>\n<h2 id=\"四、seaborn画散点图\">四、seaborn画散点图</h2>\n<p><b>1、scatterplot函数</b></p>\n<p>(1)格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.scatterplot)\n\n</code></pre>\n<pre><code>Help on function scatterplot in module seaborn.relational:\n\nscatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n    Draw a scatter plot with possibility of several semantic groupings.\n    \n    The relationship between ``x`` and ``y`` can be shown for different subsets\n    of the data using the ``hue``, ``size``, and ``style`` parameters. These\n    parameters control what visual semantics are used to identify the different\n    subsets. It is possible to show up to three dimensions independently by\n    using all three semantic types, but this style of plot can be hard to\n    interpret and is often ineffective. Using redundant semantics (i.e. both\n    ``hue`` and ``style`` for the same variable) can be helpful for making\n    graphics more accessible.\n    \n    See the :ref:`tutorial &lt;relational_tutorial&gt;` for more information.\n    \n    The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n    semantic, if present, depends on whether the variable is inferred to\n    represent &quot;numeric&quot; or &quot;categorical&quot; data. In particular, numeric variables\n    are represented with a sequential colormap by default, and the legend\n    entries show regular &quot;ticks&quot; with values that may or may not exist in the\n    data. This behavior can be controlled through various parameters, as\n    described and illustrated below.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    hue : vector or key in ``data``\n        Grouping variable that will produce points with different colors.\n        Can be either categorical or numeric, although color mapping will\n        behave differently in latter case.\n    size : vector or key in ``data``\n        Grouping variable that will produce points with different sizes.\n        Can be either categorical or numeric, although size mapping will\n        behave differently in latter case.\n    style : vector or key in ``data``\n        Grouping variable that will produce points with different markers.\n        Can have a numeric dtype but will always be treated as categorical.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    sizes : list, dict, or tuple\n        An object that determines how sizes are chosen when ``size`` is used.\n        It can always be a list of size values or a dict mapping levels of the\n        ``size`` variable to sizes. When ``size``  is numeric, it can also be\n        a tuple specifying the minimum and maximum size to use such that other\n        values are normalized within this range.\n    size_order : list\n        Specified order for appearance of the ``size`` variable levels,\n        otherwise they are determined from the data. Not relevant when the\n        ``size`` variable is numeric.\n    size_norm : tuple or Normalize object\n        Normalization in data units for scaling plot objects when the\n        ``size`` variable is numeric.\n    markers : boolean, list, or dictionary\n        Object determining how to draw the markers for different levels of the\n        ``style`` variable. Setting to ``True`` will use default markers, or\n        you can pass a list of markers or a dictionary mapping levels of the\n        ``style`` variable to markers. Setting to ``False`` will draw\n        marker-less lines.  Markers are specified as in matplotlib.\n    style_order : list\n        Specified order for appearance of the ``style`` variable levels\n        otherwise they are determined from the data. Not relevant when the\n        ``style`` variable is numeric.\n    &#123;x,y&#125;_bins : lists or arrays or functions\n        *Currently non-functional.*\n    units : vector or key in ``data``\n        Grouping variable identifying sampling units. When used, a separate\n        line will be drawn for each unit with appropriate semantics, but no\n        legend entry will be added. Useful for showing distribution of\n        experimental replicates when exact identities are not needed.\n        *Currently non-functional.*\n    estimator : name of pandas method or callable or None\n        Method for aggregating across multiple observations of the ``y``\n        variable at the same ``x`` level. If ``None``, all observations will\n        be drawn.\n        *Currently non-functional.*\n    ci : int or &quot;sd&quot; or None\n        Size of the confidence interval to draw when aggregating with an\n        estimator. &quot;sd&quot; means to draw the standard deviation of the data.\n        Setting to ``None`` will skip bootstrapping.\n        *Currently non-functional.*\n    n_boot : int\n        Number of bootstraps to use for computing the confidence interval.\n        *Currently non-functional.*\n    alpha : float\n        Proportional opacity of the points.\n    &#123;x,y&#125;_jitter : booleans or floats\n        *Currently non-functional.*\n    legend : &quot;auto&quot;, &quot;brief&quot;, &quot;full&quot;, or False\n        How to draw the legend. If &quot;brief&quot;, numeric ``hue`` and ``size``\n        variables will be represented with a sample of evenly spaced values.\n        If &quot;full&quot;, every group will get an entry in the legend. If &quot;auto&quot;,\n        choose between brief or full representation based on number of levels.\n        If ``False``, no legend data is added and no legend is drawn.\n    ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n        internally.\n    kwargs : key, value mappings\n        Other keyword arguments are passed down to\n        :meth:`matplotlib.axes.Axes.scatter`.\n    \n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n        The matplotlib axes containing the plot.\n    \n    See Also\n    --------\n    lineplot : Plot data using lines.\n    stripplot : Plot a categorical scatter with jitter.\n    swarmplot : Plot a categorical scatter with non-overlapping points.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/scatterplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）简单例子</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(&quot;tips&quot;)\n&quot;&quot;&quot;\n案例1：散点图\n&quot;&quot;&quot;\nsns.scatterplot( x=&quot;total_bill&quot;, y=&quot;tip&quot;,data=tips)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_29_0.png\" alt=\"png\"></p>\n<p><strong>分组</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(&quot;iris&quot;)\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=&quot;brief&quot;)\n#legend的取值为brief,full和False\n</code></pre>\n<p><img src=\"/img/output_31_0.png\" alt=\"png\"></p>\n<p><b>2、regplot函数</b></p>\n<p>regplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n#help(sns.regplot)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490&gt;\n</code></pre>\n<p><img src=\"/img/output_34_1.png\" alt=\"png\"></p>\n<p>如不显示拟合线，可将fit_reg设置为false</p>\n<p>regplot完整用法请参阅<br>\n<a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\">https://seaborn.pydata.org/generated/seaborn.regplot.html</a></p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520&gt;\n</code></pre>\n<p><img src=\"/img/output_36_1.png\" alt=\"png\"></p>\n<p><b>3、lmplot函数画散点图</b></p>\n<p>lmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=“species”。</p>\n<p>(1)implot 简单散点图</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_0.png\" alt=\"png\"></p>\n<p>（2）每组使用不同的标记（markers属性）</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, markers=[&quot;o&quot;, &quot;x&quot;, &quot;2&quot;])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0.png\" alt=\"png\"></p>\n<p>（3）使用不同的调色板(palette)</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=&quot;Set2&quot;)\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0.png\" alt=\"png\"></p>\n<p>（4）控制每组的颜色</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=&quot;#9b59b6&quot;, virginica=&quot;#3498db&quot;, versicolor=&quot;#95a5a6&quot;))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_46_0.png\" alt=\"png\"></p>\n<h1><center> 气泡图</center></h1>\n<p>散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。</p>\n<p>假设某个农产品的产量与温度和降雨量的关系如下表所示。<br></p>\n<pre><code class=\"language-python\">import pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf_product\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>作出产量与温度的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_52_0.png\" alt=\"png\"></p>\n<p>若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/><br>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\nrainfall=df_product[u&quot;降雨量&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_54_0.png\" alt=\"png\"></p>\n<h3 id=\"课堂练习：使用seaborn画气泡图\">课堂练习：使用seaborn画气泡图</h3>\n<p>自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n"},{"title":"重构","author":"ztq","date":"2021-04-17T16:07:00.000Z","_content":"\n# 1、什么是重构\n\n​\t\t在百度百科里给出的定义是:在不改变软件系统外部行为的前提下，改善它的内部结构。通过调整程序代码改善软件的质量、性能，使其程序的设计模式和架构更趋合理，提高软件的扩展性和维护性。\n\n​\t\t也许有人会问，为什么不在项目开始时多花些时间把设计做好，而要以后花时间来重构呢？\n\n​\t\t首先要知道一个完美得可以预见未来任何变化的设计，或一个灵活得可以容纳任何扩展的设计是不存在的。系统设计人员对即将着手的项目往往只能从大方向予以把控，而无法知道每个细枝末节。\n\n​\t\t其次永远不变的就是变化，提出需求的用户往往要在软件成型后，才开始\"品头论足\"，系统设计人员毕竟不是先知先觉的神仙，功能的变化导致设计的调整再所难免。\n\n​\t\t所以\"测试为先，持续重构\"作为良好开发习惯被越来越多的人所采纳，测试和重构像黄河的护堤，成为保证软件质量的法宝\n\n# 2、软件质量因素的定义\n\n正确性（Correctness）：系统满足规格说明和用户目标的程度，即在预定环境下能正确地完成预期功能的程度\n\n健壮性（Robustness）：在硬件发生故障、输入的数据无效或操作错误等意外环境下，系统能做出适当响应的程度\n\n效率（Efficiency）：为了完成预定的功能，系统需要的计算资源的多少\n\n完整性（Efficiency）或安全性（Security）：对未经授权的人使用软件或数据的企图，系统能够控制（禁止）的程度\n\n可用性（Usability）：系统在完成预定应该完成的功能时令人满意的程度\n\n风险（Risk）：按预定的成本和进度把系统开发出来，并且为用户所满意的概率\n\n可理解性（Comprehensibility）：理解和使用该系统的容易程度\n\n可维修性（Maintainability）：诊断和改正在运行现场发现的错误所需要的工作量的大小\n\n灵活性（Maintainability）或适应性（Adaptability）：修改或改进正在运行的系统需要的工作量的多少\n\n可再用性（Reusability）：在其他应用中该程序可以被再次使用的程度（或范围）\n\n可移植性（Portability）：把程序从一种硬件配置和（或）软件系统环境转移到另一种配置和环境时，需要的工作量多少。有一种定量度量的方法是：用原来程序设计和调试的成本除移植时需用的费用\n\n互运行性（Interoperability）：把该系统和另一个系统结合起来需要的工作量的多少\n\n重构的目的就是为了保证软件满足以上特性。\n\n# 3、重构的意义\n\n​\t\t在不改变系统功能的情况下，改变系统的实现方式。为什么要这么做？投入精力不用来满足客户关心的需求，而是仅仅改变了软件的实现方式，这是否是在浪费客户的投资呢？\n\n​\t\t重构的重要性要从软件的生命周期说起。软件不同与普通的产品，他是一种智力产品，没有具体的物理形态。一个软件不可能发生物理损耗，界面上的按钮永远不会因为按动次数太多而发生接触不良。那么为什么一个软件制造出来以后，却不能永远使用下去呢？\n\n​\t\t对软件的生命造成威胁的因素只有一个：需求的变更。一个软件总是为解决某种特定的需求而产生，时代在发展，客户的业务也在发生变化。有的需求相对稳定一些，有的需求变化的比较剧烈，还有的需求已经消失了，或者转化成了别的需求。在这种情况下，软件必须相应的改变，考虑到成本和时间等因素，当然不是所有的需求变化都要在软件系统中实现。但是总的说来，软件要适应需求的变化，以保持自己的生命力。\n\n​\t\t软件产品最初制造出来，是经过精心的设计，具有良好架构的。但是随着时间的发展、需求的变化，必须不断的修改原有的功能、追加新的功能，还免不了有一些缺陷需要修改。为了实现变更，不可避免的要违反最初的设计构架。经过一段时间以后，软件的架构就千疮百孔了。bug越来越多，越来越难维护，新的需求越来越难实现，软件的构架对新的需求渐渐的失去支持能力，而是成为一种制约。最后新需求的开发成本会超过开发一个新的软件的成本，这就是这个软件系统的生命走到尽头的时候。重构就能够最大限度的避免这样一种现象。系统发展到一定阶段后，使用重构的方式，不改变系统的外部功能，只对内部的结构进行重新的整理。通过重构，不断的调整系统的结构，使系统对于需求的变更始终具有较强的适应能力。\n\n# 4、重构实例演示\n\n​\t\t案例很简单，这是给一家出租店用的程序。计算每一位顾客的消费金额并打印详单。操作者告诉程序：顾客租了哪些影片？租期多长？程序会根据租赁时间和影片类型计算费用。影片分为三类：普通片、儿童片、新片。除了计算费用还有为顾客计算积分，积分会根据租片类型是否为新片而不同。\n\n为了实现这个功能。我们编写出了以下代码（三个类：Movie、Rental、Consumer）：\n\n![1618675966507](/img/1618675966507.png)\n\n![1618675987731](/img/1618675987731.png)\n\n![1618676025236](/img/1618676025236.png)\n\n存在的问题：\n\n（1）对于consumer里面的statement方法。\n\t\t这个方法做的事情太多，如果用户希望对系统做一点修改，首先他们希望以html格式 输出详单，这样可在网页上直接显示。这个变化的影响是：根本不可能在打印html报表的函数中复用目前statement的任何行为。唯一可以做的就是编写一个全新的htmlStatement 大量重复statement的行为。如果计费标准发生变化必须同时修改statement和htmlstatement， 不断的修改和不断的复制粘贴，在程序要保存很长时间时，造成潜在的威胁.\n\n（2）如果用户希望改变影片分类规则，但还未决定怎么改，他们设想几种方案。 这些方案都会影响消费和积分的计算方式。为了应付分类规则和计费规则的变化，程序不得不对statement做出修改，但是如果我们把statement内的代码，复制到htmlstatement函数中，就必须确保将来的任何修改在两个地方保持一致，随着各种规则变得愈来愈复杂，适当的修改点越来越难找，不犯错的机会也越来越少。\n\n（3）你的态度也行倾向于尽量少修改程序，不管怎么说。它运行的很好。你心里牢牢记着那句古老的工程谚语：如果它没坏。就不要动它 也行这个程序还没坏，但是它造成了伤害，它让你的生活比较难过，因为你发现很难完成客户所需的修改。你发现自己需要为程序添加一个特性。而代码结构使你无法方便的达到目的，那就先重构那个程序。 重构，真的是可以锻炼自己思维和代码的编写能力\n\n## Step1、重构第一步-可靠的测试\n\n01 进行重构时，我们需要依赖测试，让它告诉我们是否引入bug。好的测试是重构的根本\n\n02 重构的前提是要有一个可靠的测试，这个测试必须有自我检验能力\n\n## Step2、重构第二步-分解重组statement 找出代码的逻辑泥团并运用Extract Method\n\n这个函数太长了，代码块越小，代码的功能越好管理。代码的处理和移动也越轻松\n代码重构目标：希望将长长的函数切开，把较小的块移动到更合适的类中，最终能够降低代码重复和扩展\n将 switch这段逻辑泥团抽离为函数。\n在分析函数内的局部变量和参数，其中statement() while循环中有两个： thisAmount、each, thisAmount会被修改，each不会被修改。\n任何不会被修改的变量都可以被当成参数传入新的函数\n注意每次调整都要编译测试 \n重构技术就是以 微小 的步伐修改程序，如果你犯下错误，很容易发现它\n\n![1618676190337](/img/1618676190337.png)\n\n## Step3、重构第三步-更改amountFor中的变量名\n\n好的代码应该清楚表达出自己的功能，变量名称是代码清晰的关键。\n任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。\n代码应该表现自己的目的\n随着对程序的理解逐渐加深，我也就不断的把这些理解嵌入到代码中，这么一来才不会遗忘我曾经理解的东西\n\n![1618676232016](/img/1618676232016.png)\n\n## Step4、重构第四步-搬移金额计算代码\n\n观察amountFor时，发现这个函数没有使用来自Consumer类的信息，使用了来自Rental类的信息。\n 所以应该改这段代码搬移到Rental类。并且做相应的调整：更改方法名、参数等 更改调用处\n运用Replace Temp with Query把thisAmount除去\n\n![1618676280257](/img/1618676280257.png)\n\n第一步：先将计算金额代码搬移到Rental类中\n\n  Rental类中添加方法 getCharge\n\n第二步：针对搬移后的代码，调整Consumer类\n\n## Step5、重构第五步-运用Extract Method 参考抽取计算金额，来抽取积分\n\n![1618676358941](/img/1618676358941.png)\n\n第一步：将积分计算方法搬移到Rental类中\n\n第二步：2、更改consumer类中获取积分\n\n## Step6、重构第六步-去掉临时变量\n\n临时变量只在自己所属的函数中有效，所以它们会助长冗长而复杂的函数。\n运用Replace Temp with Query,并利用查询函数(query method)来取代totalAmount和frequentRentalPoints这两个临时变量。\n由于类中的任何函数都可以调用上述查询函数，所以它能够促成较干净的设计，而减少冗长复杂的函数。\n\n![1618676411155](/img/1618676411155.png)\n\n第一步：totalAmount和frequenRenterPoint两个临时变量\n\n第二步：使用查询函数来替代\n\n## Step7、重构第七步-运用多态取代与价格相关的条件逻辑\n\n对于switch语句，最好不要在另一个对象的属性基础上运用switch语句。如果不得不使用。也应该在对象自己的数据上使用而不是在别人的数据上使用。 这暗示getCharge应该移动到Movie中\n租期的长度来自Rental对象，计算费用的时候需要两项数据：租期长度和影片类型\n     为什么选择租期长度呢。因为本系统可能发生变化是加入新影片类型。这种变化带有不稳定性倾向\n    如果影片类型发生变化，我希望尽量控制它造成的影响。所以在Movie对象中计算费用\n\n![1618676461026](/img/1618676461026.png)\n\n## Step8、重构第八步-运用多态取代与价格相关的条件逻辑\n\n用多态替换Switch，如果创建三个子类继承Movie，调用方就必须创建具体的子类对象（违反依赖倒置原则）。\n 一个对象具有状态，并且不同状态下有不同的行为，引入State模式：\n创建接口Price作为Movie的属性，接口方法getCharge(int daysRented)，再创建三个实现类，把Switch分支的逻辑移至具体的实现类\n依赖倒置原则，调用方应该依赖抽象类或接口，不要依赖具体实现类\n\n创建price接口 将getPriceCode、getCharge getFrequentRenterPoint抽象出来\n\n![1618676506490](/img/1618676506490.png)\n\n![1618676526483](/img/1618676526483.png)\n\n创建子类继承Price 实现具体的实现\n\n# 5、总结\n\n1、每个方法只做一件事，每个方法抽象层级不能多于两层，根据这个原则抽取方法。\n\n2、根据类的职责和对象之间的依赖关系，把方法移至对应的类。\n\n3、应该调用对象的接口方法，不要直接操作对象的属性。\n\n4、尽量减少方法中的临时变量，简化逻辑，增加可读性。\n\n# 6、重构的时机\n\n## （1）、什么时候重构\n\n三次法则：事不过三，三则重构\n添加功能时重构（New Feature）\n  代码的设计无法帮助我轻松的添加我所需要的特性，如果用某种方式来设计，添加特性会简单的多。一旦完成重构，新特性的添加会更快速，更流畅\n修补错误时重构（Bug Fix）\n   调试过程中，运用重构，多半是为了让代码更具有可读性\n 复审代码时重构（Code Review)重构可以帮助我们复审代码\n\n## （2）、什么时候不重构\n\n既有代码太混乱，且不能正常工作，需要重写而不是重构。\n 项目接近最后期限时，应该避免重构。\n\n# 7、重构的手段\n\n## （1）、改善重复代码\n\n​\t\t重复的代码是坏味道中出现频率最高的情形非其莫属。如果在一个的以上地方看到相同的代码，那么就可以肯定：想办法将它们合而为一，代码会变得更好。最单纯的重复代码就是“同一个类的两个函数含有相同的表达式”，这时候可以采用抽取方法提炼出重复的代码，然后让这两个地点都调用被提炼出的那一段代码。\n\n​\t\t另一种常见情况就是“两个互为兄弟的子类内含相同的表达式”，这时候只需对两个类抽取方法，然后将提炼出的代码推入到超类中。如果代码之间只是类似而并非完全相同，那么就需要通过抽取方法将相似部分和差异部分分开，构成单独一个函数。如果有些函数以不同的算法做相同的事，可以使用比较清晰的一个替换掉其余的。\n\n## （2）、改善过长的函数、过大的类、 过长的参数列\n\n​\t\t程序员都喜欢简短的函数。拥有短函数的对象会活的比较好、比较长。不熟悉面向对象技术的人，常常觉得对象程序中只有无穷无尽的委托，根本没有进行任何计算。和此类程序共同生活数年后，你才会知道这些小小函数的价值。\n\n​\t\t应该积极地分解函数，将长长的函数变为多个短小的函数。一般会遵循这样的原则：每当感觉需要用注释来说明点什么的时候，就把需要说明的东西写进一个独立函数中，并以其用途命名。不要嫌麻烦。可以对一组甚至短短一行代码做这件事，哪怕替换后的函数调用动作比函数自身还长，只要函数名称能够解释其用途，也应毫不犹豫地那么做。关键不在于函数的长度，而在于函数“做什么”和“如何做”之间的语义距离。\n​\t\t如果想利用单个的类做太多的事情，其内往往会出现太多实例变量。一旦如此，重复的代码就接踵而来。可以将几个变量一起提炼至新类内。提炼时应该选择类内彼此相关的变量，将它们放在一起。通常如果类内的数个变量有着相同的前缀或字尾，这就意味有机会把它们提炼到某个组件内。和“太多实例变量”一样，类内如果有太多代码，也是代码重复、混乱并最终走向死亡的源头。最简单的方案是把多余的东西消弭于类内部。如果有五个“百行函数”，它们之中很多代码都相同，那么或许你可以把它们变成五个“十行函数”和十个提炼出的“双行函数”。\n\n​\t\t刚开始学编程的时候，或许都是“把函数所需的所有东西都以参数传递进去”。这样也是可以理解的，因为除此之外就只能选择全局数据，而全局数据是邪恶的东西。对象技术告诉我们，如果你手上没有所需的东西，总可以叫一个对象给你。有了对象，你就不必要把函数所需的所有东西都以参数传递给它，只需传给它足够的、让函数能从中获得自己的东西就行。太长的的参数列难以理解，太多参数会造成前后不一致、不易使用，而且一旦需要更多数据，就不得不修改它。如果将对象传递给函数，大多数修改都将没有必要，因为很可能只需增加一两条请求，就能得到更多的数据。\n\n## （3）、发散式变化\n\n​\t\t我们希望软件能够容易被修改——毕竟软件再怎么说本来就该是“软”的。一旦需要修改，我们希望能够跳到系统某一点，只在该处做修改。如果不能做到这点，你就会嗅出两种紧密相关的刺鼻味道中的一种。如果某个类经常因为不同的原因在不同的方向上发生变化，发散式变化就出现了。其主要指“一个类受多种变化的影响”。当你看着一个类说：“呃，如果新加入一个数据库，就必须修改这三个函数；如果新出现一种工具，就必须修改这四个函数。”那么此时也许将这个对象分成两个会更好，这样对每个对象就可以只因一种变化而需要修改因为不同的原因，在不同的方向上，修改同一个类。应该分解成更小的类，每个类只因一种原因而修改。多层结构系统，开发人员往往容易把全部逻辑都放在Service层，导致Service类非常庞大且不断被修改。\n\n## （4）、霾弹式修改\n\n​\t\t如果每遇到变化，都必须在许多不同的类内做出许多小修改，你所面临的坏味道就是霾弹式修改。其主要指“一种变化引发多个类相应修改”。如果需要修改的代码散布四周，不但很难找到它们，也很容易忘记某个重要的修改。这种情况可以把所有需要的代码放进同一个类。如果眼下没有合适的类可以安置这些代码，就创造一个。通常可以运用内联类把一系列相关行为放进同一个类。\n\n## （5）、 依恋情节\n\n​\t\t众所周知，对象技术的全部要点在于：其是一种“将数据和对数据的操作行为包装在一起”的技术。有一种经典的气味：函数对于某个类的兴趣高过对自己所处类的兴趣。在很多情况下，都能够看到：某个函数为了计算某个值，从另一个对象那儿调用几乎半打的取值函数。疗法也显而易见：把这个函数移至另一个地点，移到它该去的地方。‘有时候一个函数往往会用到几个类的功能，那么它究竟该被置于何处呢？处理原则通常为：判断哪个类拥有最多被此函数使用的数据，然后就把这个函数和那些数据摆在一起。\n\n## （6）、数据泥团\n\n​\t\t如果用比较形象的事物来形容数据项，我想“小孩子”是一个不错的选择，数据项就像小孩子，喜欢成群结队地呆在一块儿。常常可以在很多地方看到相同的三四项数据：两个类中相同的字段、许多函数签名中相同的参数。这些总是绑在一起出现的数据真应该拥有属于它们自己的对象。这种情况可以先找出这些数据以字段形式出现的地方，将它们提炼到一个独立对象中，然后将注意力转移到函数签名上，运用参数对象为它减肥。这样做的直接好处是可以将很多参数列缩短，简化函数调用。一个比较好的评判方法是：删掉众多数据中的一项。这么做其它数据有没有因而失去意义？如果它们不再有意义，这就是一个明确的信号：应该为它们产生一个新对象。\n\n## （7）、基本类型偏执\n\n​\t\t大多数编程环境都有两种数据：结构类型允许你将数据组织成有意义的形式；基本类型则是构成结构类型的积木块。但是请记住：结构总是会带来一定的额外开销。它们可能代表着数据库中的表，如果只为做一两件事而创建结构类型也可能显得很麻烦。 对象的一个极大价值在于：它们模糊甚至打破横亘于基本数据和体积较大的类之间的界限。如果你有一组应该总是被放在一起的字段，可以将其抽取为一个独立的类。如果你在参数列中看到基本型数据，可以引入参数对象进行处理。如果你发现自己正从数组中挑选数据，可以运用以对象取代数组进行处理。 由一个起始值和一个结束值组成的range类：如果你有大量的基本数据类型字段，就有可能将其中部分存在逻辑联系的字段组织起来，形成一个类。更进一步的是，将与这些数据有关联的方法也一并移入类中 如果你发现自己正从数组中挑选数据，可运用 以对象取代数组。\n\n## （8）、Switch惊悚现身\n\n​\t\t面向对象程序的一个较明显特征是：少用switch语句。从本质上说，switch语句的问题在于重复。你常会发现同样的switch语句散布于不同的地方。如果要为它添加一个新的case语句，就必须找到所用switch语句并修改它们。面向对象中的多态概念可为此带来优雅的解决办法。大多数时候，一看到switch语句，那就应该考虑以多态来替换它。switch语句常常根据类型码进行选择，你要的是“与该类型码相关的函数或类”，所以应该将switch语句提炼到一个独立函数中，再将它搬移到需要多态性的那个类里。\n\n## （9）、平行继承体系\n\n​\t\t平行继承体系其实是霾弹式修改的特殊情况。在这种情况下，每当为某个类增加一个子类，必须也为另一个类增加一个子类。如果发现某个继承体系的类名称前缀和另一个继承体系的类名称前缀完全相同，这种坏味道就会被嗅出。\n​\t\t消除这种重复性的一般策略是：让一个\n​\t\t继承体系的实例引用另一个继承体系的实例。\n\n ![1618677203153](/img/1618677203153.png)\n\n## （10）、冗赘类\n\n​\t\t你所创建的每一个类，都得有人去理解它、维护它，这些工作都是需要花钱的。如果一个类的所得并不值其身价，他就应该消失。项目中经常会出现这样的情况：某个类原本对得起自己的价值，但重构使它身形缩水，不再做那么多工作；或开发者事先规划了某些变化，并添加一个类来应付这些变化，但变化实际没有发生。不管是哪种原因，都应该让这个类庄严赴义吧。如果某些子类并没有做足够的工作，我们可以尝试“折叠继承体系”，将超类和子类合为一体，那样就会减少维护时间。对于那些几乎没用的组件，就应该将这个类的所有特性搬移到另一个类中，然后移除原类。\n\n## （11）、夸夸其谈未来性\n\n​\t\t我们经常会说：“我想总有一天需要做这事”，并因而企图以各样的钩子和特殊情况来处理一些非必要的事情。一旦这样，坏味道就浮现出来了。夸夸其谈未来的结果往往会造成系统更加难以理解和维护。如果所有的装置都被用到了，那就值得那么做；如果用不到，就不值得。用不上的装置只会阻挡你的路，给你添乱，那就搬开它吧。如果某个抽象类其实没有太大作用，可以将超类和子类合为一体。将不必要的委托转移到另一个类中，并消除原先的类。如果函数的某些参数未被用上，那么就将参数移走。如果函数名称带有多余的抽象意味，就应该对它重命名，让它现实一些。\n\n## （12）、令人迷惑的暂时字段\n\n​\t\t有时候你会发现：类中的某个实例变量仅为某种特定情况而设。这样的代码让人难以理解，因为你通常认为对象在所有时候都需要它的所有变量。当变量在未被使用的情况下去猜测其当初设置的目的，会让你发疯的。可以使用提炼新类为这个可怜的孤儿创造一个家，然后把所有和这个变量相关的代码都放进这个新家。也许还可以使用“将Null值替换为Null对象”在“变量不合法”的情况下创建一个Null对象，从而避免写出条件式代码。\n\n## （13）、 过度耦合的消息链\n\n​\t\t如果你看到用户向一个对象请求另一个对象，然后再向后者请求另一个对象，然后再请求另一个对象.....这就是消息链。这种方式意味着客户代码将与某些功能函数中的导航结构紧密耦合。一旦对象间的关系发生任何变化，客户端就不得不做出相应修改。 这时候我们可以隐藏“委托关系”，并在服务类上建立客户所需要的所有函数。你可以在消息链的不同位置进行这种重构手法。理论上是可以重构消息链上的任何一个对象，但是这样做往往会把一系列对象都变成“中间人”。通常更好的选择是：先观察消息链最终得到的对象是用来干什么的，再看看能否通过抽取方法把使用该对象的代码提炼到一个独立函数中，然后再将这个函数推入消息链。 \n\n```java\nString result=Class.getFile().getFileChannel().getFileSource().getFileName() \n\nString result=Class.getFile().getFileName(); \n```\n\n​\t\t常常是因为数据结构的层次很深，需要层层调用getter获取内层数据。 个人认为Message Chains如果频繁出现，考虑这个字段是否应该移到较外层的类，或者把调用链封装在较外层类的方法。\n\n## （14）、中间人\n\n​\t\t我们都知道对象的基本特征之一就是封装——对外部世界隐藏其内部细节。封装往往伴随着委托。比如你对Boss说是否有时间参加一个会议，他把这个消息“委托”给他的记事本，然后才能回答你。但是，你没有必要知道Boss到底使用传统记事本或电子记事本亦或秘书来记录自己的约会。人们可能会过度使用委托。你也许会看到某个类接口中有一半的函数都委托给其它类，这样就是过度委托。这时候就应该移除中间人，直接和真正的负责人打交道。如果这样“不干实事”的函数只有少数几个，可以将它们放进调用端。如果中间人还有其它行为，可以把它变成实责对象的子类，这样你既可以扩展原对象的行为，又不必负担那么多的委托动作。\n\n## （15）、狎昵关系\n\n​\t\t有时候你会看到两个类过于亲密，花费太多时间去探究彼此的private成分。如果这发生在两个“人”之间，我们无比做卫道士；但对于类，我们就希望它们严守清规。也许就像古代的恋人一样，过分狎昵的类必须拆散。可以通过“移动方法”和“移动字段”帮它们划清界限，从而减少狎昵行径。如果两个类实在是情投意合，可以把两者共同点提炼到一个安全地点，让它们坦荡地使用这个新类。或者通过隐藏“委托关系”让另一个类来为它们传递相思情。将双向关联改为单向关联提炼类，将两个类的共同点提炼到新类中，让它们共同使用新类","source":"_posts/重构.md","raw":"title: 重构\nauthor: ztq\ntags:\n\n  - 重构\ncategories:\n  - 设计模式\ndate: 2021-04-18 00:07:00\n\n---\n\n# 1、什么是重构\n\n​\t\t在百度百科里给出的定义是:在不改变软件系统外部行为的前提下，改善它的内部结构。通过调整程序代码改善软件的质量、性能，使其程序的设计模式和架构更趋合理，提高软件的扩展性和维护性。\n\n​\t\t也许有人会问，为什么不在项目开始时多花些时间把设计做好，而要以后花时间来重构呢？\n\n​\t\t首先要知道一个完美得可以预见未来任何变化的设计，或一个灵活得可以容纳任何扩展的设计是不存在的。系统设计人员对即将着手的项目往往只能从大方向予以把控，而无法知道每个细枝末节。\n\n​\t\t其次永远不变的就是变化，提出需求的用户往往要在软件成型后，才开始\"品头论足\"，系统设计人员毕竟不是先知先觉的神仙，功能的变化导致设计的调整再所难免。\n\n​\t\t所以\"测试为先，持续重构\"作为良好开发习惯被越来越多的人所采纳，测试和重构像黄河的护堤，成为保证软件质量的法宝\n\n# 2、软件质量因素的定义\n\n正确性（Correctness）：系统满足规格说明和用户目标的程度，即在预定环境下能正确地完成预期功能的程度\n\n健壮性（Robustness）：在硬件发生故障、输入的数据无效或操作错误等意外环境下，系统能做出适当响应的程度\n\n效率（Efficiency）：为了完成预定的功能，系统需要的计算资源的多少\n\n完整性（Efficiency）或安全性（Security）：对未经授权的人使用软件或数据的企图，系统能够控制（禁止）的程度\n\n可用性（Usability）：系统在完成预定应该完成的功能时令人满意的程度\n\n风险（Risk）：按预定的成本和进度把系统开发出来，并且为用户所满意的概率\n\n可理解性（Comprehensibility）：理解和使用该系统的容易程度\n\n可维修性（Maintainability）：诊断和改正在运行现场发现的错误所需要的工作量的大小\n\n灵活性（Maintainability）或适应性（Adaptability）：修改或改进正在运行的系统需要的工作量的多少\n\n可再用性（Reusability）：在其他应用中该程序可以被再次使用的程度（或范围）\n\n可移植性（Portability）：把程序从一种硬件配置和（或）软件系统环境转移到另一种配置和环境时，需要的工作量多少。有一种定量度量的方法是：用原来程序设计和调试的成本除移植时需用的费用\n\n互运行性（Interoperability）：把该系统和另一个系统结合起来需要的工作量的多少\n\n重构的目的就是为了保证软件满足以上特性。\n\n# 3、重构的意义\n\n​\t\t在不改变系统功能的情况下，改变系统的实现方式。为什么要这么做？投入精力不用来满足客户关心的需求，而是仅仅改变了软件的实现方式，这是否是在浪费客户的投资呢？\n\n​\t\t重构的重要性要从软件的生命周期说起。软件不同与普通的产品，他是一种智力产品，没有具体的物理形态。一个软件不可能发生物理损耗，界面上的按钮永远不会因为按动次数太多而发生接触不良。那么为什么一个软件制造出来以后，却不能永远使用下去呢？\n\n​\t\t对软件的生命造成威胁的因素只有一个：需求的变更。一个软件总是为解决某种特定的需求而产生，时代在发展，客户的业务也在发生变化。有的需求相对稳定一些，有的需求变化的比较剧烈，还有的需求已经消失了，或者转化成了别的需求。在这种情况下，软件必须相应的改变，考虑到成本和时间等因素，当然不是所有的需求变化都要在软件系统中实现。但是总的说来，软件要适应需求的变化，以保持自己的生命力。\n\n​\t\t软件产品最初制造出来，是经过精心的设计，具有良好架构的。但是随着时间的发展、需求的变化，必须不断的修改原有的功能、追加新的功能，还免不了有一些缺陷需要修改。为了实现变更，不可避免的要违反最初的设计构架。经过一段时间以后，软件的架构就千疮百孔了。bug越来越多，越来越难维护，新的需求越来越难实现，软件的构架对新的需求渐渐的失去支持能力，而是成为一种制约。最后新需求的开发成本会超过开发一个新的软件的成本，这就是这个软件系统的生命走到尽头的时候。重构就能够最大限度的避免这样一种现象。系统发展到一定阶段后，使用重构的方式，不改变系统的外部功能，只对内部的结构进行重新的整理。通过重构，不断的调整系统的结构，使系统对于需求的变更始终具有较强的适应能力。\n\n# 4、重构实例演示\n\n​\t\t案例很简单，这是给一家出租店用的程序。计算每一位顾客的消费金额并打印详单。操作者告诉程序：顾客租了哪些影片？租期多长？程序会根据租赁时间和影片类型计算费用。影片分为三类：普通片、儿童片、新片。除了计算费用还有为顾客计算积分，积分会根据租片类型是否为新片而不同。\n\n为了实现这个功能。我们编写出了以下代码（三个类：Movie、Rental、Consumer）：\n\n![1618675966507](/img/1618675966507.png)\n\n![1618675987731](/img/1618675987731.png)\n\n![1618676025236](/img/1618676025236.png)\n\n存在的问题：\n\n（1）对于consumer里面的statement方法。\n\t\t这个方法做的事情太多，如果用户希望对系统做一点修改，首先他们希望以html格式 输出详单，这样可在网页上直接显示。这个变化的影响是：根本不可能在打印html报表的函数中复用目前statement的任何行为。唯一可以做的就是编写一个全新的htmlStatement 大量重复statement的行为。如果计费标准发生变化必须同时修改statement和htmlstatement， 不断的修改和不断的复制粘贴，在程序要保存很长时间时，造成潜在的威胁.\n\n（2）如果用户希望改变影片分类规则，但还未决定怎么改，他们设想几种方案。 这些方案都会影响消费和积分的计算方式。为了应付分类规则和计费规则的变化，程序不得不对statement做出修改，但是如果我们把statement内的代码，复制到htmlstatement函数中，就必须确保将来的任何修改在两个地方保持一致，随着各种规则变得愈来愈复杂，适当的修改点越来越难找，不犯错的机会也越来越少。\n\n（3）你的态度也行倾向于尽量少修改程序，不管怎么说。它运行的很好。你心里牢牢记着那句古老的工程谚语：如果它没坏。就不要动它 也行这个程序还没坏，但是它造成了伤害，它让你的生活比较难过，因为你发现很难完成客户所需的修改。你发现自己需要为程序添加一个特性。而代码结构使你无法方便的达到目的，那就先重构那个程序。 重构，真的是可以锻炼自己思维和代码的编写能力\n\n## Step1、重构第一步-可靠的测试\n\n01 进行重构时，我们需要依赖测试，让它告诉我们是否引入bug。好的测试是重构的根本\n\n02 重构的前提是要有一个可靠的测试，这个测试必须有自我检验能力\n\n## Step2、重构第二步-分解重组statement 找出代码的逻辑泥团并运用Extract Method\n\n这个函数太长了，代码块越小，代码的功能越好管理。代码的处理和移动也越轻松\n代码重构目标：希望将长长的函数切开，把较小的块移动到更合适的类中，最终能够降低代码重复和扩展\n将 switch这段逻辑泥团抽离为函数。\n在分析函数内的局部变量和参数，其中statement() while循环中有两个： thisAmount、each, thisAmount会被修改，each不会被修改。\n任何不会被修改的变量都可以被当成参数传入新的函数\n注意每次调整都要编译测试 \n重构技术就是以 微小 的步伐修改程序，如果你犯下错误，很容易发现它\n\n![1618676190337](/img/1618676190337.png)\n\n## Step3、重构第三步-更改amountFor中的变量名\n\n好的代码应该清楚表达出自己的功能，变量名称是代码清晰的关键。\n任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。\n代码应该表现自己的目的\n随着对程序的理解逐渐加深，我也就不断的把这些理解嵌入到代码中，这么一来才不会遗忘我曾经理解的东西\n\n![1618676232016](/img/1618676232016.png)\n\n## Step4、重构第四步-搬移金额计算代码\n\n观察amountFor时，发现这个函数没有使用来自Consumer类的信息，使用了来自Rental类的信息。\n 所以应该改这段代码搬移到Rental类。并且做相应的调整：更改方法名、参数等 更改调用处\n运用Replace Temp with Query把thisAmount除去\n\n![1618676280257](/img/1618676280257.png)\n\n第一步：先将计算金额代码搬移到Rental类中\n\n  Rental类中添加方法 getCharge\n\n第二步：针对搬移后的代码，调整Consumer类\n\n## Step5、重构第五步-运用Extract Method 参考抽取计算金额，来抽取积分\n\n![1618676358941](/img/1618676358941.png)\n\n第一步：将积分计算方法搬移到Rental类中\n\n第二步：2、更改consumer类中获取积分\n\n## Step6、重构第六步-去掉临时变量\n\n临时变量只在自己所属的函数中有效，所以它们会助长冗长而复杂的函数。\n运用Replace Temp with Query,并利用查询函数(query method)来取代totalAmount和frequentRentalPoints这两个临时变量。\n由于类中的任何函数都可以调用上述查询函数，所以它能够促成较干净的设计，而减少冗长复杂的函数。\n\n![1618676411155](/img/1618676411155.png)\n\n第一步：totalAmount和frequenRenterPoint两个临时变量\n\n第二步：使用查询函数来替代\n\n## Step7、重构第七步-运用多态取代与价格相关的条件逻辑\n\n对于switch语句，最好不要在另一个对象的属性基础上运用switch语句。如果不得不使用。也应该在对象自己的数据上使用而不是在别人的数据上使用。 这暗示getCharge应该移动到Movie中\n租期的长度来自Rental对象，计算费用的时候需要两项数据：租期长度和影片类型\n     为什么选择租期长度呢。因为本系统可能发生变化是加入新影片类型。这种变化带有不稳定性倾向\n    如果影片类型发生变化，我希望尽量控制它造成的影响。所以在Movie对象中计算费用\n\n![1618676461026](/img/1618676461026.png)\n\n## Step8、重构第八步-运用多态取代与价格相关的条件逻辑\n\n用多态替换Switch，如果创建三个子类继承Movie，调用方就必须创建具体的子类对象（违反依赖倒置原则）。\n 一个对象具有状态，并且不同状态下有不同的行为，引入State模式：\n创建接口Price作为Movie的属性，接口方法getCharge(int daysRented)，再创建三个实现类，把Switch分支的逻辑移至具体的实现类\n依赖倒置原则，调用方应该依赖抽象类或接口，不要依赖具体实现类\n\n创建price接口 将getPriceCode、getCharge getFrequentRenterPoint抽象出来\n\n![1618676506490](/img/1618676506490.png)\n\n![1618676526483](/img/1618676526483.png)\n\n创建子类继承Price 实现具体的实现\n\n# 5、总结\n\n1、每个方法只做一件事，每个方法抽象层级不能多于两层，根据这个原则抽取方法。\n\n2、根据类的职责和对象之间的依赖关系，把方法移至对应的类。\n\n3、应该调用对象的接口方法，不要直接操作对象的属性。\n\n4、尽量减少方法中的临时变量，简化逻辑，增加可读性。\n\n# 6、重构的时机\n\n## （1）、什么时候重构\n\n三次法则：事不过三，三则重构\n添加功能时重构（New Feature）\n  代码的设计无法帮助我轻松的添加我所需要的特性，如果用某种方式来设计，添加特性会简单的多。一旦完成重构，新特性的添加会更快速，更流畅\n修补错误时重构（Bug Fix）\n   调试过程中，运用重构，多半是为了让代码更具有可读性\n 复审代码时重构（Code Review)重构可以帮助我们复审代码\n\n## （2）、什么时候不重构\n\n既有代码太混乱，且不能正常工作，需要重写而不是重构。\n 项目接近最后期限时，应该避免重构。\n\n# 7、重构的手段\n\n## （1）、改善重复代码\n\n​\t\t重复的代码是坏味道中出现频率最高的情形非其莫属。如果在一个的以上地方看到相同的代码，那么就可以肯定：想办法将它们合而为一，代码会变得更好。最单纯的重复代码就是“同一个类的两个函数含有相同的表达式”，这时候可以采用抽取方法提炼出重复的代码，然后让这两个地点都调用被提炼出的那一段代码。\n\n​\t\t另一种常见情况就是“两个互为兄弟的子类内含相同的表达式”，这时候只需对两个类抽取方法，然后将提炼出的代码推入到超类中。如果代码之间只是类似而并非完全相同，那么就需要通过抽取方法将相似部分和差异部分分开，构成单独一个函数。如果有些函数以不同的算法做相同的事，可以使用比较清晰的一个替换掉其余的。\n\n## （2）、改善过长的函数、过大的类、 过长的参数列\n\n​\t\t程序员都喜欢简短的函数。拥有短函数的对象会活的比较好、比较长。不熟悉面向对象技术的人，常常觉得对象程序中只有无穷无尽的委托，根本没有进行任何计算。和此类程序共同生活数年后，你才会知道这些小小函数的价值。\n\n​\t\t应该积极地分解函数，将长长的函数变为多个短小的函数。一般会遵循这样的原则：每当感觉需要用注释来说明点什么的时候，就把需要说明的东西写进一个独立函数中，并以其用途命名。不要嫌麻烦。可以对一组甚至短短一行代码做这件事，哪怕替换后的函数调用动作比函数自身还长，只要函数名称能够解释其用途，也应毫不犹豫地那么做。关键不在于函数的长度，而在于函数“做什么”和“如何做”之间的语义距离。\n​\t\t如果想利用单个的类做太多的事情，其内往往会出现太多实例变量。一旦如此，重复的代码就接踵而来。可以将几个变量一起提炼至新类内。提炼时应该选择类内彼此相关的变量，将它们放在一起。通常如果类内的数个变量有着相同的前缀或字尾，这就意味有机会把它们提炼到某个组件内。和“太多实例变量”一样，类内如果有太多代码，也是代码重复、混乱并最终走向死亡的源头。最简单的方案是把多余的东西消弭于类内部。如果有五个“百行函数”，它们之中很多代码都相同，那么或许你可以把它们变成五个“十行函数”和十个提炼出的“双行函数”。\n\n​\t\t刚开始学编程的时候，或许都是“把函数所需的所有东西都以参数传递进去”。这样也是可以理解的，因为除此之外就只能选择全局数据，而全局数据是邪恶的东西。对象技术告诉我们，如果你手上没有所需的东西，总可以叫一个对象给你。有了对象，你就不必要把函数所需的所有东西都以参数传递给它，只需传给它足够的、让函数能从中获得自己的东西就行。太长的的参数列难以理解，太多参数会造成前后不一致、不易使用，而且一旦需要更多数据，就不得不修改它。如果将对象传递给函数，大多数修改都将没有必要，因为很可能只需增加一两条请求，就能得到更多的数据。\n\n## （3）、发散式变化\n\n​\t\t我们希望软件能够容易被修改——毕竟软件再怎么说本来就该是“软”的。一旦需要修改，我们希望能够跳到系统某一点，只在该处做修改。如果不能做到这点，你就会嗅出两种紧密相关的刺鼻味道中的一种。如果某个类经常因为不同的原因在不同的方向上发生变化，发散式变化就出现了。其主要指“一个类受多种变化的影响”。当你看着一个类说：“呃，如果新加入一个数据库，就必须修改这三个函数；如果新出现一种工具，就必须修改这四个函数。”那么此时也许将这个对象分成两个会更好，这样对每个对象就可以只因一种变化而需要修改因为不同的原因，在不同的方向上，修改同一个类。应该分解成更小的类，每个类只因一种原因而修改。多层结构系统，开发人员往往容易把全部逻辑都放在Service层，导致Service类非常庞大且不断被修改。\n\n## （4）、霾弹式修改\n\n​\t\t如果每遇到变化，都必须在许多不同的类内做出许多小修改，你所面临的坏味道就是霾弹式修改。其主要指“一种变化引发多个类相应修改”。如果需要修改的代码散布四周，不但很难找到它们，也很容易忘记某个重要的修改。这种情况可以把所有需要的代码放进同一个类。如果眼下没有合适的类可以安置这些代码，就创造一个。通常可以运用内联类把一系列相关行为放进同一个类。\n\n## （5）、 依恋情节\n\n​\t\t众所周知，对象技术的全部要点在于：其是一种“将数据和对数据的操作行为包装在一起”的技术。有一种经典的气味：函数对于某个类的兴趣高过对自己所处类的兴趣。在很多情况下，都能够看到：某个函数为了计算某个值，从另一个对象那儿调用几乎半打的取值函数。疗法也显而易见：把这个函数移至另一个地点，移到它该去的地方。‘有时候一个函数往往会用到几个类的功能，那么它究竟该被置于何处呢？处理原则通常为：判断哪个类拥有最多被此函数使用的数据，然后就把这个函数和那些数据摆在一起。\n\n## （6）、数据泥团\n\n​\t\t如果用比较形象的事物来形容数据项，我想“小孩子”是一个不错的选择，数据项就像小孩子，喜欢成群结队地呆在一块儿。常常可以在很多地方看到相同的三四项数据：两个类中相同的字段、许多函数签名中相同的参数。这些总是绑在一起出现的数据真应该拥有属于它们自己的对象。这种情况可以先找出这些数据以字段形式出现的地方，将它们提炼到一个独立对象中，然后将注意力转移到函数签名上，运用参数对象为它减肥。这样做的直接好处是可以将很多参数列缩短，简化函数调用。一个比较好的评判方法是：删掉众多数据中的一项。这么做其它数据有没有因而失去意义？如果它们不再有意义，这就是一个明确的信号：应该为它们产生一个新对象。\n\n## （7）、基本类型偏执\n\n​\t\t大多数编程环境都有两种数据：结构类型允许你将数据组织成有意义的形式；基本类型则是构成结构类型的积木块。但是请记住：结构总是会带来一定的额外开销。它们可能代表着数据库中的表，如果只为做一两件事而创建结构类型也可能显得很麻烦。 对象的一个极大价值在于：它们模糊甚至打破横亘于基本数据和体积较大的类之间的界限。如果你有一组应该总是被放在一起的字段，可以将其抽取为一个独立的类。如果你在参数列中看到基本型数据，可以引入参数对象进行处理。如果你发现自己正从数组中挑选数据，可以运用以对象取代数组进行处理。 由一个起始值和一个结束值组成的range类：如果你有大量的基本数据类型字段，就有可能将其中部分存在逻辑联系的字段组织起来，形成一个类。更进一步的是，将与这些数据有关联的方法也一并移入类中 如果你发现自己正从数组中挑选数据，可运用 以对象取代数组。\n\n## （8）、Switch惊悚现身\n\n​\t\t面向对象程序的一个较明显特征是：少用switch语句。从本质上说，switch语句的问题在于重复。你常会发现同样的switch语句散布于不同的地方。如果要为它添加一个新的case语句，就必须找到所用switch语句并修改它们。面向对象中的多态概念可为此带来优雅的解决办法。大多数时候，一看到switch语句，那就应该考虑以多态来替换它。switch语句常常根据类型码进行选择，你要的是“与该类型码相关的函数或类”，所以应该将switch语句提炼到一个独立函数中，再将它搬移到需要多态性的那个类里。\n\n## （9）、平行继承体系\n\n​\t\t平行继承体系其实是霾弹式修改的特殊情况。在这种情况下，每当为某个类增加一个子类，必须也为另一个类增加一个子类。如果发现某个继承体系的类名称前缀和另一个继承体系的类名称前缀完全相同，这种坏味道就会被嗅出。\n​\t\t消除这种重复性的一般策略是：让一个\n​\t\t继承体系的实例引用另一个继承体系的实例。\n\n ![1618677203153](/img/1618677203153.png)\n\n## （10）、冗赘类\n\n​\t\t你所创建的每一个类，都得有人去理解它、维护它，这些工作都是需要花钱的。如果一个类的所得并不值其身价，他就应该消失。项目中经常会出现这样的情况：某个类原本对得起自己的价值，但重构使它身形缩水，不再做那么多工作；或开发者事先规划了某些变化，并添加一个类来应付这些变化，但变化实际没有发生。不管是哪种原因，都应该让这个类庄严赴义吧。如果某些子类并没有做足够的工作，我们可以尝试“折叠继承体系”，将超类和子类合为一体，那样就会减少维护时间。对于那些几乎没用的组件，就应该将这个类的所有特性搬移到另一个类中，然后移除原类。\n\n## （11）、夸夸其谈未来性\n\n​\t\t我们经常会说：“我想总有一天需要做这事”，并因而企图以各样的钩子和特殊情况来处理一些非必要的事情。一旦这样，坏味道就浮现出来了。夸夸其谈未来的结果往往会造成系统更加难以理解和维护。如果所有的装置都被用到了，那就值得那么做；如果用不到，就不值得。用不上的装置只会阻挡你的路，给你添乱，那就搬开它吧。如果某个抽象类其实没有太大作用，可以将超类和子类合为一体。将不必要的委托转移到另一个类中，并消除原先的类。如果函数的某些参数未被用上，那么就将参数移走。如果函数名称带有多余的抽象意味，就应该对它重命名，让它现实一些。\n\n## （12）、令人迷惑的暂时字段\n\n​\t\t有时候你会发现：类中的某个实例变量仅为某种特定情况而设。这样的代码让人难以理解，因为你通常认为对象在所有时候都需要它的所有变量。当变量在未被使用的情况下去猜测其当初设置的目的，会让你发疯的。可以使用提炼新类为这个可怜的孤儿创造一个家，然后把所有和这个变量相关的代码都放进这个新家。也许还可以使用“将Null值替换为Null对象”在“变量不合法”的情况下创建一个Null对象，从而避免写出条件式代码。\n\n## （13）、 过度耦合的消息链\n\n​\t\t如果你看到用户向一个对象请求另一个对象，然后再向后者请求另一个对象，然后再请求另一个对象.....这就是消息链。这种方式意味着客户代码将与某些功能函数中的导航结构紧密耦合。一旦对象间的关系发生任何变化，客户端就不得不做出相应修改。 这时候我们可以隐藏“委托关系”，并在服务类上建立客户所需要的所有函数。你可以在消息链的不同位置进行这种重构手法。理论上是可以重构消息链上的任何一个对象，但是这样做往往会把一系列对象都变成“中间人”。通常更好的选择是：先观察消息链最终得到的对象是用来干什么的，再看看能否通过抽取方法把使用该对象的代码提炼到一个独立函数中，然后再将这个函数推入消息链。 \n\n```java\nString result=Class.getFile().getFileChannel().getFileSource().getFileName() \n\nString result=Class.getFile().getFileName(); \n```\n\n​\t\t常常是因为数据结构的层次很深，需要层层调用getter获取内层数据。 个人认为Message Chains如果频繁出现，考虑这个字段是否应该移到较外层的类，或者把调用链封装在较外层类的方法。\n\n## （14）、中间人\n\n​\t\t我们都知道对象的基本特征之一就是封装——对外部世界隐藏其内部细节。封装往往伴随着委托。比如你对Boss说是否有时间参加一个会议，他把这个消息“委托”给他的记事本，然后才能回答你。但是，你没有必要知道Boss到底使用传统记事本或电子记事本亦或秘书来记录自己的约会。人们可能会过度使用委托。你也许会看到某个类接口中有一半的函数都委托给其它类，这样就是过度委托。这时候就应该移除中间人，直接和真正的负责人打交道。如果这样“不干实事”的函数只有少数几个，可以将它们放进调用端。如果中间人还有其它行为，可以把它变成实责对象的子类，这样你既可以扩展原对象的行为，又不必负担那么多的委托动作。\n\n## （15）、狎昵关系\n\n​\t\t有时候你会看到两个类过于亲密，花费太多时间去探究彼此的private成分。如果这发生在两个“人”之间，我们无比做卫道士；但对于类，我们就希望它们严守清规。也许就像古代的恋人一样，过分狎昵的类必须拆散。可以通过“移动方法”和“移动字段”帮它们划清界限，从而减少狎昵行径。如果两个类实在是情投意合，可以把两者共同点提炼到一个安全地点，让它们坦荡地使用这个新类。或者通过隐藏“委托关系”让另一个类来为它们传递相思情。将双向关联改为单向关联提炼类，将两个类的共同点提炼到新类中，让它们共同使用新类","slug":"重构","published":1,"updated":"2022-04-04T08:32:40.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl1vcno2700fi7kt99w391hw4","content":"<h1>1、什么是重构</h1>\n<p>​\t\t在百度百科里给出的定义是:在不改变软件系统外部行为的前提下，改善它的内部结构。通过调整程序代码改善软件的质量、性能，使其程序的设计模式和架构更趋合理，提高软件的扩展性和维护性。</p>\n<p>​\t\t也许有人会问，为什么不在项目开始时多花些时间把设计做好，而要以后花时间来重构呢？</p>\n<p>​\t\t首先要知道一个完美得可以预见未来任何变化的设计，或一个灵活得可以容纳任何扩展的设计是不存在的。系统设计人员对即将着手的项目往往只能从大方向予以把控，而无法知道每个细枝末节。</p>\n<p>​\t\t其次永远不变的就是变化，提出需求的用户往往要在软件成型后，才开始&quot;品头论足&quot;，系统设计人员毕竟不是先知先觉的神仙，功能的变化导致设计的调整再所难免。</p>\n<p>​\t\t所以&quot;测试为先，持续重构&quot;作为良好开发习惯被越来越多的人所采纳，测试和重构像黄河的护堤，成为保证软件质量的法宝</p>\n<h1>2、软件质量因素的定义</h1>\n<p>正确性（Correctness）：系统满足规格说明和用户目标的程度，即在预定环境下能正确地完成预期功能的程度</p>\n<p>健壮性（Robustness）：在硬件发生故障、输入的数据无效或操作错误等意外环境下，系统能做出适当响应的程度</p>\n<p>效率（Efficiency）：为了完成预定的功能，系统需要的计算资源的多少</p>\n<p>完整性（Efficiency）或安全性（Security）：对未经授权的人使用软件或数据的企图，系统能够控制（禁止）的程度</p>\n<p>可用性（Usability）：系统在完成预定应该完成的功能时令人满意的程度</p>\n<p>风险（Risk）：按预定的成本和进度把系统开发出来，并且为用户所满意的概率</p>\n<p>可理解性（Comprehensibility）：理解和使用该系统的容易程度</p>\n<p>可维修性（Maintainability）：诊断和改正在运行现场发现的错误所需要的工作量的大小</p>\n<p>灵活性（Maintainability）或适应性（Adaptability）：修改或改进正在运行的系统需要的工作量的多少</p>\n<p>可再用性（Reusability）：在其他应用中该程序可以被再次使用的程度（或范围）</p>\n<p>可移植性（Portability）：把程序从一种硬件配置和（或）软件系统环境转移到另一种配置和环境时，需要的工作量多少。有一种定量度量的方法是：用原来程序设计和调试的成本除移植时需用的费用</p>\n<p>互运行性（Interoperability）：把该系统和另一个系统结合起来需要的工作量的多少</p>\n<p>重构的目的就是为了保证软件满足以上特性。</p>\n<h1>3、重构的意义</h1>\n<p>​\t\t在不改变系统功能的情况下，改变系统的实现方式。为什么要这么做？投入精力不用来满足客户关心的需求，而是仅仅改变了软件的实现方式，这是否是在浪费客户的投资呢？</p>\n<p>​\t\t重构的重要性要从软件的生命周期说起。软件不同与普通的产品，他是一种智力产品，没有具体的物理形态。一个软件不可能发生物理损耗，界面上的按钮永远不会因为按动次数太多而发生接触不良。那么为什么一个软件制造出来以后，却不能永远使用下去呢？</p>\n<p>​\t\t对软件的生命造成威胁的因素只有一个：需求的变更。一个软件总是为解决某种特定的需求而产生，时代在发展，客户的业务也在发生变化。有的需求相对稳定一些，有的需求变化的比较剧烈，还有的需求已经消失了，或者转化成了别的需求。在这种情况下，软件必须相应的改变，考虑到成本和时间等因素，当然不是所有的需求变化都要在软件系统中实现。但是总的说来，软件要适应需求的变化，以保持自己的生命力。</p>\n<p>​\t\t软件产品最初制造出来，是经过精心的设计，具有良好架构的。但是随着时间的发展、需求的变化，必须不断的修改原有的功能、追加新的功能，还免不了有一些缺陷需要修改。为了实现变更，不可避免的要违反最初的设计构架。经过一段时间以后，软件的架构就千疮百孔了。bug越来越多，越来越难维护，新的需求越来越难实现，软件的构架对新的需求渐渐的失去支持能力，而是成为一种制约。最后新需求的开发成本会超过开发一个新的软件的成本，这就是这个软件系统的生命走到尽头的时候。重构就能够最大限度的避免这样一种现象。系统发展到一定阶段后，使用重构的方式，不改变系统的外部功能，只对内部的结构进行重新的整理。通过重构，不断的调整系统的结构，使系统对于需求的变更始终具有较强的适应能力。</p>\n<h1>4、重构实例演示</h1>\n<p>​\t\t案例很简单，这是给一家出租店用的程序。计算每一位顾客的消费金额并打印详单。操作者告诉程序：顾客租了哪些影片？租期多长？程序会根据租赁时间和影片类型计算费用。影片分为三类：普通片、儿童片、新片。除了计算费用还有为顾客计算积分，积分会根据租片类型是否为新片而不同。</p>\n<p>为了实现这个功能。我们编写出了以下代码（三个类：Movie、Rental、Consumer）：</p>\n<p><img src=\"/img/1618675966507.png\" alt=\"1618675966507\"></p>\n<p><img src=\"/img/1618675987731.png\" alt=\"1618675987731\"></p>\n<p><img src=\"/img/1618676025236.png\" alt=\"1618676025236\"></p>\n<p>存在的问题：</p>\n<p>（1）对于consumer里面的statement方法。<br>\n这个方法做的事情太多，如果用户希望对系统做一点修改，首先他们希望以html格式 输出详单，这样可在网页上直接显示。这个变化的影响是：根本不可能在打印html报表的函数中复用目前statement的任何行为。唯一可以做的就是编写一个全新的htmlStatement 大量重复statement的行为。如果计费标准发生变化必须同时修改statement和htmlstatement， 不断的修改和不断的复制粘贴，在程序要保存很长时间时，造成潜在的威胁.</p>\n<p>（2）如果用户希望改变影片分类规则，但还未决定怎么改，他们设想几种方案。 这些方案都会影响消费和积分的计算方式。为了应付分类规则和计费规则的变化，程序不得不对statement做出修改，但是如果我们把statement内的代码，复制到htmlstatement函数中，就必须确保将来的任何修改在两个地方保持一致，随着各种规则变得愈来愈复杂，适当的修改点越来越难找，不犯错的机会也越来越少。</p>\n<p>（3）你的态度也行倾向于尽量少修改程序，不管怎么说。它运行的很好。你心里牢牢记着那句古老的工程谚语：如果它没坏。就不要动它 也行这个程序还没坏，但是它造成了伤害，它让你的生活比较难过，因为你发现很难完成客户所需的修改。你发现自己需要为程序添加一个特性。而代码结构使你无法方便的达到目的，那就先重构那个程序。 重构，真的是可以锻炼自己思维和代码的编写能力</p>\n<h2 id=\"Step1、重构第一步-可靠的测试\">Step1、重构第一步-可靠的测试</h2>\n<p>01 进行重构时，我们需要依赖测试，让它告诉我们是否引入bug。好的测试是重构的根本</p>\n<p>02 重构的前提是要有一个可靠的测试，这个测试必须有自我检验能力</p>\n<h2 id=\"Step2、重构第二步-分解重组statement-找出代码的逻辑泥团并运用Extract-Method\">Step2、重构第二步-分解重组statement 找出代码的逻辑泥团并运用Extract Method</h2>\n<p>这个函数太长了，代码块越小，代码的功能越好管理。代码的处理和移动也越轻松<br>\n代码重构目标：希望将长长的函数切开，把较小的块移动到更合适的类中，最终能够降低代码重复和扩展<br>\n将 switch这段逻辑泥团抽离为函数。<br>\n在分析函数内的局部变量和参数，其中statement() while循环中有两个： thisAmount、each, thisAmount会被修改，each不会被修改。<br>\n任何不会被修改的变量都可以被当成参数传入新的函数<br>\n注意每次调整都要编译测试<br>\n重构技术就是以 微小 的步伐修改程序，如果你犯下错误，很容易发现它</p>\n<p><img src=\"/img/1618676190337.png\" alt=\"1618676190337\"></p>\n<h2 id=\"Step3、重构第三步-更改amountFor中的变量名\">Step3、重构第三步-更改amountFor中的变量名</h2>\n<p>好的代码应该清楚表达出自己的功能，变量名称是代码清晰的关键。<br>\n任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。<br>\n代码应该表现自己的目的<br>\n随着对程序的理解逐渐加深，我也就不断的把这些理解嵌入到代码中，这么一来才不会遗忘我曾经理解的东西</p>\n<p><img src=\"/img/1618676232016.png\" alt=\"1618676232016\"></p>\n<h2 id=\"Step4、重构第四步-搬移金额计算代码\">Step4、重构第四步-搬移金额计算代码</h2>\n<p>观察amountFor时，发现这个函数没有使用来自Consumer类的信息，使用了来自Rental类的信息。<br>\n所以应该改这段代码搬移到Rental类。并且做相应的调整：更改方法名、参数等 更改调用处<br>\n运用Replace Temp with Query把thisAmount除去</p>\n<p><img src=\"/img/1618676280257.png\" alt=\"1618676280257\"></p>\n<p>第一步：先将计算金额代码搬移到Rental类中</p>\n<p>Rental类中添加方法 getCharge</p>\n<p>第二步：针对搬移后的代码，调整Consumer类</p>\n<h2 id=\"Step5、重构第五步-运用Extract-Method-参考抽取计算金额，来抽取积分\">Step5、重构第五步-运用Extract Method 参考抽取计算金额，来抽取积分</h2>\n<p><img src=\"/img/1618676358941.png\" alt=\"1618676358941\"></p>\n<p>第一步：将积分计算方法搬移到Rental类中</p>\n<p>第二步：2、更改consumer类中获取积分</p>\n<h2 id=\"Step6、重构第六步-去掉临时变量\">Step6、重构第六步-去掉临时变量</h2>\n<p>临时变量只在自己所属的函数中有效，所以它们会助长冗长而复杂的函数。<br>\n运用Replace Temp with Query,并利用查询函数(query method)来取代totalAmount和frequentRentalPoints这两个临时变量。<br>\n由于类中的任何函数都可以调用上述查询函数，所以它能够促成较干净的设计，而减少冗长复杂的函数。</p>\n<p><img src=\"/img/1618676411155.png\" alt=\"1618676411155\"></p>\n<p>第一步：totalAmount和frequenRenterPoint两个临时变量</p>\n<p>第二步：使用查询函数来替代</p>\n<h2 id=\"Step7、重构第七步-运用多态取代与价格相关的条件逻辑\">Step7、重构第七步-运用多态取代与价格相关的条件逻辑</h2>\n<p>对于switch语句，最好不要在另一个对象的属性基础上运用switch语句。如果不得不使用。也应该在对象自己的数据上使用而不是在别人的数据上使用。 这暗示getCharge应该移动到Movie中<br>\n租期的长度来自Rental对象，计算费用的时候需要两项数据：租期长度和影片类型<br>\n为什么选择租期长度呢。因为本系统可能发生变化是加入新影片类型。这种变化带有不稳定性倾向<br>\n如果影片类型发生变化，我希望尽量控制它造成的影响。所以在Movie对象中计算费用</p>\n<p><img src=\"/img/1618676461026.png\" alt=\"1618676461026\"></p>\n<h2 id=\"Step8、重构第八步-运用多态取代与价格相关的条件逻辑\">Step8、重构第八步-运用多态取代与价格相关的条件逻辑</h2>\n<p>用多态替换Switch，如果创建三个子类继承Movie，调用方就必须创建具体的子类对象（违反依赖倒置原则）。<br>\n一个对象具有状态，并且不同状态下有不同的行为，引入State模式：<br>\n创建接口Price作为Movie的属性，接口方法getCharge(int daysRented)，再创建三个实现类，把Switch分支的逻辑移至具体的实现类<br>\n依赖倒置原则，调用方应该依赖抽象类或接口，不要依赖具体实现类</p>\n<p>创建price接口 将getPriceCode、getCharge getFrequentRenterPoint抽象出来</p>\n<p><img src=\"/img/1618676506490.png\" alt=\"1618676506490\"></p>\n<p><img src=\"/img/1618676526483.png\" alt=\"1618676526483\"></p>\n<p>创建子类继承Price 实现具体的实现</p>\n<h1>5、总结</h1>\n<p>1、每个方法只做一件事，每个方法抽象层级不能多于两层，根据这个原则抽取方法。</p>\n<p>2、根据类的职责和对象之间的依赖关系，把方法移至对应的类。</p>\n<p>3、应该调用对象的接口方法，不要直接操作对象的属性。</p>\n<p>4、尽量减少方法中的临时变量，简化逻辑，增加可读性。</p>\n<h1>6、重构的时机</h1>\n<h2 id=\"（1）、什么时候重构\">（1）、什么时候重构</h2>\n<p>三次法则：事不过三，三则重构<br>\n添加功能时重构（New Feature）<br>\n代码的设计无法帮助我轻松的添加我所需要的特性，如果用某种方式来设计，添加特性会简单的多。一旦完成重构，新特性的添加会更快速，更流畅<br>\n修补错误时重构（Bug Fix）<br>\n调试过程中，运用重构，多半是为了让代码更具有可读性<br>\n复审代码时重构（Code Review)重构可以帮助我们复审代码</p>\n<h2 id=\"（2）、什么时候不重构\">（2）、什么时候不重构</h2>\n<p>既有代码太混乱，且不能正常工作，需要重写而不是重构。<br>\n项目接近最后期限时，应该避免重构。</p>\n<h1>7、重构的手段</h1>\n<h2 id=\"（1）、改善重复代码\">（1）、改善重复代码</h2>\n<p>​\t\t重复的代码是坏味道中出现频率最高的情形非其莫属。如果在一个的以上地方看到相同的代码，那么就可以肯定：想办法将它们合而为一，代码会变得更好。最单纯的重复代码就是“同一个类的两个函数含有相同的表达式”，这时候可以采用抽取方法提炼出重复的代码，然后让这两个地点都调用被提炼出的那一段代码。</p>\n<p>​\t\t另一种常见情况就是“两个互为兄弟的子类内含相同的表达式”，这时候只需对两个类抽取方法，然后将提炼出的代码推入到超类中。如果代码之间只是类似而并非完全相同，那么就需要通过抽取方法将相似部分和差异部分分开，构成单独一个函数。如果有些函数以不同的算法做相同的事，可以使用比较清晰的一个替换掉其余的。</p>\n<h2 id=\"（2）、改善过长的函数、过大的类、-过长的参数列\">（2）、改善过长的函数、过大的类、 过长的参数列</h2>\n<p>​\t\t程序员都喜欢简短的函数。拥有短函数的对象会活的比较好、比较长。不熟悉面向对象技术的人，常常觉得对象程序中只有无穷无尽的委托，根本没有进行任何计算。和此类程序共同生活数年后，你才会知道这些小小函数的价值。</p>\n<p>​\t\t应该积极地分解函数，将长长的函数变为多个短小的函数。一般会遵循这样的原则：每当感觉需要用注释来说明点什么的时候，就把需要说明的东西写进一个独立函数中，并以其用途命名。不要嫌麻烦。可以对一组甚至短短一行代码做这件事，哪怕替换后的函数调用动作比函数自身还长，只要函数名称能够解释其用途，也应毫不犹豫地那么做。关键不在于函数的长度，而在于函数“做什么”和“如何做”之间的语义距离。<br>\n​\t\t如果想利用单个的类做太多的事情，其内往往会出现太多实例变量。一旦如此，重复的代码就接踵而来。可以将几个变量一起提炼至新类内。提炼时应该选择类内彼此相关的变量，将它们放在一起。通常如果类内的数个变量有着相同的前缀或字尾，这就意味有机会把它们提炼到某个组件内。和“太多实例变量”一样，类内如果有太多代码，也是代码重复、混乱并最终走向死亡的源头。最简单的方案是把多余的东西消弭于类内部。如果有五个“百行函数”，它们之中很多代码都相同，那么或许你可以把它们变成五个“十行函数”和十个提炼出的“双行函数”。</p>\n<p>​\t\t刚开始学编程的时候，或许都是“把函数所需的所有东西都以参数传递进去”。这样也是可以理解的，因为除此之外就只能选择全局数据，而全局数据是邪恶的东西。对象技术告诉我们，如果你手上没有所需的东西，总可以叫一个对象给你。有了对象，你就不必要把函数所需的所有东西都以参数传递给它，只需传给它足够的、让函数能从中获得自己的东西就行。太长的的参数列难以理解，太多参数会造成前后不一致、不易使用，而且一旦需要更多数据，就不得不修改它。如果将对象传递给函数，大多数修改都将没有必要，因为很可能只需增加一两条请求，就能得到更多的数据。</p>\n<h2 id=\"（3）、发散式变化\">（3）、发散式变化</h2>\n<p>​\t\t我们希望软件能够容易被修改——毕竟软件再怎么说本来就该是“软”的。一旦需要修改，我们希望能够跳到系统某一点，只在该处做修改。如果不能做到这点，你就会嗅出两种紧密相关的刺鼻味道中的一种。如果某个类经常因为不同的原因在不同的方向上发生变化，发散式变化就出现了。其主要指“一个类受多种变化的影响”。当你看着一个类说：“呃，如果新加入一个数据库，就必须修改这三个函数；如果新出现一种工具，就必须修改这四个函数。”那么此时也许将这个对象分成两个会更好，这样对每个对象就可以只因一种变化而需要修改因为不同的原因，在不同的方向上，修改同一个类。应该分解成更小的类，每个类只因一种原因而修改。多层结构系统，开发人员往往容易把全部逻辑都放在Service层，导致Service类非常庞大且不断被修改。</p>\n<h2 id=\"（4）、霾弹式修改\">（4）、霾弹式修改</h2>\n<p>​\t\t如果每遇到变化，都必须在许多不同的类内做出许多小修改，你所面临的坏味道就是霾弹式修改。其主要指“一种变化引发多个类相应修改”。如果需要修改的代码散布四周，不但很难找到它们，也很容易忘记某个重要的修改。这种情况可以把所有需要的代码放进同一个类。如果眼下没有合适的类可以安置这些代码，就创造一个。通常可以运用内联类把一系列相关行为放进同一个类。</p>\n<h2 id=\"（5）、-依恋情节\">（5）、 依恋情节</h2>\n<p>​\t\t众所周知，对象技术的全部要点在于：其是一种“将数据和对数据的操作行为包装在一起”的技术。有一种经典的气味：函数对于某个类的兴趣高过对自己所处类的兴趣。在很多情况下，都能够看到：某个函数为了计算某个值，从另一个对象那儿调用几乎半打的取值函数。疗法也显而易见：把这个函数移至另一个地点，移到它该去的地方。‘有时候一个函数往往会用到几个类的功能，那么它究竟该被置于何处呢？处理原则通常为：判断哪个类拥有最多被此函数使用的数据，然后就把这个函数和那些数据摆在一起。</p>\n<h2 id=\"（6）、数据泥团\">（6）、数据泥团</h2>\n<p>​\t\t如果用比较形象的事物来形容数据项，我想“小孩子”是一个不错的选择，数据项就像小孩子，喜欢成群结队地呆在一块儿。常常可以在很多地方看到相同的三四项数据：两个类中相同的字段、许多函数签名中相同的参数。这些总是绑在一起出现的数据真应该拥有属于它们自己的对象。这种情况可以先找出这些数据以字段形式出现的地方，将它们提炼到一个独立对象中，然后将注意力转移到函数签名上，运用参数对象为它减肥。这样做的直接好处是可以将很多参数列缩短，简化函数调用。一个比较好的评判方法是：删掉众多数据中的一项。这么做其它数据有没有因而失去意义？如果它们不再有意义，这就是一个明确的信号：应该为它们产生一个新对象。</p>\n<h2 id=\"（7）、基本类型偏执\">（7）、基本类型偏执</h2>\n<p>​\t\t大多数编程环境都有两种数据：结构类型允许你将数据组织成有意义的形式；基本类型则是构成结构类型的积木块。但是请记住：结构总是会带来一定的额外开销。它们可能代表着数据库中的表，如果只为做一两件事而创建结构类型也可能显得很麻烦。 对象的一个极大价值在于：它们模糊甚至打破横亘于基本数据和体积较大的类之间的界限。如果你有一组应该总是被放在一起的字段，可以将其抽取为一个独立的类。如果你在参数列中看到基本型数据，可以引入参数对象进行处理。如果你发现自己正从数组中挑选数据，可以运用以对象取代数组进行处理。 由一个起始值和一个结束值组成的range类：如果你有大量的基本数据类型字段，就有可能将其中部分存在逻辑联系的字段组织起来，形成一个类。更进一步的是，将与这些数据有关联的方法也一并移入类中 如果你发现自己正从数组中挑选数据，可运用 以对象取代数组。</p>\n<h2 id=\"（8）、Switch惊悚现身\">（8）、Switch惊悚现身</h2>\n<p>​\t\t面向对象程序的一个较明显特征是：少用switch语句。从本质上说，switch语句的问题在于重复。你常会发现同样的switch语句散布于不同的地方。如果要为它添加一个新的case语句，就必须找到所用switch语句并修改它们。面向对象中的多态概念可为此带来优雅的解决办法。大多数时候，一看到switch语句，那就应该考虑以多态来替换它。switch语句常常根据类型码进行选择，你要的是“与该类型码相关的函数或类”，所以应该将switch语句提炼到一个独立函数中，再将它搬移到需要多态性的那个类里。</p>\n<h2 id=\"（9）、平行继承体系\">（9）、平行继承体系</h2>\n<p>​\t\t平行继承体系其实是霾弹式修改的特殊情况。在这种情况下，每当为某个类增加一个子类，必须也为另一个类增加一个子类。如果发现某个继承体系的类名称前缀和另一个继承体系的类名称前缀完全相同，这种坏味道就会被嗅出。<br>\n​\t\t消除这种重复性的一般策略是：让一个<br>\n​\t\t继承体系的实例引用另一个继承体系的实例。</p>\n<p><img src=\"/img/1618677203153.png\" alt=\"1618677203153\"></p>\n<h2 id=\"（10）、冗赘类\">（10）、冗赘类</h2>\n<p>​\t\t你所创建的每一个类，都得有人去理解它、维护它，这些工作都是需要花钱的。如果一个类的所得并不值其身价，他就应该消失。项目中经常会出现这样的情况：某个类原本对得起自己的价值，但重构使它身形缩水，不再做那么多工作；或开发者事先规划了某些变化，并添加一个类来应付这些变化，但变化实际没有发生。不管是哪种原因，都应该让这个类庄严赴义吧。如果某些子类并没有做足够的工作，我们可以尝试“折叠继承体系”，将超类和子类合为一体，那样就会减少维护时间。对于那些几乎没用的组件，就应该将这个类的所有特性搬移到另一个类中，然后移除原类。</p>\n<h2 id=\"（11）、夸夸其谈未来性\">（11）、夸夸其谈未来性</h2>\n<p>​\t\t我们经常会说：“我想总有一天需要做这事”，并因而企图以各样的钩子和特殊情况来处理一些非必要的事情。一旦这样，坏味道就浮现出来了。夸夸其谈未来的结果往往会造成系统更加难以理解和维护。如果所有的装置都被用到了，那就值得那么做；如果用不到，就不值得。用不上的装置只会阻挡你的路，给你添乱，那就搬开它吧。如果某个抽象类其实没有太大作用，可以将超类和子类合为一体。将不必要的委托转移到另一个类中，并消除原先的类。如果函数的某些参数未被用上，那么就将参数移走。如果函数名称带有多余的抽象意味，就应该对它重命名，让它现实一些。</p>\n<h2 id=\"（12）、令人迷惑的暂时字段\">（12）、令人迷惑的暂时字段</h2>\n<p>​\t\t有时候你会发现：类中的某个实例变量仅为某种特定情况而设。这样的代码让人难以理解，因为你通常认为对象在所有时候都需要它的所有变量。当变量在未被使用的情况下去猜测其当初设置的目的，会让你发疯的。可以使用提炼新类为这个可怜的孤儿创造一个家，然后把所有和这个变量相关的代码都放进这个新家。也许还可以使用“将Null值替换为Null对象”在“变量不合法”的情况下创建一个Null对象，从而避免写出条件式代码。</p>\n<h2 id=\"（13）、-过度耦合的消息链\">（13）、 过度耦合的消息链</h2>\n<p>​\t\t如果你看到用户向一个对象请求另一个对象，然后再向后者请求另一个对象，然后再请求另一个对象…这就是消息链。这种方式意味着客户代码将与某些功能函数中的导航结构紧密耦合。一旦对象间的关系发生任何变化，客户端就不得不做出相应修改。 这时候我们可以隐藏“委托关系”，并在服务类上建立客户所需要的所有函数。你可以在消息链的不同位置进行这种重构手法。理论上是可以重构消息链上的任何一个对象，但是这样做往往会把一系列对象都变成“中间人”。通常更好的选择是：先观察消息链最终得到的对象是用来干什么的，再看看能否通过抽取方法把使用该对象的代码提炼到一个独立函数中，然后再将这个函数推入消息链。</p>\n<pre><code class=\"language-java\">String result=Class.getFile().getFileChannel().getFileSource().getFileName() \n\nString result=Class.getFile().getFileName(); \n</code></pre>\n<p>​\t\t常常是因为数据结构的层次很深，需要层层调用getter获取内层数据。 个人认为Message Chains如果频繁出现，考虑这个字段是否应该移到较外层的类，或者把调用链封装在较外层类的方法。</p>\n<h2 id=\"（14）、中间人\">（14）、中间人</h2>\n<p>​\t\t我们都知道对象的基本特征之一就是封装——对外部世界隐藏其内部细节。封装往往伴随着委托。比如你对Boss说是否有时间参加一个会议，他把这个消息“委托”给他的记事本，然后才能回答你。但是，你没有必要知道Boss到底使用传统记事本或电子记事本亦或秘书来记录自己的约会。人们可能会过度使用委托。你也许会看到某个类接口中有一半的函数都委托给其它类，这样就是过度委托。这时候就应该移除中间人，直接和真正的负责人打交道。如果这样“不干实事”的函数只有少数几个，可以将它们放进调用端。如果中间人还有其它行为，可以把它变成实责对象的子类，这样你既可以扩展原对象的行为，又不必负担那么多的委托动作。</p>\n<h2 id=\"（15）、狎昵关系\">（15）、狎昵关系</h2>\n<p>​\t\t有时候你会看到两个类过于亲密，花费太多时间去探究彼此的private成分。如果这发生在两个“人”之间，我们无比做卫道士；但对于类，我们就希望它们严守清规。也许就像古代的恋人一样，过分狎昵的类必须拆散。可以通过“移动方法”和“移动字段”帮它们划清界限，从而减少狎昵行径。如果两个类实在是情投意合，可以把两者共同点提炼到一个安全地点，让它们坦荡地使用这个新类。或者通过隐藏“委托关系”让另一个类来为它们传递相思情。将双向关联改为单向关联提炼类，将两个类的共同点提炼到新类中，让它们共同使用新类</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、什么是重构</h1>\n<p>​\t\t在百度百科里给出的定义是:在不改变软件系统外部行为的前提下，改善它的内部结构。通过调整程序代码改善软件的质量、性能，使其程序的设计模式和架构更趋合理，提高软件的扩展性和维护性。</p>\n<p>​\t\t也许有人会问，为什么不在项目开始时多花些时间把设计做好，而要以后花时间来重构呢？</p>\n<p>​\t\t首先要知道一个完美得可以预见未来任何变化的设计，或一个灵活得可以容纳任何扩展的设计是不存在的。系统设计人员对即将着手的项目往往只能从大方向予以把控，而无法知道每个细枝末节。</p>\n<p>​\t\t其次永远不变的就是变化，提出需求的用户往往要在软件成型后，才开始&quot;品头论足&quot;，系统设计人员毕竟不是先知先觉的神仙，功能的变化导致设计的调整再所难免。</p>\n<p>​\t\t所以&quot;测试为先，持续重构&quot;作为良好开发习惯被越来越多的人所采纳，测试和重构像黄河的护堤，成为保证软件质量的法宝</p>\n<h1>2、软件质量因素的定义</h1>\n<p>正确性（Correctness）：系统满足规格说明和用户目标的程度，即在预定环境下能正确地完成预期功能的程度</p>\n<p>健壮性（Robustness）：在硬件发生故障、输入的数据无效或操作错误等意外环境下，系统能做出适当响应的程度</p>\n<p>效率（Efficiency）：为了完成预定的功能，系统需要的计算资源的多少</p>\n<p>完整性（Efficiency）或安全性（Security）：对未经授权的人使用软件或数据的企图，系统能够控制（禁止）的程度</p>\n<p>可用性（Usability）：系统在完成预定应该完成的功能时令人满意的程度</p>\n<p>风险（Risk）：按预定的成本和进度把系统开发出来，并且为用户所满意的概率</p>\n<p>可理解性（Comprehensibility）：理解和使用该系统的容易程度</p>\n<p>可维修性（Maintainability）：诊断和改正在运行现场发现的错误所需要的工作量的大小</p>\n<p>灵活性（Maintainability）或适应性（Adaptability）：修改或改进正在运行的系统需要的工作量的多少</p>\n<p>可再用性（Reusability）：在其他应用中该程序可以被再次使用的程度（或范围）</p>\n<p>可移植性（Portability）：把程序从一种硬件配置和（或）软件系统环境转移到另一种配置和环境时，需要的工作量多少。有一种定量度量的方法是：用原来程序设计和调试的成本除移植时需用的费用</p>\n<p>互运行性（Interoperability）：把该系统和另一个系统结合起来需要的工作量的多少</p>\n<p>重构的目的就是为了保证软件满足以上特性。</p>\n<h1>3、重构的意义</h1>\n<p>​\t\t在不改变系统功能的情况下，改变系统的实现方式。为什么要这么做？投入精力不用来满足客户关心的需求，而是仅仅改变了软件的实现方式，这是否是在浪费客户的投资呢？</p>\n<p>​\t\t重构的重要性要从软件的生命周期说起。软件不同与普通的产品，他是一种智力产品，没有具体的物理形态。一个软件不可能发生物理损耗，界面上的按钮永远不会因为按动次数太多而发生接触不良。那么为什么一个软件制造出来以后，却不能永远使用下去呢？</p>\n<p>​\t\t对软件的生命造成威胁的因素只有一个：需求的变更。一个软件总是为解决某种特定的需求而产生，时代在发展，客户的业务也在发生变化。有的需求相对稳定一些，有的需求变化的比较剧烈，还有的需求已经消失了，或者转化成了别的需求。在这种情况下，软件必须相应的改变，考虑到成本和时间等因素，当然不是所有的需求变化都要在软件系统中实现。但是总的说来，软件要适应需求的变化，以保持自己的生命力。</p>\n<p>​\t\t软件产品最初制造出来，是经过精心的设计，具有良好架构的。但是随着时间的发展、需求的变化，必须不断的修改原有的功能、追加新的功能，还免不了有一些缺陷需要修改。为了实现变更，不可避免的要违反最初的设计构架。经过一段时间以后，软件的架构就千疮百孔了。bug越来越多，越来越难维护，新的需求越来越难实现，软件的构架对新的需求渐渐的失去支持能力，而是成为一种制约。最后新需求的开发成本会超过开发一个新的软件的成本，这就是这个软件系统的生命走到尽头的时候。重构就能够最大限度的避免这样一种现象。系统发展到一定阶段后，使用重构的方式，不改变系统的外部功能，只对内部的结构进行重新的整理。通过重构，不断的调整系统的结构，使系统对于需求的变更始终具有较强的适应能力。</p>\n<h1>4、重构实例演示</h1>\n<p>​\t\t案例很简单，这是给一家出租店用的程序。计算每一位顾客的消费金额并打印详单。操作者告诉程序：顾客租了哪些影片？租期多长？程序会根据租赁时间和影片类型计算费用。影片分为三类：普通片、儿童片、新片。除了计算费用还有为顾客计算积分，积分会根据租片类型是否为新片而不同。</p>\n<p>为了实现这个功能。我们编写出了以下代码（三个类：Movie、Rental、Consumer）：</p>\n<p><img src=\"/img/1618675966507.png\" alt=\"1618675966507\"></p>\n<p><img src=\"/img/1618675987731.png\" alt=\"1618675987731\"></p>\n<p><img src=\"/img/1618676025236.png\" alt=\"1618676025236\"></p>\n<p>存在的问题：</p>\n<p>（1）对于consumer里面的statement方法。<br>\n这个方法做的事情太多，如果用户希望对系统做一点修改，首先他们希望以html格式 输出详单，这样可在网页上直接显示。这个变化的影响是：根本不可能在打印html报表的函数中复用目前statement的任何行为。唯一可以做的就是编写一个全新的htmlStatement 大量重复statement的行为。如果计费标准发生变化必须同时修改statement和htmlstatement， 不断的修改和不断的复制粘贴，在程序要保存很长时间时，造成潜在的威胁.</p>\n<p>（2）如果用户希望改变影片分类规则，但还未决定怎么改，他们设想几种方案。 这些方案都会影响消费和积分的计算方式。为了应付分类规则和计费规则的变化，程序不得不对statement做出修改，但是如果我们把statement内的代码，复制到htmlstatement函数中，就必须确保将来的任何修改在两个地方保持一致，随着各种规则变得愈来愈复杂，适当的修改点越来越难找，不犯错的机会也越来越少。</p>\n<p>（3）你的态度也行倾向于尽量少修改程序，不管怎么说。它运行的很好。你心里牢牢记着那句古老的工程谚语：如果它没坏。就不要动它 也行这个程序还没坏，但是它造成了伤害，它让你的生活比较难过，因为你发现很难完成客户所需的修改。你发现自己需要为程序添加一个特性。而代码结构使你无法方便的达到目的，那就先重构那个程序。 重构，真的是可以锻炼自己思维和代码的编写能力</p>\n<h2 id=\"Step1、重构第一步-可靠的测试\">Step1、重构第一步-可靠的测试</h2>\n<p>01 进行重构时，我们需要依赖测试，让它告诉我们是否引入bug。好的测试是重构的根本</p>\n<p>02 重构的前提是要有一个可靠的测试，这个测试必须有自我检验能力</p>\n<h2 id=\"Step2、重构第二步-分解重组statement-找出代码的逻辑泥团并运用Extract-Method\">Step2、重构第二步-分解重组statement 找出代码的逻辑泥团并运用Extract Method</h2>\n<p>这个函数太长了，代码块越小，代码的功能越好管理。代码的处理和移动也越轻松<br>\n代码重构目标：希望将长长的函数切开，把较小的块移动到更合适的类中，最终能够降低代码重复和扩展<br>\n将 switch这段逻辑泥团抽离为函数。<br>\n在分析函数内的局部变量和参数，其中statement() while循环中有两个： thisAmount、each, thisAmount会被修改，each不会被修改。<br>\n任何不会被修改的变量都可以被当成参数传入新的函数<br>\n注意每次调整都要编译测试<br>\n重构技术就是以 微小 的步伐修改程序，如果你犯下错误，很容易发现它</p>\n<p><img src=\"/img/1618676190337.png\" alt=\"1618676190337\"></p>\n<h2 id=\"Step3、重构第三步-更改amountFor中的变量名\">Step3、重构第三步-更改amountFor中的变量名</h2>\n<p>好的代码应该清楚表达出自己的功能，变量名称是代码清晰的关键。<br>\n任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。<br>\n代码应该表现自己的目的<br>\n随着对程序的理解逐渐加深，我也就不断的把这些理解嵌入到代码中，这么一来才不会遗忘我曾经理解的东西</p>\n<p><img src=\"/img/1618676232016.png\" alt=\"1618676232016\"></p>\n<h2 id=\"Step4、重构第四步-搬移金额计算代码\">Step4、重构第四步-搬移金额计算代码</h2>\n<p>观察amountFor时，发现这个函数没有使用来自Consumer类的信息，使用了来自Rental类的信息。<br>\n所以应该改这段代码搬移到Rental类。并且做相应的调整：更改方法名、参数等 更改调用处<br>\n运用Replace Temp with Query把thisAmount除去</p>\n<p><img src=\"/img/1618676280257.png\" alt=\"1618676280257\"></p>\n<p>第一步：先将计算金额代码搬移到Rental类中</p>\n<p>Rental类中添加方法 getCharge</p>\n<p>第二步：针对搬移后的代码，调整Consumer类</p>\n<h2 id=\"Step5、重构第五步-运用Extract-Method-参考抽取计算金额，来抽取积分\">Step5、重构第五步-运用Extract Method 参考抽取计算金额，来抽取积分</h2>\n<p><img src=\"/img/1618676358941.png\" alt=\"1618676358941\"></p>\n<p>第一步：将积分计算方法搬移到Rental类中</p>\n<p>第二步：2、更改consumer类中获取积分</p>\n<h2 id=\"Step6、重构第六步-去掉临时变量\">Step6、重构第六步-去掉临时变量</h2>\n<p>临时变量只在自己所属的函数中有效，所以它们会助长冗长而复杂的函数。<br>\n运用Replace Temp with Query,并利用查询函数(query method)来取代totalAmount和frequentRentalPoints这两个临时变量。<br>\n由于类中的任何函数都可以调用上述查询函数，所以它能够促成较干净的设计，而减少冗长复杂的函数。</p>\n<p><img src=\"/img/1618676411155.png\" alt=\"1618676411155\"></p>\n<p>第一步：totalAmount和frequenRenterPoint两个临时变量</p>\n<p>第二步：使用查询函数来替代</p>\n<h2 id=\"Step7、重构第七步-运用多态取代与价格相关的条件逻辑\">Step7、重构第七步-运用多态取代与价格相关的条件逻辑</h2>\n<p>对于switch语句，最好不要在另一个对象的属性基础上运用switch语句。如果不得不使用。也应该在对象自己的数据上使用而不是在别人的数据上使用。 这暗示getCharge应该移动到Movie中<br>\n租期的长度来自Rental对象，计算费用的时候需要两项数据：租期长度和影片类型<br>\n为什么选择租期长度呢。因为本系统可能发生变化是加入新影片类型。这种变化带有不稳定性倾向<br>\n如果影片类型发生变化，我希望尽量控制它造成的影响。所以在Movie对象中计算费用</p>\n<p><img src=\"/img/1618676461026.png\" alt=\"1618676461026\"></p>\n<h2 id=\"Step8、重构第八步-运用多态取代与价格相关的条件逻辑\">Step8、重构第八步-运用多态取代与价格相关的条件逻辑</h2>\n<p>用多态替换Switch，如果创建三个子类继承Movie，调用方就必须创建具体的子类对象（违反依赖倒置原则）。<br>\n一个对象具有状态，并且不同状态下有不同的行为，引入State模式：<br>\n创建接口Price作为Movie的属性，接口方法getCharge(int daysRented)，再创建三个实现类，把Switch分支的逻辑移至具体的实现类<br>\n依赖倒置原则，调用方应该依赖抽象类或接口，不要依赖具体实现类</p>\n<p>创建price接口 将getPriceCode、getCharge getFrequentRenterPoint抽象出来</p>\n<p><img src=\"/img/1618676506490.png\" alt=\"1618676506490\"></p>\n<p><img src=\"/img/1618676526483.png\" alt=\"1618676526483\"></p>\n<p>创建子类继承Price 实现具体的实现</p>\n<h1>5、总结</h1>\n<p>1、每个方法只做一件事，每个方法抽象层级不能多于两层，根据这个原则抽取方法。</p>\n<p>2、根据类的职责和对象之间的依赖关系，把方法移至对应的类。</p>\n<p>3、应该调用对象的接口方法，不要直接操作对象的属性。</p>\n<p>4、尽量减少方法中的临时变量，简化逻辑，增加可读性。</p>\n<h1>6、重构的时机</h1>\n<h2 id=\"（1）、什么时候重构\">（1）、什么时候重构</h2>\n<p>三次法则：事不过三，三则重构<br>\n添加功能时重构（New Feature）<br>\n代码的设计无法帮助我轻松的添加我所需要的特性，如果用某种方式来设计，添加特性会简单的多。一旦完成重构，新特性的添加会更快速，更流畅<br>\n修补错误时重构（Bug Fix）<br>\n调试过程中，运用重构，多半是为了让代码更具有可读性<br>\n复审代码时重构（Code Review)重构可以帮助我们复审代码</p>\n<h2 id=\"（2）、什么时候不重构\">（2）、什么时候不重构</h2>\n<p>既有代码太混乱，且不能正常工作，需要重写而不是重构。<br>\n项目接近最后期限时，应该避免重构。</p>\n<h1>7、重构的手段</h1>\n<h2 id=\"（1）、改善重复代码\">（1）、改善重复代码</h2>\n<p>​\t\t重复的代码是坏味道中出现频率最高的情形非其莫属。如果在一个的以上地方看到相同的代码，那么就可以肯定：想办法将它们合而为一，代码会变得更好。最单纯的重复代码就是“同一个类的两个函数含有相同的表达式”，这时候可以采用抽取方法提炼出重复的代码，然后让这两个地点都调用被提炼出的那一段代码。</p>\n<p>​\t\t另一种常见情况就是“两个互为兄弟的子类内含相同的表达式”，这时候只需对两个类抽取方法，然后将提炼出的代码推入到超类中。如果代码之间只是类似而并非完全相同，那么就需要通过抽取方法将相似部分和差异部分分开，构成单独一个函数。如果有些函数以不同的算法做相同的事，可以使用比较清晰的一个替换掉其余的。</p>\n<h2 id=\"（2）、改善过长的函数、过大的类、-过长的参数列\">（2）、改善过长的函数、过大的类、 过长的参数列</h2>\n<p>​\t\t程序员都喜欢简短的函数。拥有短函数的对象会活的比较好、比较长。不熟悉面向对象技术的人，常常觉得对象程序中只有无穷无尽的委托，根本没有进行任何计算。和此类程序共同生活数年后，你才会知道这些小小函数的价值。</p>\n<p>​\t\t应该积极地分解函数，将长长的函数变为多个短小的函数。一般会遵循这样的原则：每当感觉需要用注释来说明点什么的时候，就把需要说明的东西写进一个独立函数中，并以其用途命名。不要嫌麻烦。可以对一组甚至短短一行代码做这件事，哪怕替换后的函数调用动作比函数自身还长，只要函数名称能够解释其用途，也应毫不犹豫地那么做。关键不在于函数的长度，而在于函数“做什么”和“如何做”之间的语义距离。<br>\n​\t\t如果想利用单个的类做太多的事情，其内往往会出现太多实例变量。一旦如此，重复的代码就接踵而来。可以将几个变量一起提炼至新类内。提炼时应该选择类内彼此相关的变量，将它们放在一起。通常如果类内的数个变量有着相同的前缀或字尾，这就意味有机会把它们提炼到某个组件内。和“太多实例变量”一样，类内如果有太多代码，也是代码重复、混乱并最终走向死亡的源头。最简单的方案是把多余的东西消弭于类内部。如果有五个“百行函数”，它们之中很多代码都相同，那么或许你可以把它们变成五个“十行函数”和十个提炼出的“双行函数”。</p>\n<p>​\t\t刚开始学编程的时候，或许都是“把函数所需的所有东西都以参数传递进去”。这样也是可以理解的，因为除此之外就只能选择全局数据，而全局数据是邪恶的东西。对象技术告诉我们，如果你手上没有所需的东西，总可以叫一个对象给你。有了对象，你就不必要把函数所需的所有东西都以参数传递给它，只需传给它足够的、让函数能从中获得自己的东西就行。太长的的参数列难以理解，太多参数会造成前后不一致、不易使用，而且一旦需要更多数据，就不得不修改它。如果将对象传递给函数，大多数修改都将没有必要，因为很可能只需增加一两条请求，就能得到更多的数据。</p>\n<h2 id=\"（3）、发散式变化\">（3）、发散式变化</h2>\n<p>​\t\t我们希望软件能够容易被修改——毕竟软件再怎么说本来就该是“软”的。一旦需要修改，我们希望能够跳到系统某一点，只在该处做修改。如果不能做到这点，你就会嗅出两种紧密相关的刺鼻味道中的一种。如果某个类经常因为不同的原因在不同的方向上发生变化，发散式变化就出现了。其主要指“一个类受多种变化的影响”。当你看着一个类说：“呃，如果新加入一个数据库，就必须修改这三个函数；如果新出现一种工具，就必须修改这四个函数。”那么此时也许将这个对象分成两个会更好，这样对每个对象就可以只因一种变化而需要修改因为不同的原因，在不同的方向上，修改同一个类。应该分解成更小的类，每个类只因一种原因而修改。多层结构系统，开发人员往往容易把全部逻辑都放在Service层，导致Service类非常庞大且不断被修改。</p>\n<h2 id=\"（4）、霾弹式修改\">（4）、霾弹式修改</h2>\n<p>​\t\t如果每遇到变化，都必须在许多不同的类内做出许多小修改，你所面临的坏味道就是霾弹式修改。其主要指“一种变化引发多个类相应修改”。如果需要修改的代码散布四周，不但很难找到它们，也很容易忘记某个重要的修改。这种情况可以把所有需要的代码放进同一个类。如果眼下没有合适的类可以安置这些代码，就创造一个。通常可以运用内联类把一系列相关行为放进同一个类。</p>\n<h2 id=\"（5）、-依恋情节\">（5）、 依恋情节</h2>\n<p>​\t\t众所周知，对象技术的全部要点在于：其是一种“将数据和对数据的操作行为包装在一起”的技术。有一种经典的气味：函数对于某个类的兴趣高过对自己所处类的兴趣。在很多情况下，都能够看到：某个函数为了计算某个值，从另一个对象那儿调用几乎半打的取值函数。疗法也显而易见：把这个函数移至另一个地点，移到它该去的地方。‘有时候一个函数往往会用到几个类的功能，那么它究竟该被置于何处呢？处理原则通常为：判断哪个类拥有最多被此函数使用的数据，然后就把这个函数和那些数据摆在一起。</p>\n<h2 id=\"（6）、数据泥团\">（6）、数据泥团</h2>\n<p>​\t\t如果用比较形象的事物来形容数据项，我想“小孩子”是一个不错的选择，数据项就像小孩子，喜欢成群结队地呆在一块儿。常常可以在很多地方看到相同的三四项数据：两个类中相同的字段、许多函数签名中相同的参数。这些总是绑在一起出现的数据真应该拥有属于它们自己的对象。这种情况可以先找出这些数据以字段形式出现的地方，将它们提炼到一个独立对象中，然后将注意力转移到函数签名上，运用参数对象为它减肥。这样做的直接好处是可以将很多参数列缩短，简化函数调用。一个比较好的评判方法是：删掉众多数据中的一项。这么做其它数据有没有因而失去意义？如果它们不再有意义，这就是一个明确的信号：应该为它们产生一个新对象。</p>\n<h2 id=\"（7）、基本类型偏执\">（7）、基本类型偏执</h2>\n<p>​\t\t大多数编程环境都有两种数据：结构类型允许你将数据组织成有意义的形式；基本类型则是构成结构类型的积木块。但是请记住：结构总是会带来一定的额外开销。它们可能代表着数据库中的表，如果只为做一两件事而创建结构类型也可能显得很麻烦。 对象的一个极大价值在于：它们模糊甚至打破横亘于基本数据和体积较大的类之间的界限。如果你有一组应该总是被放在一起的字段，可以将其抽取为一个独立的类。如果你在参数列中看到基本型数据，可以引入参数对象进行处理。如果你发现自己正从数组中挑选数据，可以运用以对象取代数组进行处理。 由一个起始值和一个结束值组成的range类：如果你有大量的基本数据类型字段，就有可能将其中部分存在逻辑联系的字段组织起来，形成一个类。更进一步的是，将与这些数据有关联的方法也一并移入类中 如果你发现自己正从数组中挑选数据，可运用 以对象取代数组。</p>\n<h2 id=\"（8）、Switch惊悚现身\">（8）、Switch惊悚现身</h2>\n<p>​\t\t面向对象程序的一个较明显特征是：少用switch语句。从本质上说，switch语句的问题在于重复。你常会发现同样的switch语句散布于不同的地方。如果要为它添加一个新的case语句，就必须找到所用switch语句并修改它们。面向对象中的多态概念可为此带来优雅的解决办法。大多数时候，一看到switch语句，那就应该考虑以多态来替换它。switch语句常常根据类型码进行选择，你要的是“与该类型码相关的函数或类”，所以应该将switch语句提炼到一个独立函数中，再将它搬移到需要多态性的那个类里。</p>\n<h2 id=\"（9）、平行继承体系\">（9）、平行继承体系</h2>\n<p>​\t\t平行继承体系其实是霾弹式修改的特殊情况。在这种情况下，每当为某个类增加一个子类，必须也为另一个类增加一个子类。如果发现某个继承体系的类名称前缀和另一个继承体系的类名称前缀完全相同，这种坏味道就会被嗅出。<br>\n​\t\t消除这种重复性的一般策略是：让一个<br>\n​\t\t继承体系的实例引用另一个继承体系的实例。</p>\n<p><img src=\"/img/1618677203153.png\" alt=\"1618677203153\"></p>\n<h2 id=\"（10）、冗赘类\">（10）、冗赘类</h2>\n<p>​\t\t你所创建的每一个类，都得有人去理解它、维护它，这些工作都是需要花钱的。如果一个类的所得并不值其身价，他就应该消失。项目中经常会出现这样的情况：某个类原本对得起自己的价值，但重构使它身形缩水，不再做那么多工作；或开发者事先规划了某些变化，并添加一个类来应付这些变化，但变化实际没有发生。不管是哪种原因，都应该让这个类庄严赴义吧。如果某些子类并没有做足够的工作，我们可以尝试“折叠继承体系”，将超类和子类合为一体，那样就会减少维护时间。对于那些几乎没用的组件，就应该将这个类的所有特性搬移到另一个类中，然后移除原类。</p>\n<h2 id=\"（11）、夸夸其谈未来性\">（11）、夸夸其谈未来性</h2>\n<p>​\t\t我们经常会说：“我想总有一天需要做这事”，并因而企图以各样的钩子和特殊情况来处理一些非必要的事情。一旦这样，坏味道就浮现出来了。夸夸其谈未来的结果往往会造成系统更加难以理解和维护。如果所有的装置都被用到了，那就值得那么做；如果用不到，就不值得。用不上的装置只会阻挡你的路，给你添乱，那就搬开它吧。如果某个抽象类其实没有太大作用，可以将超类和子类合为一体。将不必要的委托转移到另一个类中，并消除原先的类。如果函数的某些参数未被用上，那么就将参数移走。如果函数名称带有多余的抽象意味，就应该对它重命名，让它现实一些。</p>\n<h2 id=\"（12）、令人迷惑的暂时字段\">（12）、令人迷惑的暂时字段</h2>\n<p>​\t\t有时候你会发现：类中的某个实例变量仅为某种特定情况而设。这样的代码让人难以理解，因为你通常认为对象在所有时候都需要它的所有变量。当变量在未被使用的情况下去猜测其当初设置的目的，会让你发疯的。可以使用提炼新类为这个可怜的孤儿创造一个家，然后把所有和这个变量相关的代码都放进这个新家。也许还可以使用“将Null值替换为Null对象”在“变量不合法”的情况下创建一个Null对象，从而避免写出条件式代码。</p>\n<h2 id=\"（13）、-过度耦合的消息链\">（13）、 过度耦合的消息链</h2>\n<p>​\t\t如果你看到用户向一个对象请求另一个对象，然后再向后者请求另一个对象，然后再请求另一个对象…这就是消息链。这种方式意味着客户代码将与某些功能函数中的导航结构紧密耦合。一旦对象间的关系发生任何变化，客户端就不得不做出相应修改。 这时候我们可以隐藏“委托关系”，并在服务类上建立客户所需要的所有函数。你可以在消息链的不同位置进行这种重构手法。理论上是可以重构消息链上的任何一个对象，但是这样做往往会把一系列对象都变成“中间人”。通常更好的选择是：先观察消息链最终得到的对象是用来干什么的，再看看能否通过抽取方法把使用该对象的代码提炼到一个独立函数中，然后再将这个函数推入消息链。</p>\n<pre><code class=\"language-java\">String result=Class.getFile().getFileChannel().getFileSource().getFileName() \n\nString result=Class.getFile().getFileName(); \n</code></pre>\n<p>​\t\t常常是因为数据结构的层次很深，需要层层调用getter获取内层数据。 个人认为Message Chains如果频繁出现，考虑这个字段是否应该移到较外层的类，或者把调用链封装在较外层类的方法。</p>\n<h2 id=\"（14）、中间人\">（14）、中间人</h2>\n<p>​\t\t我们都知道对象的基本特征之一就是封装——对外部世界隐藏其内部细节。封装往往伴随着委托。比如你对Boss说是否有时间参加一个会议，他把这个消息“委托”给他的记事本，然后才能回答你。但是，你没有必要知道Boss到底使用传统记事本或电子记事本亦或秘书来记录自己的约会。人们可能会过度使用委托。你也许会看到某个类接口中有一半的函数都委托给其它类，这样就是过度委托。这时候就应该移除中间人，直接和真正的负责人打交道。如果这样“不干实事”的函数只有少数几个，可以将它们放进调用端。如果中间人还有其它行为，可以把它变成实责对象的子类，这样你既可以扩展原对象的行为，又不必负担那么多的委托动作。</p>\n<h2 id=\"（15）、狎昵关系\">（15）、狎昵关系</h2>\n<p>​\t\t有时候你会看到两个类过于亲密，花费太多时间去探究彼此的private成分。如果这发生在两个“人”之间，我们无比做卫道士；但对于类，我们就希望它们严守清规。也许就像古代的恋人一样，过分狎昵的类必须拆散。可以通过“移动方法”和“移动字段”帮它们划清界限，从而减少狎昵行径。如果两个类实在是情投意合，可以把两者共同点提炼到一个安全地点，让它们坦荡地使用这个新类。或者通过隐藏“委托关系”让另一个类来为它们传递相思情。将双向关联改为单向关联提炼类，将两个类的共同点提炼到新类中，让它们共同使用新类</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cl1vcnny900017kt95ktv7ni8","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnyh000e7kt989f92p5r"},{"post_id":"cl1vcnnyd00077kt97hca36ba","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnyi000h7kt9bwf40xvb"},{"post_id":"cl1vcnnya00027kt9dedk6eaq","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnyi000k7kt9ce8igpcl"},{"post_id":"cl1vcnnyc00057kt912i74ybb","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnyl000r7kt9e50chwii"},{"post_id":"cl1vcnnyc00067kt9ftwka5m2","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnym000w7kt91xtu7cuo"},{"post_id":"cl1vcnnyl000t7kt90qu6b00x","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcnnyo00127kt9h1bgb277"},{"post_id":"cl1vcnnyf000b7kt9bks41sxs","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcnnyp00157kt91a03bzia"},{"post_id":"cl1vcnnyg000c7kt99ach53aa","category_id":"cl1vcnnyn000x7kt9ga2y5hdw","_id":"cl1vcnnyq00197kt98sto0t1d"},{"post_id":"cl1vcnnyh000g7kt96c594hcn","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcnnyu001h7kt9b9h74gau"},{"post_id":"cl1vcnnyq001a7kt9hkoj8lew","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnyv001k7kt9dx375a9c"},{"post_id":"cl1vcnnyi000j7kt9gqxnemcw","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcnnyw001o7kt96wfad46w"},{"post_id":"cl1vcnnyj000n7kt945y8c2o2","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcnnyx001u7kt98z0qfyh3"},{"post_id":"cl1vcnnyk000p7kt9606y5gxc","category_id":"cl1vcnnyn000x7kt9ga2y5hdw","_id":"cl1vcnnyz001z7kt90h8qcacz"},{"post_id":"cl1vcnnym000v7kt9h23ce7f1","category_id":"cl1vcnnyx001t7kt97fme2mu3","_id":"cl1vcnnz000257kt90jxv8ju0"},{"post_id":"cl1vcnnyz00217kt98y0w51n1","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcnnz3002a7kt90owp6bhw"},{"post_id":"cl1vcnnyn000y7kt9ftsyfvg2","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnz4002e7kt9e9oqhfhl"},{"post_id":"cl1vcnnz000247kt949vzc52d","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnz5002h7kt9c9cw5y8v"},{"post_id":"cl1vcnnz100277kt94c2p7v9y","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcnnz5002l7kt9ae8c6ryr"},{"post_id":"cl1vcnnyo00117kt9ag3762r3","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnz6002p7kt91ilm7zdm"},{"post_id":"cl1vcnnz3002d7kt9cj5r5rco","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnz7002t7kt94728eg1j"},{"post_id":"cl1vcnnyo00147kt9gfj1aoe0","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnz8002x7kt9ajhmf18n"},{"post_id":"cl1vcnnz4002g7kt9391hb0oq","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnz900317kt96zmpepmn"},{"post_id":"cl1vcnnz5002k7kt9fc6rf7oa","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnza00337kt9a2fa3hyy"},{"post_id":"cl1vcnnyp00177kt9dxyz29ux","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzb00377kt972cxdioa"},{"post_id":"cl1vcnnz6002o7kt9cf1ibtoi","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnzb00397kt9dk0lahvw"},{"post_id":"cl1vcnnz7002s7kt9by3r4tua","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnzd003e7kt9blhz6k25"},{"post_id":"cl1vcnnyt001d7kt924s9ghz4","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzd003h7kt96dsggoc3"},{"post_id":"cl1vcnnz8002w7kt94b39alfc","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcnnzf003m7kt983ur4ex0"},{"post_id":"cl1vcnnyt001f7kt9h3zt9bz3","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzg003o7kt999ahetl7"},{"post_id":"cl1vcnnz900327kt9ebyge62n","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzh003s7kt93i4l03nt"},{"post_id":"cl1vcnnza00367kt91qqd5mvz","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzj003v7kt99l1f8oxx"},{"post_id":"cl1vcnnyu001j7kt92j3d6gqk","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzk00407kt99irb6ros"},{"post_id":"cl1vcnnyv001m7kt92417djj1","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzl00437kt9biyfejwb"},{"post_id":"cl1vcnnyw001p7kt9htw45mxa","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzm00477kt92lmc6on4"},{"post_id":"cl1vcnnzf003n7kt95ti7bewu","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnzn004a7kt97msc01s4"},{"post_id":"cl1vcnnzh003r7kt9a9h7g7ae","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzo004e7kt992go8sv3"},{"post_id":"cl1vcnnyx001s7kt988sa6mfy","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzp004h7kt98efxb10i"},{"post_id":"cl1vcnnzi003u7kt9gsww4z1x","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcnnzp004l7kt9c1qihuyq"},{"post_id":"cl1vcnnyy001v7kt9fll11q4v","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzq004o7kt9700jf4bi"},{"post_id":"cl1vcnnyy001y7kt96j6750ry","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcnnzr004s7kt9ax0hbeth"},{"post_id":"cl1vcnnzn004d7kt9abfj6lxb","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzs004v7kt93qq3dlat"},{"post_id":"cl1vcnnz200297kt9b7c8116o","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzt004y7kt985a48d77"},{"post_id":"cl1vcnnzo004g7kt9bajo3xsz","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzu00527kt9atic7hav"},{"post_id":"cl1vcnnzp004k7kt92utz6fb2","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzv00557kt9hic5glov"},{"post_id":"cl1vcnnz900307kt9ce8b0ivr","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzw00597kt93u9s6ork"},{"post_id":"cl1vcnnzq004n7kt93kp80967","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzw005c7kt9flv585zq"},{"post_id":"cl1vcnnzr004r7kt90d304k0l","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzx005g7kt98g81hoe8"},{"post_id":"cl1vcnnzs004u7kt9c8xx3frj","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzy005j7kt98z165922"},{"post_id":"cl1vcnnzb00387kt9e8mo6eex","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcnnzz005n7kt9fvdo9qhn"},{"post_id":"cl1vcnnzs004x7kt96idget6k","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcnnzz005p7kt9fuosezts"},{"post_id":"cl1vcnnzt00517kt977pu01l8","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno01005u7kt92g7newb4"},{"post_id":"cl1vcnnzc003d7kt9cwys7jdg","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno02005x7kt91nxa5v7c"},{"post_id":"cl1vcnnzv00587kt96f6y15qc","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno0300627kt96mnh5c70"},{"post_id":"cl1vcnnzd003g7kt90ieue2gh","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0400657kt9c53dhcb6"},{"post_id":"cl1vcnnzw005b7kt9a9um35yf","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno05006a7kt9gd9ghgdb"},{"post_id":"cl1vcnnzx005f7kt98cjuggo2","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcno06006d7kt92a7qerso"},{"post_id":"cl1vcnnze003l7kt96hgq4rae","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno08006h7kt985zdhwo1"},{"post_id":"cl1vcnnzy005i7kt9cpfv0rra","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcno09006k7kt9gfma1vxf"},{"post_id":"cl1vcnnzy005m7kt92m7i2pjh","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcno09006o7kt905cq3iwj"},{"post_id":"cl1vcnnzk003z7kt97i4j4hv4","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno0a006r7kt9aqcn9kjm"},{"post_id":"cl1vcnnzz005o7kt9haoig9h5","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0b006u7kt93i3d1ei1"},{"post_id":"cl1vcno00005t7kt9anh540n9","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcno0c006x7kt97lkxci6b"},{"post_id":"cl1vcnnzl00427kt937lj9sst","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno0d00707kt99r5s0dnp"},{"post_id":"cl1vcno02005w7kt95m2ca9ms","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcno0d00737kt9b33be5aw"},{"post_id":"cl1vcno0300617kt9a21a76xa","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcno0e00757kt92kk22vdh"},{"post_id":"cl1vcnnzm00467kt95dnz1p3m","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno0f00797kt99cgz16fu"},{"post_id":"cl1vcno0400647kt95x5048oj","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcno0f007b7kt9gadc1jxw"},{"post_id":"cl1vcno0500697kt98i116rz8","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcno0g007f7kt9b70l1gwg"},{"post_id":"cl1vcnnzn00497kt91734f6s6","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno0g007h7kt91mhqfze5"},{"post_id":"cl1vcno05006c7kt9fp83fnpl","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0h007l7kt92cku8nbs"},{"post_id":"cl1vcno08006g7kt9dmev1y2s","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0j007o7kt9cfu6atzw"},{"post_id":"cl1vcno08006j7kt93fkb81tl","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcno0k007r7kt9fs2bhkq4"},{"post_id":"cl1vcno09006n7kt9c7z0bt3n","category_id":"cl1vcnnyo00137kt9bqyl98up","_id":"cl1vcno0k007t7kt9aa06hd9i"},{"post_id":"cl1vcno0a006q7kt91fxf6ja1","category_id":"cl1vcnnyx001t7kt97fme2mu3","_id":"cl1vcno0l007w7kt92rw2gmmf"},{"post_id":"cl1vcno0b006t7kt9gzbu7c38","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0o007z7kt9d9d2e4fd"},{"post_id":"cl1vcno0b006w7kt9ctvdddnt","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0p00837kt9cpc85tzu"},{"post_id":"cl1vcno0c006z7kt988alfu9s","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0q00867kt9f3wqc9gd"},{"post_id":"cl1vcno0d00727kt97iho2wrx","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0r008a7kt9br59bhmy"},{"post_id":"cl1vcno0d00747kt934xscje4","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0r008d7kt9c95lgikc"},{"post_id":"cl1vcno0e00787kt9399h6c3x","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0s008g7kt99j4yey87"},{"post_id":"cl1vcno0f007a7kt98nxhcvbu","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0t008l7kt9cqxnceuu"},{"post_id":"cl1vcno0g007g7kt9c1stcdtf","category_id":"cl1vcnnyx001t7kt97fme2mu3","_id":"cl1vcno0t008o7kt90bi6g7aq"},{"post_id":"cl1vcno0h007k7kt9e4cj7jjb","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0u008t7kt99ybp5sub"},{"post_id":"cl1vcno0i007n7kt9e7b7fzzn","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno0v008w7kt92cmd1n9p"},{"post_id":"cl1vcno0g007e7kt9e52j50jx","category_id":"cl1vcno0h007j7kt917c91q45","_id":"cl1vcno0w00907kt9992vbll7"},{"post_id":"cl1vcno0j007q7kt9h79r7oqa","category_id":"cl1vcnnyn000x7kt9ga2y5hdw","_id":"cl1vcno0x00947kt926pn9yt7"},{"post_id":"cl1vcno0k007s7kt97a0rfmab","category_id":"cl1vcnnyn000x7kt9ga2y5hdw","_id":"cl1vcno0y00977kt9ccshg89n"},{"post_id":"cl1vcno0l007v7kt90jzlhazb","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno0z009a7kt9ey99g10n"},{"post_id":"cl1vcno0l007y7kt9hsdjdwre","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno10009e7kt9900yhptw"},{"post_id":"cl1vcno0o00827kt9h3rwenli","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno11009h7kt9ast9bclv"},{"post_id":"cl1vcno0q00857kt9gazu1eec","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno12009l7kt93cdq6n7g"},{"post_id":"cl1vcno0q00897kt939lo4nvc","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno13009o7kt9f1chc6qb"},{"post_id":"cl1vcno0t008n7kt9a17rga93","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno13009s7kt99ozkflyb"},{"post_id":"cl1vcno0r008c7kt9gyedhhoa","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno14009v7kt9c8dqbtmw"},{"post_id":"cl1vcno0u008r7kt9btl5dhzs","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno15009z7kt91qq175o6"},{"post_id":"cl1vcno0v008v7kt97alqblc5","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1700a27kt918j62r2b"},{"post_id":"cl1vcno0s008f7kt9awvvhfyq","category_id":"cl1vcno0u008s7kt98dmf08ix","_id":"cl1vcno1800a67kt90om5cubq"},{"post_id":"cl1vcno0w008z7kt91evi4d5n","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno1800a97kt94vo1e9co"},{"post_id":"cl1vcno0x00937kt97hj1dxs7","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1900ad7kt9901ac96j"},{"post_id":"cl1vcno0s008k7kt990hoel8z","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno1a00ag7kt9bt1gcjzy"},{"post_id":"cl1vcno0y00967kt9dxpk1sxw","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1a00ak7kt98d0d1jzm"},{"post_id":"cl1vcno0y00997kt9c29f118b","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1b00ao7kt93wc3e586"},{"post_id":"cl1vcno0z009d7kt9foqi3dul","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1c00as7kt95unyb7pb"},{"post_id":"cl1vcno11009g7kt9esm458fl","category_id":"cl1vcnnyg000d7kt93yqcd3kt","_id":"cl1vcno1d00av7kt93lvghnl9"},{"post_id":"cl1vcno12009k7kt907y77qop","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno1e00az7kt9h36d1oka"},{"post_id":"cl1vcno12009n7kt92nvja4mx","category_id":"cl1vcnnzn004b7kt9hr086xan","_id":"cl1vcno1e00b27kt997i508es"},{"post_id":"cl1vcno13009r7kt91nj24flk","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1f00b57kt9b1vt1740"},{"post_id":"cl1vcno14009u7kt93sfa2kde","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1g00b87kt9gpt0c3ir"},{"post_id":"cl1vcno15009y7kt91kn45bf9","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno1h00bb7kt94ybf8ehs"},{"post_id":"cl1vcno1600a17kt91xxddepo","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1h00bd7kt9043d5n7w"},{"post_id":"cl1vcno1700a57kt99xgw4uaa","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1h00bf7kt9fni51v8o"},{"post_id":"cl1vcno1800a87kt96ombgy8m","category_id":"cl1vcnnyz00207kt9dl0105gr","_id":"cl1vcno1i00bh7kt99seo3we4"},{"post_id":"cl1vcno1900ac7kt9ckli24fx","category_id":"cl1vcnnyl000q7kt95058e27c","_id":"cl1vcno1i00bj7kt9e997027z"},{"post_id":"cl1vcno1a00aj7kt90sdj56at","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1i00bl7kt978w3a40r"},{"post_id":"cl1vcno1b00an7kt94i2w4c4h","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1i00bn7kt91flx8osv"},{"post_id":"cl1vcno1c00ar7kt91ytx2o67","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1i00bp7kt9gf1x8tci"},{"post_id":"cl1vcno1900af7kt91f6vdfhq","category_id":"cl1vcno1b00am7kt90ixz088t","_id":"cl1vcno1j00br7kt9geuohqyz"},{"post_id":"cl1vcno1c00au7kt97emwh7mt","category_id":"cl1vcno0u008s7kt98dmf08ix","_id":"cl1vcno1j00bt7kt98um8gi2k"},{"post_id":"cl1vcno1d00ay7kt95sq5gj2o","category_id":"cl1vcno0s008j7kt9f6qjeahs","_id":"cl1vcno1j00bv7kt90igb5luk"},{"post_id":"cl1vcno1e00b17kt9g82ta19v","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1j00bx7kt9hwf6e1m6"},{"post_id":"cl1vcno1f00b47kt971y37yoj","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1j00bz7kt9hq315z3k"},{"post_id":"cl1vcno1f00b77kt9b520amiu","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1j00c07kt90lpt7mvk"},{"post_id":"cl1vcno1g00ba7kt95m96hoyi","category_id":"cl1vcnnyb00037kt92f1xc3pn","_id":"cl1vcno1j00c17kt9fd2n7pqe"},{"post_id":"cl1vcno2300f47kt93srt1ygq","category_id":"cl1vcno1b00am7kt90ixz088t","_id":"cl1vcno2600fa7kt93psr66xp"},{"post_id":"cl1vcno2400f57kt959xqbw35","category_id":"cl1vcno1b00am7kt90ixz088t","_id":"cl1vcno2600fd7kt9aneq826y"},{"post_id":"cl1vcno2400f77kt9fjhvbrmd","category_id":"cl1vcnnzq004p7kt94gy73a3x","_id":"cl1vcno2700fg7kt961y57lc3"},{"post_id":"cl1vcno2500f97kt94qug0h5q","category_id":"cl1vcno1b00am7kt90ixz088t","_id":"cl1vcno2800fj7kt9fvv47rp2"},{"post_id":"cl1vcno2600fc7kt921oyepz3","category_id":"cl1vcnnyn000x7kt9ga2y5hdw","_id":"cl1vcno2800fl7kt9f9ny9pw7"},{"post_id":"cl1vcno2600ff7kt96p3l77is","category_id":"cl1vcno1b00am7kt90ixz088t","_id":"cl1vcno2800fn7kt9bioia3gz"},{"post_id":"cl1vcno2700fi7kt99w391hw4","category_id":"cl1vcno0u008s7kt98dmf08ix","_id":"cl1vcno2800fp7kt923vchrky"}],"PostTag":[{"post_id":"cl1vcnny900017kt95ktv7ni8","tag_id":"cl1vcnnyb00047kt980nx5px5","_id":"cl1vcnnyf000a7kt94tz2hdse"},{"post_id":"cl1vcnnya00027kt9dedk6eaq","tag_id":"cl1vcnnyd00097kt96fx8duff","_id":"cl1vcnnyi000i7kt9b0lpexkb"},{"post_id":"cl1vcnnyc00057kt912i74ybb","tag_id":"cl1vcnnyh000f7kt9fxix2jc1","_id":"cl1vcnnyj000o7kt9f9vx9f5i"},{"post_id":"cl1vcnnyc00067kt9ftwka5m2","tag_id":"cl1vcnnyj000m7kt9dyhn3v7j","_id":"cl1vcnnym000u7kt9cmlw6ly6"},{"post_id":"cl1vcnnyd00077kt97hca36ba","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcnnyo00107kt9cis2f26y"},{"post_id":"cl1vcnnyf000b7kt9bks41sxs","tag_id":"cl1vcnnyn000z7kt9grqp7ode","_id":"cl1vcnnyq00187kt96s2uggs9"},{"post_id":"cl1vcnnyg000c7kt99ach53aa","tag_id":"cl1vcnnyp00167kt96rtrbgjr","_id":"cl1vcnnyt001e7kt99bjseycp"},{"post_id":"cl1vcnnyh000g7kt96c594hcn","tag_id":"cl1vcnnyr001c7kt9acxtdcat","_id":"cl1vcnnyv001l7kt943umbidk"},{"post_id":"cl1vcnnyi000j7kt9gqxnemcw","tag_id":"cl1vcnnyu001i7kt9h55j384e","_id":"cl1vcnnyx001r7kt90e9ta59x"},{"post_id":"cl1vcnnyj000n7kt945y8c2o2","tag_id":"cl1vcnnyu001i7kt9h55j384e","_id":"cl1vcnnyy001x7kt94aciawqb"},{"post_id":"cl1vcnnyk000p7kt9606y5gxc","tag_id":"cl1vcnnyp00167kt96rtrbgjr","_id":"cl1vcnnz000237kt96mhlc42t"},{"post_id":"cl1vcnnyl000t7kt90qu6b00x","tag_id":"cl1vcnnz000227kt9fmp24q74","_id":"cl1vcnnz3002b7kt935kb2s01"},{"post_id":"cl1vcnnym000v7kt9h23ce7f1","tag_id":"cl1vcnnz200287kt92tze1zv4","_id":"cl1vcnnz5002i7kt934z37nry"},{"post_id":"cl1vcnnz3002d7kt9cj5r5rco","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcnnz5002m7kt98gklc3o3"},{"post_id":"cl1vcnnz4002g7kt9391hb0oq","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcnnz7002q7kt9anetffya"},{"post_id":"cl1vcnnyn000y7kt9ftsyfvg2","tag_id":"cl1vcnnz4002f7kt9dkjabg91","_id":"cl1vcnnz7002u7kt92qh43egj"},{"post_id":"cl1vcnnyo00117kt9ag3762r3","tag_id":"cl1vcnnz4002f7kt9dkjabg91","_id":"cl1vcnnz9002z7kt90h603wr8"},{"post_id":"cl1vcnnza00367kt91qqd5mvz","tag_id":"cl1vcnnz4002f7kt9dkjabg91","_id":"cl1vcnnzc003c7kt9er6v16je"},{"post_id":"cl1vcnnyo00147kt9gfj1aoe0","tag_id":"cl1vcnnz8002v7kt90ggd7wqf","_id":"cl1vcnnzd003f7kt9hu1nc8p3"},{"post_id":"cl1vcnnyo00147kt9gfj1aoe0","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcnnze003k7kt988fzgmtj"},{"post_id":"cl1vcnnyp00177kt9dxyz29ux","tag_id":"cl1vcnnz8002v7kt90ggd7wqf","_id":"cl1vcnnzh003t7kt9cqdmf0hz"},{"post_id":"cl1vcnnyp00177kt9dxyz29ux","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcnnzj003w7kt9hjlphg05"},{"post_id":"cl1vcnnyq001a7kt9hkoj8lew","tag_id":"cl1vcnnzg003p7kt9bqsg3quu","_id":"cl1vcnnzk00417kt9a6w98fuu"},{"post_id":"cl1vcnnyt001d7kt924s9ghz4","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzm00487kt96nh3cla1"},{"post_id":"cl1vcnnyt001f7kt9h3zt9bz3","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzo004f7kt9gfy0gfkp"},{"post_id":"cl1vcnnyu001j7kt92j3d6gqk","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzq004m7kt90g034bay"},{"post_id":"cl1vcnnyv001m7kt92417djj1","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzs004t7kt9esad0ksj"},{"post_id":"cl1vcnnyw001p7kt9htw45mxa","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzt00507kt950f54h27"},{"post_id":"cl1vcnnyx001s7kt988sa6mfy","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzv00577kt9cmfd2w77"},{"post_id":"cl1vcnnyy001v7kt9fll11q4v","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzx005e7kt983lc95wt"},{"post_id":"cl1vcnnyy001y7kt96j6750ry","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcnnzy005l7kt92yk17nyy"},{"post_id":"cl1vcnnyz00217kt98y0w51n1","tag_id":"cl1vcnnzx005h7kt95tjr46qf","_id":"cl1vcno00005s7kt9hl6ie7j7"},{"post_id":"cl1vcnnzz005o7kt9haoig9h5","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno01005v7kt9ht7rba4a"},{"post_id":"cl1vcnnz000247kt949vzc52d","tag_id":"cl1vcnnzj003y7kt9bgv8duh1","_id":"cl1vcno0300607kt96n2kcpog"},{"post_id":"cl1vcno02005w7kt95m2ca9ms","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcno0400637kt91cbrahgg"},{"post_id":"cl1vcno0300617kt9a21a76xa","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcno0500687kt91te4eb7d"},{"post_id":"cl1vcnnz100277kt94c2p7v9y","tag_id":"cl1vcno03005z7kt98tsw8ebj","_id":"cl1vcno05006b7kt9gq2ldsw4"},{"post_id":"cl1vcno0400647kt95x5048oj","tag_id":"cl1vcnnyr001c7kt9acxtdcat","_id":"cl1vcno07006f7kt9cg9jg4l5"},{"post_id":"cl1vcnnz200297kt9b7c8116o","tag_id":"cl1vcno0400677kt943qb37c1","_id":"cl1vcno08006i7kt919lx7avr"},{"post_id":"cl1vcno08006g7kt9dmev1y2s","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno09006m7kt99aa2c150"},{"post_id":"cl1vcnnz5002k7kt9fc6rf7oa","tag_id":"cl1vcno07006e7kt97v289c0l","_id":"cl1vcno0a006p7kt93tqqhliq"},{"post_id":"cl1vcnnz6002o7kt9cf1ibtoi","tag_id":"cl1vcno07006e7kt97v289c0l","_id":"cl1vcno0b006v7kt912j2hn93"},{"post_id":"cl1vcnnz8002w7kt94b39alfc","tag_id":"cl1vcno07006e7kt97v289c0l","_id":"cl1vcno0d00717kt9767lfc7o"},{"post_id":"cl1vcnnz900307kt9ce8b0ivr","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno0e00777kt988cc6t5c"},{"post_id":"cl1vcnnz900327kt9ebyge62n","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcno0f007d7kt9e8r18wl4"},{"post_id":"cl1vcnnzb00387kt9e8mo6eex","tag_id":"cl1vcno0f007c7kt96v524qgz","_id":"cl1vcno0i007m7kt99txj4ecs"},{"post_id":"cl1vcnnzc003d7kt9cwys7jdg","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno0l007x7kt9dltogplh"},{"post_id":"cl1vcnnzc003d7kt9cwys7jdg","tag_id":"cl1vcno0j007p7kt97ra0ecc7","_id":"cl1vcno0o00807kt95dn0bbkt"},{"post_id":"cl1vcno0l007v7kt90jzlhazb","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0p00847kt93hzqgvi0"},{"post_id":"cl1vcnnzd003g7kt90ieue2gh","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0q00877kt92fgi0b5d"},{"post_id":"cl1vcno0l007y7kt9hsdjdwre","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0r008b7kt999ff0pt6"},{"post_id":"cl1vcno0o00827kt9h3rwenli","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0r008e7kt9assn3u62"},{"post_id":"cl1vcnnze003l7kt96hgq4rae","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0s008i7kt984oj1p1g"},{"post_id":"cl1vcno0q00857kt9gazu1eec","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0t008m7kt9e9o48rl8"},{"post_id":"cl1vcno0q00897kt939lo4nvc","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno0t008q7kt90rlqazrz"},{"post_id":"cl1vcnnzf003n7kt95ti7bewu","tag_id":"cl1vcno0q00887kt97zc4fgun","_id":"cl1vcno0u008u7kt96ywx636v"},{"post_id":"cl1vcnnzf003n7kt95ti7bewu","tag_id":"cl1vcno0s008h7kt941j27hxm","_id":"cl1vcno0w008x7kt9aobp9vv0"},{"post_id":"cl1vcnnzh003r7kt9a9h7g7ae","tag_id":"cl1vcno0t008p7kt90ycf1e3q","_id":"cl1vcno0w00927kt9ay86bjwo"},{"post_id":"cl1vcnnzi003u7kt9gsww4z1x","tag_id":"cl1vcno0400677kt943qb37c1","_id":"cl1vcno0y00987kt91ntchy5p"},{"post_id":"cl1vcno0x00937kt97hj1dxs7","tag_id":"cl1vcnnyb00047kt980nx5px5","_id":"cl1vcno0z009b7kt9flng5n9z"},{"post_id":"cl1vcno0y00967kt9dxpk1sxw","tag_id":"cl1vcnnyb00047kt980nx5px5","_id":"cl1vcno10009f7kt90gdn6tu1"},{"post_id":"cl1vcnnzk003z7kt97i4j4hv4","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno11009i7kt9hdhwd6xo"},{"post_id":"cl1vcno0y00997kt9c29f118b","tag_id":"cl1vcnnyb00047kt980nx5px5","_id":"cl1vcno12009m7kt935uafrco"},{"post_id":"cl1vcnnzl00427kt937lj9sst","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno13009p7kt9cgzk472c"},{"post_id":"cl1vcno11009g7kt9esm458fl","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno14009t7kt933c4h0ft"},{"post_id":"cl1vcnnzm00467kt95dnz1p3m","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno14009w7kt950ui975p"},{"post_id":"cl1vcno12009n7kt92nvja4mx","tag_id":"cl1vcno0400677kt943qb37c1","_id":"cl1vcno1600a07kt98srh3o1f"},{"post_id":"cl1vcnnzn00497kt91734f6s6","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1700a37kt9dx8550ur"},{"post_id":"cl1vcno14009u7kt93sfa2kde","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno1800a77kt944r7crwm"},{"post_id":"cl1vcnnzn004d7kt9abfj6lxb","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1800aa7kt9esav0mdj"},{"post_id":"cl1vcno1700a57kt99xgw4uaa","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno1900ae7kt99m9e5moy"},{"post_id":"cl1vcno1700a57kt99xgw4uaa","tag_id":"cl1vcnnyd00097kt96fx8duff","_id":"cl1vcno1a00ah7kt9d40w3ri0"},{"post_id":"cl1vcnnzo004g7kt9bajo3xsz","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1b00al7kt95zvmel8a"},{"post_id":"cl1vcno1800a87kt96ombgy8m","tag_id":"cl1vcnnza00347kt9dhhu7huh","_id":"cl1vcno1b00ap7kt966v28g2d"},{"post_id":"cl1vcnnzp004k7kt92utz6fb2","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1c00at7kt91xea0raq"},{"post_id":"cl1vcnnzq004n7kt93kp80967","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1d00aw7kt99wqlg7ll"},{"post_id":"cl1vcnnzr004r7kt90d304k0l","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1e00b07kt90oi68lcu"},{"post_id":"cl1vcnnzs004u7kt9c8xx3frj","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1f00b67kt9afyg5pfm"},{"post_id":"cl1vcnnzs004x7kt96idget6k","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1h00bc7kt91e9fh6w9"},{"post_id":"cl1vcnnzt00517kt977pu01l8","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1h00bg7kt93a4z9aa8"},{"post_id":"cl1vcnnzu00547kt9gbs61pd8","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1i00bk7kt96x9k89vk"},{"post_id":"cl1vcnnzv00587kt96f6y15qc","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1i00bo7kt9e8j02a3x"},{"post_id":"cl1vcnnzw005b7kt9a9um35yf","tag_id":"cl1vcno0c006y7kt9er1wdw6r","_id":"cl1vcno1j00bs7kt91ks0grpu"},{"post_id":"cl1vcnnzx005f7kt98cjuggo2","tag_id":"cl1vcno1i00bq7kt92i18dlwt","_id":"cl1vcno1j00bw7kt9dh5i1r7u"},{"post_id":"cl1vcnnzy005i7kt9cpfv0rra","tag_id":"cl1vcno1i00bq7kt92i18dlwt","_id":"cl1vcno1k00c37kt98v73fchc"},{"post_id":"cl1vcnnzy005i7kt9cpfv0rra","tag_id":"cl1vcno1j00by7kt9fq6j7ifu","_id":"cl1vcno1k00c47kt9hieobjhn"},{"post_id":"cl1vcnnzy005m7kt92m7i2pjh","tag_id":"cl1vcno1i00bq7kt92i18dlwt","_id":"cl1vcno1k00c67kt9dlfs76wd"},{"post_id":"cl1vcno00005t7kt9anh540n9","tag_id":"cl1vcno1k00c57kt96zmzbizq","_id":"cl1vcno1k00c87kt92fr75c56"},{"post_id":"cl1vcno0500697kt98i116rz8","tag_id":"cl1vcno1k00c77kt9e3vnehki","_id":"cl1vcno1l00ca7kt9707p2chv"},{"post_id":"cl1vcno0500697kt98i116rz8","tag_id":"cl1vcnnyr001c7kt9acxtdcat","_id":"cl1vcno1l00cb7kt9e1ta17g8"},{"post_id":"cl1vcno05006c7kt9fp83fnpl","tag_id":"cl1vcno1l00c97kt9dsf5doxp","_id":"cl1vcno1m00cd7kt9480sgqq0"},{"post_id":"cl1vcno08006j7kt93fkb81tl","tag_id":"cl1vcno1k00c77kt9e3vnehki","_id":"cl1vcno1m00cf7kt921m2g3ix"},{"post_id":"cl1vcno09006n7kt9c7z0bt3n","tag_id":"cl1vcno1k00c77kt9e3vnehki","_id":"cl1vcno1m00ci7kt9ac0k7tmo"},{"post_id":"cl1vcno09006n7kt9c7z0bt3n","tag_id":"cl1vcno1m00cg7kt96pvwgs6t","_id":"cl1vcno1m00cj7kt9c9etar54"},{"post_id":"cl1vcno0a006q7kt91fxf6ja1","tag_id":"cl1vcno1m00ch7kt9cilshukf","_id":"cl1vcno1n00cl7kt9hl4kafir"},{"post_id":"cl1vcno0b006t7kt9gzbu7c38","tag_id":"cl1vcno1m00ck7kt9bdb575hj","_id":"cl1vcno1n00cp7kt97vkud6ww"},{"post_id":"cl1vcno0b006t7kt9gzbu7c38","tag_id":"cl1vcno1n00cm7kt98l9s1iew","_id":"cl1vcno1n00cq7kt9d1p33cif"},{"post_id":"cl1vcno0b006t7kt9gzbu7c38","tag_id":"cl1vcno1n00cn7kt9hn2ueq5d","_id":"cl1vcno1o00cs7kt973j9hzug"},{"post_id":"cl1vcno0b006w7kt9ctvdddnt","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1o00ct7kt9hd81fwb9"},{"post_id":"cl1vcno0c006z7kt988alfu9s","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1o00cv7kt9dlgo1qjw"},{"post_id":"cl1vcno0d00727kt97iho2wrx","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1o00cx7kt9g8qo7fg8"},{"post_id":"cl1vcno0d00747kt934xscje4","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1p00cz7kt98qg39it8"},{"post_id":"cl1vcno0e00787kt9399h6c3x","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1p00d17kt96zv719dx"},{"post_id":"cl1vcno0f007a7kt98nxhcvbu","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1p00d37kt95s3b3ept"},{"post_id":"cl1vcno0g007e7kt9e52j50jx","tag_id":"cl1vcno1p00d27kt9bbin8z06","_id":"cl1vcno1p00d57kt9ax1f6hnf"},{"post_id":"cl1vcno0g007g7kt9c1stcdtf","tag_id":"cl1vcno1p00d47kt94uuw86f5","_id":"cl1vcno1q00d77kt959z243p6"},{"post_id":"cl1vcno0h007k7kt9e4cj7jjb","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1q00d97kt90ln27w9f"},{"post_id":"cl1vcno0i007n7kt9e7b7fzzn","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1r00db7kt90zad1iy9"},{"post_id":"cl1vcno0j007q7kt9h79r7oqa","tag_id":"cl1vcno1q00da7kt92ikr0nzx","_id":"cl1vcno1r00dd7kt99zobgsnm"},{"post_id":"cl1vcno0k007s7kt97a0rfmab","tag_id":"cl1vcno1r00dc7kt98fbdab4a","_id":"cl1vcno1r00df7kt946w7bp6o"},{"post_id":"cl1vcno0r008c7kt9gyedhhoa","tag_id":"cl1vcno1r00de7kt92hdu6fh0","_id":"cl1vcno1s00di7kt99c8l2zbz"},{"post_id":"cl1vcno0r008c7kt9gyedhhoa","tag_id":"cl1vcno1r00dg7kt909xv2sbp","_id":"cl1vcno1s00dj7kt9aec08d87"},{"post_id":"cl1vcno0s008f7kt9awvvhfyq","tag_id":"cl1vcno1r00dh7kt91e9bch52","_id":"cl1vcno1s00dl7kt9ewo7enuj"},{"post_id":"cl1vcno0s008k7kt990hoel8z","tag_id":"cl1vcno1s00dk7kt99mkc1o0h","_id":"cl1vcno1s00dn7kt96tmvc8oq"},{"post_id":"cl1vcno0t008n7kt9a17rga93","tag_id":"cl1vcno1r00de7kt92hdu6fh0","_id":"cl1vcno1s00dp7kt9fskj8fjp"},{"post_id":"cl1vcno0u008r7kt9btl5dhzs","tag_id":"cl1vcno1s00do7kt952f7exm0","_id":"cl1vcno1t00ds7kt98ogzgreb"},{"post_id":"cl1vcno0u008r7kt9btl5dhzs","tag_id":"cl1vcno1s00dq7kt9a4bf79r8","_id":"cl1vcno1t00dt7kt976wb1ee7"},{"post_id":"cl1vcno0v008v7kt97alqblc5","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1t00dv7kt99aenhj49"},{"post_id":"cl1vcno0w008z7kt91evi4d5n","tag_id":"cl1vcno1r00de7kt92hdu6fh0","_id":"cl1vcno1u00dy7kt98srree50"},{"post_id":"cl1vcno0w008z7kt91evi4d5n","tag_id":"cl1vcno1r00dg7kt909xv2sbp","_id":"cl1vcno1u00dz7kt9ckrugcpq"},{"post_id":"cl1vcno0z009d7kt9foqi3dul","tag_id":"cl1vcno1u00dx7kt9g37bdl6k","_id":"cl1vcno1u00e17kt9a88wfct3"},{"post_id":"cl1vcno12009k7kt907y77qop","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1v00e47kt9ekab5318"},{"post_id":"cl1vcno12009k7kt907y77qop","tag_id":"cl1vcno1n00co7kt94gs1aq63","_id":"cl1vcno1v00e57kt9azgd8owu"},{"post_id":"cl1vcno13009r7kt91nj24flk","tag_id":"cl1vcnnyl000s7kt93kr595nb","_id":"cl1vcno1v00e77kt93at8b9cr"},{"post_id":"cl1vcno13009r7kt91nj24flk","tag_id":"cl1vcno1v00e37kt94rvyhyz6","_id":"cl1vcno1v00e87kt9ayt71mhj"},{"post_id":"cl1vcno15009y7kt91kn45bf9","tag_id":"cl1vcno1r00de7kt92hdu6fh0","_id":"cl1vcno1w00eb7kt9be12eacg"},{"post_id":"cl1vcno15009y7kt91kn45bf9","tag_id":"cl1vcno1v00e97kt93p139ljd","_id":"cl1vcno1w00ec7kt94iun9a51"},{"post_id":"cl1vcno1600a17kt91xxddepo","tag_id":"cl1vcno1w00ea7kt91dvo3p9l","_id":"cl1vcno1w00ee7kt91syu4dtu"},{"post_id":"cl1vcno1900ac7kt9ckli24fx","tag_id":"cl1vcno1w00ed7kt96vypb0d3","_id":"cl1vcno1x00eh7kt9cglw9r7j"},{"post_id":"cl1vcno1900ac7kt9ckli24fx","tag_id":"cl1vcno1w00ef7kt97pcdbk3b","_id":"cl1vcno1x00ei7kt9cash1j4e"},{"post_id":"cl1vcno1900af7kt91f6vdfhq","tag_id":"cl1vcno1x00eg7kt926yr1lly","_id":"cl1vcno1x00ek7kt9frbaae1v"},{"post_id":"cl1vcno1a00aj7kt90sdj56at","tag_id":"cl1vcno1x00ej7kt9ewqf2lnj","_id":"cl1vcno1y00em7kt9f1y18g3w"},{"post_id":"cl1vcno1b00an7kt94i2w4c4h","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1y00eo7kt9cshnalzl"},{"post_id":"cl1vcno1c00ar7kt91ytx2o67","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno1z00eq7kt939eh0ue1"},{"post_id":"cl1vcno1c00au7kt97emwh7mt","tag_id":"cl1vcno1r00dh7kt91e9bch52","_id":"cl1vcno1z00es7kt92gerb3g8"},{"post_id":"cl1vcno1d00ay7kt95sq5gj2o","tag_id":"cl1vcno1z00er7kt9au1dc3mb","_id":"cl1vcno1z00ev7kt951da8r84"},{"post_id":"cl1vcno1d00ay7kt95sq5gj2o","tag_id":"cl1vcno1r00de7kt92hdu6fh0","_id":"cl1vcno1z00ew7kt9g26h0gn0"},{"post_id":"cl1vcno1e00b17kt9g82ta19v","tag_id":"cl1vcno1z00eu7kt95zl1ecxf","_id":"cl1vcno2000ey7kt98su7dza0"},{"post_id":"cl1vcno1f00b47kt971y37yoj","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno2000f07kt9d5uz8mgt"},{"post_id":"cl1vcno1f00b77kt9b520amiu","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno2000f27kt957kg0ki4"},{"post_id":"cl1vcno1g00ba7kt95m96hoyi","tag_id":"cl1vcno1p00d07kt9dojb7lu3","_id":"cl1vcno2100f37kt9dgak7978"},{"post_id":"cl1vcno2300f47kt93srt1ygq","tag_id":"cl1vcno1x00eg7kt926yr1lly","_id":"cl1vcno2400f67kt9aw224t5z"},{"post_id":"cl1vcno2400f57kt959xqbw35","tag_id":"cl1vcno1x00eg7kt926yr1lly","_id":"cl1vcno2500f87kt93vh23v3j"},{"post_id":"cl1vcno2400f77kt9fjhvbrmd","tag_id":"cl1vcno0k007u7kt9f8w869ik","_id":"cl1vcno2600fb7kt95hxr04f0"},{"post_id":"cl1vcno2500f97kt94qug0h5q","tag_id":"cl1vcno1x00eg7kt926yr1lly","_id":"cl1vcno2600fe7kt91n939dq8"},{"post_id":"cl1vcno2600ff7kt96p3l77is","tag_id":"cl1vcno1x00eg7kt926yr1lly","_id":"cl1vcno2800fk7kt9fr1o9u1z"},{"post_id":"cl1vcno2600fc7kt921oyepz3","tag_id":"cl1vcno2700fh7kt97lp2aa8p","_id":"cl1vcno2800fo7kt9hs82eqy6"},{"post_id":"cl1vcno2700fi7kt99w391hw4","tag_id":"cl1vcno2800fm7kt999sk9i4o","_id":"cl1vcno2800fq7kt9ehwhcrob"}],"Tag":[{"name":"javaagent","_id":"cl1vcnnyb00047kt980nx5px5"},{"name":"多线程","_id":"cl1vcnnyd00097kt96fx8duff"},{"name":"java基础","_id":"cl1vcnnyh000f7kt9fxix2jc1"},{"name":"排序","_id":"cl1vcnnyj000m7kt9dyhn3v7j"},{"name":"java","_id":"cl1vcnnyl000s7kt93kr595nb"},{"name":"DNS","_id":"cl1vcnnyn000z7kt9grqp7ode"},{"name":"Disruptor","_id":"cl1vcnnyp00167kt96rtrbgjr"},{"name":"docker","_id":"cl1vcnnyr001c7kt9acxtdcat"},{"name":"es","_id":"cl1vcnnyu001i7kt9h55j384e"},{"name":"GET/POST","_id":"cl1vcnnz000227kt9fmp24q74"},{"name":"git","_id":"cl1vcnnz200287kt92tze1zv4"},{"name":"hadoop","_id":"cl1vcnnz4002f7kt9dkjabg91"},{"name":"HDFS","_id":"cl1vcnnz8002v7kt90ggd7wqf"},{"name":"HADOOP","_id":"cl1vcnnza00347kt9dhhu7huh"},{"name":"哈希表","_id":"cl1vcnnzg003p7kt9bqsg3quu"},{"name":"hive","_id":"cl1vcnnzj003y7kt9bgv8duh1"},{"name":"httpclient","_id":"cl1vcnnzx005h7kt95tjr46qf"},{"name":"https","_id":"cl1vcno03005z7kt98tsw8ebj"},{"name":"spring","_id":"cl1vcno0400677kt943qb37c1"},{"name":"JVM","_id":"cl1vcno07006e7kt97v289c0l"},{"name":"SpringCloud","_id":"cl1vcno0c006y7kt9er1wdw6r"},{"name":"分布式存储","_id":"cl1vcno0f007c7kt96v524qgz"},{"name":"nacos-config","_id":"cl1vcno0j007p7kt97ra0ecc7"},{"name":"redis","_id":"cl1vcno0k007u7kt9f8w869ik"},{"name":"并发","_id":"cl1vcno0q00887kt97zc4fgun"},{"name":"线程安全","_id":"cl1vcno0s008h7kt941j27hxm"},{"name":"Spark","_id":"cl1vcno0t008p7kt90ycf1e3q"},{"name":"TCP/IP","_id":"cl1vcno1i00bq7kt92i18dlwt"},{"name":"UDP","_id":"cl1vcno1j00by7kt9fq6j7ifu"},{"name":"tomcat","_id":"cl1vcno1k00c57kt96zmzbizq"},{"name":"k8s","_id":"cl1vcno1k00c77kt9e3vnehki"},{"name":"JDK1.8新特性","_id":"cl1vcno1l00c97kt9dsf5doxp"},{"name":"elk","_id":"cl1vcno1m00cg7kt96pvwgs6t"},{"name":"maven","_id":"cl1vcno1m00ch7kt9cilshukf"},{"name":"String","_id":"cl1vcno1m00ck7kt9bdb575hj"},{"name":"StringBuilder","_id":"cl1vcno1n00cm7kt98l9s1iew"},{"name":"StringBuffer","_id":"cl1vcno1n00cn7kt9hn2ueq5d"},{"name":"mysql","_id":"cl1vcno1n00co7kt94gs1aq63"},{"name":"锁","_id":"cl1vcno1p00d07kt9dojb7lu3"},{"name":"伪共享","_id":"cl1vcno1p00d27kt9bbin8z06"},{"name":"信息系统项目管理师","_id":"cl1vcno1p00d47kt94uuw86f5"},{"name":"CAP","_id":"cl1vcno1q00da7kt92ikr0nzx"},{"name":"分布式","_id":"cl1vcno1r00dc7kt98fbdab4a"},{"name":"可信","_id":"cl1vcno1r00de7kt92hdu6fh0"},{"name":"密码学","_id":"cl1vcno1r00dg7kt909xv2sbp"},{"name":"设计模式","_id":"cl1vcno1r00dh7kt91e9bch52"},{"name":"可信计算","_id":"cl1vcno1s00dk7kt99mkc1o0h"},{"name":"可靠","_id":"cl1vcno1s00do7kt952f7exm0"},{"name":"容错","_id":"cl1vcno1s00dq7kt9a4bf79r8"},{"name":"内存模型","_id":"cl1vcno1u00dx7kt9g37bdl6k"},{"name":"数据结构","_id":"cl1vcno1v00e37kt94rvyhyz6"},{"name":"加密算法","_id":"cl1vcno1v00e97kt93p139ljd"},{"name":"文件上传","_id":"cl1vcno1w00ea7kt91dvo3p9l"},{"name":"IO","_id":"cl1vcno1w00ed7kt96vypb0d3"},{"name":"阻塞与非阻塞","_id":"cl1vcno1w00ef7kt97pcdbk3b"},{"name":"python","_id":"cl1vcno1x00eg7kt926yr1lly"},{"name":"线程","_id":"cl1vcno1x00ej7kt9ewqf2lnj"},{"name":"网络安全","_id":"cl1vcno1z00er7kt9au1dc3mb"},{"name":"运算符","_id":"cl1vcno1z00eu7kt95zl1ecxf"},{"name":"分布式锁","_id":"cl1vcno2700fh7kt97lp2aa8p"},{"name":"重构","_id":"cl1vcno2800fm7kt999sk9i4o"}]}}
{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/3-hexo/source/css/gitalk.css","path":"css/gitalk.css","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/mobile.styl","path":"css/mobile.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/gitalk.js","path":"js/gitalk.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/gitment.js","path":"js/gitment.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/iconfont.js","path":"js/iconfont.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","path":"js/jquery.pjax.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/titleTip.js","path":"js/titleTip.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/alipay.jpg","path":"img/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","path":"img/article-list-background.jpeg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/avatar.jpg","path":"img/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/brown-papersq.png","path":"img/brown-papersq.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/gov.png","path":"img/gov.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/weixin.jpg","path":"img/weixin.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/school-book.png","path":"img/school-book.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","path":"css/fonts/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","path":"css/fonts/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","path":"css/fonts/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","path":"css/fonts/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","path":"css/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","path":"css/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","path":"css/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","path":"css/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff2","path":"css/fonts/iconfont.woff2","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/selection.json","path":"css/fonts/selection.json","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","path":"css/hl_theme/atom-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","path":"css/hl_theme/atom-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","path":"css/hl_theme/brown-paper.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","path":"css/hl_theme/darcula.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","path":"css/hl_theme/github-gist.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","path":"css/hl_theme/github.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","path":"css/hl_theme/gruvbox-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","path":"css/hl_theme/gruvbox-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","path":"css/hl_theme/kimbie-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","path":"css/hl_theme/kimbie-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","path":"css/hl_theme/railscasts.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","path":"css/hl_theme/rainbow.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","path":"css/hl_theme/school-book.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","path":"css/hl_theme/sublime.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","path":"css/hl_theme/sunburst.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","path":"css/hl_theme/zenbum.styl","modified":1,"renderable":1},{"_id":"source/img/1337b059.png","path":"img/1337b059.png","modified":1,"renderable":0},{"_id":"source/img/145d5664.png","path":"img/145d5664.png","modified":1,"renderable":0},{"_id":"source/img/1618244002479.png","path":"img/1618244002479.png","modified":1,"renderable":0},{"_id":"source/img/1618244110010.png","path":"img/1618244110010.png","modified":1,"renderable":0},{"_id":"source/img/1618244137853.png","path":"img/1618244137853.png","modified":1,"renderable":0},{"_id":"source/img/1618244073306.png","path":"img/1618244073306.png","modified":1,"renderable":0},{"_id":"source/img/1618244166891.png","path":"img/1618244166891.png","modified":1,"renderable":0},{"_id":"source/img/1618244193984.png","path":"img/1618244193984.png","modified":1,"renderable":0},{"_id":"source/img/1618244361109.png","path":"img/1618244361109.png","modified":1,"renderable":0},{"_id":"source/img/1618244251013.png","path":"img/1618244251013.png","modified":1,"renderable":0},{"_id":"source/img/1618244289269.png","path":"img/1618244289269.png","modified":1,"renderable":0},{"_id":"source/img/1618244387386.png","path":"img/1618244387386.png","modified":1,"renderable":0},{"_id":"source/img/1618244497271.png","path":"img/1618244497271.png","modified":1,"renderable":0},{"_id":"source/img/1618244522945.png","path":"img/1618244522945.png","modified":1,"renderable":0},{"_id":"source/img/1618244544994.png","path":"img/1618244544994.png","modified":1,"renderable":0},{"_id":"source/img/1618244566899.png","path":"img/1618244566899.png","modified":1,"renderable":0},{"_id":"source/img/1618244592782.png","path":"img/1618244592782.png","modified":1,"renderable":0},{"_id":"source/img/1618244623527.png","path":"img/1618244623527.png","modified":1,"renderable":0},{"_id":"source/img/1618295759372.png","path":"img/1618295759372.png","modified":1,"renderable":0},{"_id":"source/img/227d0458.png","path":"img/227d0458.png","modified":1,"renderable":0},{"_id":"source/img/480c47d9.png","path":"img/480c47d9.png","modified":1,"renderable":0},{"_id":"source/img/4e12cc06.png","path":"img/4e12cc06.png","modified":1,"renderable":0},{"_id":"source/img/5a377b3b.png","path":"img/5a377b3b.png","modified":1,"renderable":0},{"_id":"source/img/5a824e2f.png","path":"img/5a824e2f.png","modified":1,"renderable":0},{"_id":"source/img/70db7f87.jpg","path":"img/70db7f87.jpg","modified":1,"renderable":0},{"_id":"source/img/743e6cec.jpg","path":"img/743e6cec.jpg","modified":1,"renderable":0},{"_id":"source/img/7f3f75ca.png","path":"img/7f3f75ca.png","modified":1,"renderable":0},{"_id":"source/img/8fb0cd0c.png","path":"img/8fb0cd0c.png","modified":1,"renderable":0},{"_id":"source/img/8ab72b98.png","path":"img/8ab72b98.png","modified":1,"renderable":0},{"_id":"source/img/9cb319ab.png","path":"img/9cb319ab.png","modified":1,"renderable":0},{"_id":"source/img/DNS1.png","path":"img/DNS1.png","modified":1,"renderable":0},{"_id":"source/img/DNS2.png","path":"img/DNS2.png","modified":1,"renderable":0},{"_id":"source/img/GETPOST.png","path":"img/GETPOST.png","modified":1,"renderable":0},{"_id":"source/img/Git工作流程.png","path":"img/Git工作流程.png","modified":1,"renderable":0},{"_id":"source/img/HDFS-liucheng.png","path":"img/HDFS-liucheng.png","modified":1,"renderable":0},{"_id":"source/img/HDFS上传.png","path":"img/HDFS上传.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS1.png","path":"img/HTTPS1.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS2.png","path":"img/HTTPS2.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS3.png","path":"img/HTTPS3.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS4.png","path":"img/HTTPS4.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS5.png","path":"img/HTTPS5.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS6.png","path":"img/HTTPS6.png","modified":1,"renderable":0},{"_id":"source/img/Hive-运算2.png","path":"img/Hive-运算2.png","modified":1,"renderable":0},{"_id":"source/img/IO复用select模型.png","path":"img/IO复用select模型.png","modified":1,"renderable":0},{"_id":"source/img/JVM Memory.png","path":"img/JVM Memory.png","modified":1,"renderable":0},{"_id":"source/img/JVM类加载过程.png","path":"img/JVM类加载过程.png","modified":1,"renderable":0},{"_id":"source/img/Socket通讯模型.png","path":"img/Socket通讯模型.png","modified":1,"renderable":0},{"_id":"source/img/Spark.png","path":"img/Spark.png","modified":1,"renderable":0},{"_id":"source/img/StringBuilder.png","path":"img/StringBuilder.png","modified":1,"renderable":0},{"_id":"source/img/StringStringBuilderStringBuffer.png","path":"img/StringStringBuilderStringBuffer.png","modified":1,"renderable":0},{"_id":"source/img/SpringBean3.png","path":"img/SpringBean3.png","modified":1,"renderable":0},{"_id":"source/img/StringUpdate.png","path":"img/StringUpdate.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP服务器接收请求.png","path":"img/TCPIP服务器接收请求.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP模型.png","path":"img/TCPIP模型.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP用户发送请求.png","path":"img/TCPIP用户发送请求.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程1.png","path":"img/TCP协议通讯过程1.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程.png","path":"img/TCP协议通讯过程.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程2.png","path":"img/TCP协议通讯过程2.png","modified":1,"renderable":0},{"_id":"source/img/ThreadLocal内部存储.png","path":"img/ThreadLocal内部存储.png","modified":1,"renderable":0},{"_id":"source/img/Yarn.png","path":"img/Yarn.png","modified":1,"renderable":0},{"_id":"source/img/UDPHeader.png","path":"img/UDPHeader.png","modified":1,"renderable":0},{"_id":"source/img/UDPTCPcompare.png","path":"img/UDPTCPcompare.png","modified":1,"renderable":0},{"_id":"source/img/a7ade6c9.png","path":"img/a7ade6c9.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime.png","path":"img/agent-costtime.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime2.png","path":"img/agent-costtime2.png","modified":1,"renderable":0},{"_id":"source/img/b235f114.png","path":"img/b235f114.png","modified":1,"renderable":0},{"_id":"source/img/b4d68165.png","path":"img/b4d68165.png","modified":1,"renderable":0},{"_id":"source/img/chartype.png","path":"img/chartype.png","modified":1,"renderable":0},{"_id":"source/img/clip_image002.png","path":"img/clip_image002.png","modified":1,"renderable":0},{"_id":"source/img/clip_image004.png","path":"img/clip_image004.png","modified":1,"renderable":0},{"_id":"source/img/d94cd34e.png","path":"img/d94cd34e.png","modified":1,"renderable":0},{"_id":"source/img/data-collect.png","path":"img/data-collect.png","modified":1,"renderable":0},{"_id":"source/img/d325d29a.png","path":"img/d325d29a.png","modified":1,"renderable":0},{"_id":"source/img/data-collect-analysis.png","path":"img/data-collect-analysis.png","modified":1,"renderable":0},{"_id":"source/img/dataSource-behaviour-relative.png","path":"img/dataSource-behaviour-relative.png","modified":1,"renderable":0},{"_id":"source/img/e18c9709.jpg","path":"img/e18c9709.jpg","modified":1,"renderable":0},{"_id":"source/img/ea179a09.png","path":"img/ea179a09.png","modified":1,"renderable":0},{"_id":"source/img/es2.png","path":"img/es2.png","modified":1,"renderable":0},{"_id":"source/img/fac717a5.png","path":"img/fac717a5.png","modified":1,"renderable":0},{"_id":"source/img/es1.png","path":"img/es1.png","modified":1,"renderable":0},{"_id":"source/img/fb523dd6.png","path":"img/fb523dd6.png","modified":1,"renderable":0},{"_id":"source/img/floattype.png","path":"img/floattype.png","modified":1,"renderable":0},{"_id":"source/img/hdfs-read-file.png","path":"img/hdfs-read-file.png","modified":1,"renderable":0},{"_id":"source/img/hdfs.png","path":"img/hdfs.png","modified":1,"renderable":0},{"_id":"source/img/hive-partition.png","path":"img/hive-partition.png","modified":1,"renderable":0},{"_id":"source/img/hdfs-write-file.png","path":"img/hdfs-write-file.png","modified":1,"renderable":0},{"_id":"source/img/hdfs创建文件夹.png","path":"img/hdfs创建文件夹.png","modified":1,"renderable":0},{"_id":"source/img/hive-数学函数.png","path":"img/hive-数学函数.png","modified":1,"renderable":0},{"_id":"source/img/hive-算数运算符.png","path":"img/hive-算数运算符.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合2.png","path":"img/hive-聚合2.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合1.png","path":"img/hive-聚合1.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合3.png","path":"img/hive-聚合3.png","modified":1,"renderable":0},{"_id":"source/img/hive-表生成函数.png","path":"img/hive-表生成函数.png","modified":1,"renderable":0},{"_id":"source/img/hive-运算3.png","path":"img/hive-运算3.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构.png","path":"img/hive数据结构.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构1.png","path":"img/hive数据结构1.png","modified":1,"renderable":0},{"_id":"source/img/hive运算1.png","path":"img/hive运算1.png","modified":1,"renderable":0},{"_id":"source/img/hive文本文件数据编码.png","path":"img/hive文本文件数据编码.png","modified":1,"renderable":0},{"_id":"source/img/hive集合数据类型.png","path":"img/hive集合数据类型.png","modified":1,"renderable":0},{"_id":"source/img/https-intro.png","path":"img/https-intro.png","modified":1,"renderable":0},{"_id":"source/img/image-20200117113506023.png","path":"img/image-20200117113506023.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115459406.png","path":"img/image-20201206115459406.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115600801.png","path":"img/image-20201206115600801.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115653987.png","path":"img/image-20201206115653987.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115738831.png","path":"img/image-20201206115738831.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206115832025.png","path":"img/image-20201206115832025.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121627187.png","path":"img/image-20201206121627187.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121711716.png","path":"img/image-20201206121711716.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121650739.png","path":"img/image-20201206121650739.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206121829515.png","path":"img/image-20201206121829515.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214114013294.png","path":"img/image-20201214114013294.png","modified":1,"renderable":0},{"_id":"source/img/image-20201206122126739.png","path":"img/image-20201206122126739.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121332594.png","path":"img/image-20201214121332594.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121613146.png","path":"img/image-20201214121613146.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121645728.png","path":"img/image-20201214121645728.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214121721100.png","path":"img/image-20201214121721100.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214122255720.png","path":"img/image-20201214122255720.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214123508379.png","path":"img/image-20201214123508379.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214124300675.png","path":"img/image-20201214124300675.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125020584.png","path":"img/image-20201214125020584.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125032862.png","path":"img/image-20201214125032862.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125402694.png","path":"img/image-20201214125402694.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214124603810.png","path":"img/image-20201214124603810.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125453643.png","path":"img/image-20201214125453643.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125504552.png","path":"img/image-20201214125504552.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214125529023.png","path":"img/image-20201214125529023.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214130757812.png","path":"img/image-20201214130757812.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131527522.png","path":"img/image-20201214131527522.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131543066.png","path":"img/image-20201214131543066.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131734289.png","path":"img/image-20201214131734289.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131814678.png","path":"img/image-20201214131814678.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131948506.png","path":"img/image-20201214131948506.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214131847691.png","path":"img/image-20201214131847691.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132014480.png","path":"img/image-20201214132014480.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132127621.png","path":"img/image-20201214132127621.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132215125.png","path":"img/image-20201214132215125.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132059091.png","path":"img/image-20201214132059091.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132254636.png","path":"img/image-20201214132254636.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132309723.png","path":"img/image-20201214132309723.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132326043.png","path":"img/image-20201214132326043.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132343909.png","path":"img/image-20201214132343909.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132401207.png","path":"img/image-20201214132401207.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214132416329.png","path":"img/image-20201214132416329.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133601256.png","path":"img/image-20201214133601256.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133612299.png","path":"img/image-20201214133612299.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133830361.png","path":"img/image-20201214133830361.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133935704.png","path":"img/image-20201214133935704.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214133955605.png","path":"img/image-20201214133955605.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134132982.png","path":"img/image-20201214134132982.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134208507.png","path":"img/image-20201214134208507.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134235050.png","path":"img/image-20201214134235050.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214134902337.png","path":"img/image-20201214134902337.png","modified":1,"renderable":0},{"_id":"source/img/image-20201214135053884.png","path":"img/image-20201214135053884.png","modified":1,"renderable":0},{"_id":"source/img/image-20210415193556837.png","path":"img/image-20210415193556837.png","modified":1,"renderable":0},{"_id":"source/img/inttype.png","path":"img/inttype.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储.png","path":"img/java对象存储.png","modified":1,"renderable":0},{"_id":"source/img/javaagent1.png","path":"img/javaagent1.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储3.png","path":"img/java对象存储3.png","modified":1,"renderable":0},{"_id":"source/img/jvm1.png","path":"img/jvm1.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储2.png","path":"img/java对象存储2.png","modified":1,"renderable":0},{"_id":"source/img/jvm3.png","path":"img/jvm3.png","modified":1,"renderable":0},{"_id":"source/img/jvm2.png","path":"img/jvm2.png","modified":1,"renderable":0},{"_id":"source/img/jvmHeapStructure.png","path":"img/jvmHeapStructure.png","modified":1,"renderable":0},{"_id":"source/img/maven配置.png","path":"img/maven配置.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序1.png","path":"img/mysql排序1.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序3.png","path":"img/mysql排序3.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序2.png","path":"img/mysql排序2.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序4.png","path":"img/mysql排序4.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序5.png","path":"img/mysql排序5.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序6.png","path":"img/mysql排序6.png","modified":1,"renderable":0},{"_id":"source/img/mysql时间存储.png","path":"img/mysql时间存储.png","modified":1,"renderable":0},{"_id":"source/img/mysql的ip存储.png","path":"img/mysql的ip存储.png","modified":1,"renderable":0},{"_id":"source/img/nacos-producer.png","path":"img/nacos-producer.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud1.png","path":"img/nacos-springCloud1.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud2.png","path":"img/nacos-springCloud2.png","modified":1,"renderable":0},{"_id":"source/img/nacos1.png","path":"img/nacos1.png","modified":1,"renderable":0},{"_id":"source/img/output_11_0.png","path":"img/output_11_0.png","modified":1,"renderable":0},{"_id":"source/img/output_11_1.png","path":"img/output_11_1.png","modified":1,"renderable":0},{"_id":"source/img/output_11_111.png","path":"img/output_11_111.png","modified":1,"renderable":0},{"_id":"source/img/output_13_0.png","path":"img/output_13_0.png","modified":1,"renderable":0},{"_id":"source/img/output_13_011.png","path":"img/output_13_011.png","modified":1,"renderable":0},{"_id":"source/img/output_13_1.png","path":"img/output_13_1.png","modified":1,"renderable":0},{"_id":"source/img/output_13_111.png","path":"img/output_13_111.png","modified":1,"renderable":0},{"_id":"source/img/output_14_0.png","path":"img/output_14_0.png","modified":1,"renderable":0},{"_id":"source/img/output_15_0.png","path":"img/output_15_0.png","modified":1,"renderable":0},{"_id":"source/img/output_15_011.png","path":"img/output_15_011.png","modified":1,"renderable":0},{"_id":"source/img/output_17_0.png","path":"img/output_17_0.png","modified":1,"renderable":0},{"_id":"source/img/output_19_0.png","path":"img/output_19_0.png","modified":1,"renderable":0},{"_id":"source/img/output_20_1.png","path":"img/output_20_1.png","modified":1,"renderable":0},{"_id":"source/img/output_22_0.png","path":"img/output_22_0.png","modified":1,"renderable":0},{"_id":"source/img/output_26_1.png","path":"img/output_26_1.png","modified":1,"renderable":0},{"_id":"source/img/output_26_111.png","path":"img/output_26_111.png","modified":1,"renderable":0},{"_id":"source/img/output_29_0.png","path":"img/output_29_0.png","modified":1,"renderable":0},{"_id":"source/img/output_31_0.png","path":"img/output_31_0.png","modified":1,"renderable":0},{"_id":"source/img/output_32_1.png","path":"img/output_32_1.png","modified":1,"renderable":0},{"_id":"source/img/output_32_111.png","path":"img/output_32_111.png","modified":1,"renderable":0},{"_id":"source/img/output_33_1.png","path":"img/output_33_1.png","modified":1,"renderable":0},{"_id":"source/img/output_33_111.png","path":"img/output_33_111.png","modified":1,"renderable":0},{"_id":"source/img/output_34_1.png","path":"img/output_34_1.png","modified":1,"renderable":0},{"_id":"source/img/output_36_1.png","path":"img/output_36_1.png","modified":1,"renderable":0},{"_id":"source/img/output_42_0.png","path":"img/output_42_0.png","modified":1,"renderable":0},{"_id":"source/img/output_40_0.png","path":"img/output_40_0.png","modified":1,"renderable":0},{"_id":"source/img/output_44_0.png","path":"img/output_44_0.png","modified":1,"renderable":0},{"_id":"source/img/output_46_0.png","path":"img/output_46_0.png","modified":1,"renderable":0},{"_id":"source/img/output_52_0.png","path":"img/output_52_0.png","modified":1,"renderable":0},{"_id":"source/img/output_54_0.png","path":"img/output_54_0.png","modified":1,"renderable":0},{"_id":"source/img/output_8_0.png","path":"img/output_8_0.png","modified":1,"renderable":0},{"_id":"source/img/output_8_1.png","path":"img/output_8_1.png","modified":1,"renderable":0},{"_id":"source/img/output_8_111.png","path":"img/output_8_111.png","modified":1,"renderable":0},{"_id":"source/img/pasted-0.png","path":"img/pasted-0.png","modified":1,"renderable":0},{"_id":"source/img/secondaryNameNode.jpg","path":"img/secondaryNameNode.jpg","modified":1,"renderable":0},{"_id":"source/img/select、epoll模型对比.png","path":"img/select、epoll模型对比.png","modified":1,"renderable":0},{"_id":"source/img/simpleDateFormat-alibaba.png","path":"img/simpleDateFormat-alibaba.png","modified":1,"renderable":0},{"_id":"source/img/spark+hdfs.png","path":"img/spark+hdfs.png","modified":1,"renderable":0},{"_id":"source/img/spark-all.png","path":"img/spark-all.png","modified":1,"renderable":0},{"_id":"source/img/transaction.png","path":"img/transaction.png","modified":1,"renderable":0},{"_id":"source/img/typetrans.png","path":"img/typetrans.png","modified":1,"renderable":0},{"_id":"source/img/user-behaviour.png","path":"img/user-behaviour.png","modified":1,"renderable":0},{"_id":"source/img/user-log.png","path":"img/user-log.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-map.png","path":"img/wordcount-map.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-reduce.png","path":"img/wordcount-reduce.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-split.png","path":"img/wordcount-split.png","modified":1,"renderable":0},{"_id":"source/img/wordcount.png","path":"img/wordcount.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议2.png","path":"img/三次握手协议2.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议1.png","path":"img/三次握手协议1.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议3.png","path":"img/三次握手协议3.png","modified":1,"renderable":0},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","path":"img/主动免疫可信架构信任链传递示意图.png","modified":1,"renderable":0},{"_id":"source/img/使用协议进行通讯.png","path":"img/使用协议进行通讯.png","modified":1,"renderable":0},{"_id":"source/img/信任链.png","path":"img/信任链.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥1.png","path":"img/公钥私钥1.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥10.png","path":"img/公钥私钥10.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥12.png","path":"img/公钥私钥12.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥11.png","path":"img/公钥私钥11.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥2.png","path":"img/公钥私钥2.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥13.png","path":"img/公钥私钥13.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥3.png","path":"img/公钥私钥3.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥4.png","path":"img/公钥私钥4.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥5.png","path":"img/公钥私钥5.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥6.png","path":"img/公钥私钥6.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥7.png","path":"img/公钥私钥7.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥8.png","path":"img/公钥私钥8.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥9.png","path":"img/公钥私钥9.png","modified":1,"renderable":0},{"_id":"source/img/加密算法.png","path":"img/加密算法.png","modified":1,"renderable":0},{"_id":"source/img/可信在云平台的基础架构.png","path":"img/可信在云平台的基础架构.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议.png","path":"img/四次挥手协议.png","modified":1,"renderable":0},{"_id":"source/img/可信根.png","path":"img/可信根.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议2.png","path":"img/四次挥手协议2.png","modified":1,"renderable":0},{"_id":"source/img/对象存储1.png","path":"img/对象存储1.png","modified":1,"renderable":0},{"_id":"source/img/对称加密算法.png","path":"img/对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩1.png","path":"img/指针压缩1.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩2.png","path":"img/指针压缩2.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩3.png","path":"img/指针压缩3.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩4.png","path":"img/指针压缩4.png","modified":1,"renderable":0},{"_id":"source/img/数据库分布式ID生成.png","path":"img/数据库分布式ID生成.png","modified":1,"renderable":0},{"_id":"source/img/椭圆曲线算法的基本原理.png","path":"img/椭圆曲线算法的基本原理.png","modified":1,"renderable":0},{"_id":"source/img/混合加密的方式.png","path":"img/混合加密的方式.png","modified":1,"renderable":0},{"_id":"source/img/线程相关1.jpg","path":"img/线程相关1.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关2.jpg","path":"img/线程相关2.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关3.jpg","path":"img/线程相关3.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关4.jpg","path":"img/线程相关4.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关5.jpg","path":"img/线程相关5.jpg","modified":1,"renderable":0},{"_id":"source/img/网络连接模型.png","path":"img/网络连接模型.png","modified":1,"renderable":0},{"_id":"source/img/读写锁.png","path":"img/读写锁.png","modified":1,"renderable":0},{"_id":"source/img/锁的创建.png","path":"img/锁的创建.png","modified":1,"renderable":0},{"_id":"source/img/锁的创建2.png","path":"img/锁的创建2.png","modified":1,"renderable":0},{"_id":"source/img/阻塞IO.png","path":"img/阻塞IO.png","modified":1,"renderable":0},{"_id":"source/img/阻塞与非阻塞调用对比.png","path":"img/阻塞与非阻塞调用对比.png","modified":1,"renderable":0},{"_id":"source/img/雪花算法.png","path":"img/雪花算法.png","modified":1,"renderable":0},{"_id":"source/img/非对称加密算法.png","path":"img/非对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/非阻塞IO.png","path":"img/非阻塞IO.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_discarded/hello-world.md","hash":"3de445ce428eb9d987b765b94bd6e0428eaebdbc","modified":1567305094926},{"_id":"source/_drafts/信息系统项目管理师-信息化.md","hash":"232e4854291552bb30925fc583e601c134da6dfe","modified":1618654518587},{"_id":"source/img/145d5664.png","hash":"5de6263d58ac723c22250323e760162284317a71","modified":1618295475379},{"_id":"source/img/1618244110010.png","hash":"947439297da765710fa815ea74e0920af546e46b","modified":1618244112681},{"_id":"source/img/1618244166891.png","hash":"5fbb1c37f8bc0d5a9d12e931dbf7b24da9d180ce","modified":1618244169653},{"_id":"source/img/1618244251013.png","hash":"a4a3edf1be8118db34c7d6ccd951dbcfe29ad50c","modified":1618244253272},{"_id":"source/img/1618244289269.png","hash":"3505425f677bf63c3351d9bb723193659be5f0f1","modified":1618244291354},{"_id":"source/img/1618244387386.png","hash":"f57bf41422b093082b4b2f4b42afa9488f0bb8de","modified":1618244393658},{"_id":"source/img/1618244497271.png","hash":"6e3f32bbe2c044ddbdfde51fda828ed6eb4190c4","modified":1618244499346},{"_id":"source/img/1618244522945.png","hash":"3484b30a26d62ae602e3b7a6faf8b7b49bab0295","modified":1618244528607},{"_id":"source/img/1618244566899.png","hash":"7a5274a43d170806b63e4f3457c2f0af774a86cd","modified":1618244569094},{"_id":"source/img/227d0458.png","hash":"2840cba30e73d089195ec8d60e2bd567e96485e6","modified":1618295461056},{"_id":"source/img/480c47d9.png","hash":"d1b8b8bd0eb74760f8c54dee986e6fef466d6b6e","modified":1618297710569},{"_id":"source/img/4e12cc06.png","hash":"750185f3c87805cafe374b9ed6c6b4d6909c8d3a","modified":1618295452515},{"_id":"source/img/5a377b3b.png","hash":"7a8b1a4998362f443f0989fd88d2424a93adb05e","modified":1618295415549},{"_id":"source/img/5a824e2f.png","hash":"7a187c1968dcd312f4480515f5ddb7b8a0815774","modified":1618297733007},{"_id":"source/img/70db7f87.jpg","hash":"354e54d0f10bf2d4bb92ad58e51a542d66095e2c","modified":1618296481441},{"_id":"source/img/743e6cec.jpg","hash":"eda743b32da786d5287d69c54f66600b02dfb48a","modified":1618296489447},{"_id":"source/img/8fb0cd0c.png","hash":"3656d384f8348607e72d6ddb69f23ec7e3e1801b","modified":1618297719517},{"_id":"source/img/Git工作流程.png","hash":"13f5329292c5f0b42cb969b2981141981bc62c12","modified":1567048479116},{"_id":"source/img/JVM Memory.png","hash":"053217d209af791dd65e97e321268cf17df92275","modified":1601179028295},{"_id":"source/img/JVM类加载过程.png","hash":"aa87f840b42869db7b76bd2789d1eaa8dcda2d0f","modified":1601178007056},{"_id":"source/img/StringBuilder.png","hash":"eaa519848fcd7948c1820e5e2a2f2ad719277708","modified":1595317529009},{"_id":"source/img/StringStringBuilderStringBuffer.png","hash":"d836b560a0cf006cfa6d8d833c3453f131db7c12","modified":1595317835896},{"_id":"source/img/TCPIP服务器接收请求.png","hash":"dd6986ec9df5f3ffe5ff743e97408b53147743fc","modified":1567149345292},{"_id":"source/img/TCPIP用户发送请求.png","hash":"f4428ca4bf21e4d067da5b944bb742892939d131","modified":1567149324326},{"_id":"source/img/UDPHeader.png","hash":"03991c09cb8a800bc68acc974430c1d6df3073d2","modified":1595290558000},{"_id":"source/img/UDPTCPcompare.png","hash":"fc2c1380a717ad5e1ac5a85250d26d21e56a9f92","modified":1595290931067},{"_id":"source/img/a7ade6c9.png","hash":"f3c6bb2c3cf9073b99b71d5ed8c48bf7d9f85d77","modified":1618295438850},{"_id":"source/img/b235f114.png","hash":"605d8626ed4d395ea711cb82a141c2ce1e36f4d8","modified":1618295481837},{"_id":"source/img/b4d68165.png","hash":"5d7c542053cb704983fde9b21bc7cd74c9053628","modified":1618295489652},{"_id":"source/img/clip_image002.png","hash":"773f0b16657a294e5b51f44c9adc7cf0d4813728","modified":1573721616774},{"_id":"source/img/d94cd34e.png","hash":"d29a3d8cbf07390c741d3b0a1706461fc85e6590","modified":1618295468781},{"_id":"source/img/d325d29a.png","hash":"962a746a65c34d0e4ea2b5a3e9d23a30f1d35201","modified":1618295508050},{"_id":"source/img/dataSource-behaviour-relative.png","hash":"702859e0f00906daf67b67515b0bd683148f8109","modified":1577073211083},{"_id":"source/img/e18c9709.jpg","hash":"06375b48ff834c994e894405ee00742bb12d962d","modified":1618297696888},{"_id":"source/img/ea179a09.png","hash":"145556ada619dcda0eb7f503930ea65ef73b3465","modified":1618297726285},{"_id":"source/img/es2.png","hash":"07f92f31d703b5659cf537a5b31b2ff9c55cbdc1","modified":1594782445385},{"_id":"source/img/fac717a5.png","hash":"793349e13d727eef0ec51f496e78116cdb7604ca","modified":1618297233871},{"_id":"source/img/es1.png","hash":"89281b1bef0d51742b32b70b774525c6ae08c95f","modified":1594782221769},{"_id":"source/img/fb523dd6.png","hash":"cf674b0aa3f9fedd73a0e9501121aaeadce2f834","modified":1618297679279},{"_id":"source/img/floattype.png","hash":"a7c29f3c863dfe4b4f3ea2ccea6b35002b7eb7ba","modified":1578294604312},{"_id":"source/img/hdfs创建文件夹.png","hash":"27daebbd4cf39ca74ffa88aa9865257de46beeb2","modified":1607226754825},{"_id":"source/img/hive-聚合2.png","hash":"69ab607095185fb63a23851f371ac09c3261728a","modified":1579415860533},{"_id":"source/img/https-intro.png","hash":"14850fa8463ff0892e5258b5884b4e92cb66f481","modified":1607224523988},{"_id":"source/img/image-20201206115459406.png","hash":"b459dcbaa97aba5f099acae0f39145ad3daa61e5","modified":1607226904134},{"_id":"source/img/image-20201206115832025.png","hash":"83d8dac7f6b2b8c02031d46c5d9a8748d2242228","modified":1607227118026},{"_id":"source/img/image-20201206121627187.png","hash":"071a741e5d52a45d3ca56dccb5af33a190597f7f","modified":1607228191278},{"_id":"source/img/image-20201206121711716.png","hash":"e70f981b0e2661a084fe730e068bc889b4efb7ac","modified":1607228236858},{"_id":"source/img/image-20201206121650739.png","hash":"2ad0274ed10bb42df9bddc574b430e023858e661","modified":1607228215512},{"_id":"source/img/image-20201214114013294.png","hash":"b88b3083b0bf5a8ca307695fc265dd07fd2e9bd9","modified":1607917220551},{"_id":"source/img/image-20201214121332594.png","hash":"4ab43a767fa96e721de32a3084870e7ef785bc8a","modified":1607919216748},{"_id":"source/img/image-20201214121613146.png","hash":"0855b48f6f372b5f20905d273cc9d780ea2c19dc","modified":1607919376721},{"_id":"source/img/image-20201214121645728.png","hash":"fdea15c1fff089408871ff3150b2c8b6c107173a","modified":1607919409425},{"_id":"source/img/image-20201214121721100.png","hash":"feade0b22e88fe8bdf2d97cf7d939f39b3466735","modified":1607919443737},{"_id":"source/img/image-20201214122255720.png","hash":"0238a48f3dcd45892e5263bda2e67d455ecdc631","modified":1607919778587},{"_id":"source/img/image-20201214124300675.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1607920983411},{"_id":"source/img/image-20201214125020584.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1607921423736},{"_id":"source/img/image-20201214125032862.png","hash":"2d9a0431305510a118e7136455247f2918f68708","modified":1607921435928},{"_id":"source/img/image-20201214124603810.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1607921167502},{"_id":"source/img/image-20201214125453643.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1607921695961},{"_id":"source/img/image-20201214125504552.png","hash":"0a1ac24eef9da3dbdb800c4d46175abc60eba89a","modified":1607921706939},{"_id":"source/img/image-20201214125529023.png","hash":"82d57e9f373789317595d49aa2e379260acb5a9f","modified":1607921731648},{"_id":"source/img/image-20201214130757812.png","hash":"5d935f2e74081b23dff7c75ce43b3f088906ece6","modified":1607922480980},{"_id":"source/img/image-20201214131734289.png","hash":"f63b13d6700ce20d9e476e880b0ef1d7f230e249","modified":1607923056477},{"_id":"source/img/image-20201214131814678.png","hash":"e86681a9d954b4ab9ceeccef85949166532d22d7","modified":1607923098001},{"_id":"source/img/image-20201214131948506.png","hash":"8eb5e9f7c9d9f5f47e19509c702632a8ee881711","modified":1607923190772},{"_id":"source/img/image-20201214131847691.png","hash":"8c841533dd0f0e9d2965695be830eac48c571f04","modified":1607923130344},{"_id":"source/img/image-20201214132014480.png","hash":"692652f4e430312e6d6a3414304b0c64e46d6d36","modified":1607923216526},{"_id":"source/img/image-20201214132127621.png","hash":"5e48bbea9baa60cb25cdff8a3b5ce28a0f0b935f","modified":1607923290211},{"_id":"source/img/image-20201214132215125.png","hash":"fedaacf9919b879a2539c2faacb748456b0b3e62","modified":1607923338029},{"_id":"source/img/image-20201214132059091.png","hash":"0e9f8c4f95304e59cc8af6adff897b0656925bea","modified":1607923261334},{"_id":"source/img/image-20201214132309723.png","hash":"9ee9276322c6160b294b49127b0ccfdd31006d6a","modified":1607923391880},{"_id":"source/img/image-20201214132326043.png","hash":"e2fcc5164dcfdd9125fd0f2b2a785de9c8668c3c","modified":1607923408522},{"_id":"source/img/image-20201214132343909.png","hash":"90f9e923863dcdeddfe91f1794c42e904011d33d","modified":1607923426042},{"_id":"source/img/image-20201214132401207.png","hash":"29faa2ade6819a751e21d2ed734326369ba67afb","modified":1607923443812},{"_id":"source/img/image-20201214132416329.png","hash":"06b6ecf11a65a2616a7ba9b317bc4efb0dd2fb9c","modified":1607923458837},{"_id":"source/img/image-20201214133601256.png","hash":"30359512fcf46fca69e040fd4446ea477b5c2646","modified":1607924163960},{"_id":"source/img/image-20201214133935704.png","hash":"53cb6b400f9247b609d3d6df67a79f65eabe1f55","modified":1607924377755},{"_id":"source/img/image-20201214133955605.png","hash":"df38a2d88ed44358447df314b1b05cb762fb3e34","modified":1607924397869},{"_id":"source/img/image-20201214134132982.png","hash":"4f48f02bee07e0e6be8cd93a463a70750abf12f6","modified":1607924495896},{"_id":"source/img/image-20201214134208507.png","hash":"a2ad991b50538e8a070fd5b390cdafd800c3affd","modified":1607924531059},{"_id":"source/img/image-20201214135053884.png","hash":"f6c523e663d41c1df848333892389670db4b14e5","modified":1607925056342},{"_id":"source/img/java对象存储.png","hash":"78231e4a1c56b2078cdd4454d20593ba3e2b48e3","modified":1567229269140},{"_id":"source/img/javaagent1.png","hash":"93c2b34bbbbb8ce1fccb0d0eb0daff97a7a2bdac","modified":1594963356047},{"_id":"source/img/java对象存储3.png","hash":"4b83f6c1accd8ea2753e0dc16f52820d52abb700","modified":1574250991794},{"_id":"source/img/java对象存储2.png","hash":"a5e540b6699183dcad89a9912c25c6a0c5eed9f3","modified":1574250903564},{"_id":"source/img/maven配置.png","hash":"c966dcfb8e36138b51cbaa08a8e15bc73a615778","modified":1567045274281},{"_id":"source/img/mysql排序1.png","hash":"09e8d6d507cd2b06ed4e6e9a76c832e226981ed0","modified":1574253764887},{"_id":"source/img/mysql排序2.png","hash":"0f0e1c1417f85b8e5f7bf1bb929ad63223569faf","modified":1574253839369},{"_id":"source/img/mysql排序4.png","hash":"9ef66bb2173d553fb58106a4959b3c0a5751d482","modified":1574254046965},{"_id":"source/img/mysql排序5.png","hash":"044b6bc8c3b4d33f3a25598b9b99771efe75bccf","modified":1574254222538},{"_id":"source/img/mysql排序6.png","hash":"254e269dc8da6b5358e883e82a16c52243a4286e","modified":1574254481724},{"_id":"source/img/nacos-producer.png","hash":"36eefd319d40a5a11e9343d3d817d66098d9d043","modified":1575358053823},{"_id":"source/img/nacos-springCloud1.png","hash":"afd0806eb1d72ed4988811e08d1c2e7cb621682b","modified":1574848818956},{"_id":"source/img/nacos-springCloud2.png","hash":"6ce7c90123f1a1eb0e3ebbe93ae50eded7392603","modified":1574848848065},{"_id":"source/img/nacos1.png","hash":"07c9cfee060b4090c04d5a9d422b97e2dd1cbbd2","modified":1574671501527},{"_id":"source/img/output_11_0.png","hash":"feae0728c34b976d209edf511796c8d86b374266","modified":1618654416000},{"_id":"source/img/output_11_1.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1618655238000},{"_id":"source/img/output_11_111.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1618655353812},{"_id":"source/img/output_13_0.png","hash":"74537a7efb66b8f35c874c0b69c1d0282ab92eec","modified":1618654416000},{"_id":"source/img/output_13_011.png","hash":"00d6df80990d980af9b8d244faba51db58904ebc","modified":1618655360541},{"_id":"source/img/output_13_1.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1618655238000},{"_id":"source/img/output_13_111.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1618655367417},{"_id":"source/img/output_14_0.png","hash":"ec71387d1c4c05e7a86a33c13aa3a08568e75827","modified":1618654416000},{"_id":"source/img/output_15_0.png","hash":"cc3a6ab17830f239b1e82500c967f6a3b40093f4","modified":1618654416000},{"_id":"source/img/output_15_011.png","hash":"d699a9e55c1d99653f37a36ab91be019a53e4bbf","modified":1618655373081},{"_id":"source/img/output_17_0.png","hash":"e4f13d81304ba2ac471c18b70558fc4cea417666","modified":1618654416000},{"_id":"source/img/output_19_0.png","hash":"d278d46ba923bcc6dfdbe29ca0d8547898ab1f2a","modified":1618654416000},{"_id":"source/img/output_20_1.png","hash":"65f2cbd977d34618d64be151e40aa7c5dbe9ec9f","modified":1618654416000},{"_id":"source/img/output_22_0.png","hash":"9b8f87f0342ee9093359c92047f1438a4c474dff","modified":1618654416000},{"_id":"source/img/output_26_1.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1618655238000},{"_id":"source/img/output_26_111.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1618655379253},{"_id":"source/img/output_29_0.png","hash":"e4c9e227e39a0e97a133e1b96b9577534ff3cb4c","modified":1618654416000},{"_id":"source/img/output_31_0.png","hash":"0850ffa53150038f984d9861c87bebcc127662c2","modified":1618654416000},{"_id":"source/img/output_32_1.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1618655238000},{"_id":"source/img/output_32_111.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1618655387136},{"_id":"source/img/output_33_111.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1618655391998},{"_id":"source/img/output_33_1.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1618655238000},{"_id":"source/img/output_34_1.png","hash":"219b0f0516b12045f827b0461e1be52eed5afa35","modified":1618654416000},{"_id":"source/img/output_36_1.png","hash":"14241a13199df5a0b3daeb5ab3c628f9bca15763","modified":1618654416000},{"_id":"source/img/output_42_0.png","hash":"5f66e78fbf687a4e955d043ca3c949036f445847","modified":1618654416000},{"_id":"source/img/output_40_0.png","hash":"a13785823254893b0bf7a2be92b5347507c44e01","modified":1618654416000},{"_id":"source/img/output_44_0.png","hash":"8d3277c861b982ddcea78e95020eed54f412af3d","modified":1618654416000},{"_id":"source/img/output_46_0.png","hash":"26fbd45ab4c82c99b073d1c940767c4732aa6de1","modified":1618654416000},{"_id":"source/img/output_52_0.png","hash":"0abbeb362227d28f260cef8812606514f6530b9c","modified":1618654416000},{"_id":"source/img/output_54_0.png","hash":"e889c6a42c4956042d69b1fa6c6bf75e60a9b41c","modified":1618654416000},{"_id":"source/img/output_8_0.png","hash":"0bcb929ddd78eab93836703fc4f859dfeef6c3ae","modified":1618654416000},{"_id":"source/img/output_8_1.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1618655238000},{"_id":"source/img/output_8_111.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1618655344899},{"_id":"source/img/pasted-0.png","hash":"d4f20b5880ebe1cd4114af86e182d3a8042f29b6","modified":1571142530740},{"_id":"source/img/secondaryNameNode.jpg","hash":"6186b1ee9e43435bafb7abefde1824d5f66e8f8b","modified":1576479793155},{"_id":"source/img/user-log.png","hash":"f7bd7155b8a465ad32afb87f8ae386768833c64f","modified":1577072956150},{"_id":"source/img/wordcount.png","hash":"78dbf6fe038d27891bb860321f1087fa9f642e3c","modified":1576488969747},{"_id":"source/img/三次握手协议2.png","hash":"149fa2d315b1471325a89b68ee659409c709c10c","modified":1567152281871},{"_id":"source/img/三次握手协议1.png","hash":"b7cf27e8ba8b7e299c4e02519394600d0269ae84","modified":1567152263777},{"_id":"source/img/使用协议进行通讯.png","hash":"fe93735a4d871a544feb31b0b713f98ab4c715fb","modified":1567157267759},{"_id":"source/img/信任链.png","hash":"23517850fc98c07ad66d80bac4556b77e0688214","modified":1569663179090},{"_id":"source/img/公钥私钥6.png","hash":"c7104a322604a0b51de2d988bd9522b5deab2045","modified":1569332262735},{"_id":"source/img/公钥私钥8.png","hash":"6b13644b98068f7f44ddb51532f17030906031c0","modified":1569332306598},{"_id":"source/img/加密算法.png","hash":"5a0a4d8fbdc43f929dd5aa86aded45086ba38cbb","modified":1571142476298},{"_id":"source/img/可信根.png","hash":"875a1cdae0d77993f07a1df1dd968f907a2c1d1e","modified":1569664599137},{"_id":"source/img/对称加密算法.png","hash":"6a7ed179fb8b48e9054caba56308b1691b6ebe92","modified":1567425941515},{"_id":"source/img/数据库分布式ID生成.png","hash":"7376ea76ee62cc47d2e0389f90ca9fa1a3173f74","modified":1570600820250},{"_id":"source/img/线程相关1.jpg","hash":"cd2dfeb2b0a367d208e46d1fec4ed9241164256e","modified":1574250522288},{"_id":"source/img/线程相关2.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1574250550341},{"_id":"source/img/线程相关3.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1574250560177},{"_id":"source/img/线程相关4.jpg","hash":"3268689c6020136cda73d95d1553dfa6816aa60b","modified":1574250574158},{"_id":"source/img/线程相关5.jpg","hash":"54f4bd928b107b4da2faaa87fa74484b76900c0b","modified":1574250583745},{"_id":"source/img/锁的创建.png","hash":"ac72c945c263be025d83970d2cfdb6c240c38bbc","modified":1571144077063},{"_id":"source/img/锁的创建2.png","hash":"ebf05f8b81c02d9c880b9081e9ccc0b2b6c5e4ae","modified":1571144110083},{"_id":"source/img/阻塞IO.png","hash":"f5cfd961b871078b9b202d1e8d4810f3f126aaef","modified":1567157770850},{"_id":"source/img/雪花算法.png","hash":"522b23e1a5bc4bdf46b1d285233d815004847271","modified":1570600786906},{"_id":"source/_posts/AtomicInteger.md","hash":"89cae95ed4bebeeb5a10ffddf13c9fe444199566","modified":1595846430037},{"_id":"source/_posts/DNS.md","hash":"acbf1fe4f74db68c1d6374548b8ccabe69d59fd7","modified":1595337964670},{"_id":"source/_posts/Disruptor.md","hash":"02a7838361a2ba19c822a2efef30787989adb382","modified":1618295513290},{"_id":"source/_posts/Disruptor中发布事件相关类.md","hash":"fbbd89b8085b8435e6fbd5cc315ded9cae10f32c","modified":1618295795621},{"_id":"source/_posts/ElasticSearch客户端.md","hash":"621b45b40b9ba1638b20c2edf9d5da0dfad60fba","modified":1618296390041},{"_id":"source/_posts/Docker入门.md","hash":"a43bb3cc1317e572333c2710c6e8e8115d30b75b","modified":1607923733336},{"_id":"source/_posts/ElasticSearch近实时性介绍.md","hash":"51a113085013b3897764afa733feb3400fd1b09b","modified":1618296417049},{"_id":"source/_posts/GET与POST区别.md","hash":"99282f35ceadcd64eab6a9b404316087c228c435","modified":1595295043727},{"_id":"source/_posts/Git梳理.md","hash":"d0a674882ec53bb07989a7f0dfaba94e0799342a","modified":1571142092785},{"_id":"source/_posts/HDFS-shell操作（1）.md","hash":"90628e1f94837c07894a58ba2e9fff418d38ce87","modified":1607227295062},{"_id":"source/_posts/HDFS-shell操作（2）.md","hash":"82b830a49715618406b55a0c2300128bbd7fa1e1","modified":1607227839580},{"_id":"source/_posts/HDFS文件操作.md","hash":"02424d27fdab1eab02babaf246e4c4f09edf904a","modified":1576487061726},{"_id":"source/_posts/HDFS概述.md","hash":"f93a0a44dd6c86d661add6adb695ac900dad5e16","modified":1576482443395},{"_id":"source/_posts/Hash解决冲突的方法.md","hash":"dcb776fc0c0db2dae7f56f81630719ba5b0f927a","modified":1600409078475},{"_id":"source/_posts/HiveQL视图.md","hash":"d5530bb5709c2e1afc05e1416194f4455f1a33a3","modified":1579575513416},{"_id":"source/_posts/Hive数据定义.md","hash":"9e687194aeec3eeaf1b2f88d8f3576cb464e2cd8","modified":1579247585936},{"_id":"source/_posts/Hive数据操作.md","hash":"d0d1b62844f29132a921c4034cbda39d27ee0fed","modified":1579420181742},{"_id":"source/_posts/Hive数据操作（2）.md","hash":"13a8dd7c6c5d339fe38f11345d387756211b082f","modified":1579502773866},{"_id":"source/_posts/Hive数据操作（3）.md","hash":"dee288d667445de9a892708dfffbc6c8192e47ae","modified":1585358257098},{"_id":"source/_posts/Hive数据类型和文件格式.md","hash":"5bb268f69c76b18c83e8b3045b08ebc348bf09e7","modified":1579241571383},{"_id":"source/_posts/Hive模式设计.md","hash":"caf50befcb1e9dfb86c6b7f868b2d76594b834b4","modified":1579596483705},{"_id":"source/_posts/Hive索引.md","hash":"08cc7d276a1037eed15156d7497a61816d956cf8","modified":1579585803179},{"_id":"source/_posts/Hive调优.md","hash":"14ff0ea2973448cd9121e83728042952c7333859","modified":1579600013242},{"_id":"source/_posts/HttpClient.md","hash":"5e8869ed7e12977ecd3e754f196405ec5ce3d1b5","modified":1594823957273},{"_id":"source/_posts/Http和Https的区别.md","hash":"80ae7b705f21a68faece10279fc66744fb1cca4d","modified":1607224592003},{"_id":"source/_posts/IOC中的基本反射步骤.md","hash":"01863c7f235caa925f4efbca12468c9d983db013","modified":1599960324128},{"_id":"source/_posts/JAVA数据类型.md","hash":"26d4e513161224daad2a53674d5ffade057f621c","modified":1578304400441},{"_id":"source/_posts/JVM垃圾回收算法.md","hash":"75754d12be87cec563de72d42351505ad54a5de6","modified":1600404935000},{"_id":"source/_posts/JVM内存结构.md","hash":"238efbb8701bcee73dc5bef2190ceb20d08796d6","modified":1601180056575},{"_id":"source/_posts/JVM类加载过程.md","hash":"8fb28feef5f1812fd4c9106bf291382db007097b","modified":1601178857818},{"_id":"source/_posts/Kafka的简单使用.md","hash":"bbaeadd8b73446b83d193d64d56bf0652a9e8a0c","modified":1618297905783},{"_id":"source/_posts/JVM性能优化整理.md","hash":"f3b2aed092723c2b5b733b57b6ad73dc45bcc4f3","modified":1605603411613},{"_id":"source/_posts/MapReduce平均数计算.md","hash":"47432b7b7cfe44868d95c36f960ea30de76c5c69","modified":1607228504982},{"_id":"source/_posts/MapReduce概述.md","hash":"e24632c1742518951c8b5bb2b378a54a7cc8dbac","modified":1576490842822},{"_id":"source/_posts/Nacos配置中心使用.md","hash":"52d8c2c31332f622c8e5a4e1a7e03e9b96dd1556","modified":1618297932904},{"_id":"source/_posts/SimpleDateFormat引发的线程安全问题.md","hash":"d8c4c82a697598498aa98a78adab7cb0b0c383b6","modified":1570882111315},{"_id":"source/_posts/Spark相关概述.md","hash":"51e1ebe27a5fddc703347127dd8c172e6515fc64","modified":1576660372500},{"_id":"source/_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","hash":"8892875af4dc9c06f91b6be4dd047fea8336cafd","modified":1618297974786},{"_id":"source/_posts/Spring-Bean生命周期.md","hash":"cce576cd1e7c9260398e5ce7dbbf93f78ad65fa3","modified":1573723161652},{"_id":"source/_posts/SpringCloud-Hystrix参数配置.md","hash":"b01f6f18a2f6ec92579d3258503beb121b0a9cbb","modified":1618297993985},{"_id":"source/_posts/SpringCloud-Ribbon参数配置.md","hash":"d655c7cfd39403c8855b4379a6c59eac800cb3c8","modified":1618298011814},{"_id":"source/_posts/SpringCloud-client配置.md","hash":"93d4db29e7c047c951f0c9d4587a26ada5dca07f","modified":1618298028225},{"_id":"source/_posts/SpringCloud使用Feign-Ribbon-Hystrix.md","hash":"3df2c395f448bbf79d2b7a68078601cf45c502cb","modified":1618297811594},{"_id":"source/_posts/SpringCloud使用Feign-Ribbon.md","hash":"4bea5ff574c636e9f69fe314900736055e13ae64","modified":1618297794331},{"_id":"source/_posts/SpringCloud使用Feign.md","hash":"6124b6fcc97b4dbfad644a5a84669d0d4f5066b8","modified":1618297819992},{"_id":"source/_posts/SpringCloud使用RestTemplate.md","hash":"7ac836a89b73cca9a71e8572a3ec28ced42e5fc4","modified":1618297827651},{"_id":"source/_posts/SpringCloud健康检查.md","hash":"6cc9783a60f80aa2857695301760778865611a46","modified":1618297858521},{"_id":"source/_posts/SpringCloud服务构建.md","hash":"34a2c9a73207ddef903177dfc501fabb64604dca","modified":1618297849104},{"_id":"source/_posts/SpringCloud异常配置.md","hash":"4c8c2764047571f7ebf1a4bec91c4af24646c920","modified":1618297838522},{"_id":"source/_posts/SpringCloud服务注册.md","hash":"a18130537beec571079dce1f8a327276031a4213","modified":1618297888615},{"_id":"source/_posts/SpringCloud服务消费.md","hash":"e05477ed09b8c462e6b48f0ade6cb3f68b6db48f","modified":1607919985577},{"_id":"source/_posts/SpringCloud管理配置页面.md","hash":"dc4302d3ebedb038a5c6fb05495f9333b22fc5b5","modified":1618297866850},{"_id":"source/_posts/SpringCloud运维接口.md","hash":"e0eac5bbe4198c66607718e324969737c27acca9","modified":1618297877368},{"_id":"source/_posts/TCP与UDP的区别.md","hash":"2ea423fb37ccdcabb7eba166a5ea84b4bb9e0c8d","modified":1595290967635},{"_id":"source/_posts/TCP-IP四层网络模型.md","hash":"ad3086d71dd922d2a5fbf1e53d45ed848ebd16dd","modified":1571141979272},{"_id":"source/_posts/TCP握手、挥手协议.md","hash":"707e37bd71fa2bf44d653374fff403f8eaa0396e","modified":1595287084754},{"_id":"source/_posts/ThreadLocal.md","hash":"d4b27d71fd595e522dedf09d3e51688caf8fcb66","modified":1585660357975},{"_id":"source/_posts/Tomcat性能优化整理.md","hash":"ae4c76c6bd93fe99d01c646f3a07ddce45ce2850","modified":1605596293216},{"_id":"source/_posts/WordCount简析.md","hash":"ed975037b0341d6c16022e5914f000ac38f00a4d","modified":1576641550021},{"_id":"source/_posts/java8新特性.md","hash":"f4389eb1abf000d7ff97b9a8cedc1dac2030f4b7","modified":1573720237235},{"_id":"source/_posts/Yarn概述.md","hash":"b126a9351edb7982bb71c5a55d509408c6b51c84","modified":1576635779255},{"_id":"source/_posts/java中的Queue队列.md","hash":"c13b39731e9caad5dc4d0b3209f31fa918aade8d","modified":1607922695486},{"_id":"source/_posts/k8s构建ELK日志平台.md","hash":"c02dec8c0e4c2faefc06f7838765c860ebda5f25","modified":1618297741143},{"_id":"source/_posts/l-String、-StringBuilder、StringBuffer区别.md","hash":"886a68c19f776c072aa95f93bfd6401601487495","modified":1595317907729},{"_id":"source/_posts/maven梳理.md","hash":"4cd19343841ad32a13e7c04e25551ead74e1683c","modified":1618244736944},{"_id":"source/_posts/mysql排序.md","hash":"b4a56da0c4b2024b2bbe87c8450cd3076106282a","modified":1574753513949},{"_id":"source/_posts/mysql事务.md","hash":"5fbed327cd3bb603a192f689031685f7ebc72ce5","modified":1595312002677},{"_id":"source/_posts/mysql表设计及优化.md","hash":"1e592b36e153b129de094310283598e4694a75fa","modified":1571134218948},{"_id":"source/_posts/互斥锁.md","hash":"a2a6a0b26fd73482c98c84867c93c02e851c9fb0","modified":1571134239998},{"_id":"source/_posts/伪共享.md","hash":"f04e6defd49d8a64929ca05428e41733face4a97","modified":1607924788244},{"_id":"source/_posts/信息系统项目管理师-信息化-1.md","hash":"89f4c8eb71087a501e32bcca24fc2d8b79c06873","modified":1618654518587},{"_id":"source/_posts/偏向锁.md","hash":"47b6a6a3fec8455311a590cbb47a66f788fd7cb4","modified":1585357234402},{"_id":"source/_posts/公平锁、非公平锁.md","hash":"7d520324daf56a0c36885fef17f2366800438a0f","modified":1571133968660},{"_id":"source/_posts/分布式CAP概念.md","hash":"e7badee94e29603efbe4726098bb14a854363020","modified":1607925396424},{"_id":"source/_posts/分布式全局唯一ID生成策略.md","hash":"a060a38e0f2c4a5ef4633cc2be911bcd9a1bb64e","modified":1571133944357},{"_id":"source/_posts/加密解密.md","hash":"edf660583a53e0d44ba9a0a74785a3c5a98b6626","modified":1571143218778},{"_id":"source/_posts/单例模式.md","hash":"2ead30e35f5bf0dda43ab6908563876337154980","modified":1578043442502},{"_id":"source/_posts/可信与可信计算.md","hash":"622f1a97783b9830838ec63f7eafd319107daf78","modified":1571134041402},{"_id":"source/_posts/原子性，有序性，可见性.md","hash":"61e5feebb95c7ed0ce8b78477493a068a0289e48","modified":1597115095668},{"_id":"source/_posts/可信基本概念.md","hash":"8e83f8be54cd21559f858755e6abb9ea577c487f","modified":1571134252269},{"_id":"source/_posts/可重入锁.md","hash":"a62dbbe3f15a6527490ea786371b62b31f782e79","modified":1571134060790},{"_id":"source/_posts/可靠性和容错技术.md","hash":"a397449f53f9042cb8de56580572210a9d18ba2e","modified":1569998915455},{"_id":"source/_posts/图解公钥与私钥.md","hash":"0a89b8fa785c291ff753fdfaa6bcc7668d80e733","modified":1569333117731},{"_id":"source/_posts/基于JavaAgent的全链路监控（1）.md","hash":"e6447921b7a38289c85539bade9d521edf640d33","modified":1595170251434},{"_id":"source/_posts/基于JavaAgent的全链路监控（2）.md","hash":"ebe71baa24d4986588097f66652b76adf4b638e8","modified":1595170244890},{"_id":"source/_posts/基于JavaAgent的全链路监控（3）.md","hash":"4174d8f8dce9bbefd4c42f981a6a45882257cd34","modified":1595170237560},{"_id":"source/_posts/基于JavaAgent的全链路监控（4）.md","hash":"aa66dca05ebd0f57a2c0cbc8cd7c553bd2a67b78","modified":1595287075292},{"_id":"source/_posts/对象存储与指针压缩.md","hash":"690c2453d97b741ffe8dcdfeb93d81d787c0bcfd","modified":1574251423218},{"_id":"source/_posts/并发编程总结.md","hash":"f5c74369135b8473f73da401cac2eaa4745595c1","modified":1605606928668},{"_id":"source/_posts/悲观锁、乐观锁.md","hash":"d18befc289385445e68c92be35441c0aaaf692c3","modified":1571141864237},{"_id":"source/_posts/手写一个简单Autowired.md","hash":"3e3731c835e580279f997c5e73e8bf661d0d76b9","modified":1599793649281},{"_id":"source/_posts/排序之比较器.md","hash":"225e3478c1e2bc7ab8c6dd1e0245dadc7b0d8d39","modified":1577936716885},{"_id":"source/_posts/排序之比较器Comparator-T.md","hash":"fc1ed5224ef4318537061477d1f5a4b40e784b89","modified":1577937612166},{"_id":"source/_posts/散点图.md","hash":"bdc16efdaa8500b38944fa2875cadcd33cce0321","modified":1618655185042},{"_id":"source/_posts/排序方法.md","hash":"005b453b3972d300a8921db595224b6e5751ba85","modified":1603439395206},{"_id":"source/_posts/数字签名.md","hash":"bf07570f5afa7175d432492564c548654191a38e","modified":1571142533710},{"_id":"source/_posts/池化之线程池.md","hash":"c77f5acddf612ad5f2743864bf4edcc4a6dd6786","modified":1571133890697},{"_id":"source/_posts/文件上传.md","hash":"b77879b87814fef9745a3a203e53c79db3c5cebe","modified":1571141394589},{"_id":"source/_posts/特征提取-简单流程.md","hash":"72c62832ec836dd303b214806c7ce2d0fc63cd85","modified":1577095126583},{"_id":"source/_posts/理解IO阻塞与非阻塞.md","hash":"09c9be752527e1c704a7012e7474f4d01f6fa741","modified":1595316240310},{"_id":"source/_posts/直方图.md","hash":"ac7363dd073dd6d182c4860516c7a2a647bc4585","modified":1618655499262},{"_id":"source/_posts/线程相关的知识.md","hash":"dae1ea9b088c4125eb2a0d41bae42c9d5bbdc94b","modified":1574250624878},{"_id":"source/_posts/自旋锁.md","hash":"d7fe8a130eba821be23ddb2c9e6aea2b0b2d2b67","modified":1567227561273},{"_id":"source/_posts/读写锁.md","hash":"3e17aded1f87f787910f807cf8696e0b2d428c3d","modified":1571133919181},{"_id":"source/_posts/责任链模式.md","hash":"2cbfcbb8a09a899bc3d20d9597ab8c4e1d8055cb","modified":1578038298113},{"_id":"source/_posts/轻量级锁.md","hash":"fbf5342b86be3b20dca77f472c390340256d49ec","modified":1567235430223},{"_id":"source/_posts/运算符.md","hash":"09c6ae8c201b08b26d9877a15e681f7371a8c043","modified":1598926051136},{"_id":"source/_posts/重放攻击.md","hash":"b00712cae3ee556704f0f0163aba3a45da842264","modified":1567603483315},{"_id":"source/_posts/锁粗化.md","hash":"d96860384ca8bdd805950ce24cfb782282c6b178","modified":1571142931866},{"_id":"source/_posts/阻塞锁.md","hash":"bc39dd0b2789e9b4efaa084473dbbd7f0545ff26","modified":1567227873756},{"_id":"source/img/1337b059.png","hash":"934fedad21bc55fd4848e8478221aa8de7c30ee7","modified":1618297672254},{"_id":"source/img/1618244002479.png","hash":"476a48c86fe9853e6e3814d62967932180e87aac","modified":1618244008713},{"_id":"source/img/1618244073306.png","hash":"c23f3b75c0e00674c191e398da326fbabfdc1155","modified":1618244081040},{"_id":"source/img/1618244193984.png","hash":"5f54d9d4ca1822d23caf1b86f07d11775d996d3c","modified":1618244199174},{"_id":"source/img/1618244361109.png","hash":"565b7cfe75ec1b5d599bdaa2be09166c5ffb7275","modified":1618244363161},{"_id":"source/img/1618244592782.png","hash":"4f2779efd3450377236256ecf7cc6c7525fcf57c","modified":1618244594684},{"_id":"source/img/7f3f75ca.png","hash":"3af248ce70e75081d255e4eaa66dd9e2aba6b5bb","modified":1618295425863},{"_id":"source/img/9cb319ab.png","hash":"7a7cd79948bc7091586b5b20a61aa70de5ce1827","modified":1618295497876},{"_id":"source/img/GETPOST.png","hash":"2a4f00d2d7cf57f885f347c1f6cdd5d0f477408c","modified":1595293877166},{"_id":"source/img/HDFS-liucheng.png","hash":"b37eef3d1e61aeaf650ea9860b40acff7c5fc1e4","modified":1577071488198},{"_id":"source/img/HDFS上传.png","hash":"7ca87724851bc20365821ed02ffcbe9de0768a0c","modified":1607226849819},{"_id":"source/img/HTTPS2.png","hash":"b792dee35f602f38fce149164dad4593c64a9650","modified":1569332481676},{"_id":"source/img/HTTPS3.png","hash":"e3e58b8e4961a4533ac2a63154aee3c49322e379","modified":1569332500332},{"_id":"source/img/HTTPS4.png","hash":"98c925d3de7c588950cb1ce2be36b3346a0ee848","modified":1569332518053},{"_id":"source/img/HTTPS5.png","hash":"7f96023634200693631288ec5047f024ee53f91c","modified":1569332538597},{"_id":"source/img/IO复用select模型.png","hash":"9f0c021b9b258025552f261e3ab8cbb2f96093ad","modified":1567157813191},{"_id":"source/img/Spark.png","hash":"ab45bdfcdd54f893ab046360fea5ccd51b887b3f","modified":1576647742604},{"_id":"source/img/StringUpdate.png","hash":"1cad86bc5aa015dfba1e0929e4dd98c708401011","modified":1595317765839},{"_id":"source/img/TCPIP模型.png","hash":"69c031664c0698c88a8b5d40b7b8e7e7afc53748","modified":1595290168314},{"_id":"source/img/TCP协议通讯过程.png","hash":"46efae085ca09e1445b9eac506c02c3462e2a16b","modified":1567157460145},{"_id":"source/img/ThreadLocal内部存储.png","hash":"1bcf081078acb1a514dd19ef48b740663837e899","modified":1585659506550},{"_id":"source/img/agent-costtime.png","hash":"1659bf6791dac773e8eb6d5ee3357b79d8702c12","modified":1595149419543},{"_id":"source/img/agent-costtime2.png","hash":"657b28cdec63d882c44e6010a7a78fdffbd6c186","modified":1595149535210},{"_id":"source/img/chartype.png","hash":"965a1655bf9364c0ec0b7a8773c74d8ce9d91805","modified":1578295245919},{"_id":"source/img/clip_image004.png","hash":"762418fea78372f2c8b67fc6f4dffb7045e9d449","modified":1573721637689},{"_id":"source/img/data-collect.png","hash":"f46c55acbc3e188b8c9cce341fb61d2db1027f31","modified":1577071668167},{"_id":"source/img/data-collect-analysis.png","hash":"19e4706ad65dfa32bf285624af839d4437db17df","modified":1577073489053},{"_id":"source/img/hdfs-read-file.png","hash":"861f90f65c5c1eac3ffea16430c75833aff5e089","modified":1576482855424},{"_id":"source/img/hive数据结构.png","hash":"6d0a423af256a0aecb19cd1ef7d34f4eb327efde","modified":1579229663822},{"_id":"source/img/hive数据结构1.png","hash":"1251b48e87a11c926b56969b3de0db1077627a3a","modified":1579229765391},{"_id":"source/img/hive文本文件数据编码.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1579232129199},{"_id":"source/img/image-20200117113506023.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1579232453859},{"_id":"source/img/image-20201206115600801.png","hash":"793cd10108ad7b33685387becb060e215a2eb045","modified":1607226968287},{"_id":"source/img/image-20201206115738831.png","hash":"03a84a42d3f64d15ac10a3d3449e554dc184be28","modified":1607227064520},{"_id":"source/img/image-20201206122126739.png","hash":"57cc146ead1c09f0fb6d2e54ad6f3bec553db218","modified":1607228497602},{"_id":"source/img/image-20201214123508379.png","hash":"36aa63f8051ab7294256ea771bdde1930d350db3","modified":1607920511260},{"_id":"source/img/image-20201214125402694.png","hash":"c5023ea858552e39671ae6d0f4aeef4189cc1801","modified":1607921661295},{"_id":"source/img/image-20201214131527522.png","hash":"9cb523d61f7f7178a933e7cee84a538d729a4026","modified":1607922930420},{"_id":"source/img/image-20201214131543066.png","hash":"00c552d3184a69128eb6b263119cb1c5cf6976ed","modified":1607922946631},{"_id":"source/img/image-20201214132254636.png","hash":"c580027a831986c0f1ada1ec7cf5e47726a29d99","modified":1607923377299},{"_id":"source/img/image-20201214133830361.png","hash":"eb52ab258a8bf959e9402b5abf071c9d3db1a2cf","modified":1607924312591},{"_id":"source/img/image-20201214133612299.png","hash":"c8f1d908f463400ae615da444654759a79c9511b","modified":1607924174766},{"_id":"source/img/image-20201214134235050.png","hash":"da065bb97d038439d13aca2178dc7f204d397c9f","modified":1607924558029},{"_id":"source/img/inttype.png","hash":"e01f1199570a67cd2ae26c3012227718e2575056","modified":1578292566587},{"_id":"source/img/jvm1.png","hash":"3cd098688ff2de0e071bc49739e257c18d0046af","modified":1600392421028},{"_id":"source/img/jvm3.png","hash":"e3b1fd453e531b527b569c4e28eaa04776f28fd5","modified":1600392487733},{"_id":"source/img/jvm2.png","hash":"6839ef36387dfa9daccb9d6395f486f9c6e73a4f","modified":1600392455838},{"_id":"source/img/mysql时间存储.png","hash":"1db17e59e7b6dad480009153803c159d5a97cbfe","modified":1567298126454},{"_id":"source/img/mysql的ip存储.png","hash":"d06d3619438f57be43aa1236ec9ace4e1bf3d126","modified":1567298167198},{"_id":"source/img/transaction.png","hash":"787175d99cc453a5ff1692029e2043060b1a7514","modified":1595302411291},{"_id":"source/img/typetrans.png","hash":"485490a772062455bc238936c0607d48d4aa93b7","modified":1578297875081},{"_id":"source/img/user-behaviour.png","hash":"b0ce3ff640e61461a545a39c56d01a3b6447a68a","modified":1577072979634},{"_id":"source/img/wordcount-map.png","hash":"a1c10dc70e0fdef09e3db7f305e16cd2fdf631a1","modified":1576489400587},{"_id":"source/img/wordcount-split.png","hash":"27610016538c3dca06409dcfbd57f8a805f86624","modified":1576489083300},{"_id":"source/img/公钥私钥1.png","hash":"498ea1f36ce52b0ff2964c9423bf5eb5ab0ae804","modified":1569332132500},{"_id":"source/img/公钥私钥10.png","hash":"767005c089b3be3b91213d12fab4e462718a7787","modified":1569332359302},{"_id":"source/img/公钥私钥11.png","hash":"10755e3bcfc39d9bf3c8b9b5060ec5682a18cad3","modified":1569332383529},{"_id":"source/img/公钥私钥2.png","hash":"c60ac4f1a39665d9a2b923f3a78541fc663b0e6b","modified":1569332150578},{"_id":"source/img/公钥私钥13.png","hash":"afd0297e43e61b9193b6dcbf9a71da610938dd2f","modified":1569332430586},{"_id":"source/img/公钥私钥3.png","hash":"63b5f3e415343c2a65d3274fe8d6e4ce4c26f490","modified":1569332201849},{"_id":"source/img/公钥私钥4.png","hash":"683543865f578ea89be62b1af374df4de0f42069","modified":1569332222890},{"_id":"source/img/公钥私钥5.png","hash":"b2721631bbbac0306b12e70466fa49531f8bae30","modified":1569332242107},{"_id":"source/img/公钥私钥7.png","hash":"e7f712acb8d5e47d717f18494d4c5e22ea9b5468","modified":1569332285257},{"_id":"source/img/公钥私钥9.png","hash":"4c4d25e78aea05b34ccf9cd0c5025cfcb7bf94e3","modified":1569332337273},{"_id":"source/img/指针压缩2.png","hash":"2db8b3b15563eedb0a35b192c37760fec8f9c8fa","modified":1574251229188},{"_id":"source/img/指针压缩3.png","hash":"f142f7d0e15d58e03d02d99c0eba541e51e30a7f","modified":1574251277019},{"_id":"source/img/指针压缩4.png","hash":"945b359ce34488105eab6422d8f397d8c76b4ea0","modified":1574251320221},{"_id":"source/img/非对称加密算法.png","hash":"260798ec83168388a70e55c60b1a7e5a47e78246","modified":1567425887927},{"_id":"source/img/1618244137853.png","hash":"90a6d21417b376515ad5d1652f85eba0d61545c2","modified":1618244144526},{"_id":"source/img/1618244623527.png","hash":"813c175d735bb6274abc069750ff9bda641937b7","modified":1618244625529},{"_id":"source/img/HTTPS1.png","hash":"2cc9b05768fb05bd25da12dd3428e6f02dfe50ec","modified":1569332459290},{"_id":"source/img/hive-算数运算符.png","hash":"54c7db33bbd7b5b8484727d7a988869609539d4c","modified":1579415475356},{"_id":"source/img/hive-聚合3.png","hash":"e3efe8d25521a72b1180deab85b38de9ecda0f5f","modified":1579415874687},{"_id":"source/img/image-20201214134902337.png","hash":"5d56cc52e66348ca9f9cc41c1b7b93fa30659f29","modified":1607924944995},{"_id":"source/img/mysql排序3.png","hash":"a91f727d4c99fb6289678a86f64ddd1c246568c7","modified":1574253993339},{"_id":"source/img/simpleDateFormat-alibaba.png","hash":"f45845a64160deda103dcca59d212712bfe544e6","modified":1585640320743},{"_id":"source/img/spark+hdfs.png","hash":"5ea801d62e8a34b8f08c627826d7f71369c2b749","modified":1576649860216},{"_id":"source/img/可信在云平台的基础架构.png","hash":"b507acb0ecbe5f39f783b7a82340a30bb6fd42d4","modified":1567327147941},{"_id":"source/img/非阻塞IO.png","hash":"58573268ff130afca209099ecda84d1c4eeff17f","modified":1567157791409},{"_id":"source/img/1618244544994.png","hash":"92f81e311f76ad0ac43170b8629248563e2af10f","modified":1618244550635},{"_id":"source/img/DNS2.png","hash":"0e8ca00be831d63e2ba6f36efc36dac00b590330","modified":1595332632492},{"_id":"source/img/HTTPS6.png","hash":"be9aa6a1578dd12e1970db4731f7c9727f9bc17e","modified":1569332587298},{"_id":"source/img/SpringBean3.png","hash":"2b96172eecc1cbd1e41ab9756c0a12836ebad948","modified":1573721725755},{"_id":"source/img/TCP协议通讯过程2.png","hash":"9ae8696d16fd4ffdc46b6c005b598999dec3f9c1","modified":1567157484636},{"_id":"source/img/Yarn.png","hash":"8b15f23ee56b0f7e2e5aa00157c1de50a0979da7","modified":1576631940342},{"_id":"source/img/hdfs-write-file.png","hash":"3482163375f44f53c6ff7791e005d5ebc42bf0f8","modified":1576485576556},{"_id":"source/img/hive-数学函数.png","hash":"e15b0006e6290c17b2348d0840272b52be732e0e","modified":1579415575739},{"_id":"source/img/image-20201206115653987.png","hash":"6ecd4c60466dc5e4278f30761e2354568150992a","modified":1607227020380},{"_id":"source/img/spark-all.png","hash":"d52bb136b90c8f63acaba9398811dbf1642de23f","modified":1576649188404},{"_id":"source/img/公钥私钥12.png","hash":"fdadf38dfcfd50d4afb6939f7403946ebcf1217b","modified":1569332408375},{"_id":"source/img/椭圆曲线算法的基本原理.png","hash":"5f848a66630c6ba515286fa96dc36ed3a9a11106","modified":1567343921566},{"_id":"source/img/DNS1.png","hash":"6ab8d7e53c1f65db2993710c9974276b7c6724fe","modified":1595332571282},{"_id":"source/img/Hive-运算2.png","hash":"1422e43a9e0aba7dac5ea4b6eecddbf6adc6bfe7","modified":1579415672114},{"_id":"source/img/hdfs.png","hash":"c3be7dde1cdd24c4765a87ddf07c75c70148e970","modified":1576476667162},{"_id":"source/img/hive集合数据类型.png","hash":"3803153d332cc20f22be920a84a9e371617099f3","modified":1579230723575},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","hash":"ec45ff26518a046bd0f6274a8f7317222f05ea81","modified":1567337278597},{"_id":"source/img/四次挥手协议.png","hash":"112d408966bbd5f77373bda95aaa07fd952e23d4","modified":1567152300617},{"_id":"source/img/指针压缩1.png","hash":"d26943d292b23b4cf544cee8b1a73aa4172a8920","modified":1574251137573},{"_id":"source/img/混合加密的方式.png","hash":"45b8a92df2e3c57f6af2d483c444a96e59929843","modified":1567426174461},{"_id":"source/img/1618295759372.png","hash":"369d456db024815c115664ca926d0356e0d9af2d","modified":1618295762547},{"_id":"source/img/8ab72b98.png","hash":"04732de500f3fc10dada0c0c2b2f35004c87624b","modified":1618295520632},{"_id":"source/img/TCP协议通讯过程1.png","hash":"100f6ea49b8b585a10e0ac88e2486a902e221a5c","modified":1567157472656},{"_id":"source/img/hive-运算3.png","hash":"5bc45de4ce61992130c1c3636d2031969dd81cb1","modified":1579415710576},{"_id":"source/img/读写锁.png","hash":"f5c76702b3a276fc48835d2ad1a02859a4767e08","modified":1567228164910},{"_id":"source/img/select、epoll模型对比.png","hash":"cdc93cacbedcf65fb68878bd36da3929e6333e2b","modified":1567157832363},{"_id":"source/img/wordcount-reduce.png","hash":"99f7334d973e7a4ffaf5d34e7870b55d0016858b","modified":1576489761247},{"_id":"source/img/对象存储1.png","hash":"281b0b0c7d4c55df4000800b3f00888663e5ee25","modified":1574250820011},{"_id":"source/img/阻塞与非阻塞调用对比.png","hash":"cdee42373e7940d8f09575bf844ce08f49b4d1af","modified":1567157755596},{"_id":"source/img/jvmHeapStructure.png","hash":"002db89529955be1ab4647f6628bcb41474c9ec6","modified":1600401526035},{"_id":"themes/3-hexo/.DS_Store","hash":"0770f9d42bfdd8d420de48fed463015e001cf579","modified":1618482058221},{"_id":"themes/3-hexo/LICENSE","hash":"b04140c5f682db2b300428f97bb164fd7f5f18bd","modified":1618482058221},{"_id":"themes/3-hexo/README.md","hash":"19b8cfe6690c28427492f342e74dda5ed49a1664","modified":1618482058222},{"_id":"themes/3-hexo/_config.yml","hash":"b0779a6e34cf461a6252d95cf41ff5c3a5fbbd4d","modified":1618655736934},{"_id":"themes/3-hexo/languages/en.yml","hash":"616e02c035c86033ab4a97c5ae9e0a9e5f0b8ea3","modified":1618482058223},{"_id":"themes/3-hexo/languages/zh-CN.yml","hash":"83633d45420c96dfac41333aeac3f3616dca5286","modified":1618482058223},{"_id":"themes/3-hexo/layout/index.ejs","hash":"1c185288c2925a652d577965626718e12df07f65","modified":1618482058231},{"_id":"themes/3-hexo/layout/indexs.md","hash":"51e7c86d8f0a6620de511719a535b1a872760c42","modified":1618655806250},{"_id":"themes/3-hexo/layout/post.ejs","hash":"a0eaba41e7ec9db5843af482470a45531049b457","modified":1618482058231},{"_id":"themes/3-hexo/source/.DS_Store","hash":"fdcc907c46e093a14b153c5dc8c038461997ed3c","modified":1618482058232},{"_id":"themes/3-hexo/layout/_partial/article.ejs","hash":"9e5afcc26f47f93c165072b0a2b5cbf72efb7ef9","modified":1618482058224},{"_id":"themes/3-hexo/layout/_partial/article_copyright.ejs","hash":"9e1cdec49d5b9b44399348d96ecd7331f3ee7d85","modified":1618482058224},{"_id":"themes/3-hexo/layout/_partial/comment.ejs","hash":"d18f94e04ef0cf7abb432a8e707ccb3abc7fe435","modified":1618482058224},{"_id":"themes/3-hexo/layout/_partial/copyright.ejs","hash":"4c09f47e899fe36bfe36d92b12996219c2b5f622","modified":1618482058227},{"_id":"themes/3-hexo/layout/_partial/dashang.ejs","hash":"b2a01cc1f0326965f0a186ce3c9b3c991fd4e2c9","modified":1618482058227},{"_id":"themes/3-hexo/layout/_partial/footer.ejs","hash":"9087af9647a87c3fa9ef87632de5427ba4abe9c4","modified":1618482058228},{"_id":"themes/3-hexo/layout/_partial/friends.ejs","hash":"e6dd90be668195016d6e1c51a6baefb50676e6ab","modified":1618482058228},{"_id":"themes/3-hexo/layout/_partial/full-toc.ejs","hash":"60a085fab3165ea1fc6370abac0bd6ab1b2f2510","modified":1618482058228},{"_id":"themes/3-hexo/layout/_partial/header.ejs","hash":"1e04b617fe38acca8b3d3774c5dbfcb74a02db6b","modified":1618482058229},{"_id":"themes/3-hexo/layout/_partial/mathjax.ejs","hash":"e2be0e37f3d48e63e65a47d819bfb800b9aa3784","modified":1618482058229},{"_id":"themes/3-hexo/layout/_partial/nav-left.ejs","hash":"f3cc395fbb4e308776a38e369faefbc9e5891807","modified":1618482058230},{"_id":"themes/3-hexo/layout/_partial/meta.ejs","hash":"ab6329ddd908b0567c18f39ac6a8553c6fec67c5","modified":1618482058229},{"_id":"themes/3-hexo/layout/_partial/nav-right.ejs","hash":"7942c661b48e15fced4a97acf86fac7fea013378","modified":1618482058230},{"_id":"themes/3-hexo/layout/_partial/toc-ref.ejs","hash":"33f7a4bfca1bb9835ec8f0d1e73188d1f56cc8b9","modified":1618482058230},{"_id":"themes/3-hexo/layout/_partial/tag.ejs","hash":"8704e6bd833d270cc6a494d4e7cf1dfeddedba40","modified":1618482058230},{"_id":"themes/3-hexo/source/css/gitalk.css","hash":"3dc58e9a3fd63a3144d5fe850eb55e3dc885c9fb","modified":1618482058241},{"_id":"themes/3-hexo/source/css/mobile.styl","hash":"1c2f8b7d7cf46f219adb3a628bdf380f29ff4a6b","modified":1618482058246},{"_id":"themes/3-hexo/source/css/style.styl","hash":"29fa7f6619519c2dcfec4efac4314c5af659a92a","modified":1618482058246},{"_id":"themes/3-hexo/source/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1618482058257},{"_id":"themes/3-hexo/source/js/script.js","hash":"36275888d57fecb6afd2c6f9291c46c2b3894ac5","modified":1618482058258},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","hash":"8c2a4f10a4da3d9615a3a81542494c6d21479b3d","modified":1618482058258},{"_id":"themes/3-hexo/source/js/search.js","hash":"788c610149a5f9361295f9f0207c8523f37ddb8b","modified":1618482058259},{"_id":"themes/3-hexo/source/js/titleTip.js","hash":"7299ac046ddd6e6a4267d435f7b4c8198baaaccc","modified":1618482058260},{"_id":"themes/3-hexo/source/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1567299308141},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1618482058248},{"_id":"themes/3-hexo/source/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1618482058249},{"_id":"themes/3-hexo/source/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1618482058249},{"_id":"themes/3-hexo/source/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1618482058250},{"_id":"themes/3-hexo/layout/_partial/comments/click2show.ejs","hash":"05b09c45b379ffeb4f48c1604044d88829f90799","modified":1618482058225},{"_id":"themes/3-hexo/layout/_partial/comments/disqus.ejs","hash":"32ce7b48d366b9c888ff2ceb911a3cd82f864537","modified":1618482058225},{"_id":"themes/3-hexo/layout/_partial/comments/gitalk.ejs","hash":"01567e010cf4f2dd141fe2019490d3f0d5aa2529","modified":1618482058226},{"_id":"themes/3-hexo/layout/_partial/comments/gitment.ejs","hash":"eaf2b6f297282606b630ad55fb9e38af7e2829dc","modified":1618482058226},{"_id":"themes/3-hexo/layout/_partial/comments/livere.ejs","hash":"2d115e79cadedc2d5d8f4b5618559640d986e01f","modified":1618482058226},{"_id":"themes/3-hexo/layout/_partial/comments/utteranc.ejs","hash":"8f2d4f42fbad351677c82e72420224587a5bd666","modified":1618482058227},{"_id":"themes/3-hexo/source/css/_partial/comment.styl","hash":"d5fa333970a2eac66937d42eeb16fdb362e121ed","modified":1618482058233},{"_id":"themes/3-hexo/source/css/_partial/dashang.styl","hash":"f0eac1dc1f5dbed1769d032bb5fd5f002faaee26","modified":1618482058233},{"_id":"themes/3-hexo/source/css/_partial/fade.styl","hash":"02c7510a26f306e240f23ddbf772a69be2c890dd","modified":1618482058233},{"_id":"themes/3-hexo/source/css/_partial/font.styl","hash":"3db01e603985e6dbcacb6b0f13dbd804f5849e3c","modified":1618482058234},{"_id":"themes/3-hexo/source/css/_partial/full-toc.styl","hash":"9a732af065d0a80c9e420934be0f3582bf0129dc","modified":1618482058234},{"_id":"themes/3-hexo/source/css/_partial/nav-left.styl","hash":"0a067ced25025000aa33c8f5017c87fff0971378","modified":1618482058234},{"_id":"themes/3-hexo/source/css/_partial/nav-right.styl","hash":"588b75e3b83ed95e526154bf3c0336c6f33e2be7","modified":1618482058235},{"_id":"themes/3-hexo/source/css/_partial/nprogress.styl","hash":"2620a02169a6aeb75137fd368eac2c36423d8498","modified":1618482058235},{"_id":"themes/3-hexo/source/css/_partial/num-load.styl","hash":"f7ef35459ece22e1da950b86126be1c2bfe97fcf","modified":1618482058235},{"_id":"themes/3-hexo/source/css/_partial/post.styl","hash":"f1251e2a3b5334af3a22b51fc0293c2456568b50","modified":1618482058236},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1618482058236},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","hash":"b5e7562c8494b0ddb3a70ecc5545ef7340d8e971","modified":1618482058237},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1618482058237},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1618482058238},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1618482058238},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1618482058239},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","hash":"3630aabf2f9c0417f483ebd03d9e429dbc2594e0","modified":1618482058239},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1618482058239},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1618482058240},{"_id":"themes/3-hexo/source/css/fonts/selection.json","hash":"b6456a4eabcffd95e822d1d7adce96da524d481a","modified":1618482058240},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","hash":"f3eb4e5feda9cbd6242ccf44ca064e2979b5d719","modified":1618482058241},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","hash":"553987211d3323a7dfc0b08786b183a3435978c9","modified":1618482058242},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","hash":"03af387edcc1cf8c18d12e9c440fd51b6cf425b6","modified":1618482058242},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","hash":"2bfc14f27ccca108b4b3755782de8366e8bd001e","modified":1618482058242},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","hash":"5e05b19832c1099bd9d284bc3ed00dc8a3d7ee23","modified":1618482058242},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","hash":"53276ff1f224f691dfe811e82c0af7f4476abf5d","modified":1618482058243},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","hash":"315ad610d303caba9eac80a7d51002193a15478a","modified":1618482058243},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","hash":"1bece084b1dbbbd4af064f05feffd8c332b96a48","modified":1618482058243},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","hash":"e9c190f9ffc37a13cac430512e4e0c760205be4a","modified":1618482058244},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","hash":"0c3ccd0d64e7504c7061d246dc32737f502f64e4","modified":1618482058244},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","hash":"a6e8cfd2202afd7893f5268f3437421e35066e7b","modified":1618482058244},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","hash":"e5c37646a9d9c1094f9aab7a7c65a4b242e8db00","modified":1618482058245},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","hash":"51659351b391a2be5c68728bb51b7ad467c5e0db","modified":1618482058245},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","hash":"501d75ef0f4385bea24d9b9b4cc434ba68d4be27","modified":1618482058245},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","hash":"2aa9817e68fb2ed216781ea04b733039ebe18214","modified":1618482058245},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","hash":"92941a6ae73b74f44ad7c559c5548c44073c644a","modified":1618482058246},{"_id":"source/img/hive-partition.png","hash":"c587e39d656360af68418b82172cf91d1b0e7cd2","modified":1579406393393},{"_id":"source/img/hive-表生成函数.png","hash":"cf8bb24a3e4d1263993cc3df54d7777a3931eb64","modified":1579416817156},{"_id":"source/img/hive运算1.png","hash":"472120109358e0ec58e980b9d0fc9ab533764d4c","modified":1579415684134},{"_id":"source/img/image-20201206121829515.png","hash":"c5523dda003b29efcb8231211c4d2045d61b0dec","modified":1607228315435},{"_id":"themes/3-hexo/source/img/avatar.jpg","hash":"525dc2b6ef38fee9d4c66e554f928934eadd9117","modified":1567299464243},{"_id":"themes/3-hexo/source/img/weixin.jpg","hash":"59cf33d0f6ce9324dabe183f3d4551620959e987","modified":1567299346911},{"_id":"source/img/Socket通讯模型.png","hash":"2e260e5f30b7e17b7dba072e65ac7860e8d7e6ce","modified":1567156905642},{"_id":"source/img/hive-聚合1.png","hash":"070d7feb1ef4626223019a5b205d43b24116dc4a","modified":1579415841935},{"_id":"source/img/网络连接模型.png","hash":"f2f48effb8cb134f597011fda639932ea0d2fac2","modified":1567149380436},{"_id":"themes/3-hexo/source/js/gitment.js","hash":"67984b83cd46ff4300d4fd959bf6c17dd66b4136","modified":1618482058257},{"_id":"source/img/三次握手协议3.png","hash":"e6fe59d8473f94be2b361f4bda5be7d791022eec","modified":1567152339366},{"_id":"source/img/四次挥手协议2.png","hash":"1a4890549075feacfa4010df20911537fc726e4b","modified":1567152739865},{"_id":"themes/3-hexo/source/js/gitalk.js","hash":"a95b598d998c4723f978ed21614127150075bf40","modified":1618482058256},{"_id":"source/img/image-20210415193556837.png","hash":"56b313e3e79284892d99656549cc4b7f0335e296","modified":1618654518597},{"_id":"public/2021/04/17/直方图/index.html","hash":"7d5d533ad5b72e4a3895c41e4c03410cbf5392d1","modified":1618655815709},{"_id":"public/2021/04/17/散点图/index.html","hash":"7f0517b2a2b08ca841060460ca84f38be118ab32","modified":1618655815709},{"_id":"public/2021/04/15/信息系统项目管理师-信息化-1/index.html","hash":"e776526feed65ae6ea3a96bc398b758270bf2770","modified":1618655815709},{"_id":"public/2021/04/13/k8s构建ELK日志平台/index.html","hash":"0330e26c72a3fb285056cd4cf2e4a5ed8b33e0f5","modified":1618655815709},{"_id":"public/2021/04/13/Disruptor中发布事件相关类/index.html","hash":"e4ac7e56c85c4c5c17c9cee93847978cd932dd5e","modified":1618655815709},{"_id":"public/2020/12/14/分布式CAP概念/index.html","hash":"ce310d0a9aa3038f01dcb65210041969d23a65e4","modified":1618655815709},{"_id":"public/2021/04/13/Disruptor/index.html","hash":"e190d00beaca9c3c5913c2dd789912d1b52ebda8","modified":1618655815709},{"_id":"public/2020/12/14/伪共享/index.html","hash":"cb8cd777bdb87989f5fa4f483de1190292182ca9","modified":1618655815709},{"_id":"public/2020/12/14/java中的Queue队列/index.html","hash":"1c99a49c2658b5b6ce7101fc081ce702f0891586","modified":1618655815709},{"_id":"public/2020/12/14/Docker入门/index.html","hash":"6326a030450f16a7e4f48812e72267cfadd409f9","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud使用Feign-Ribbon-Hystrix/index.html","hash":"0303753d5d4be993bf630cafc1b6aacc1b8cd482","modified":1618655815709},{"_id":"public/2020/12/14/Kafka的简单使用/index.html","hash":"73734141b0e4486087a1a7a84f7ef752ac582bbb","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud使用Feign-Ribbon/index.html","hash":"4310d474acd6dabdb195f3aabfa5b6af71aae181","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud使用Feign/index.html","hash":"9454595618aa21c6f664f1a5615f191dbfede1ac","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud使用RestTemplate/index.html","hash":"da8b95091ba957f5fb7c439666865388a25387f3","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud异常配置/index.html","hash":"5fdfd3db885d11bffbf89405eefb133c36eb3f25","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud-Hystrix参数配置/index.html","hash":"9df15a27c80f88f976f006f3c337958c359663c4","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud-client配置/index.html","hash":"e807e15f1224f43d80ee28de48a93689b811b59a","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud-Ribbon参数配置/index.html","hash":"187f5d7b5c1cf02d10918c3629b45feb548d9052","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud服务注册/index.html","hash":"9963aefff5b9faed860dc1648019a57250e0d470","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud服务消费/index.html","hash":"2c7097ab86026adae009144cd946b5771a547495","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud健康检查/index.html","hash":"3a05b2ee9ea5c0c3a6d89cd0f8df5ebaf06af65b","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud管理配置页面/index.html","hash":"754a4177f8857b195a571859b2ea208ff766417e","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud运维接口/index.html","hash":"ef6b6c31425770bf8505a75ba90dfbb5e8d69b45","modified":1618655815709},{"_id":"public/2020/12/14/SpringCloud服务构建/index.html","hash":"f2a49cb3b9f43e0bd037eb9a30c975885714d819","modified":1618655815709},{"_id":"public/2020/12/06/MapReduce平均数计算/index.html","hash":"fd6b62c7b27c45c1f15eae77f4a64417bab46edf","modified":1618655815709},{"_id":"public/2020/12/06/HDFS-shell操作（2）/index.html","hash":"ba4edd6d031b41a3d43f9c1698f5a78b89061bd3","modified":1618655815709},{"_id":"public/2020/12/06/HDFS-shell操作（1）/index.html","hash":"560e63e539decf3d18f5fb13605ed19b35cba274","modified":1618655815709},{"_id":"public/2020/11/17/并发编程总结/index.html","hash":"7303fa75a0dabe05676fd20d9042f5fd74fea632","modified":1618655815709},{"_id":"public/2020/11/17/JVM性能优化整理/index.html","hash":"913e7beedb66a55b7166ff92b4509bc00e1cc0f7","modified":1618655815709},{"_id":"public/2020/11/17/Tomcat性能优化整理/index.html","hash":"4ebcff83d4960979730553ea153985ab4e7116eb","modified":1618655815709},{"_id":"public/2020/10/23/排序方法/index.html","hash":"0a5b3eac37f4ec846beffd0f950758c0014cd101","modified":1618655815709},{"_id":"public/2020/09/27/JVM内存结构/index.html","hash":"e014eeb4e03bfcaf8e3c0cb5d9a1b32544073028","modified":1618655815709},{"_id":"public/2020/09/27/JVM类加载过程/index.html","hash":"2f08a5ff1497d912a31af73584b8198daeaca9d7","modified":1618655815709},{"_id":"public/2020/09/18/Hash解决冲突的方法/index.html","hash":"282bd9bc4ea134210c5e1a429a65fa8e464e4255","modified":1618655815709},{"_id":"public/2020/09/18/JVM垃圾回收算法/index.html","hash":"219ff53007a9564767ea237e6d969400326e67c1","modified":1618655815709},{"_id":"public/2020/09/13/IOC中的基本反射步骤/index.html","hash":"1e841ca4f90d2c29c9bfa0e7c052414c10f911b1","modified":1618655815709},{"_id":"public/2020/09/11/Http和Https的区别/index.html","hash":"f7bb7cc611483a2107cb810a21e217029dc54709","modified":1618655815709},{"_id":"public/2020/09/11/手写一个简单Autowired/index.html","hash":"741688849f14c49fa7805c543c1678983febb5e4","modified":1618655815709},{"_id":"public/2020/09/01/运算符/index.html","hash":"50ae4c7846165bf9014678a7437466aa29c117c5","modified":1618655815709},{"_id":"public/2020/08/11/原子性，有序性，可见性/index.html","hash":"cf753108088508df1789c2b63ff2642b3192cb74","modified":1618655815709},{"_id":"public/2020/07/24/AtomicInteger/index.html","hash":"53d239420aa4c6619de48ee0200fc5ae6c5a7cb4","modified":1618655815709},{"_id":"public/2020/07/21/DNS/index.html","hash":"2399e265baf787c4b464d827f1980e0970ceb8ff","modified":1618655815709},{"_id":"public/2020/07/21/l-String、-StringBuilder、StringBuffer区别/index.html","hash":"7d703e7933ea55e213fcb8f99afea60bd5a2d801","modified":1618655815709},{"_id":"public/2020/07/21/mysql事务/index.html","hash":"4a50fadb33fd6e0030c72a49f02f4b9a6a211463","modified":1618655815709},{"_id":"public/2020/07/21/GET与POST区别/index.html","hash":"020c8db7eb5540299dc937e422457cd161294d32","modified":1618655815709},{"_id":"public/2020/07/21/TCP与UDP的区别/index.html","hash":"5541d5f8edd3aefb0d239491f6600439dab4293a","modified":1618655815709},{"_id":"public/2020/07/19/基于JavaAgent的全链路监控（4）/index.html","hash":"9e38cce7553831435c7ef8db16f7b5c3624e00ba","modified":1618655815709},{"_id":"public/2020/07/19/基于JavaAgent的全链路监控（3）/index.html","hash":"7ac586c398a8fe178cae7be4ebc3d55b80973099","modified":1618655815709},{"_id":"public/2020/07/19/基于JavaAgent的全链路监控（2）/index.html","hash":"8974cf65853693fa4715439f802dfd884633085f","modified":1618655815709},{"_id":"public/2020/07/17/基于JavaAgent的全链路监控（1）/index.html","hash":"b49a0743c81eec01d9b39f8686e87a15be5d4b8c","modified":1618655815709},{"_id":"public/2020/07/15/HttpClient/index.html","hash":"c9ea49116ba150b6596dfc3a6a27ddb16ffcba4e","modified":1618655815709},{"_id":"public/2020/07/15/ElasticSearch客户端/index.html","hash":"57ad3ec3f19fa6983357efcb634637fa1e14c461","modified":1618655815709},{"_id":"public/2020/07/15/ElasticSearch近实时性介绍/index.html","hash":"4b0c6d3e558e4708b6a4b95cfc7a8a04ae70084a","modified":1618655815709},{"_id":"public/2020/03/28/ThreadLocal/index.html","hash":"ddc78116c09666bd06894b9c2588cff798159eff","modified":1618655815709},{"_id":"public/2020/01/21/Hive调优/index.html","hash":"e81ae9b8c05a4d7efd571d6d6a2d8fa809e9ac0f","modified":1618655815709},{"_id":"public/2020/01/21/Hive模式设计/index.html","hash":"b439b3d68097dbcc4ff714a07c8e125c4199921b","modified":1618655815709},{"_id":"public/2020/01/21/Hive索引/index.html","hash":"8dc08acd8f92a7bd65ddf7cf7d1c3a51fc8354f5","modified":1618655815709},{"_id":"public/2020/01/20/HiveQL视图/index.html","hash":"934e3c98a3bd80f7d9afa4d1c358e47c71b0c182","modified":1618655815709},{"_id":"public/2020/01/20/Hive数据操作（3）/index.html","hash":"470f8c98654ca4aa1b4822bbd13af73eeec777c6","modified":1618655815709},{"_id":"public/2020/01/19/Hive数据操作（2）/index.html","hash":"9022f56d182a5e38ec17f109ea737f8f9230c56e","modified":1618655815709},{"_id":"public/2020/01/17/Hive数据操作/index.html","hash":"f28a2974e454b829265969fedf62fe58d33544e3","modified":1618655815709},{"_id":"public/2020/01/17/Hive数据定义/index.html","hash":"1280b3c281ad933fb8d5ac8bb32d7e08defbc1c5","modified":1618655815709},{"_id":"public/2020/01/17/Hive数据类型和文件格式/index.html","hash":"c4ecd4e2d15dc6f40ebb73e134084ffa70be812c","modified":1618655815709},{"_id":"public/2020/01/06/JAVA数据类型/index.html","hash":"7e51173d021686213bf94c63134094101b7f15a0","modified":1618655815709},{"_id":"public/2020/01/03/单例模式/index.html","hash":"6863f3a773976fe85ef03ecd341869c8782bfefb","modified":1618655815709},{"_id":"public/2020/01/03/责任链模式/index.html","hash":"1ee5e26d7066957a29dab8ed4df2b3c2ded767b0","modified":1618655815709},{"_id":"public/2020/01/02/排序之比较器Comparator-T/index.html","hash":"c36cf8d8803884a65706ec5d40cdfaaaca4d4574","modified":1618655815709},{"_id":"public/2020/01/02/排序之比较器/index.html","hash":"0a172d20c0532fc8e9b503d9832722c8baca8f8d","modified":1618655815709},{"_id":"public/2019/12/23/特征提取-简单流程/index.html","hash":"286363601db7bd0a5426ca4364d31eda4a8670d7","modified":1618655815709},{"_id":"public/2019/12/18/Spark相关概述/index.html","hash":"462d2d103434acc895307918eb79e5833e549630","modified":1618655815709},{"_id":"public/2019/12/18/WordCount简析/index.html","hash":"aacdfde2728b13a3a67c8de12ac66c6bf55e7de7","modified":1618655815709},{"_id":"public/2019/12/18/Yarn概述/index.html","hash":"8c368eb3040e694ef062a1922f6963d41925491b","modified":1618655815709},{"_id":"public/2019/12/16/MapReduce概述/index.html","hash":"c53d277cf7f56c2136660e9a92d3476d0160bc54","modified":1618655815709},{"_id":"public/2019/12/16/HDFS文件操作/index.html","hash":"2efc8e89a8324aeb6b5336b4e6293e3f4ebf0b17","modified":1618655815709},{"_id":"public/2019/12/16/HDFS概述/index.html","hash":"6c6a4664fbfee516f1fc9021f2cd3195a4f78f90","modified":1618655815709},{"_id":"public/2019/12/03/SpringCloud-Alibaba整合Nacos服务注册发现/index.html","hash":"b684d8a300d5d32a3c5bcbd51b0203e1dd80e1ab","modified":1618655815709},{"_id":"public/2019/11/25/Nacos配置中心使用/index.html","hash":"e5c0776032fa3ffbce80b9fb05a8c81f0e4abb60","modified":1618655815709},{"_id":"public/2019/11/20/mysql排序/index.html","hash":"0d0757d4d7a20e7630617ce4af392b3214958275","modified":1618655815709},{"_id":"public/2019/11/20/对象存储与指针压缩/index.html","hash":"e41a608c1c992f74dd573cf4142fb1f8781d43d7","modified":1618655815709},{"_id":"public/2019/11/20/线程相关的知识/index.html","hash":"2c8419280c35920c26d87969536c3826aa8c48cf","modified":1618655815709},{"_id":"public/2019/11/14/Spring-Bean生命周期/index.html","hash":"dca7a56dfdf808f99d1810d7cc1a7b94a9da8c8f","modified":1618655815709},{"_id":"public/2019/11/14/java8新特性/index.html","hash":"06a9f27cfe5242bf3c63699dcc9e28bcaa0f3d04","modified":1618655815709},{"_id":"public/2019/10/12/SimpleDateFormat引发的线程安全问题/index.html","hash":"f87b16c2b02524dd4baa2ed3e11443c613d5f8d2","modified":1618655815709},{"_id":"public/2019/10/09/分布式全局唯一ID生成策略/index.html","hash":"1aae6cbc2405af4b356e13f8c41d313e76d339df","modified":1618655815709},{"_id":"public/2019/10/02/可靠性和容错技术/index.html","hash":"db523e7bcb6601343846c15a1256328ccb3c0d96","modified":1618655815709},{"_id":"public/2019/09/28/可信与可信计算/index.html","hash":"e502d6b8c8d44b8ad54425a5720008a65baebe62","modified":1618655815709},{"_id":"public/2019/09/24/图解公钥与私钥/index.html","hash":"b9c794481cbf87ec357aa022cdb2add61d48be21","modified":1618655815709},{"_id":"public/2019/09/04/重放攻击/index.html","hash":"b95deb92a1beb646b648cdb4a8f5d4b3147f13bb","modified":1618655815709},{"_id":"public/2019/09/02/加密解密/index.html","hash":"49899eb3694f61aeaf71d032c624e72d6244e145","modified":1618655815709},{"_id":"public/2019/09/01/数字签名/index.html","hash":"6468a74d5066330320b6b9189e8a8d92f9651164","modified":1618655815709},{"_id":"public/2019/09/01/可信基本概念/index.html","hash":"9f031e9b620b87983863a9b5bf8ae25bc3b58f91","modified":1618655815709},{"_id":"public/2019/09/01/池化之线程池/index.html","hash":"a1913d8b73258166933a5f4df3685ded2e4ac9ee","modified":1618655815709},{"_id":"public/2019/08/31/文件上传/index.html","hash":"4be266bc723d9b0d5fba29fc89ec927a0e5c6c7b","modified":1618655815709},{"_id":"public/2019/08/31/mysql表设计及优化/index.html","hash":"e873c51b3fbac7ece08fb1c209b83fbb8c286bf6","modified":1618655815709},{"_id":"public/2019/08/31/轻量级锁/index.html","hash":"0b1a761369018151266cc657e54caed6a78aaf3a","modified":1618655815709},{"_id":"public/2019/08/31/锁粗化/index.html","hash":"a90232d4cc9fcfdab66e64aa9c8958a15048621f","modified":1618655815709},{"_id":"public/2019/08/31/偏向锁/index.html","hash":"4a14b366bdaeb65b41ffcc5f92d13b02bf3d6893","modified":1618655815709},{"_id":"public/2019/08/31/公平锁、非公平锁/index.html","hash":"05ce8005ec13b37ae176886cb8e94e1e2e3025f7","modified":1618655815709},{"_id":"public/2019/08/31/悲观锁、乐观锁/index.html","hash":"c6e59eab3ae218f122f92e8bc325b17e98a87cf1","modified":1618655815709},{"_id":"public/2019/08/31/互斥锁/index.html","hash":"0d622bf5296ceee45956d95be99a2e242d26e301","modified":1618655815709},{"_id":"public/2019/08/31/读写锁/index.html","hash":"b6e03d8de257978867051b93f4b627fe8aebcd6c","modified":1618655815709},{"_id":"public/2019/08/31/可重入锁/index.html","hash":"92b26f310626cbf9d66ea797391ad70f49b0ba62","modified":1618655815709},{"_id":"public/2019/08/31/阻塞锁/index.html","hash":"083cd4d15c2cd0e65436cb41258fff689cd07776","modified":1618655815709},{"_id":"public/2019/08/30/理解IO阻塞与非阻塞/index.html","hash":"134e714453238ed5d1747c5d04c6f441ba737999","modified":1618655815709},{"_id":"public/2019/08/31/自旋锁/index.html","hash":"7240a59a590525da1db2a86241a49332d173498a","modified":1618655815709},{"_id":"public/2019/08/30/TCP握手、挥手协议/index.html","hash":"9d3e6314b1e83f4faf873fbb9860bf61a2b52152","modified":1618655815709},{"_id":"public/2019/08/30/TCP-IP四层网络模型/index.html","hash":"b35a02f623f14a6c1afe51429b2793d8cf189d43","modified":1618655815709},{"_id":"public/archives/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/2019/08/29/Git梳理/index.html","hash":"482dcf629774ecfc3cd7b8d51db10b7efb71fc59","modified":1618655815709},{"_id":"public/2019/08/28/maven梳理/index.html","hash":"4520071d299aecf3cbf701270fe129af6e2c754f","modified":1618655815709},{"_id":"public/archives/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/4/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/5/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/6/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/8/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/7/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/9/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/10/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/page/11/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/page/4/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/page/5/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/08/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/08/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/09/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/10/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/11/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2019/12/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/5/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/4/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/6/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/page/7/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/01/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/01/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/03/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/07/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/07/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/08/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/09/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/10/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/11/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/12/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/12/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2020/12/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2021/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/archives/2021/04/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/4/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/5/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/6/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/8/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/7/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/9/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/10/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/page/11/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/面试/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/网络/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/分布式/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/CICD/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/软件管理/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/大数据/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/大数据/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/spring/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/spring/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/java基础/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/java基础/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/java基础/page/3/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/数据库/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/可信/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/操作系统/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/设计模式/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/java基础/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/categories/python基础/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/DNS/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/Disruptor/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/es/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/docker/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/GET-POST/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/hadoop/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/git/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/HDFS/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/HADOOP/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/哈希表/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/hive/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/httpclient/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/spring/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/https/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/java/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/JVM/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/SpringCloud/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/SpringCloud/page/2/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/nacos-config/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/并发/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/线程安全/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/Spark/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/TCP-IP/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/UDP/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/tomcat/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/JDK1-8新特性/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/k8s/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/elk/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/String/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/StringBuilder/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/StringBuffer/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/maven/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/mysql/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/锁/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/伪共享/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/信息系统项目管理师/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/CAP/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/分布式/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/可信/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/密码学/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/设计模式/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/可信计算/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/多线程/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/可靠/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/容错/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/javaagent/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/内存模型/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/数据结构/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/排序/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/加密算法/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/文件上传/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/IO/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/阻塞与非阻塞/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/python/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/线程/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/运算符/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/tags/网络安全/index.html","hash":"19210d2533929f9ddce037a8a46ec9a3b323e80f","modified":1618655815709},{"_id":"public/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1618655815709},{"_id":"public/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1618655815709},{"_id":"public/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1618655815709},{"_id":"public/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1618655815709},{"_id":"public/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1618655815709},{"_id":"public/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1618655815709},{"_id":"public/css/fonts/icomoon.svg","hash":"b5e7562c8494b0ddb3a70ecc5545ef7340d8e971","modified":1618655815709},{"_id":"public/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1618655815709},{"_id":"public/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1618655815709},{"_id":"public/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1618655815709},{"_id":"public/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1618655815709},{"_id":"public/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1618655815709},{"_id":"public/css/fonts/iconfont.svg","hash":"3630aabf2f9c0417f483ebd03d9e429dbc2594e0","modified":1618655815709},{"_id":"public/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1618655815709},{"_id":"public/img/145d5664.png","hash":"5de6263d58ac723c22250323e760162284317a71","modified":1618655815709},{"_id":"public/img/1618244110010.png","hash":"947439297da765710fa815ea74e0920af546e46b","modified":1618655815709},{"_id":"public/img/1618244166891.png","hash":"5fbb1c37f8bc0d5a9d12e931dbf7b24da9d180ce","modified":1618655815709},{"_id":"public/img/1618244289269.png","hash":"3505425f677bf63c3351d9bb723193659be5f0f1","modified":1618655815709},{"_id":"public/img/1618244497271.png","hash":"6e3f32bbe2c044ddbdfde51fda828ed6eb4190c4","modified":1618655815709},{"_id":"public/img/1618244387386.png","hash":"f57bf41422b093082b4b2f4b42afa9488f0bb8de","modified":1618655815709},{"_id":"public/img/1618244522945.png","hash":"3484b30a26d62ae602e3b7a6faf8b7b49bab0295","modified":1618655815709},{"_id":"public/img/1618244566899.png","hash":"7a5274a43d170806b63e4f3457c2f0af774a86cd","modified":1618655815709},{"_id":"public/img/227d0458.png","hash":"2840cba30e73d089195ec8d60e2bd567e96485e6","modified":1618655815709},{"_id":"public/img/480c47d9.png","hash":"d1b8b8bd0eb74760f8c54dee986e6fef466d6b6e","modified":1618655815709},{"_id":"public/img/4e12cc06.png","hash":"750185f3c87805cafe374b9ed6c6b4d6909c8d3a","modified":1618655815709},{"_id":"public/img/5a824e2f.png","hash":"7a187c1968dcd312f4480515f5ddb7b8a0815774","modified":1618655815709},{"_id":"public/img/5a377b3b.png","hash":"7a8b1a4998362f443f0989fd88d2424a93adb05e","modified":1618655815709},{"_id":"public/img/70db7f87.jpg","hash":"354e54d0f10bf2d4bb92ad58e51a542d66095e2c","modified":1618655815709},{"_id":"public/img/743e6cec.jpg","hash":"eda743b32da786d5287d69c54f66600b02dfb48a","modified":1618655815709},{"_id":"public/img/8fb0cd0c.png","hash":"3656d384f8348607e72d6ddb69f23ec7e3e1801b","modified":1618655815709},{"_id":"public/img/Git工作流程.png","hash":"13f5329292c5f0b42cb969b2981141981bc62c12","modified":1618655815709},{"_id":"public/img/JVM类加载过程.png","hash":"aa87f840b42869db7b76bd2789d1eaa8dcda2d0f","modified":1618655815709},{"_id":"public/img/StringBuilder.png","hash":"eaa519848fcd7948c1820e5e2a2f2ad719277708","modified":1618655815709},{"_id":"public/img/TCPIP服务器接收请求.png","hash":"dd6986ec9df5f3ffe5ff743e97408b53147743fc","modified":1618655815709},{"_id":"public/img/TCPIP用户发送请求.png","hash":"f4428ca4bf21e4d067da5b944bb742892939d131","modified":1618655815709},{"_id":"public/img/UDPTCPcompare.png","hash":"fc2c1380a717ad5e1ac5a85250d26d21e56a9f92","modified":1618655815709},{"_id":"public/img/a7ade6c9.png","hash":"f3c6bb2c3cf9073b99b71d5ed8c48bf7d9f85d77","modified":1618655815709},{"_id":"public/img/d94cd34e.png","hash":"d29a3d8cbf07390c741d3b0a1706461fc85e6590","modified":1618655815709},{"_id":"public/img/d325d29a.png","hash":"962a746a65c34d0e4ea2b5a3e9d23a30f1d35201","modified":1618655815709},{"_id":"public/img/e18c9709.jpg","hash":"06375b48ff834c994e894405ee00742bb12d962d","modified":1618655815709},{"_id":"public/img/ea179a09.png","hash":"145556ada619dcda0eb7f503930ea65ef73b3465","modified":1618655815709},{"_id":"public/img/es2.png","hash":"07f92f31d703b5659cf537a5b31b2ff9c55cbdc1","modified":1618655815709},{"_id":"public/img/fac717a5.png","hash":"793349e13d727eef0ec51f496e78116cdb7604ca","modified":1618655815709},{"_id":"public/img/https-intro.png","hash":"14850fa8463ff0892e5258b5884b4e92cb66f481","modified":1618655815709},{"_id":"public/img/image-20201206115832025.png","hash":"83d8dac7f6b2b8c02031d46c5d9a8748d2242228","modified":1618655815709},{"_id":"public/img/image-20201206121711716.png","hash":"e70f981b0e2661a084fe730e068bc889b4efb7ac","modified":1618655815709},{"_id":"public/img/image-20201206121627187.png","hash":"071a741e5d52a45d3ca56dccb5af33a190597f7f","modified":1618655815709},{"_id":"public/img/image-20201206121650739.png","hash":"2ad0274ed10bb42df9bddc574b430e023858e661","modified":1618655815709},{"_id":"public/img/image-20201214114013294.png","hash":"b88b3083b0bf5a8ca307695fc265dd07fd2e9bd9","modified":1618655815709},{"_id":"public/img/image-20201214121332594.png","hash":"4ab43a767fa96e721de32a3084870e7ef785bc8a","modified":1618655815709},{"_id":"public/img/image-20201214121613146.png","hash":"0855b48f6f372b5f20905d273cc9d780ea2c19dc","modified":1618655815709},{"_id":"public/img/image-20201214121721100.png","hash":"feade0b22e88fe8bdf2d97cf7d939f39b3466735","modified":1618655815709},{"_id":"public/img/image-20201214121645728.png","hash":"fdea15c1fff089408871ff3150b2c8b6c107173a","modified":1618655815709},{"_id":"public/img/image-20201214122255720.png","hash":"0238a48f3dcd45892e5263bda2e67d455ecdc631","modified":1618655815709},{"_id":"public/img/image-20201214125020584.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1618655815709},{"_id":"public/img/image-20201214124300675.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1618655815709},{"_id":"public/img/image-20201214125032862.png","hash":"2d9a0431305510a118e7136455247f2918f68708","modified":1618655815709},{"_id":"public/img/image-20201214124603810.png","hash":"57e972afad663ccbe766072af5e6bb8b3ec7115d","modified":1618655815709},{"_id":"public/img/image-20201214125453643.png","hash":"6ab7eff3ad3550f1a254bee7df5c505d1402955b","modified":1618655815709},{"_id":"public/img/image-20201214130757812.png","hash":"5d935f2e74081b23dff7c75ce43b3f088906ece6","modified":1618655815709},{"_id":"public/img/image-20201214131814678.png","hash":"e86681a9d954b4ab9ceeccef85949166532d22d7","modified":1618655815709},{"_id":"public/img/image-20201214131948506.png","hash":"8eb5e9f7c9d9f5f47e19509c702632a8ee881711","modified":1618655815709},{"_id":"public/img/image-20201214132127621.png","hash":"5e48bbea9baa60cb25cdff8a3b5ce28a0f0b935f","modified":1618655815709},{"_id":"public/img/image-20201214132014480.png","hash":"692652f4e430312e6d6a3414304b0c64e46d6d36","modified":1618655815709},{"_id":"public/img/image-20201214132059091.png","hash":"0e9f8c4f95304e59cc8af6adff897b0656925bea","modified":1618655815709},{"_id":"public/img/image-20201214132309723.png","hash":"9ee9276322c6160b294b49127b0ccfdd31006d6a","modified":1618655815709},{"_id":"public/img/image-20201214132326043.png","hash":"e2fcc5164dcfdd9125fd0f2b2a785de9c8668c3c","modified":1618655815709},{"_id":"public/img/image-20201214132343909.png","hash":"90f9e923863dcdeddfe91f1794c42e904011d33d","modified":1618655815709},{"_id":"public/img/image-20201214132416329.png","hash":"06b6ecf11a65a2616a7ba9b317bc4efb0dd2fb9c","modified":1618655815709},{"_id":"public/img/image-20201214132401207.png","hash":"29faa2ade6819a751e21d2ed734326369ba67afb","modified":1618655815709},{"_id":"public/img/image-20201214133601256.png","hash":"30359512fcf46fca69e040fd4446ea477b5c2646","modified":1618655815709},{"_id":"public/img/image-20201214133955605.png","hash":"df38a2d88ed44358447df314b1b05cb762fb3e34","modified":1618655815709},{"_id":"public/img/image-20201214133935704.png","hash":"53cb6b400f9247b609d3d6df67a79f65eabe1f55","modified":1618655815709},{"_id":"public/img/image-20201214134132982.png","hash":"4f48f02bee07e0e6be8cd93a463a70750abf12f6","modified":1618655815709},{"_id":"public/img/image-20201214134208507.png","hash":"a2ad991b50538e8a070fd5b390cdafd800c3affd","modified":1618655815709},{"_id":"public/img/image-20201214135053884.png","hash":"f6c523e663d41c1df848333892389670db4b14e5","modified":1618655815709},{"_id":"public/img/java对象存储.png","hash":"78231e4a1c56b2078cdd4454d20593ba3e2b48e3","modified":1618655815709},{"_id":"public/img/java对象存储3.png","hash":"4b83f6c1accd8ea2753e0dc16f52820d52abb700","modified":1618655815709},{"_id":"public/img/javaagent1.png","hash":"93c2b34bbbbb8ce1fccb0d0eb0daff97a7a2bdac","modified":1618655815709},{"_id":"public/img/java对象存储2.png","hash":"a5e540b6699183dcad89a9912c25c6a0c5eed9f3","modified":1618655815709},{"_id":"public/img/maven配置.png","hash":"c966dcfb8e36138b51cbaa08a8e15bc73a615778","modified":1618655815709},{"_id":"public/img/mysql排序1.png","hash":"09e8d6d507cd2b06ed4e6e9a76c832e226981ed0","modified":1618655815709},{"_id":"public/img/mysql排序4.png","hash":"9ef66bb2173d553fb58106a4959b3c0a5751d482","modified":1618655815709},{"_id":"public/img/mysql排序6.png","hash":"254e269dc8da6b5358e883e82a16c52243a4286e","modified":1618655815709},{"_id":"public/img/mysql排序2.png","hash":"0f0e1c1417f85b8e5f7bf1bb929ad63223569faf","modified":1618655815709},{"_id":"public/img/mysql排序5.png","hash":"044b6bc8c3b4d33f3a25598b9b99771efe75bccf","modified":1618655815709},{"_id":"public/img/nacos-producer.png","hash":"36eefd319d40a5a11e9343d3d817d66098d9d043","modified":1618655815709},{"_id":"public/img/nacos-springCloud1.png","hash":"afd0806eb1d72ed4988811e08d1c2e7cb621682b","modified":1618655815709},{"_id":"public/img/nacos-springCloud2.png","hash":"6ce7c90123f1a1eb0e3ebbe93ae50eded7392603","modified":1618655815709},{"_id":"public/img/nacos1.png","hash":"07c9cfee060b4090c04d5a9d422b97e2dd1cbbd2","modified":1618655815709},{"_id":"public/img/output_11_0.png","hash":"feae0728c34b976d209edf511796c8d86b374266","modified":1618655815709},{"_id":"public/img/output_11_1.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1618655815709},{"_id":"public/img/output_11_111.png","hash":"70dda01a5b3d039db7afac6313ef0697c4a21f5a","modified":1618655815709},{"_id":"public/img/output_13_0.png","hash":"74537a7efb66b8f35c874c0b69c1d0282ab92eec","modified":1618655815709},{"_id":"public/img/output_13_011.png","hash":"00d6df80990d980af9b8d244faba51db58904ebc","modified":1618655815709},{"_id":"public/img/output_14_0.png","hash":"ec71387d1c4c05e7a86a33c13aa3a08568e75827","modified":1618655815709},{"_id":"public/img/output_13_1.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1618655815709},{"_id":"public/img/output_13_111.png","hash":"d8cd2210777d18fff20b5ab5f3a043390bc9bcf9","modified":1618655815709},{"_id":"public/img/output_15_0.png","hash":"cc3a6ab17830f239b1e82500c967f6a3b40093f4","modified":1618655815709},{"_id":"public/img/output_19_0.png","hash":"d278d46ba923bcc6dfdbe29ca0d8547898ab1f2a","modified":1618655815709},{"_id":"public/img/output_15_011.png","hash":"d699a9e55c1d99653f37a36ab91be019a53e4bbf","modified":1618655815709},{"_id":"public/img/output_20_1.png","hash":"65f2cbd977d34618d64be151e40aa7c5dbe9ec9f","modified":1618655815709},{"_id":"public/img/output_17_0.png","hash":"e4f13d81304ba2ac471c18b70558fc4cea417666","modified":1618655815709},{"_id":"public/img/output_26_1.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1618655815709},{"_id":"public/img/output_22_0.png","hash":"9b8f87f0342ee9093359c92047f1438a4c474dff","modified":1618655815709},{"_id":"public/img/output_29_0.png","hash":"e4c9e227e39a0e97a133e1b96b9577534ff3cb4c","modified":1618655815709},{"_id":"public/img/output_26_111.png","hash":"874cb72d983cd6ce926c4f27aa5a0a11b19b2957","modified":1618655815709},{"_id":"public/img/output_31_0.png","hash":"0850ffa53150038f984d9861c87bebcc127662c2","modified":1618655815709},{"_id":"public/img/output_32_1.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1618655815709},{"_id":"public/img/output_32_111.png","hash":"7af28dd388673de04da3bfb3bf8467690a225348","modified":1618655815709},{"_id":"public/img/output_33_1.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1618655815709},{"_id":"public/img/output_33_111.png","hash":"ccc4cd949a206730c2f043f20f583ba0edcb3011","modified":1618655815709},{"_id":"public/img/output_34_1.png","hash":"219b0f0516b12045f827b0461e1be52eed5afa35","modified":1618655815709},{"_id":"public/img/output_36_1.png","hash":"14241a13199df5a0b3daeb5ab3c628f9bca15763","modified":1618655815709},{"_id":"public/img/output_42_0.png","hash":"5f66e78fbf687a4e955d043ca3c949036f445847","modified":1618655815709},{"_id":"public/img/output_40_0.png","hash":"a13785823254893b0bf7a2be92b5347507c44e01","modified":1618655815709},{"_id":"public/img/output_46_0.png","hash":"26fbd45ab4c82c99b073d1c940767c4732aa6de1","modified":1618655815709},{"_id":"public/img/output_52_0.png","hash":"0abbeb362227d28f260cef8812606514f6530b9c","modified":1618655815709},{"_id":"public/img/output_54_0.png","hash":"e889c6a42c4956042d69b1fa6c6bf75e60a9b41c","modified":1618655815709},{"_id":"public/img/output_44_0.png","hash":"8d3277c861b982ddcea78e95020eed54f412af3d","modified":1618655815709},{"_id":"public/img/output_8_0.png","hash":"0bcb929ddd78eab93836703fc4f859dfeef6c3ae","modified":1618655815709},{"_id":"public/img/output_8_1.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1618655815709},{"_id":"public/img/pasted-0.png","hash":"d4f20b5880ebe1cd4114af86e182d3a8042f29b6","modified":1618655815709},{"_id":"public/img/secondaryNameNode.jpg","hash":"6186b1ee9e43435bafb7abefde1824d5f66e8f8b","modified":1618655815709},{"_id":"public/img/output_8_111.png","hash":"f8434432105278525fea3d9c5db099805042cbb1","modified":1618655815709},{"_id":"public/img/user-log.png","hash":"f7bd7155b8a465ad32afb87f8ae386768833c64f","modified":1618655815709},{"_id":"public/img/wordcount.png","hash":"78dbf6fe038d27891bb860321f1087fa9f642e3c","modified":1618655815709},{"_id":"public/img/三次握手协议2.png","hash":"149fa2d315b1471325a89b68ee659409c709c10c","modified":1618655815709},{"_id":"public/img/三次握手协议1.png","hash":"b7cf27e8ba8b7e299c4e02519394600d0269ae84","modified":1618655815709},{"_id":"public/img/使用协议进行通讯.png","hash":"fe93735a4d871a544feb31b0b713f98ab4c715fb","modified":1618655815709},{"_id":"public/img/信任链.png","hash":"23517850fc98c07ad66d80bac4556b77e0688214","modified":1618655815709},{"_id":"public/img/公钥私钥6.png","hash":"c7104a322604a0b51de2d988bd9522b5deab2045","modified":1618655815709},{"_id":"public/img/公钥私钥8.png","hash":"6b13644b98068f7f44ddb51532f17030906031c0","modified":1618655815709},{"_id":"public/img/加密算法.png","hash":"5a0a4d8fbdc43f929dd5aa86aded45086ba38cbb","modified":1618655815709},{"_id":"public/img/可信根.png","hash":"875a1cdae0d77993f07a1df1dd968f907a2c1d1e","modified":1618655815709},{"_id":"public/img/对称加密算法.png","hash":"6a7ed179fb8b48e9054caba56308b1691b6ebe92","modified":1618655815709},{"_id":"public/img/数据库分布式ID生成.png","hash":"7376ea76ee62cc47d2e0389f90ca9fa1a3173f74","modified":1618655815709},{"_id":"public/img/线程相关1.jpg","hash":"cd2dfeb2b0a367d208e46d1fec4ed9241164256e","modified":1618655815709},{"_id":"public/img/线程相关2.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1618655815709},{"_id":"public/img/线程相关3.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1618655815709},{"_id":"public/img/线程相关4.jpg","hash":"3268689c6020136cda73d95d1553dfa6816aa60b","modified":1618655815709},{"_id":"public/img/线程相关5.jpg","hash":"54f4bd928b107b4da2faaa87fa74484b76900c0b","modified":1618655815709},{"_id":"public/img/锁的创建2.png","hash":"ebf05f8b81c02d9c880b9081e9ccc0b2b6c5e4ae","modified":1618655815709},{"_id":"public/img/锁的创建.png","hash":"ac72c945c263be025d83970d2cfdb6c240c38bbc","modified":1618655815709},{"_id":"public/img/阻塞IO.png","hash":"f5cfd961b871078b9b202d1e8d4810f3f126aaef","modified":1618655815709},{"_id":"public/img/雪花算法.png","hash":"522b23e1a5bc4bdf46b1d285233d815004847271","modified":1618655815709},{"_id":"public/img/avatar.jpg","hash":"525dc2b6ef38fee9d4c66e554f928934eadd9117","modified":1618655815709},{"_id":"public/img/weixin.jpg","hash":"59cf33d0f6ce9324dabe183f3d4551620959e987","modified":1618655815709},{"_id":"public/img/1337b059.png","hash":"934fedad21bc55fd4848e8478221aa8de7c30ee7","modified":1618655815709},{"_id":"public/img/1618244002479.png","hash":"476a48c86fe9853e6e3814d62967932180e87aac","modified":1618655815709},{"_id":"public/img/1618244193984.png","hash":"5f54d9d4ca1822d23caf1b86f07d11775d996d3c","modified":1618655815709},{"_id":"public/img/1618244251013.png","hash":"a4a3edf1be8118db34c7d6ccd951dbcfe29ad50c","modified":1618655815709},{"_id":"public/img/1618244361109.png","hash":"565b7cfe75ec1b5d599bdaa2be09166c5ffb7275","modified":1618655815709},{"_id":"public/img/1618244592782.png","hash":"4f2779efd3450377236256ecf7cc6c7525fcf57c","modified":1618655815709},{"_id":"public/img/7f3f75ca.png","hash":"3af248ce70e75081d255e4eaa66dd9e2aba6b5bb","modified":1618655815709},{"_id":"public/img/9cb319ab.png","hash":"7a7cd79948bc7091586b5b20a61aa70de5ce1827","modified":1618655815709},{"_id":"public/img/GETPOST.png","hash":"2a4f00d2d7cf57f885f347c1f6cdd5d0f477408c","modified":1618655815709},{"_id":"public/img/HDFS-liucheng.png","hash":"b37eef3d1e61aeaf650ea9860b40acff7c5fc1e4","modified":1618655815709},{"_id":"public/img/HDFS上传.png","hash":"7ca87724851bc20365821ed02ffcbe9de0768a0c","modified":1618655815709},{"_id":"public/img/HTTPS3.png","hash":"e3e58b8e4961a4533ac2a63154aee3c49322e379","modified":1618655815709},{"_id":"public/img/HTTPS4.png","hash":"98c925d3de7c588950cb1ce2be36b3346a0ee848","modified":1618655815709},{"_id":"public/img/HTTPS2.png","hash":"b792dee35f602f38fce149164dad4593c64a9650","modified":1618655815709},{"_id":"public/img/HTTPS5.png","hash":"7f96023634200693631288ec5047f024ee53f91c","modified":1618655815709},{"_id":"public/img/JVM Memory.png","hash":"053217d209af791dd65e97e321268cf17df92275","modified":1618655815709},{"_id":"public/img/Spark.png","hash":"ab45bdfcdd54f893ab046360fea5ccd51b887b3f","modified":1618655815709},{"_id":"public/img/StringStringBuilderStringBuffer.png","hash":"d836b560a0cf006cfa6d8d833c3453f131db7c12","modified":1618655815709},{"_id":"public/img/TCPIP模型.png","hash":"69c031664c0698c88a8b5d40b7b8e7e7afc53748","modified":1618655815709},{"_id":"public/img/TCP协议通讯过程.png","hash":"46efae085ca09e1445b9eac506c02c3462e2a16b","modified":1618655815709},{"_id":"public/img/UDPHeader.png","hash":"03991c09cb8a800bc68acc974430c1d6df3073d2","modified":1618655815709},{"_id":"public/img/ThreadLocal内部存储.png","hash":"1bcf081078acb1a514dd19ef48b740663837e899","modified":1618655815709},{"_id":"public/img/agent-costtime.png","hash":"1659bf6791dac773e8eb6d5ee3357b79d8702c12","modified":1618655815709},{"_id":"public/img/agent-costtime2.png","hash":"657b28cdec63d882c44e6010a7a78fdffbd6c186","modified":1618655815709},{"_id":"public/img/b235f114.png","hash":"605d8626ed4d395ea711cb82a141c2ce1e36f4d8","modified":1618655815709},{"_id":"public/img/b4d68165.png","hash":"5d7c542053cb704983fde9b21bc7cd74c9053628","modified":1618655815709},{"_id":"public/img/chartype.png","hash":"965a1655bf9364c0ec0b7a8773c74d8ce9d91805","modified":1618655815709},{"_id":"public/img/clip_image002.png","hash":"773f0b16657a294e5b51f44c9adc7cf0d4813728","modified":1618655815709},{"_id":"public/img/dataSource-behaviour-relative.png","hash":"702859e0f00906daf67b67515b0bd683148f8109","modified":1618655815709},{"_id":"public/img/es1.png","hash":"89281b1bef0d51742b32b70b774525c6ae08c95f","modified":1618655815709},{"_id":"public/img/fb523dd6.png","hash":"cf674b0aa3f9fedd73a0e9501121aaeadce2f834","modified":1618655815709},{"_id":"public/img/floattype.png","hash":"a7c29f3c863dfe4b4f3ea2ccea6b35002b7eb7ba","modified":1618655815709},{"_id":"public/img/hdfs创建文件夹.png","hash":"27daebbd4cf39ca74ffa88aa9865257de46beeb2","modified":1618655815709},{"_id":"public/img/hive-聚合2.png","hash":"69ab607095185fb63a23851f371ac09c3261728a","modified":1618655815709},{"_id":"public/img/hive数据结构.png","hash":"6d0a423af256a0aecb19cd1ef7d34f4eb327efde","modified":1618655815709},{"_id":"public/img/hive数据结构1.png","hash":"1251b48e87a11c926b56969b3de0db1077627a3a","modified":1618655815709},{"_id":"public/img/hive文本文件数据编码.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1618655815709},{"_id":"public/img/image-20201206115459406.png","hash":"b459dcbaa97aba5f099acae0f39145ad3daa61e5","modified":1618655815709},{"_id":"public/img/image-20201206115600801.png","hash":"793cd10108ad7b33685387becb060e215a2eb045","modified":1618655815709},{"_id":"public/img/image-20201206115738831.png","hash":"03a84a42d3f64d15ac10a3d3449e554dc184be28","modified":1618655815709},{"_id":"public/img/image-20201206122126739.png","hash":"57cc146ead1c09f0fb6d2e54ad6f3bec553db218","modified":1618655815709},{"_id":"public/img/image-20201214123508379.png","hash":"36aa63f8051ab7294256ea771bdde1930d350db3","modified":1618655815709},{"_id":"public/img/image-20201214125504552.png","hash":"0a1ac24eef9da3dbdb800c4d46175abc60eba89a","modified":1618655815709},{"_id":"public/img/image-20201214125529023.png","hash":"82d57e9f373789317595d49aa2e379260acb5a9f","modified":1618655815709},{"_id":"public/img/image-20201214131527522.png","hash":"9cb523d61f7f7178a933e7cee84a538d729a4026","modified":1618655815709},{"_id":"public/img/image-20201214131543066.png","hash":"00c552d3184a69128eb6b263119cb1c5cf6976ed","modified":1618655815709},{"_id":"public/img/image-20201214131734289.png","hash":"f63b13d6700ce20d9e476e880b0ef1d7f230e249","modified":1618655815709},{"_id":"public/img/image-20201214131847691.png","hash":"8c841533dd0f0e9d2965695be830eac48c571f04","modified":1618655815709},{"_id":"public/img/image-20201214132215125.png","hash":"fedaacf9919b879a2539c2faacb748456b0b3e62","modified":1618655815709},{"_id":"public/img/image-20201214132254636.png","hash":"c580027a831986c0f1ada1ec7cf5e47726a29d99","modified":1618655815709},{"_id":"public/img/image-20201214133612299.png","hash":"c8f1d908f463400ae615da444654759a79c9511b","modified":1618655815709},{"_id":"public/img/image-20201214133830361.png","hash":"eb52ab258a8bf959e9402b5abf071c9d3db1a2cf","modified":1618655815709},{"_id":"public/img/image-20201214134235050.png","hash":"da065bb97d038439d13aca2178dc7f204d397c9f","modified":1618655815709},{"_id":"public/img/inttype.png","hash":"e01f1199570a67cd2ae26c3012227718e2575056","modified":1618655815709},{"_id":"public/img/jvm2.png","hash":"6839ef36387dfa9daccb9d6395f486f9c6e73a4f","modified":1618655815709},{"_id":"public/img/jvm1.png","hash":"3cd098688ff2de0e071bc49739e257c18d0046af","modified":1618655815709},{"_id":"public/img/jvm3.png","hash":"e3b1fd453e531b527b569c4e28eaa04776f28fd5","modified":1618655815709},{"_id":"public/img/mysql时间存储.png","hash":"1db17e59e7b6dad480009153803c159d5a97cbfe","modified":1618655815709},{"_id":"public/img/mysql的ip存储.png","hash":"d06d3619438f57be43aa1236ec9ace4e1bf3d126","modified":1618655815709},{"_id":"public/img/transaction.png","hash":"787175d99cc453a5ff1692029e2043060b1a7514","modified":1618655815709},{"_id":"public/img/typetrans.png","hash":"485490a772062455bc238936c0607d48d4aa93b7","modified":1618655815709},{"_id":"public/img/user-behaviour.png","hash":"b0ce3ff640e61461a545a39c56d01a3b6447a68a","modified":1618655815709},{"_id":"public/img/wordcount-map.png","hash":"a1c10dc70e0fdef09e3db7f305e16cd2fdf631a1","modified":1618655815709},{"_id":"public/img/wordcount-split.png","hash":"27610016538c3dca06409dcfbd57f8a805f86624","modified":1618655815709},{"_id":"public/img/公钥私钥1.png","hash":"498ea1f36ce52b0ff2964c9423bf5eb5ab0ae804","modified":1618655815709},{"_id":"public/img/公钥私钥10.png","hash":"767005c089b3be3b91213d12fab4e462718a7787","modified":1618655815709},{"_id":"public/img/公钥私钥11.png","hash":"10755e3bcfc39d9bf3c8b9b5060ec5682a18cad3","modified":1618655815709},{"_id":"public/img/公钥私钥2.png","hash":"c60ac4f1a39665d9a2b923f3a78541fc663b0e6b","modified":1618655815709},{"_id":"public/img/公钥私钥3.png","hash":"63b5f3e415343c2a65d3274fe8d6e4ce4c26f490","modified":1618655815709},{"_id":"public/img/公钥私钥13.png","hash":"afd0297e43e61b9193b6dcbf9a71da610938dd2f","modified":1618655815709},{"_id":"public/img/公钥私钥4.png","hash":"683543865f578ea89be62b1af374df4de0f42069","modified":1618655815709},{"_id":"public/img/公钥私钥5.png","hash":"b2721631bbbac0306b12e70466fa49531f8bae30","modified":1618655815709},{"_id":"public/img/公钥私钥7.png","hash":"e7f712acb8d5e47d717f18494d4c5e22ea9b5468","modified":1618655815709},{"_id":"public/img/公钥私钥9.png","hash":"4c4d25e78aea05b34ccf9cd0c5025cfcb7bf94e3","modified":1618655815709},{"_id":"public/img/指针压缩2.png","hash":"2db8b3b15563eedb0a35b192c37760fec8f9c8fa","modified":1618655815709},{"_id":"public/img/指针压缩4.png","hash":"945b359ce34488105eab6422d8f397d8c76b4ea0","modified":1618655815709},{"_id":"public/img/指针压缩3.png","hash":"f142f7d0e15d58e03d02d99c0eba541e51e30a7f","modified":1618655815709},{"_id":"public/img/非对称加密算法.png","hash":"260798ec83168388a70e55c60b1a7e5a47e78246","modified":1618655815709},{"_id":"public/img/1618244544994.png","hash":"92f81e311f76ad0ac43170b8629248563e2af10f","modified":1618655815709},{"_id":"public/img/IO复用select模型.png","hash":"9f0c021b9b258025552f261e3ab8cbb2f96093ad","modified":1618655815709},{"_id":"public/img/StringUpdate.png","hash":"1cad86bc5aa015dfba1e0929e4dd98c708401011","modified":1618655815709},{"_id":"public/img/clip_image004.png","hash":"762418fea78372f2c8b67fc6f4dffb7045e9d449","modified":1618655815709},{"_id":"public/img/data-collect-analysis.png","hash":"19e4706ad65dfa32bf285624af839d4437db17df","modified":1618655815709},{"_id":"public/img/hive-聚合3.png","hash":"e3efe8d25521a72b1180deab85b38de9ecda0f5f","modified":1618655815709},{"_id":"public/img/image-20200117113506023.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1618655815709},{"_id":"public/img/mysql排序3.png","hash":"a91f727d4c99fb6289678a86f64ddd1c246568c7","modified":1618655815709},{"_id":"public/img/simpleDateFormat-alibaba.png","hash":"f45845a64160deda103dcca59d212712bfe544e6","modified":1618655815709},{"_id":"public/img/spark+hdfs.png","hash":"5ea801d62e8a34b8f08c627826d7f71369c2b749","modified":1618655815709},{"_id":"public/img/可信在云平台的基础架构.png","hash":"b507acb0ecbe5f39f783b7a82340a30bb6fd42d4","modified":1618655815709},{"_id":"public/css/mobile.css","hash":"5998f6fc27998596beb1e40e4bc3c43be2ed764c","modified":1618655815709},{"_id":"public/js/search.js","hash":"c80c9a231ee040c7adc07a477793873fb85ce8bc","modified":1618655815709},{"_id":"public/js/titleTip.js","hash":"81dca549063e29ba3a4a278f0f4388eba8a2167b","modified":1618655815709},{"_id":"public/css/hl_theme/atom-dark.css","hash":"88d11052a24e8100af6248eb4dbe1ce7b0e96408","modified":1618655815709},{"_id":"public/css/hl_theme/brown-paper.css","hash":"500c8e750373f6656ff49a7857c871ceedcf8777","modified":1618655815709},{"_id":"public/css/hl_theme/atom-light.css","hash":"d31edb9816dae6b01410028bceb91757a962f780","modified":1618655815709},{"_id":"public/css/hl_theme/darcula.css","hash":"4341074bae4bc9f0b86e32b623e27babc0159b6e","modified":1618655815709},{"_id":"public/css/hl_theme/github.css","hash":"e05a0806a508a26b9f3f3794b6b588ec6504ad3f","modified":1618655815709},{"_id":"public/css/hl_theme/github-gist.css","hash":"7a41c1c479d09df875f99f1f6d94aac42e9e2ad0","modified":1618655815709},{"_id":"public/css/hl_theme/gruvbox-dark.css","hash":"8c440d9b4ee19ac03eaee3c6af78ba52e5ba5535","modified":1618655815709},{"_id":"public/css/hl_theme/gruvbox-light.css","hash":"30514aaa242a34647aa666cfca4fc74c595ea8f2","modified":1618655815709},{"_id":"public/css/hl_theme/kimbie-light.css","hash":"0c61926c989163faefb031d27bce3e287d6e10f2","modified":1618655815709},{"_id":"public/css/hl_theme/kimbie-dark.css","hash":"728527fcc308da454722c119b89e6da3025bd1e3","modified":1618655815709},{"_id":"public/css/hl_theme/railscasts.css","hash":"511f2fd2a84d426e5da5cb17880cc08f73beb002","modified":1618655815709},{"_id":"public/css/hl_theme/rainbow.css","hash":"7ff4251938076ddb7e4e49413db82653e5b61321","modified":1618655815709},{"_id":"public/css/hl_theme/sublime.css","hash":"f65c5b116d9213afb9c324384a2f3bc86cb71121","modified":1618655815709},{"_id":"public/css/hl_theme/school-book.css","hash":"ffbbcd13a74ac2404262c50b7a43053dfd0096ff","modified":1618655815709},{"_id":"public/css/hl_theme/zenbum.css","hash":"0a78f74a93568e20b32ca7427c719e9bae9a0b55","modified":1618655815709},{"_id":"public/css/hl_theme/sunburst.css","hash":"8a135abac1512cf430d1d1ad2304b79afa1a4d6e","modified":1618655815709},{"_id":"public/css/style.css","hash":"db3b1cc156de2bd7563399cf74eeabf9abde50a7","modified":1618655815709},{"_id":"public/img/1618244073306.png","hash":"c23f3b75c0e00674c191e398da326fbabfdc1155","modified":1618655815709},{"_id":"public/img/HTTPS1.png","hash":"2cc9b05768fb05bd25da12dd3428e6f02dfe50ec","modified":1618655815709},{"_id":"public/img/DNS2.png","hash":"0e8ca00be831d63e2ba6f36efc36dac00b590330","modified":1618655815709},{"_id":"public/img/SpringBean3.png","hash":"2b96172eecc1cbd1e41ab9756c0a12836ebad948","modified":1618655815709},{"_id":"public/img/data-collect.png","hash":"f46c55acbc3e188b8c9cce341fb61d2db1027f31","modified":1618655815709},{"_id":"public/img/hdfs-read-file.png","hash":"861f90f65c5c1eac3ffea16430c75833aff5e089","modified":1618655815709},{"_id":"public/img/hdfs-write-file.png","hash":"3482163375f44f53c6ff7791e005d5ebc42bf0f8","modified":1618655815709},{"_id":"public/img/hive-数学函数.png","hash":"e15b0006e6290c17b2348d0840272b52be732e0e","modified":1618655815709},{"_id":"public/img/image-20201214125402694.png","hash":"c5023ea858552e39671ae6d0f4aeef4189cc1801","modified":1618655815709},{"_id":"public/img/image-20201214134902337.png","hash":"5d56cc52e66348ca9f9cc41c1b7b93fa30659f29","modified":1618655815709},{"_id":"public/img/spark-all.png","hash":"d52bb136b90c8f63acaba9398811dbf1642de23f","modified":1618655815709},{"_id":"public/img/公钥私钥12.png","hash":"fdadf38dfcfd50d4afb6939f7403946ebcf1217b","modified":1618655815709},{"_id":"public/img/椭圆曲线算法的基本原理.png","hash":"5f848a66630c6ba515286fa96dc36ed3a9a11106","modified":1618655815709},{"_id":"public/img/混合加密的方式.png","hash":"45b8a92df2e3c57f6af2d483c444a96e59929843","modified":1618655815709},{"_id":"public/img/非阻塞IO.png","hash":"58573268ff130afca209099ecda84d1c4eeff17f","modified":1618655815709},{"_id":"public/css/gitalk.css","hash":"58177ce227c50ee359fbf99a4fdd26058887afc5","modified":1618655815709},{"_id":"public/js/jquery.pjax.js","hash":"191c49fdb40dff115a49cfd2b30dffb888d86550","modified":1618655815709},{"_id":"public/css/fonts/selection.json","hash":"047b615ea32dc48dae5b964061427d41feaaafdf","modified":1618655815709},{"_id":"public/img/8ab72b98.png","hash":"04732de500f3fc10dada0c0c2b2f35004c87624b","modified":1618655815709},{"_id":"public/img/Hive-运算2.png","hash":"1422e43a9e0aba7dac5ea4b6eecddbf6adc6bfe7","modified":1618655815709},{"_id":"public/img/HTTPS6.png","hash":"be9aa6a1578dd12e1970db4731f7c9727f9bc17e","modified":1618655815709},{"_id":"public/img/TCP协议通讯过程2.png","hash":"9ae8696d16fd4ffdc46b6c005b598999dec3f9c1","modified":1618655815709},{"_id":"public/img/image-20201206115653987.png","hash":"6ecd4c60466dc5e4278f30761e2354568150992a","modified":1618655815709},{"_id":"public/img/主动免疫可信架构信任链传递示意图.png","hash":"ec45ff26518a046bd0f6274a8f7317222f05ea81","modified":1618655815709},{"_id":"public/img/指针压缩1.png","hash":"d26943d292b23b4cf544cee8b1a73aa4172a8920","modified":1618655815709},{"_id":"public/img/读写锁.png","hash":"f5c76702b3a276fc48835d2ad1a02859a4767e08","modified":1618655815709},{"_id":"public/js/script.js","hash":"d7efd27ade371c6e50d0d7481ffc0ec47018bad2","modified":1618655815709},{"_id":"public/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1618655815709},{"_id":"public/img/1618244137853.png","hash":"90a6d21417b376515ad5d1652f85eba0d61545c2","modified":1618655815709},{"_id":"public/img/1618295759372.png","hash":"369d456db024815c115664ca926d0356e0d9af2d","modified":1618655815709},{"_id":"public/img/Yarn.png","hash":"8b15f23ee56b0f7e2e5aa00157c1de50a0979da7","modified":1618655815709},{"_id":"public/img/hdfs.png","hash":"c3be7dde1cdd24c4765a87ddf07c75c70148e970","modified":1618655815709},{"_id":"public/img/四次挥手协议.png","hash":"112d408966bbd5f77373bda95aaa07fd952e23d4","modified":1618655815709},{"_id":"public/img/1618244623527.png","hash":"813c175d735bb6274abc069750ff9bda641937b7","modified":1618655815709},{"_id":"public/img/TCP协议通讯过程1.png","hash":"100f6ea49b8b585a10e0ac88e2486a902e221a5c","modified":1618655815709},{"_id":"public/img/hive-运算3.png","hash":"5bc45de4ce61992130c1c3636d2031969dd81cb1","modified":1618655815709},{"_id":"public/img/hive集合数据类型.png","hash":"3803153d332cc20f22be920a84a9e371617099f3","modified":1618655815709},{"_id":"public/img/jvmHeapStructure.png","hash":"002db89529955be1ab4647f6628bcb41474c9ec6","modified":1618655815709},{"_id":"public/img/select、epoll模型对比.png","hash":"cdc93cacbedcf65fb68878bd36da3929e6333e2b","modified":1618655815709},{"_id":"public/img/wordcount-reduce.png","hash":"99f7334d973e7a4ffaf5d34e7870b55d0016858b","modified":1618655815709},{"_id":"public/img/对象存储1.png","hash":"281b0b0c7d4c55df4000800b3f00888663e5ee25","modified":1618655815709},{"_id":"public/img/阻塞与非阻塞调用对比.png","hash":"cdee42373e7940d8f09575bf844ce08f49b4d1af","modified":1618655815709},{"_id":"public/img/DNS1.png","hash":"6ab8d7e53c1f65db2993710c9974276b7c6724fe","modified":1618655815709},{"_id":"public/img/hive-partition.png","hash":"c587e39d656360af68418b82172cf91d1b0e7cd2","modified":1618655815709},{"_id":"public/img/hive-算数运算符.png","hash":"54c7db33bbd7b5b8484727d7a988869609539d4c","modified":1618655815709},{"_id":"public/img/hive运算1.png","hash":"472120109358e0ec58e980b9d0fc9ab533764d4c","modified":1618655815709},{"_id":"public/img/hive-表生成函数.png","hash":"cf8bb24a3e4d1263993cc3df54d7777a3931eb64","modified":1618655815709},{"_id":"public/img/image-20201206121829515.png","hash":"c5523dda003b29efcb8231211c4d2045d61b0dec","modified":1618655815709},{"_id":"public/img/网络连接模型.png","hash":"f2f48effb8cb134f597011fda639932ea0d2fac2","modified":1618655815709},{"_id":"public/img/hive-聚合1.png","hash":"070d7feb1ef4626223019a5b205d43b24116dc4a","modified":1618655815709},{"_id":"public/img/Socket通讯模型.png","hash":"2e260e5f30b7e17b7dba072e65ac7860e8d7e6ce","modified":1618655815709},{"_id":"public/img/三次握手协议3.png","hash":"e6fe59d8473f94be2b361f4bda5be7d791022eec","modified":1618655815709},{"_id":"public/js/gitment.js","hash":"59a1e03f2b0ce61dd9bd405d3c52d3e07cc10dec","modified":1618655815709},{"_id":"public/img/四次挥手协议2.png","hash":"1a4890549075feacfa4010df20911537fc726e4b","modified":1618655815709},{"_id":"public/img/image-20210415193556837.png","hash":"56b313e3e79284892d99656549cc4b7f0335e296","modified":1618655815709},{"_id":"public/js/gitalk.js","hash":"a75ead28e6a1fab2a006cc7332ca2d2e868ce8e1","modified":1618655815709}],"Category":[{"name":"面试","_id":"cknllvpj80003l0t95tgc0d99"},{"name":"网络","_id":"cknllvpjb0008l0t9c49yd6hi"},{"name":"分布式","_id":"cknllvpjd000el0t94vmzcqng"},{"name":"CICD","_id":"cknllvpjj000ql0t9a0ej2kxk"},{"name":"软件管理","_id":"cknllvpjl000xl0t950z1g48h"},{"name":"大数据","_id":"cknllvpjt001gl0t9hfg3hjvj"},{"name":"spring","_id":"cknllvpkc003el0t92huv2fz4"},{"name":"java基础","_id":"cknllvpke003ll0t96nxhg4bf"},{"name":"数据库","_id":"cknllvpl3005wl0t95wx0djm3"},{"name":"操作系统","_id":"cknllvpl9006il0t94j24ayu2"},{"name":"可信","_id":"cknllvplc006tl0t93lwm7jrh"},{"name":"设计模式","_id":"cknllvple0072l0t9dkmqe3wb"},{"name":"python基础","_id":"cknllvplv0090l0t95wfc5nb9"}],"Data":[],"Page":[],"Post":[{"title":"信息系统项目管理师-信息化","author":"ztq","_content":"","source":"_drafts/信息系统项目管理师-信息化.md","raw":"---\ntitle: 信息系统项目管理师-信息化\nauthor: ztq\ntags:\n---\n","slug":"信息系统项目管理师-信息化","published":0,"date":"2021-04-17T10:15:18.586Z","updated":"2021-04-17T10:15:18.587Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpj10000l0t9fh877254","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"AtomicInteger","author":"郑天祺","date":"2020-07-24T07:52:00.000Z","_content":"\n# 1、介绍\t\t\n\nAtomicInteger属于JUC并发包下的原子类，继承关系如下：\n\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable\n```\n\njava 的并发机制中有三个特性：原子性、可见性和有序性。\n\nsynchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。\n\n# 2、先看一个例子：\n\n```java\npackage cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main {\n    private static volatile int a = 1;\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n        System.out.println(a);\n    }\n}\n```\n\n结果：\n\n```java\n1\n4\n4\n3\n5\n2\n```\n\n定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：\n\n（1）每个线程从内存中读取a的值\n\n（2）对a进行+1操作\n\n（3）把a重新刷新回内存\n\n当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。\n\n# 3、再看一个例子：\n\n```java\npackage cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main {\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n\t\tSystem.out.println(a);\n    }\n}\n\n```\n\n结果：\n\n```java\n2\n4\n1\n3\n5\n5\n```\n\n利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：\n\n# 4、AtomicInteger如何保证原子性\n\n## （1）、源码分析\n\nAtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）\n\n```java\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n```\n\n底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE\n\n### （1）U\n\n```java\nprivate static final Unsafe U = Unsafe.getUnsafe();\n```\n\n利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。\n\nCAS操作包含三个操作数：内存位置、预期原值及新值。\n\n执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。\n\nUnsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。\n\n### （2）VALUE\n\n```java\n private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n```\n\nVALUE是 long 类型的，代表的含义就是对象的地址的偏移量。\n\n```java\nU.getAndAddInt(this, VALUE, 1) + 1;\n```\n\nU 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；\n\n那么怎么保证 getAndAddInt() 方法是最新的值呢？\n\n```java\n@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) {\n    int v;\n    do {\n        v = getIntVolatile(o, offset);\n    } while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n}\n```\n\n底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：\n\nparam1：o 是当前对象\n\nparam2：offset 表示内存地址的偏移量\n\nparam3：v + delta 表示要增加的值\n\nAtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。\n\nCAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。\n\n\n\n## （2）、源码注释\n\n```java\npackage java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable {\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) {\n        value = initialValue;\n    }\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() {\n    }\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() {\n        return value;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) {\n        value = newValue;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) {\n        U.putIntRelease(this, VALUE, newValue);\n    }\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) {\n        return U.getAndSetInt(this, VALUE, newValue);\n    }\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) {\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue}\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) {\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() {\n        return U.getAndAddInt(this, VALUE, 1);\n    }\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() {\n        return U.getAndAddInt(this, VALUE, -1);\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) {\n        return U.getAndAddInt(this, VALUE, delta);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() {\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) {\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\t.................\n}\n```\n\n","source":"_posts/AtomicInteger.md","raw":"title: AtomicInteger\nauthor: 郑天祺\ntags:\n\n  - java基础\ncategories:\n  - 面试\ndate: 2020-07-24 15:52:00\n\n---\n\n# 1、介绍\t\t\n\nAtomicInteger属于JUC并发包下的原子类，继承关系如下：\n\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable\n```\n\njava 的并发机制中有三个特性：原子性、可见性和有序性。\n\nsynchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。\n\n# 2、先看一个例子：\n\n```java\npackage cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main {\n    private static volatile int a = 1;\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n        System.out.println(a);\n    }\n}\n```\n\n结果：\n\n```java\n1\n4\n4\n3\n5\n2\n```\n\n定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：\n\n（1）每个线程从内存中读取a的值\n\n（2）对a进行+1操作\n\n（3）把a重新刷新回内存\n\n当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。\n\n# 3、再看一个例子：\n\n```java\npackage cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main {\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) {\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue<>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i < 5; i++) {\n            pool.submit(() -> {\n                try {\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n            });\n        }\n\t\tSystem.out.println(a);\n    }\n}\n\n```\n\n结果：\n\n```java\n2\n4\n1\n3\n5\n5\n```\n\n利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：\n\n# 4、AtomicInteger如何保证原子性\n\n## （1）、源码分析\n\nAtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）\n\n```java\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n```\n\n底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE\n\n### （1）U\n\n```java\nprivate static final Unsafe U = Unsafe.getUnsafe();\n```\n\n利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。\n\nCAS操作包含三个操作数：内存位置、预期原值及新值。\n\n执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。\n\nUnsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。\n\n### （2）VALUE\n\n```java\n private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, \"value\");\n```\n\nVALUE是 long 类型的，代表的含义就是对象的地址的偏移量。\n\n```java\nU.getAndAddInt(this, VALUE, 1) + 1;\n```\n\nU 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；\n\n那么怎么保证 getAndAddInt() 方法是最新的值呢？\n\n```java\n@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) {\n    int v;\n    do {\n        v = getIntVolatile(o, offset);\n    } while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n}\n```\n\n底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：\n\nparam1：o 是当前对象\n\nparam2：offset 表示内存地址的偏移量\n\nparam3：v + delta 表示要增加的值\n\nAtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。\n\nCAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。\n\n\n\n## （2）、源码注释\n\n```java\npackage java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable {\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, \"value\");\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) {\n        value = initialValue;\n    }\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() {\n    }\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() {\n        return value;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) {\n        value = newValue;\n    }\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) {\n        U.putIntRelease(this, VALUE, newValue);\n    }\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) {\n        return U.getAndSetInt(this, VALUE, newValue);\n    }\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) {\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue}\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) {\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() {\n        return U.getAndAddInt(this, VALUE, 1);\n    }\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() {\n        return U.getAndAddInt(this, VALUE, -1);\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) {\n        return U.getAndAddInt(this, VALUE, delta);\n    }\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() {\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    }\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() {\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    }\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) {\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) {\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) {\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        }\n    }\n\t.................\n}\n```\n\n","slug":"AtomicInteger","published":1,"updated":"2020-07-27T10:40:30.037Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpj60001l0t977hteo77","content":"<h1>1、介绍</h1>\n<p>AtomicInteger属于JUC并发包下的原子类，继承关系如下：</p>\n<pre><code class=\"language-java\">public class AtomicInteger extends Number implements java.io.Serializable\n</code></pre>\n<p>java 的并发机制中有三个特性：原子性、可见性和有序性。</p>\n<p>synchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。</p>\n<h1>2、先看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main &#123;\n    private static volatile int a = 1;\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n        System.out.println(a);\n    &#125;\n&#125;\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">1\n4\n4\n3\n5\n2\n</code></pre>\n<p>定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：</p>\n<p>（1）每个线程从内存中读取a的值</p>\n<p>（2）对a进行+1操作</p>\n<p>（3）把a重新刷新回内存</p>\n<p>当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。</p>\n<h1>3、再看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main &#123;\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n\t\tSystem.out.println(a);\n    &#125;\n&#125;\n\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">2\n4\n1\n3\n5\n5\n</code></pre>\n<p>利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：</p>\n<h1>4、AtomicInteger如何保证原子性</h1>\n<h2 id=\"（1）、源码分析\">（1）、源码分析</h2>\n<p>AtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）</p>\n<pre><code class=\"language-java\">    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n</code></pre>\n<p>底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE</p>\n<h3 id=\"（1）U\">（1）U</h3>\n<pre><code class=\"language-java\">private static final Unsafe U = Unsafe.getUnsafe();\n</code></pre>\n<p>利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。</p>\n<p>CAS操作包含三个操作数：内存位置、预期原值及新值。</p>\n<p>执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。</p>\n<p>Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。</p>\n<h3 id=\"（2）VALUE\">（2）VALUE</h3>\n<pre><code class=\"language-java\"> private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n</code></pre>\n<p>VALUE是 long 类型的，代表的含义就是对象的地址的偏移量。</p>\n<pre><code class=\"language-java\">U.getAndAddInt(this, VALUE, 1) + 1;\n</code></pre>\n<p>U 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；</p>\n<p>那么怎么保证 getAndAddInt() 方法是最新的值呢？</p>\n<pre><code class=\"language-java\">@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) &#123;\n    int v;\n    do &#123;\n        v = getIntVolatile(o, offset);\n    &#125; while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n&#125;\n</code></pre>\n<p>底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：</p>\n<p>param1：o 是当前对象</p>\n<p>param2：offset 表示内存地址的偏移量</p>\n<p>param3：v + delta 表示要增加的值</p>\n<p>AtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。</p>\n<p>CAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。</p>\n<h2 id=\"（2）、源码注释\">（2）、源码注释</h2>\n<pre><code class=\"language-java\">package java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable &#123;\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) &#123;\n        value = initialValue;\n    &#125;\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() &#123;\n    &#125;\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() &#123;\n        return value;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) &#123;\n        value = newValue;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) &#123;\n        U.putIntRelease(this, VALUE, newValue);\n    &#125;\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) &#123;\n        return U.getAndSetInt(this, VALUE, newValue);\n    &#125;\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) &#123;\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue&#125;\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) &#123;\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() &#123;\n        return U.getAndAddInt(this, VALUE, 1);\n    &#125;\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() &#123;\n        return U.getAndAddInt(this, VALUE, -1);\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\t.................\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>AtomicInteger属于JUC并发包下的原子类，继承关系如下：</p>\n<pre><code class=\"language-java\">public class AtomicInteger extends Number implements java.io.Serializable\n</code></pre>\n<p>java 的并发机制中有三个特性：原子性、可见性和有序性。</p>\n<p>synchronized可以保证可见性、有序性，无法保证原子性，AtomicInteger作用是保证原子性。</p>\n<h1>2、先看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\npublic class Main &#123;\n    private static volatile int a = 1;\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \tSystem.out.println(a++);\n                \tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n        System.out.println(a);\n    &#125;\n&#125;\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">1\n4\n4\n3\n5\n2\n</code></pre>\n<p>定义变量a，保证a的可见性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，且最后结果不是5，因为：</p>\n<p>（1）每个线程从内存中读取a的值</p>\n<p>（2）对a进行+1操作</p>\n<p>（3）把a重新刷新回内存</p>\n<p>当CPU切换分片  或者   第三步（3）线程未刷新回内存，此时我们线程就读相当于脏数据。</p>\n<h1>3、再看一个例子：</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.util.concurrent.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class Main &#123;\n    private static AtomicInteger a = new AtomicInteger();\n\n    public static void main(String[] args) &#123;\n        // 创建线程工厂实例\n        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n        // 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\n        ExecutorService pool = new ThreadPoolExecutor(5, 50, 0L, TimeUnit.MILLISECONDS,\n                new LinkedBlockingQueue&lt;&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n        // \n        for (int i = 0; i &lt; 5; i++) &#123;\n            pool.submit(() -&gt; &#123;\n                try &#123;\n                \t\tSystem.out.println(a.incrementAndGet());\n                \t\tThread.sleep(500);\n\t\t\t\t&#125; catch (InterruptedException e) &#123;\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t&#125;\n            &#125;);\n        &#125;\n\t\tSystem.out.println(a);\n    &#125;\n&#125;\n\n</code></pre>\n<p>结果：</p>\n<pre><code class=\"language-java\">2\n4\n1\n3\n5\n5\n</code></pre>\n<p>利用AtomicInteger定义变量a，保证a的原子性。用5个线程分别a++，但是结果不是5，每次都有不同的结果，但是最后结果是5，因为：</p>\n<h1>4、AtomicInteger如何保证原子性</h1>\n<h2 id=\"（1）、源码分析\">（1）、源码分析</h2>\n<p>AtomicInteger使用了incrementAndGet函数（类中还有很多个API都是利用相同的方式保证原子性）</p>\n<pre><code class=\"language-java\">    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n\t/**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n</code></pre>\n<p>底层使用的是unsafe的getAndAddInt方法，对象 U 和 参数 VALUE</p>\n<h3 id=\"（1）U\">（1）U</h3>\n<pre><code class=\"language-java\">private static final Unsafe U = Unsafe.getUnsafe();\n</code></pre>\n<p>利用的是compareAndSwapInt，又称CAS，即比较并替换，实现并发算法时常用到的一种技术。</p>\n<p>CAS操作包含三个操作数：内存位置、预期原值及新值。</p>\n<p>执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。</p>\n<p>Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。在jdk1.9中，对Usafe进行了删除。</p>\n<h3 id=\"（2）VALUE\">（2）VALUE</h3>\n<pre><code class=\"language-java\"> private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n</code></pre>\n<p>VALUE是 long 类型的，代表的含义就是对象的地址的偏移量。</p>\n<pre><code class=\"language-java\">U.getAndAddInt(this, VALUE, 1) + 1;\n</code></pre>\n<p>U 通过 getAndAddInt()  方法，对原先对象的地址进行了加 1 操作，得到一个最新的值，然后+1；</p>\n<p>那么怎么保证 getAndAddInt() 方法是最新的值呢？</p>\n<pre><code class=\"language-java\">@HotSpotIntrinsicCandidate\npublic final int getAndAddInt(Object o, long offset, int delta) &#123;\n    int v;\n    do &#123;\n        v = getIntVolatile(o, offset);\n    &#125; while (!weakCompareAndSetInt(o, offset, v, v + delta));\n    return v;\n&#125;\n</code></pre>\n<p>底层通过 weakCompareAndSetInt 这个CAS机制来完成的增加操作：</p>\n<p>param1：o 是当前对象</p>\n<p>param2：offset 表示内存地址的偏移量</p>\n<p>param3：v + delta 表示要增加的值</p>\n<p>AtomicInteger的原理就是这，主要是通过Usafe的方式来完成的。Usafe又是通过CAS机制来实现的。</p>\n<p>CAS算法是乐观锁的一种，Java原子类中的递增操作就通过CAS自旋实现的。</p>\n<h2 id=\"（2）、源码注释\">（2）、源码注释</h2>\n<pre><code class=\"language-java\">package java.util.concurrent.atomic;\n\nimport java.lang.invoke.VarHandle;\nimport java.util.function.IntBinaryOperator;\nimport java.util.function.IntUnaryOperator;\nimport jdk.internal.misc.Unsafe;\n\n/**\n * 一个int值，可以进行原子更新\n */\npublic class AtomicInteger extends Number implements java.io.Serializable &#123;\n    private static final long serialVersionUID = 6214790243416807050L;\n\n    /*\n     * 该类打算使用VarHandles实现，但存在未解析的循环启动依赖项。\n     */\n    private static final Unsafe U = Unsafe.getUnsafe();\n    private static final long VALUE\n        = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);\n\n    private volatile int value;\n\n    /**\n     * 构造函数，使用给定的初始值创建新的AtomicInteger\n     *\n     * @param initialValue the initial value\n     */\n    public AtomicInteger(int initialValue) &#123;\n        value = initialValue;\n    &#125;\n\n    /**\n     * 构造函数，默认的AtomicInteger的value为0\n     */\n    public AtomicInteger() &#123;\n    &#125;\n\n    /**\n     * @return 返回当前值\n     * \n     */\n    public final int get() &#123;\n        return value;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     */\n    public final void set(int newValue) &#123;\n        value = newValue;\n    &#125;\n\n    /**\n     * 将值设置为newValue\n     * \n     * @param newValue 指定的新值\n     * @since 1.6\n     */\n    public final void lazySet(int newValue) &#123;\n        U.putIntRelease(this, VALUE, newValue);\n    &#125;\n\n    /**\n     *\n     *原子性地将值设置为newValue，并返回旧值\n     *\n     * @param newValue 指定的新值\n     * @return 返回旧值\n     */\n    public final int getAndSet(int newValue) &#123;\n        return U.getAndSetInt(this, VALUE, newValue);\n    &#125;\n\n    /**\n     * CAS\n     *\n     * 如果当前值等于expectedValue，原子将该值设置为newValue\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true，实际值与预期值不相等返回false\n     */\n    public final boolean compareAndSet(int expectedValue, int newValue) &#123;\n        return U.compareAndSetInt(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 如果当前值等于expectedValue，原子将该值设置为newValue&#125;\n     *\n     * @param expectedValue 指定的期望值\n     * @param newValue 指定的新值\n     * @return 如果成功返回true\n     * @since 9\n     */\n    public final boolean weakCompareAndSetPlain(int expectedValue, int newValue) &#123;\n        return U.weakCompareAndSetIntPlain(this, VALUE, expectedValue, newValue);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于getAndAdd(1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndIncrement() &#123;\n        return U.getAndAddInt(this, VALUE, 1);\n    &#125;\n\n    /**\n     * 原子递减当前值,\n     *\n     * 相当于getAndAdd(-1)\n     *\n     * @return 先前的值\n     */\n    public final int getAndDecrement() &#123;\n        return U.getAndAddInt(this, VALUE, -1);\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要加的值\n     * @return 先前的值\n     */\n    public final int getAndAdd(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta);\n    &#125;\n\n    /**\n     * 以原子方式递增当前值,\n     *\n     * 相当于addAndGet(1)\n     *\n     * @return 更新的值\n     */\n    public final int incrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, 1) + 1;\n    &#125;\n\n    /**\n     * 以原子方式递减当前值\n\n     * 相当于addAndGet(-1)\n     *\n     * @return 更新的值\n     */\n    public final int decrementAndGet() &#123;\n        return U.getAndAddInt(this, VALUE, -1) - 1;\n    &#125;\n\n    /**\n     * 以原子方式将给定值与当前值相加\n     * \n     * @param delta 要添加的值\n     * @return 更新的值\n     */\n    public final int addAndGet(int delta) &#123;\n        return U.getAndAddInt(this, VALUE, delta) + delta;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回先前的值\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 先前的值\n     * @since 1.8\n     */\n    public final int getAndUpdate(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return prev;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\n    /**\n     * 使用应用给定函数的结果以原子方式更新当前值，返回更新后的值。\n     * 该函数应该没有副作用，因为当尝试的更新由于线程之间的争用而失败时，可以重新应用该函数\n     *\n     * @param updateFunction 无副作用的功能\n     * @return 更新后的值\n     * @since 1.8\n     */\n    public final int updateAndGet(IntUnaryOperator updateFunction) &#123;\n        int prev = get(), next = 0;\n        for (boolean haveNext = false;;) &#123;\n            if (!haveNext)\n                next = updateFunction.applyAsInt(prev);\n            if (weakCompareAndSetVolatile(prev, next))\n                return next;\n            haveNext = (prev == (prev = get()));\n        &#125;\n    &#125;\n\t.................\n&#125;\n</code></pre>\n"},{"title":"DNS","author":"郑天祺","date":"2020-07-21T07:57:00.000Z","_content":"\n# 1、介绍\n\n​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。\n\n​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。\n\n​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。\n\n​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(如.com .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。\n\n​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。\n\n​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：\n\n​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;\n\n​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;\n\n​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。\n\n![image-20200721195559652](/img/DNS1.png)\n\n# 2、DNS解析原理\n\n![image-20200721195702373](/img/DNS2.png)\n\n\n\n通过9步来诠释DNS解析过程：\n\n（1）用户请求通过浏览器输入要访问网站的地址，例如：www.toutiao.com。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。\n\n（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步\n\n（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步\n\n（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址\n\n（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的\n\n（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL\n\n- A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。\n- CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。\n- TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server\n\n（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server\n\n​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。\n\n​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。\n\n（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步\n\n（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上\n\n\n\n\n\n\n\n借鉴于：https://network.51cto.com/art/202003/613009.htm \n\n作者：崔皓来源：[51CTO技术栈](https://www.toutiao.com/i6807234747488535054/)|2020-03-23 15:08","source":"_posts/DNS.md","raw":"title: DNS\nauthor: 郑天祺\ntags:\n\n  - DNS\ncategories:\n  - 网络\ndate: 2020-07-21 15:57:00\n\n---\n\n# 1、介绍\n\n​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。\n\n​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。\n\n​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。\n\n​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(如.com .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。\n\n​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。\n\n​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：\n\n​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;\n\n​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;\n\n​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。\n\n![image-20200721195559652](/img/DNS1.png)\n\n# 2、DNS解析原理\n\n![image-20200721195702373](/img/DNS2.png)\n\n\n\n通过9步来诠释DNS解析过程：\n\n（1）用户请求通过浏览器输入要访问网站的地址，例如：www.toutiao.com。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。\n\n（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步\n\n（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步\n\n（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址\n\n（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的\n\n（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL\n\n- A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。\n- CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。\n- TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server\n\n（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server\n\n​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。\n\n​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。\n\n（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步\n\n（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上\n\n\n\n\n\n\n\n借鉴于：https://network.51cto.com/art/202003/613009.htm \n\n作者：崔皓来源：[51CTO技术栈](https://www.toutiao.com/i6807234747488535054/)|2020-03-23 15:08","slug":"DNS","published":1,"updated":"2020-07-21T13:26:04.670Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpj70002l0t93yf25snr","content":"<h1>1、介绍</h1>\n<p>​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。</p>\n<p>​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。</p>\n<p>​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。</p>\n<p>​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(<a href=\"http://xn--bvs.com\">如.com</a> .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。</p>\n<p>​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。</p>\n<p>​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：</p>\n<p>​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;</p>\n<p>​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;</p>\n<p>​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。</p>\n<p><img src=\"/img/DNS1.png\" alt=\"image-20200721195559652\"></p>\n<h1>2、DNS解析原理</h1>\n<p><img src=\"/img/DNS2.png\" alt=\"image-20200721195702373\"></p>\n<p>通过9步来诠释DNS解析过程：</p>\n<p>（1）用户请求通过浏览器输入要访问网站的地址，例如：<a href=\"http://www.toutiao.com\">www.toutiao.com</a>。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。</p>\n<p>（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步</p>\n<p>（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步</p>\n<p>（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址</p>\n<p>（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的</p>\n<p>（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL</p>\n<ul>\n<li>A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。</li>\n<li>CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“<a href=\"http://host.mydomain.com\">host.mydomain.com</a>”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。</li>\n<li>TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server</li>\n</ul>\n<p>（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server</p>\n<p>​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。</p>\n<p>​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。</p>\n<p>（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步</p>\n<p>（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上</p>\n<p>借鉴于：<a href=\"https://network.51cto.com/art/202003/613009.htm\">https://network.51cto.com/art/202003/613009.htm</a></p>\n<p>作者：崔皓来源：<a href=\"https://www.toutiao.com/i6807234747488535054/\">51CTO技术栈</a>|2020-03-23 15:08</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t在互联网中是用IP来标识一台服务器的。IP地址虽然能够代表一台设备，但是由于记忆起来比较困难，所以将其替换成一个能够理解和识别的名字，这个名字我们称作为域名。</p>\n<p>​\t\t在域名后面会定义一个IP地址用来指向网站服务器。DNS负责域名到IP地址的对应。</p>\n<p>​\t\tDNS 是域名系统(Domain Name System，缩写：DNS)是互联网的一项服务。它将域名和IP地址相互映射的一个分布式数据库，在数据库中保存域名与IP的对照关系，从而使人更方便地访问互联网。</p>\n<p>​\t\tDNS解析是分布式存储的，从结构上来说最顶层是，根域名服务器(ROOT DNS Server)，存储260个顶级域名服务器的IP地址。对于Ipv4来说全球有13个根域名服务器，它储存了每个域(<a href=\"http://xn--bvs.com\">如.com</a> .net .cn)的解析和域名服务器的地址信息。简单的说，根域名服务器就是存放顶级域名服务器地址的。</p>\n<p>​\t\t在根域名服务器下一级就是，顶级域名服务器。例如.com的域名服务器，存储的是一些一级域名的权威DNS服务器地址(如toutiao.com的DNS)。</p>\n<p>​\t\t顶级域名又称一级域名，顶级域名可以分为三类，即gTLD、ccTLD和New gTLD：</p>\n<p>​\t\tgTLD：国际顶级域名(generic top-level domains，gTLD)，例如：.com/.net/.org等都属于gTLD;</p>\n<p>​\t\tccTLD：国家和地区顶级域名(country code top-level domains，简称ccTLD)，例如：中国是.cn域名，日本是.jp域名;</p>\n<p>​\t\tNew gTLD：新顶级域名(New gTLD)，例如：.xyz/.top/.red/.help等新顶级域名。</p>\n<p><img src=\"/img/DNS1.png\" alt=\"image-20200721195559652\"></p>\n<h1>2、DNS解析原理</h1>\n<p><img src=\"/img/DNS2.png\" alt=\"image-20200721195702373\"></p>\n<p>通过9步来诠释DNS解析过程：</p>\n<p>（1）用户请求通过浏览器输入要访问网站的地址，例如：<a href=\"http://www.toutiao.com\">www.toutiao.com</a>。浏览器会在自己的缓存中查找URL对应IP地址。如果之前访问过，保存了这个URL对应IP地址的缓存，那么就直接访问IP地址。如果没有缓存，进入到第2步。</p>\n<p>（2）通过计算机本地的Host文件配置，可以设置URL和IP地址的映射关系。比如windows下是通过C:\\windwos\\system32\\driver\\etc\\hosts文件来设置的，linux中则是/etc/named.confg文件。这里查找本地的Host文件，看是有IP地址的缓存。如果在文件中依旧没有找到映射关系，进入第3步</p>\n<p>（3）请求Local DNS Server，通过本地运营商获取URL和IP的映射关系。如果在校园网，DNS服务器就在学校，如果是小区网络，DNS服务器是运营商提供的。总之这个服务器在物理位置上离发起请求的计算机比较近。Local DNS Server缓存了大量的DNS解析结果。由于它的性能较好，物理上的距离又比较近，它通常会在很短的时间内返回指定域名的解析结果。80%的DNS解析需求在这一步就满足了。如果在这一步还是没有完成DNS解析，进入第4步</p>\n<p>（4）通过Root DNS Server进行解析，ROOT DNS Server会根据请求的URL 返回给Local DNS Server顶级域名服务器的地址。例如：查询的是”.com”的域名，就查询 gTL对应的域名服务器的地址</p>\n<p>（5）返回顶级域名服务器的地址以后，访问对应的顶级域名服务器(gTLD、ccTLD、New gTLD)，并且返回Name Server服务器地址。这个Name Server就是网站注册的域名服务器，上面包含了网站URL和IP的对应信息。例如你在某个域名服务提供商申请的域名，这个域名就由他们的服务器来解析。这个Name Server是由域名提供商维护的</p>\n<p>（6）Name Server会把指定域名的A记录或者CNAME返回给Local DNS Server，并且设置一个TTL</p>\n<ul>\n<li>A (Address) 记录是用来指定主机名(或域名)对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。</li>\n<li>CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“<a href=\"http://host.mydomain.com\">host.mydomain.com</a>”(A记录)。它同时提供WWW和MAIL服务，为了便于用户访问服务。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。</li>\n<li>TTL(Time To Live)：也就是设置这个DNS解析在Local DNS Server上面的过期时间。超过了这个过期时间，URL和IP的映射就会被删除，需要获取还要请求Name Server</li>\n</ul>\n<p>（7）如果此时获取的是A记录，那么就可以直接访问网站的IP了。但是通常来说大型的网站都会返回CNAME，然后将其传给GTM Server</p>\n<p>​\t\tGTM(Global Traffic Manager的简写)即全局流量管理，基于网宿智能DNS、分布式监控体系，实现实时故障切换及全球负载均衡，保障应用服务的持续高可用性。传给GTM的目的就是希望通过GTM的负载均衡机制，帮助用户找到最适合自己的服务器IP。</p>\n<p>​\t\t也就是离自己最近，性能最好，服务器状态最健康的。而且大多数的网站会做CDN缓存，此时就更需要使用GTM帮你找到网络节点中适合你的CDN缓存服务器。</p>\n<p>（8）找到CDN缓存服务器以后，可以直接从服务器上面获取一些静态资源，例如：HTML、CSS、JS和图片。但是一些动态资源，例如商品信息，订单信息，需要通过第9步</p>\n<p>（9）对于没有缓存的动态资源需要从应用服务器获取，在应用服务器与互联网之间通常有一层负载均衡器负责反向代理。有它路由到应用服务器上</p>\n<p>借鉴于：<a href=\"https://network.51cto.com/art/202003/613009.htm\">https://network.51cto.com/art/202003/613009.htm</a></p>\n<p>作者：崔皓来源：<a href=\"https://www.toutiao.com/i6807234747488535054/\">51CTO技术栈</a>|2020-03-23 15:08</p>\n"},{"title":"Disruptor","author":"ztq","date":"2021-04-13T06:17:00.000Z","_content":"\n# [disruptor](https://github.com/LMAX-Exchange/disruptor)\n\n------高性能的线程间消息传递框架\n\n## 介绍：\n\nDisruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。\n\n但是，Disruptor提供了与队列不同的关键功能：\n\n1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)\n\n2、为事件（events）预先分配内存空间\n\n3、针对极高的性能目标而实现的极度优化和无锁的设计；\n\n​    应用场景：\n\n## 类图：\n\n![img](/img/5a377b3b.png)\n\n## 核心概念：\n\n[RingBuffer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java) 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。\n\n[Sequence](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java) 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。\n\n[Sequencer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java) Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。\n\n [SequenceBarrier](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java) 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。\n\n [WaitStrategy](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java) 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。\n\nEvent 从生产者传递给消费者的数据单位**。**\n\n[EventProcessor](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java) 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。\n\n[EventHandler](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java) 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。\n\nProducer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。\n\n \n\n## DSL图：\n\n![img](/img/7f3f75ca.png)\n\n \n\nDisruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；\n\nRingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；\n\nEventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；\n\nEventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api\n\n## RingBuffer实现：\n\nRingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。\n\n1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）\n\n2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。\n\n3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。\n\n \n\n![img](/img/a7ade6c9.png)\n\n \n\nRingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。\n\n**消费者读取数据：**\n\n​    ![img](/img/4e12cc06.png)\n\n \n\n消费者(*Consumer*)是一个想从*RingBuffer*里读取数据的线程，它可以访问*ConsumerBarrier*对象——这个对象由*RingBuffer*创建并且代表消费者与*RingBuffer*进行交互。就像*RingBuffer*显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了*RingBuffer*里序号*8*之前（包括*8*）的所有数据，那么它期待访问的下一个序号是*9*。\n\n消费者可以调用*ConsumerBarrier*对象的*waitFor()*方法，传递它所需要的下一个序号.\n\nfinal long availableSeq = consumerBarrier.waitFor(nextSequence);\n\n*ConsumerBarrier*返回*RingBuffer*的最大可访问序号——在上面的例子中是*12*。\n\n接下来，消费者会一直原地停留，等待更多数据被写入*RingBuffer*。并且，一旦数据写入后消费者会收到通知——节点*9*，*10*，*11*和*12* 已写入。现在序号*12*到了，消费者可以让*ConsumerBarrier*去拿这些序号节点里的数据了\n\n![img](/img/227d0458.png)\n\n \n\n拿到了数据后，消费者(***Consumer***)会更新自己的标识(***cursor***)。\n\n \n\n## 这样做有助于平缓延迟的峰值？\n\n以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(*Consumer*)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（*RingBuffer*本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(*Consumer)*去读同一个*RingBuffer* ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在*Disruptor*的协调下实现真正的并发数据处理。\n\n## 生产者写入数据：\n\n \n\n 写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。\n\n RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。\n\nProducerBarrier如何防止RingBuffer重叠\n\n![img](/img/d94cd34e.png)\n\n \n\n在这幅图中，我们假设只有一个生产者写入 RingBuffer。\n\n ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。\n\n在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。\n\n现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。\n\n## 申请下一个节点：\n\n![img](/img/145d5664.png)\n\n \n\nProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。\n\n \n\n## 提交新的数据：\n\n![img](/img/b235f114.png)\n\n \n\n当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。\n\nProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。\n\n## ProducerBarrier上的批处理\n\n  Disruptor 可以同时在生产者和消费者两端实现批处理。\n\n![img](/img/b4d68165.png)\n\nProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。\n\n## 多个生产者的场景\n\n![img](/img/9cb319ab.png)\n\n现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。\n\n \n\nRingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。\n\n资料：\n\n官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction\n\n官翻https://www.cnblogs.com/daoqidelv/p/6995888.html\n\n博客http://ifeve.com/dissecting-disruptor-whats-so-special/\n\nhttps://my.oschina.net/u/1765168/blog/1807887\n\nhttps://www.jianshu.com/p/f6d0d0c2a647\n\nhttps://www.jianshu.com/p/4a202ef547cc\n\n补充：\n\n流程简图：\n\n![img](/img/d325d29a.png)\n\n等待策略\n\nBlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高\n\nBusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。\n\nLiteBlockingWaitStrategy实现方法也是阻塞等待\n\nSleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。\n\nTimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。\n\nYieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。\n\nPhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 \n\n新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享\n\n \n\n ","source":"_posts/Disruptor.md","raw":"title: Disruptor\nauthor: ztq\ntags:\n\n  - Disruptor\ncategories:\n  - 分布式\ndate: 2021-04-13 14:17:00\n---\n\n# [disruptor](https://github.com/LMAX-Exchange/disruptor)\n\n------高性能的线程间消息传递框架\n\n## 介绍：\n\nDisruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。\n\n但是，Disruptor提供了与队列不同的关键功能：\n\n1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)\n\n2、为事件（events）预先分配内存空间\n\n3、针对极高的性能目标而实现的极度优化和无锁的设计；\n\n​    应用场景：\n\n## 类图：\n\n![img](/img/5a377b3b.png)\n\n## 核心概念：\n\n[RingBuffer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java) 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。\n\n[Sequence](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java) 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。\n\n[Sequencer](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java) Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。\n\n [SequenceBarrier](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java) 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。\n\n [WaitStrategy](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java) 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。\n\nEvent 从生产者传递给消费者的数据单位**。**\n\n[EventProcessor](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java) 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。\n\n[EventHandler](https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java) 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。\n\nProducer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。\n\n \n\n## DSL图：\n\n![img](/img/7f3f75ca.png)\n\n \n\nDisruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；\n\nRingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；\n\nEventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；\n\nEventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api\n\n## RingBuffer实现：\n\nRingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。\n\n1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）\n\n2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。\n\n3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。\n\n \n\n![img](/img/a7ade6c9.png)\n\n \n\nRingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。\n\n**消费者读取数据：**\n\n​    ![img](/img/4e12cc06.png)\n\n \n\n消费者(*Consumer*)是一个想从*RingBuffer*里读取数据的线程，它可以访问*ConsumerBarrier*对象——这个对象由*RingBuffer*创建并且代表消费者与*RingBuffer*进行交互。就像*RingBuffer*显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了*RingBuffer*里序号*8*之前（包括*8*）的所有数据，那么它期待访问的下一个序号是*9*。\n\n消费者可以调用*ConsumerBarrier*对象的*waitFor()*方法，传递它所需要的下一个序号.\n\nfinal long availableSeq = consumerBarrier.waitFor(nextSequence);\n\n*ConsumerBarrier*返回*RingBuffer*的最大可访问序号——在上面的例子中是*12*。\n\n接下来，消费者会一直原地停留，等待更多数据被写入*RingBuffer*。并且，一旦数据写入后消费者会收到通知——节点*9*，*10*，*11*和*12* 已写入。现在序号*12*到了，消费者可以让*ConsumerBarrier*去拿这些序号节点里的数据了\n\n![img](/img/227d0458.png)\n\n \n\n拿到了数据后，消费者(***Consumer***)会更新自己的标识(***cursor***)。\n\n \n\n## 这样做有助于平缓延迟的峰值？\n\n以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(*Consumer*)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（*RingBuffer*本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(*Consumer)*去读同一个*RingBuffer* ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在*Disruptor*的协调下实现真正的并发数据处理。\n\n## 生产者写入数据：\n\n \n\n 写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。\n\n RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。\n\nProducerBarrier如何防止RingBuffer重叠\n\n![img](/img/d94cd34e.png)\n\n \n\n在这幅图中，我们假设只有一个生产者写入 RingBuffer。\n\n ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。\n\n在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。\n\n现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。\n\n## 申请下一个节点：\n\n![img](/img/145d5664.png)\n\n \n\nProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。\n\n \n\n## 提交新的数据：\n\n![img](/img/b235f114.png)\n\n \n\n当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。\n\nProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。\n\n## ProducerBarrier上的批处理\n\n  Disruptor 可以同时在生产者和消费者两端实现批处理。\n\n![img](/img/b4d68165.png)\n\nProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。\n\n## 多个生产者的场景\n\n![img](/img/9cb319ab.png)\n\n现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。\n\n \n\nRingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。\n\n资料：\n\n官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction\n\n官翻https://www.cnblogs.com/daoqidelv/p/6995888.html\n\n博客http://ifeve.com/dissecting-disruptor-whats-so-special/\n\nhttps://my.oschina.net/u/1765168/blog/1807887\n\nhttps://www.jianshu.com/p/f6d0d0c2a647\n\nhttps://www.jianshu.com/p/4a202ef547cc\n\n补充：\n\n流程简图：\n\n![img](/img/d325d29a.png)\n\n等待策略\n\nBlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高\n\nBusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。\n\nLiteBlockingWaitStrategy实现方法也是阻塞等待\n\nSleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。\n\nTimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。\n\nYieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。\n\nPhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 \n\n新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享\n\n \n\n ","slug":"Disruptor","published":1,"updated":"2021-04-13T06:31:53.290Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpj90005l0t9eg2t7xu1","content":"<h1><a href=\"https://github.com/LMAX-Exchange/disruptor\">disruptor</a></h1>\n<p>------高性能的线程间消息传递框架</p>\n<h2 id=\"介绍：\">介绍：</h2>\n<p>Disruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。</p>\n<p>但是，Disruptor提供了与队列不同的关键功能：</p>\n<p>1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)</p>\n<p>2、为事件（events）预先分配内存空间</p>\n<p>3、针对极高的性能目标而实现的极度优化和无锁的设计；</p>\n<p>​    应用场景：</p>\n<h2 id=\"类图：\">类图：</h2>\n<p><img src=\"/img/5a377b3b.png\" alt=\"img\"></p>\n<h2 id=\"核心概念：\">核心概念：</h2>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java\">RingBuffer</a> 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java\">Sequence</a> 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java\">Sequencer</a> Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java\">SequenceBarrier</a> 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java\">WaitStrategy</a> 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。</p>\n<p>Event 从生产者传递给消费者的数据单位**。**</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java\">EventProcessor</a> 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java\">EventHandler</a> 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。</p>\n<p>Producer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。</p>\n<h2 id=\"DSL图：\">DSL图：</h2>\n<p><img src=\"/img/7f3f75ca.png\" alt=\"img\"></p>\n<p>Disruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；</p>\n<p>RingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；</p>\n<p>EventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；</p>\n<p>EventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api</p>\n<h2 id=\"RingBuffer实现：\">RingBuffer实现：</h2>\n<p>RingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。</p>\n<p>1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）</p>\n<p>2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。</p>\n<p>3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。</p>\n<p><img src=\"/img/a7ade6c9.png\" alt=\"img\"></p>\n<p>RingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。</p>\n<p><strong>消费者读取数据：</strong></p>\n<p>​    <img src=\"/img/4e12cc06.png\" alt=\"img\"></p>\n<p>消费者(<em>Consumer</em>)是一个想从<em>RingBuffer</em>里读取数据的线程，它可以访问<em>ConsumerBarrier</em>对象——这个对象由<em>RingBuffer</em>创建并且代表消费者与<em>RingBuffer</em>进行交互。就像<em>RingBuffer</em>显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了<em>RingBuffer</em>里序号<em>8</em>之前（包括<em>8</em>）的所有数据，那么它期待访问的下一个序号是<em>9</em>。</p>\n<p>消费者可以调用<em>ConsumerBarrier</em>对象的*waitFor()*方法，传递它所需要的下一个序号.</p>\n<p>final long availableSeq = consumerBarrier.waitFor(nextSequence);</p>\n<p><em>ConsumerBarrier</em>返回<em>RingBuffer</em>的最大可访问序号——在上面的例子中是<em>12</em>。</p>\n<p>接下来，消费者会一直原地停留，等待更多数据被写入<em>RingBuffer</em>。并且，一旦数据写入后消费者会收到通知——节点<em>9</em>，<em>10</em>，<em>11</em>和<em>12</em> 已写入。现在序号<em>12</em>到了，消费者可以让<em>ConsumerBarrier</em>去拿这些序号节点里的数据了</p>\n<p><img src=\"/img/227d0458.png\" alt=\"img\"></p>\n<p>拿到了数据后，消费者(<em><strong>Consumer</strong></em>)会更新自己的标识(<em><strong>cursor</strong></em>)。</p>\n<h2 id=\"这样做有助于平缓延迟的峰值？\">这样做有助于平缓延迟的峰值？</h2>\n<p>以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(<em>Consumer</em>)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（<em>RingBuffer</em>本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(<em>Consumer)<em>去读同一个</em>RingBuffer</em> ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在<em>Disruptor</em>的协调下实现真正的并发数据处理。</p>\n<h2 id=\"生产者写入数据：\">生产者写入数据：</h2>\n<p>写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。</p>\n<p>RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。</p>\n<p>ProducerBarrier如何防止RingBuffer重叠</p>\n<p><img src=\"/img/d94cd34e.png\" alt=\"img\"></p>\n<p>在这幅图中，我们假设只有一个生产者写入 RingBuffer。</p>\n<p>ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。</p>\n<p>在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。</p>\n<p>现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。</p>\n<h2 id=\"申请下一个节点：\">申请下一个节点：</h2>\n<p><img src=\"/img/145d5664.png\" alt=\"img\"></p>\n<p>ProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。</p>\n<h2 id=\"提交新的数据：\">提交新的数据：</h2>\n<p><img src=\"/img/b235f114.png\" alt=\"img\"></p>\n<p>当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。</p>\n<p>ProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。</p>\n<h2 id=\"ProducerBarrier上的批处理\">ProducerBarrier上的批处理</h2>\n<p>Disruptor 可以同时在生产者和消费者两端实现批处理。</p>\n<p><img src=\"/img/b4d68165.png\" alt=\"img\"></p>\n<p>ProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。</p>\n<h2 id=\"多个生产者的场景\">多个生产者的场景</h2>\n<p><img src=\"/img/9cb319ab.png\" alt=\"img\"></p>\n<p>现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。</p>\n<p>RingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。</p>\n<p>资料：</p>\n<p>官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction</p>\n<p>官翻https://www.cnblogs.com/daoqidelv/p/6995888.html</p>\n<p>博客http://ifeve.com/dissecting-disruptor-whats-so-special/</p>\n<p><a href=\"https://my.oschina.net/u/1765168/blog/1807887\">https://my.oschina.net/u/1765168/blog/1807887</a></p>\n<p><a href=\"https://www.jianshu.com/p/f6d0d0c2a647\">https://www.jianshu.com/p/f6d0d0c2a647</a></p>\n<p><a href=\"https://www.jianshu.com/p/4a202ef547cc\">https://www.jianshu.com/p/4a202ef547cc</a></p>\n<p>补充：</p>\n<p>流程简图：</p>\n<p><img src=\"/img/d325d29a.png\" alt=\"img\"></p>\n<p>等待策略</p>\n<p>BlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高</p>\n<p>BusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。</p>\n<p>LiteBlockingWaitStrategy实现方法也是阻塞等待</p>\n<p>SleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。</p>\n<p>TimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。</p>\n<p>YieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。</p>\n<p>PhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<p>新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享</p>\n","site":{"data":{}},"excerpt":"","more":"<h1><a href=\"https://github.com/LMAX-Exchange/disruptor\">disruptor</a></h1>\n<p>------高性能的线程间消息传递框架</p>\n<h2 id=\"介绍：\">介绍：</h2>\n<p>Disruptor类似于java的BlockingQueue。与队列一样，Disruptor的目的是在同一进程内的线程之间传递数据。</p>\n<p>但是，Disruptor提供了与队列不同的关键功能：</p>\n<p>1、同一个“事件”可以有多个消费者，消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序(形成一个依赖图)</p>\n<p>2、为事件（events）预先分配内存空间</p>\n<p>3、针对极高的性能目标而实现的极度优化和无锁的设计；</p>\n<p>​    应用场景：</p>\n<h2 id=\"类图：\">类图：</h2>\n<p><img src=\"/img/5a377b3b.png\" alt=\"img\"></p>\n<h2 id=\"核心概念：\">核心概念：</h2>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/RingBuffer.java\">RingBuffer</a> 从3.0开始，RingBuffer仅负责存储和更新通过Disruptor的数据（事件）。它是Disruptor底层数据结构实现，核心类，是线程间交换数据的中转地。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequence.java\">Sequence</a> 序号，声明一个序号，用于跟踪ringbuffer中任务的变化和消费者的消费情况。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/Sequencer.java\">Sequencer</a> Sequencer 是 Disruptor 的真正核心。此接口有两个实现类 SingleProducerSequencer、MultiProducerSequencer ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/SequenceBarrier.java\">SequenceBarrier</a> 序号栅栏，管理和协调生产者的游标序号和各个消费者的序号，确保生产者不会覆盖消费者未来得及处理的消息，确保存在依赖的消费者之间能够按照正确的顺序处理。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WaitStrategy.java\">WaitStrategy</a> 定义Consumer如何进行等待下一个事件的策略（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现）。</p>\n<p>Event 从生产者传递给消费者的数据单位**。**</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventProcessor.java\">EventProcessor</a> 事件处理器，监听RingBuffer的事件，并消费可用事件，从RingBuffer读取的事件会交由实际的生产者实现类来消费；它会一直侦听下一个可用的序号，直到该序号对应的事件已经准备好。</p>\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/EventHandler.java\">EventHandler</a> 业务处理器，是实际消费者的接口，完成具体的业务逻辑实现，用户实现该接口，代表着消费者。</p>\n<p>Producer 生产者，用户线程充当该角色，producer向RingBuffer写入事件。</p>\n<h2 id=\"DSL图：\">DSL图：</h2>\n<p><img src=\"/img/7f3f75ca.png\" alt=\"img\"></p>\n<p>Disruptor——对外暴露的门面类，提供start()，stop()，消费者事件注册，生产者事件发布等api；</p>\n<p>RingBuffer——对生产者提供下一序号获取、entry元素获取、entry数据更改等api；</p>\n<p>EventHandler——消费者的接口定义，提供onEvent()方法，负责具体业务逻辑实现；</p>\n<p>EventHandlerGroup——业务处理器分组，管理多个业务处理器的依赖关系，提供then()、before()、after()等api</p>\n<h2 id=\"RingBuffer实现：\">RingBuffer实现：</h2>\n<p>RingBuffer顾名思义，就是一个内存环，每一次读写操作都循环利用这个内存环，从而避免频繁分配和回收内存，减轻GC压力，同时由于RingBuffer可以实现为无锁的队列，从而整体上大幅提高系统性能。</p>\n<p>1.RingBuffer是由一个大数组组成的。（比链表快，对CPU缓存友好）</p>\n<p>2.RingBuffer的“指针”（也称为序列或游标）是java long类型的（64位有符号数），指针采用往上计数自增的方式。</p>\n<p>3.RingBuffer中的指针进行按RingBuffer的size取模找出数组的下标来定位入口。为了提高性能，通常将RingBuffer的size大小设置成实际使用的2倍。</p>\n<p><img src=\"/img/a7ade6c9.png\" alt=\"img\"></p>\n<p>RingBuffer没有尾指针，只维护一个指向下一个可用位置的序号。RingBuffer和常用的队列之间的区别是，不删除buffer中的数据，也就是说这些数据一直存放在buffer中，直到新的数据覆盖他们。</p>\n<p><strong>消费者读取数据：</strong></p>\n<p>​    <img src=\"/img/4e12cc06.png\" alt=\"img\"></p>\n<p>消费者(<em>Consumer</em>)是一个想从<em>RingBuffer</em>里读取数据的线程，它可以访问<em>ConsumerBarrier</em>对象——这个对象由<em>RingBuffer</em>创建并且代表消费者与<em>RingBuffer</em>进行交互。就像<em>RingBuffer</em>显然需要一个序号才能找到下一个可用节点一样，消费者也需要知道它将要处理的序号——每个消费者都需要找到下一个它要访问的序号。在上面的例子中，消费者处理完了<em>RingBuffer</em>里序号<em>8</em>之前（包括<em>8</em>）的所有数据，那么它期待访问的下一个序号是<em>9</em>。</p>\n<p>消费者可以调用<em>ConsumerBarrier</em>对象的*waitFor()*方法，传递它所需要的下一个序号.</p>\n<p>final long availableSeq = consumerBarrier.waitFor(nextSequence);</p>\n<p><em>ConsumerBarrier</em>返回<em>RingBuffer</em>的最大可访问序号——在上面的例子中是<em>12</em>。</p>\n<p>接下来，消费者会一直原地停留，等待更多数据被写入<em>RingBuffer</em>。并且，一旦数据写入后消费者会收到通知——节点<em>9</em>，<em>10</em>，<em>11</em>和<em>12</em> 已写入。现在序号<em>12</em>到了，消费者可以让<em>ConsumerBarrier</em>去拿这些序号节点里的数据了</p>\n<p><img src=\"/img/227d0458.png\" alt=\"img\"></p>\n<p>拿到了数据后，消费者(<em><strong>Consumer</strong></em>)会更新自己的标识(<em><strong>cursor</strong></em>)。</p>\n<h2 id=\"这样做有助于平缓延迟的峰值？\">这样做有助于平缓延迟的峰值？</h2>\n<p>以前需要逐个节点地询问“我可以拿下一个数据吗？现在可以了么？现在呢？”，消费者(<em>Consumer</em>)现在只需要简单的说“当你拿到的数字比我这个要大的时候请告诉我”，函数返回值会告诉它有多少个新的节点可以读取数据了。因为这些新的节点的确已经写入了数据（<em>RingBuffer</em>本身的序号已经更新），而且消费者对这些节点的唯一操作是读而不是写，因此访问不用加锁。这太好了，不仅代码实现起来可以更加安全和简单，而且不用加锁使得速度更快。另一个好处是你可以用多个消费者(<em>Consumer)<em>去读同一个</em>RingBuffer</em> ，不需要加锁，也不需要用另外的队列来协调不同的线程(消费者)。这样你可以在<em>Disruptor</em>的协调下实现真正的并发数据处理。</p>\n<h2 id=\"生产者写入数据：\">生产者写入数据：</h2>\n<p>写入 RingBuffer 的过程涉及到两阶段提交 (two-phase commit)。首先，你的生产者需要申请 buffer 里的下一个节点。然后，当生产者向节点写完数据，它将会调用 ProducerBarrier 的 commit 方法。</p>\n<p>RingBuffer 还是与消费端一样提供了一个 ProducerBarrier 对象，让生产者通过它来写入 RingBuffer。</p>\n<p>ProducerBarrier如何防止RingBuffer重叠</p>\n<p><img src=\"/img/d94cd34e.png\" alt=\"img\"></p>\n<p>在这幅图中，我们假设只有一个生产者写入 RingBuffer。</p>\n<p>ConsumerTrackingProducerBarrier对象拥有所有正在访问 RingBuffer 的消费者列表。Disruptor 由消费者负责通知它们处理到了哪个序列号，而不是 RingBuffer。所以，如果我们想确定我们没有让 RingBuffer 重叠，需要检查所有的消费者们都读到了哪里。</p>\n<p>在上图中，有一个 消费者 顺利的读到了最大序号 12（用红色/粉色高亮）。第二个消费者 有点儿落后——可能它在做 I/O 操作之类的——它停在序号 3。因此消费者 2 在赶上消费者 1 之前要跑完整个RingBuffer一圈的距离。</p>\n<p>现在生产者想要写入 RingBuffer 中序号 3 占据的节点，因为它是 RingBuffer 当前游标的下一个节点。但是 ProducerBarrier 明白现在不能写入，因为有一个消费者正在占用它。所以，ProducerBarrier 停下来自旋 (spins)，等待，直到那个消费者离开。</p>\n<h2 id=\"申请下一个节点：\">申请下一个节点：</h2>\n<p><img src=\"/img/145d5664.png\" alt=\"img\"></p>\n<p>ProducerBarier会看到下一个节点——序号 3 那个已经可以用了。它会抢占这个节点上的 Entry（它是一个放写入到某个序号的 RingBuffer 数据的桶），把下一个序号（13）更新成 Entry 的序号，然后把 Entry 返回给生产者。生产者可以接着往 Entry 里写入数据。</p>\n<h2 id=\"提交新的数据：\">提交新的数据：</h2>\n<p><img src=\"/img/b235f114.png\" alt=\"img\"></p>\n<p>当生产者结束向 Entry 写入数据后，它会要求 ProducerBarrier 提交。</p>\n<p>ProducerBarrier先等待RingBuffer的游标追上当前的位置（对于单生产者这毫无意义－比如，我们已经知道游标到了 12 ，而且没有其他人正在写入RingBuffer）。然后 ProducerBarrier 更新 RingBuffer 的游标到刚才写入的 Entry 序号－在我们这儿是 13。接下来，ProducerBarrier 会让消费者知道buffer 中有新东西了。它戳一下 ConsumerBarrier 上的 WaitStrategy 对象说－“喂，醒醒！有事情发生了！”（注意－不同的 WaitStrategy 实现以不同的方式来实现提醒，取决于它是否采用阻塞模式）。现在消费者 1 可以读 Entry 13 的数据，消费者 2 可以读 Entry 13 以及前面的所有数据。</p>\n<h2 id=\"ProducerBarrier上的批处理\">ProducerBarrier上的批处理</h2>\n<p>Disruptor 可以同时在生产者和消费者两端实现批处理。</p>\n<p><img src=\"/img/b4d68165.png\" alt=\"img\"></p>\n<p>ProducerBarrier 知道 RingBuffer 的游标指向 12，而最慢的消费者在 9 的位置，它就可以让生产者写入节点 3，4，5，6，7 和 8，中间不需要再次检查消费者的位置。</p>\n<h2 id=\"多个生产者的场景\">多个生产者的场景</h2>\n<p><img src=\"/img/9cb319ab.png\" alt=\"img\"></p>\n<p>现在生产者 1 申请提交节点 13 的数据（生产者 1 发出的绿色箭头代表这个请求）。ProducerBarrier 让 ClaimStrategy 先等待 RingBuffer 的游标到达序号 12，当然现在已经到了。因此 RingBuffer 移动游标到 13，让 ProducerBarrier 戳一下 WaitStrategy 告诉所有人都知道 RingBuffer 有更新了。现在 ProducerBarrier 可以完成生产者 2 的请求，让 RingBuffer 移动游标到 14，并且通知所有人都知道。</p>\n<p>RingBuffer的内容顺序总是会遵循nextEntry()的初始调用顺序。也就是说，如果一个生产者在写入 RingBuffer 的时候暂停了，只有当它解除暂停后，其他等待中的提交才会立即执行。</p>\n<p>资料：</p>\n<p>官方https://github.com/LMAX-Exchange/disruptor/wiki/Introduction</p>\n<p>官翻https://www.cnblogs.com/daoqidelv/p/6995888.html</p>\n<p>博客http://ifeve.com/dissecting-disruptor-whats-so-special/</p>\n<p><a href=\"https://my.oschina.net/u/1765168/blog/1807887\">https://my.oschina.net/u/1765168/blog/1807887</a></p>\n<p><a href=\"https://www.jianshu.com/p/f6d0d0c2a647\">https://www.jianshu.com/p/f6d0d0c2a647</a></p>\n<p><a href=\"https://www.jianshu.com/p/4a202ef547cc\">https://www.jianshu.com/p/4a202ef547cc</a></p>\n<p>补充：</p>\n<p>流程简图：</p>\n<p><img src=\"/img/d325d29a.png\" alt=\"img\"></p>\n<p>等待策略</p>\n<p>BlockingWaitStrategy默认的等待策略。利用锁和等待机制的WaitStrategyCPU消耗少但是延迟比较高</p>\n<p>BusySpinWaitStrategy自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动当线程可以绑定到指定CPU(核)的时候可以使用这个策略。</p>\n<p>LiteBlockingWaitStrategy实现方法也是阻塞等待</p>\n<p>SleepingWaitStrategy是另一种较为平衡CPU消耗与延迟的WaitStrategy在不同次数的重试后采用不同的策略选择继续尝试或者让出CPU或者sleep。这种策略延迟不均匀。</p>\n<p>TimeoutBlockingWaitStrategy实现方法是阻塞给定的时间超过时间的话会抛出超时异常。</p>\n<p>YieldingWaitStrategy实现方法是先自旋(100次)不行再临时让出调度(yield)。和SleepingWaitStrategy一样也是一种高性能与CPU资源之间取舍的折中方案但这个策略不会带来显著的延迟抖动。</p>\n<p>PhasedBackoffWaitStrategy实现方法是先自旋(10000次)不行再临时让出调度(yield)不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<p>新消费者，怎么获取下标，每个核心类怎么用 实现方式。伪共享</p>\n"},{"title":"Disruptor中发布事件相关类","author":"ztq","date":"2021-04-13T06:27:00.000Z","_content":"\n \n\n### Disruptor中发布事件相关类\n\n#### RingBuffer、EventFactory\n\nEventFactory：提供给RingBuffer做事件预填充\n\nEvent事件：\n\n1、从生产者到消费者过程中所处理的数据单元；\n\n2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；\n\n \n\n### Disruptor中的等待策略\n\n#### WaitStrategy\n\n等待策略的接口\n\n#### BlockingWaitStrategy\n\nBlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略\n\n \n\n#### BusySpinWaitStrategy\n\nBusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。\n\n \n\n#### LiteBlockingWaitStrategy\n\n试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。\n\n从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。\n\n \n\n#### LiteTimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒\n\n \n\n#### PhasedBackoffWaitStrategy\n\nPhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。\n\n \n\n#### SleepingWaitStrategy\n\nSleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。\n\n \n\n#### TimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。\n\n \n\n#### YieldingWaitStrategy\n\nYielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。\n\n \n\n### 总结\n\n| 等待策略  所在包com.Imax.disruptor    | 描述                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| Class BlockingWaitStrategy            | 阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。 |\n| Class BusySpinWaitStrategy            | 自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。 |\n| Class LiteBlockingWaitStrategy        | 阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。 |\n| Class TimeoutBlockingWaitStrategy     | 阻塞给定的时间，超过时间的话会抛出超时异常。                 |\n| Class LiteTimeoutBlockingWaitStrategy | TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。 |\n| Class PhasedBackoffWaitStrategy       | 先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 |\n| Class SleepingWaitStrategy            | 先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。 |\n| Class YieldingWaitStrategy            | 在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。 |\n| Interface WaitStrategy                | 上述等待策略实现接口                                         |\n\n\n\n| 工具类 所在包com.imax.disruptor.util          | 描述                                                         |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| Enum DaemonThreadFactory                      | 访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程 |\n| Class ThreadHints                             | 用于运行时提高代码性能的提示，                               |\n| Class Util                                    | 主要用于计算的工具类                                         |\n| Enum BasicExecutor （com.lmax.disruptor.dsl） | 只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象 |\n| Class BasicExecutor（com.lmax.disruptor.dsl） | 默认提供的线程池对象                                         |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n\n\n\n| 序列类 所在包com.imax.disruptor         | 描述                                                         |\n| --------------------------------------- | ------------------------------------------------------------ |\n| Class Sequence                          | 环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作 |\n| Class SequenceGroup                     | 继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。 |\n| Class SequenceGroups                    | 用于管理SequenceGroup对象的静态方法                          |\n| Class FixedSequenceGroup                | 包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的\"写\"方法都不支持。 |\n| Interface Sequencer                     | 通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier |\n| Interface SequenceBarrier               | 消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。 |\n| Interface ProcessingSequenceBarrier     | SequenceBarrier的具体实现                                    |\n| Class AbstractSequencer                 | AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列 |\n| Class SingleProducerSequencer           | 申请序列，发布序列，唤醒消费者                               |\n| Class MultiProducerSequencer            | 适用于多线程的消费者，申请序列，发布序列，唤醒消费者         |\n| Interface Sequenced                     | Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的 |\n| Interface Cursored                      | Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。 |\n| Interface SequenceReportingEventHandler | 在完成消费事件时通知并设置回调                               |\n\n\n\n| 队列类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface DataProvider          | 提供了按序列值来获取数据的功能                               |\n| Interface EventSink             | EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。 |\n| Class RingBuffer                | 数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。 |\n| Class DataProvider              | DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 * |\n|                                 |                                                              |\n|                                 |                                                              |\n|                                 |                                                              |\n\n \n\n| 异常处理类 所在包com.imax.disruptor                     |                                                              |\n| ------------------------------------------------------- | ------------------------------------------------------------ |\n| Interface ExceptionHandler                              | 事件处理周期中未捕获异常的回调处理程序的接口                 |\n| Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)   | 异常处理的包装类                                             |\n| Class IgnoreExceptionHandler                            | INFO的异常处理程序的便捷实现                                 |\n| Class FatalExceptionHandler                             | SEVERE(严重)的异常处理程序的便捷实                           |\n| Class InsufficientCapacityException                     | 如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常 |\n| Class ExceptionHandlerSetting（com.lmax.disruptor.dsl） | 为特定消费者设置异常处理程序的支持类                         |\n|                                                         |                                                              |\n\n\n\n| 事件类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Inetface EventSink              | 这个类主要是提供发布事件(就是往队列上放数据)的功能           |\n| Interface EventFactory          | 由RingBuffer调用，以预先调用所有事件以填充RingBuffer         |\n| Interface EventHandler          | 回调接口，用于处理RingBuffer中可用的事件                     |\n| Class EventPoller               | 用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller |\n| Interface EventProcessor        | 事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件 |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface EventTranslator       | 在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。 |\n\n \n\n时序图\n\n![img](/img/8ab72b98.png)\n\n 类图\n\n![1618295759372](/img/1618295759372.png)\n\n参考资料：https://brokendreams.iteye.com/blog/2255720\n\nhttp://www.ibigdata.io/?p=92\n\n ","source":"_posts/Disruptor中发布事件相关类.md","raw":"title: Disruptor中发布事件相关类\nauthor: ztq\ntags:\n\n  - Disruptor\ncategories:\n  - 分布式\ndate: 2021-04-13 14:27:00\n\n---\n\n \n\n### Disruptor中发布事件相关类\n\n#### RingBuffer、EventFactory\n\nEventFactory：提供给RingBuffer做事件预填充\n\nEvent事件：\n\n1、从生产者到消费者过程中所处理的数据单元；\n\n2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；\n\n \n\n### Disruptor中的等待策略\n\n#### WaitStrategy\n\n等待策略的接口\n\n#### BlockingWaitStrategy\n\nBlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略\n\n \n\n#### BusySpinWaitStrategy\n\nBusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。\n\n \n\n#### LiteBlockingWaitStrategy\n\n试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。\n\n从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。\n\n \n\n#### LiteTimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒\n\n \n\n#### PhasedBackoffWaitStrategy\n\nPhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。\n\n \n\n#### SleepingWaitStrategy\n\nSleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。\n\n \n\n#### TimeoutBlockingWaitStrategy\n\nTimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。\n\n \n\n#### YieldingWaitStrategy\n\nYielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。\n\n \n\n### 总结\n\n| 等待策略  所在包com.Imax.disruptor    | 描述                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| Class BlockingWaitStrategy            | 阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。 |\n| Class BusySpinWaitStrategy            | 自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。 |\n| Class LiteBlockingWaitStrategy        | 阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。 |\n| Class TimeoutBlockingWaitStrategy     | 阻塞给定的时间，超过时间的话会抛出超时异常。                 |\n| Class LiteTimeoutBlockingWaitStrategy | TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。 |\n| Class PhasedBackoffWaitStrategy       | 先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。 |\n| Class SleepingWaitStrategy            | 先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。 |\n| Class YieldingWaitStrategy            | 在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。 |\n| Interface WaitStrategy                | 上述等待策略实现接口                                         |\n\n\n\n| 工具类 所在包com.imax.disruptor.util          | 描述                                                         |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| Enum DaemonThreadFactory                      | 访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程 |\n| Class ThreadHints                             | 用于运行时提高代码性能的提示，                               |\n| Class Util                                    | 主要用于计算的工具类                                         |\n| Enum BasicExecutor （com.lmax.disruptor.dsl） | 只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象 |\n| Class BasicExecutor（com.lmax.disruptor.dsl） | 默认提供的线程池对象                                         |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n|                                               |                                                              |\n\n\n\n| 序列类 所在包com.imax.disruptor         | 描述                                                         |\n| --------------------------------------- | ------------------------------------------------------------ |\n| Class Sequence                          | 环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作 |\n| Class SequenceGroup                     | 继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。 |\n| Class SequenceGroups                    | 用于管理SequenceGroup对象的静态方法                          |\n| Class FixedSequenceGroup                | 包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的\"写\"方法都不支持。 |\n| Interface Sequencer                     | 通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier |\n| Interface SequenceBarrier               | 消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。 |\n| Interface ProcessingSequenceBarrier     | SequenceBarrier的具体实现                                    |\n| Class AbstractSequencer                 | AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列 |\n| Class SingleProducerSequencer           | 申请序列，发布序列，唤醒消费者                               |\n| Class MultiProducerSequencer            | 适用于多线程的消费者，申请序列，发布序列，唤醒消费者         |\n| Interface Sequenced                     | Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的 |\n| Interface Cursored                      | Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。 |\n| Interface SequenceReportingEventHandler | 在完成消费事件时通知并设置回调                               |\n\n\n\n| 队列类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface DataProvider          | 提供了按序列值来获取数据的功能                               |\n| Interface EventSink             | EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。 |\n| Class RingBuffer                | 数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。 |\n| Class DataProvider              | DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 * |\n|                                 |                                                              |\n|                                 |                                                              |\n|                                 |                                                              |\n\n \n\n| 异常处理类 所在包com.imax.disruptor                     |                                                              |\n| ------------------------------------------------------- | ------------------------------------------------------------ |\n| Interface ExceptionHandler                              | 事件处理周期中未捕获异常的回调处理程序的接口                 |\n| Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)   | 异常处理的包装类                                             |\n| Class IgnoreExceptionHandler                            | INFO的异常处理程序的便捷实现                                 |\n| Class FatalExceptionHandler                             | SEVERE(严重)的异常处理程序的便捷实                           |\n| Class InsufficientCapacityException                     | 如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常 |\n| Class ExceptionHandlerSetting（com.lmax.disruptor.dsl） | 为特定消费者设置异常处理程序的支持类                         |\n|                                                         |                                                              |\n\n\n\n| 事件类 所在包com.imax.disruptor |                                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| Inetface EventSink              | 这个类主要是提供发布事件(就是往队列上放数据)的功能           |\n| Interface EventFactory          | 由RingBuffer调用，以预先调用所有事件以填充RingBuffer         |\n| Interface EventHandler          | 回调接口，用于处理RingBuffer中可用的事件                     |\n| Class EventPoller               | 用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller |\n| Interface EventProcessor        | 事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件 |\n| Interface EventSequencer        | EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。 |\n| Interface EventTranslator       | 在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。 |\n\n \n\n时序图\n\n![img](/img/8ab72b98.png)\n\n 类图\n\n![1618295759372](/img/1618295759372.png)\n\n参考资料：https://brokendreams.iteye.com/blog/2255720\n\nhttp://www.ibigdata.io/?p=92\n\n ","slug":"Disruptor中发布事件相关类","published":1,"updated":"2021-04-13T06:36:35.621Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpja0006l0t9byt49oqd","content":"<h3 id=\"Disruptor中发布事件相关类\">Disruptor中发布事件相关类</h3>\n<h4 id=\"RingBuffer、EventFactory\">RingBuffer、EventFactory</h4>\n<p>EventFactory：提供给RingBuffer做事件预填充</p>\n<p>Event事件：</p>\n<p>1、从生产者到消费者过程中所处理的数据单元；</p>\n<p>2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；</p>\n<h3 id=\"Disruptor中的等待策略\">Disruptor中的等待策略</h3>\n<h4 id=\"WaitStrategy\">WaitStrategy</h4>\n<p>等待策略的接口</p>\n<h4 id=\"BlockingWaitStrategy\">BlockingWaitStrategy</h4>\n<p>BlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略</p>\n<h4 id=\"BusySpinWaitStrategy\">BusySpinWaitStrategy</h4>\n<p>BusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</p>\n<h4 id=\"LiteBlockingWaitStrategy\">LiteBlockingWaitStrategy</h4>\n<p>试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。</p>\n<p>从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</p>\n<h4 id=\"LiteTimeoutBlockingWaitStrategy\">LiteTimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒</p>\n<h4 id=\"PhasedBackoffWaitStrategy\">PhasedBackoffWaitStrategy</h4>\n<p>PhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<h4 id=\"SleepingWaitStrategy\">SleepingWaitStrategy</h4>\n<p>SleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。<br>\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</p>\n<h4 id=\"TimeoutBlockingWaitStrategy\">TimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。</p>\n<h4 id=\"YieldingWaitStrategy\">YieldingWaitStrategy</h4>\n<p>Yielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。</p>\n<h3 id=\"总结\">总结</h3>\n<table>\n<thead>\n<tr>\n<th>等待策略  所在包com.Imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class BlockingWaitStrategy</td>\n<td>阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。</td>\n</tr>\n<tr>\n<td>Class BusySpinWaitStrategy</td>\n<td>自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</td>\n</tr>\n<tr>\n<td>Class LiteBlockingWaitStrategy</td>\n<td>阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</td>\n</tr>\n<tr>\n<td>Class TimeoutBlockingWaitStrategy</td>\n<td>阻塞给定的时间，超过时间的话会抛出超时异常。</td>\n</tr>\n<tr>\n<td>Class LiteTimeoutBlockingWaitStrategy</td>\n<td>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。</td>\n</tr>\n<tr>\n<td>Class PhasedBackoffWaitStrategy</td>\n<td>先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</td>\n</tr>\n<tr>\n<td>Class SleepingWaitStrategy</td>\n<td>先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</td>\n</tr>\n<tr>\n<td>Class YieldingWaitStrategy</td>\n<td>在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。</td>\n</tr>\n<tr>\n<td>Interface WaitStrategy</td>\n<td>上述等待策略实现接口</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>工具类 所在包com.imax.disruptor.util</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Enum DaemonThreadFactory</td>\n<td>访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程</td>\n</tr>\n<tr>\n<td>Class ThreadHints</td>\n<td>用于运行时提高代码性能的提示，</td>\n</tr>\n<tr>\n<td>Class Util</td>\n<td>主要用于计算的工具类</td>\n</tr>\n<tr>\n<td>Enum BasicExecutor （com.lmax.disruptor.dsl）</td>\n<td>只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象</td>\n</tr>\n<tr>\n<td>Class BasicExecutor（com.lmax.disruptor.dsl）</td>\n<td>默认提供的线程池对象</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>序列类 所在包com.imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class Sequence</td>\n<td>环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作</td>\n</tr>\n<tr>\n<td>Class SequenceGroup</td>\n<td>继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。</td>\n</tr>\n<tr>\n<td>Class SequenceGroups</td>\n<td>用于管理SequenceGroup对象的静态方法</td>\n</tr>\n<tr>\n<td>Class FixedSequenceGroup</td>\n<td>包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的&quot;写&quot;方法都不支持。</td>\n</tr>\n<tr>\n<td>Interface Sequencer</td>\n<td>通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier</td>\n</tr>\n<tr>\n<td>Interface SequenceBarrier</td>\n<td>消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。</td>\n</tr>\n<tr>\n<td>Interface ProcessingSequenceBarrier</td>\n<td>SequenceBarrier的具体实现</td>\n</tr>\n<tr>\n<td>Class AbstractSequencer</td>\n<td>AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列</td>\n</tr>\n<tr>\n<td>Class SingleProducerSequencer</td>\n<td>申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Class MultiProducerSequencer</td>\n<td>适用于多线程的消费者，申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Interface Sequenced</td>\n<td>Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的</td>\n</tr>\n<tr>\n<td>Interface Cursored</td>\n<td>Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。</td>\n</tr>\n<tr>\n<td>Interface SequenceReportingEventHandler</td>\n<td>在完成消费事件时通知并设置回调</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>队列类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface DataProvider</td>\n<td>提供了按序列值来获取数据的功能</td>\n</tr>\n<tr>\n<td>Interface EventSink</td>\n<td>EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。</td>\n</tr>\n<tr>\n<td>Class RingBuffer</td>\n<td>数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。</td>\n</tr>\n<tr>\n<td>Class DataProvider</td>\n<td>DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 *</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>异常处理类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface ExceptionHandler</td>\n<td>事件处理周期中未捕获异常的回调处理程序的接口</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)</td>\n<td>异常处理的包装类</td>\n</tr>\n<tr>\n<td>Class IgnoreExceptionHandler</td>\n<td>INFO的异常处理程序的便捷实现</td>\n</tr>\n<tr>\n<td>Class FatalExceptionHandler</td>\n<td>SEVERE(严重)的异常处理程序的便捷实</td>\n</tr>\n<tr>\n<td>Class InsufficientCapacityException</td>\n<td>如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerSetting（com.lmax.disruptor.dsl）</td>\n<td>为特定消费者设置异常处理程序的支持类</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>事件类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Inetface EventSink</td>\n<td>这个类主要是提供发布事件(就是往队列上放数据)的功能</td>\n</tr>\n<tr>\n<td>Interface EventFactory</td>\n<td>由RingBuffer调用，以预先调用所有事件以填充RingBuffer</td>\n</tr>\n<tr>\n<td>Interface EventHandler</td>\n<td>回调接口，用于处理RingBuffer中可用的事件</td>\n</tr>\n<tr>\n<td>Class EventPoller</td>\n<td>用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller</td>\n</tr>\n<tr>\n<td>Interface EventProcessor</td>\n<td>事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件</td>\n</tr>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface EventTranslator</td>\n<td>在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。</td>\n</tr>\n</tbody>\n</table>\n<p>时序图</p>\n<p><img src=\"/img/8ab72b98.png\" alt=\"img\"></p>\n<p>类图</p>\n<p><img src=\"/img/1618295759372.png\" alt=\"1618295759372\"></p>\n<p>参考资料：<a href=\"https://brokendreams.iteye.com/blog/2255720\">https://brokendreams.iteye.com/blog/2255720</a></p>\n<p><a href=\"http://www.ibigdata.io/?p=92\">http://www.ibigdata.io/?p=92</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Disruptor中发布事件相关类\">Disruptor中发布事件相关类</h3>\n<h4 id=\"RingBuffer、EventFactory\">RingBuffer、EventFactory</h4>\n<p>EventFactory：提供给RingBuffer做事件预填充</p>\n<p>Event事件：</p>\n<p>1、从生产者到消费者过程中所处理的数据单元；</p>\n<p>2、在Disruptor框架中没有类表示Event，因为它完全是由用户定义的，在Disruptor框架中是用泛型表示的；</p>\n<h3 id=\"Disruptor中的等待策略\">Disruptor中的等待策略</h3>\n<h4 id=\"WaitStrategy\">WaitStrategy</h4>\n<p>等待策略的接口</p>\n<h4 id=\"BlockingWaitStrategy\">BlockingWaitStrategy</h4>\n<p>BlockingWaitStrategy的实现方法是阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略</p>\n<h4 id=\"BusySpinWaitStrategy\">BusySpinWaitStrategy</h4>\n<p>BusySpinWaitStrategy的实现方法是自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</p>\n<h4 id=\"LiteBlockingWaitStrategy\">LiteBlockingWaitStrategy</h4>\n<p>试图消除有条件的唤醒。相比BlockingWaitStrategy，LiteBlockingWaitStrategy的实现方法也是阻塞等待，但它会减少一些不必要的唤醒。</p>\n<p>从源码的注释上看，这个策略在基准性能测试上是会表现出一些性能提升。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</p>\n<h4 id=\"LiteTimeoutBlockingWaitStrategy\">LiteTimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒</p>\n<h4 id=\"PhasedBackoffWaitStrategy\">PhasedBackoffWaitStrategy</h4>\n<p>PhasedBackoffWaitStrategy的实现方法是先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</p>\n<h4 id=\"SleepingWaitStrategy\">SleepingWaitStrategy</h4>\n<p>SleepingWaitStrategy的实现方法是先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。<br>\n对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</p>\n<h4 id=\"TimeoutBlockingWaitStrategy\">TimeoutBlockingWaitStrategy</h4>\n<p>TimeoutBlockingWaitStrategy的实现方法是阻塞给定的时间，超过时间的话会抛出超时异常。</p>\n<h4 id=\"YieldingWaitStrategy\">YieldingWaitStrategy</h4>\n<p>Yielding 策略：在自旋100次尝试后，让出cpu资源，等待下次cpu调度后再行尝试。这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。</p>\n<h3 id=\"总结\">总结</h3>\n<table>\n<thead>\n<tr>\n<th>等待策略  所在包com.Imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class BlockingWaitStrategy</td>\n<td>阻塞等待。当要求节省CPU资源，而不要求高吞吐量和低延迟的时候使用这个策略。</td>\n</tr>\n<tr>\n<td>Class BusySpinWaitStrategy</td>\n<td>自旋等待。这种策略会利用CPU资源来避免系统调用带来的延迟抖动，当线程可以绑定到指定CPU(核)的时候，最好使用这个策略。</td>\n</tr>\n<tr>\n<td>Class LiteBlockingWaitStrategy</td>\n<td>阻塞等待。相比BlockingWaitStrategy，它会减少一些不必要的唤醒。从而性能好。这种等待策略应该被认为是实验性的，因为官方作者还没有完全证明锁定省略代码的正确性。</td>\n</tr>\n<tr>\n<td>Class TimeoutBlockingWaitStrategy</td>\n<td>阻塞给定的时间，超过时间的话会抛出超时异常。</td>\n</tr>\n<tr>\n<td>Class LiteTimeoutBlockingWaitStrategy</td>\n<td>TimeoutBlockingWaitStrategy的一个变形，当锁无效时，试图无条件唤醒。</td>\n</tr>\n<tr>\n<td>Class PhasedBackoffWaitStrategy</td>\n<td>先自旋(10000次)，不行再临时让出调度(yield)，不行再使用其他的策略进行等待。可以根据具体场景自行设置自旋时间、yield时间和备用等待策略。</td>\n</tr>\n<tr>\n<td>Class SleepingWaitStrategy</td>\n<td>先自旋，不行再临时让出调度(Thread.yield())，不行再短暂的阻塞等待。对于既想取得高性能，由不想太浪费CPU资源的场景，这个策略是一种比较好的折中方案。</td>\n</tr>\n<tr>\n<td>Class YieldingWaitStrategy</td>\n<td>在自旋100次尝试后，让出cpu资源这个策略会100%消耗CPU，如果其他线程需要CPU资源，但是比忙碌旋转策略（busy spin strategy）更容易放弃CPU。该策略在高性能与CPU资源之间取舍的折中方案，这个策略不会带来显著的延迟抖动。，等待下次cpu调度后再行尝试。</td>\n</tr>\n<tr>\n<td>Interface WaitStrategy</td>\n<td>上述等待策略实现接口</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>工具类 所在包com.imax.disruptor.util</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Enum DaemonThreadFactory</td>\n<td>访问ThreadFactory实例。 所有线程都是使用setDaemon(true)创建的守护线程</td>\n</tr>\n<tr>\n<td>Class ThreadHints</td>\n<td>用于运行时提高代码性能的提示，</td>\n</tr>\n<tr>\n<td>Class Util</td>\n<td>主要用于计算的工具类</td>\n</tr>\n<tr>\n<td>Enum BasicExecutor （com.lmax.disruptor.dsl）</td>\n<td>只是简单的实现了Executor接口,用于解决没有传递Executor对象的时候使用默认的BasicExecutor即可,可以理解就是默认提供的线程池对象</td>\n</tr>\n<tr>\n<td>Class BasicExecutor（com.lmax.disruptor.dsl）</td>\n<td>默认提供的线程池对象</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>序列类 所在包com.imax.disruptor</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Class Sequence</td>\n<td>环真正的序列。除了缓存行的填充。Sequence类的其他set、get等方法都是通过UNSAFE对象实现对value值的原子操作</td>\n</tr>\n<tr>\n<td>Class SequenceGroup</td>\n<td>继承Sequence，序列组，是用来对sequences属性进行原子更新的，这个类里的sequences数组可以动态的进行增加、删减。</td>\n</tr>\n<tr>\n<td>Class SequenceGroups</td>\n<td>用于管理SequenceGroup对象的静态方法</td>\n</tr>\n<tr>\n<td>Class FixedSequenceGroup</td>\n<td>包含了若干序列的一个包装类，继承了Sequence只重写了get方法、获取内部序列组中最小的序列值，但其他的&quot;写&quot;方法都不支持。</td>\n</tr>\n<tr>\n<td>Interface Sequencer</td>\n<td>通过Sequencer的大部分功能来使用序列。通过Sequencer可以得到一个SequenceBarrier</td>\n</tr>\n<tr>\n<td>Interface SequenceBarrier</td>\n<td>消费者主要通过SequenceBarrier来使用序列。读取当前序列值。判断序列是否可用，是否可以消费。对消费者进通知。</td>\n</tr>\n<tr>\n<td>Interface ProcessingSequenceBarrier</td>\n<td>SequenceBarrier的具体实现</td>\n</tr>\n<tr>\n<td>Class AbstractSequencer</td>\n<td>AbstractSequencer实现了Sequencer，是SingleProducerSequencer和MultiProducerSequencer的基类，基本上的作用就是管理追踪序列和关联当前序列</td>\n</tr>\n<tr>\n<td>Class SingleProducerSequencer</td>\n<td>申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Class MultiProducerSequencer</td>\n<td>适用于多线程的消费者，申请序列，发布序列，唤醒消费者</td>\n</tr>\n<tr>\n<td>Interface Sequenced</td>\n<td>Sequenced接口提供的方法都是用来给生产者使用，用于申请序列，发布序列的</td>\n</tr>\n<tr>\n<td>Interface Cursored</td>\n<td>Cursored接口只有一个方法，getCursor就是用来获取当前游标的位置，也就是用来获取当前生产者的实时位置。</td>\n</tr>\n<tr>\n<td>Interface SequenceReportingEventHandler</td>\n<td>在完成消费事件时通知并设置回调</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>队列类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface DataProvider</td>\n<td>提供了按序列值来获取数据的功能</td>\n</tr>\n<tr>\n<td>Interface EventSink</td>\n<td>EventSink主要是提供发布事件(就是往队列上放数据)的功能，接口上定义了以各种姿势发布事件的方法。</td>\n</tr>\n<tr>\n<td>Class RingBuffer</td>\n<td>数组实现的内部队列。RingBuffer提供了静态工厂方法分别针对单事件发布者和多事件发布者的情况进行RingBuffer实例创建。</td>\n</tr>\n<tr>\n<td>Class DataProvider</td>\n<td>DataProvider 提供了根据序列获取对应的对象有两个地方调用。第一是这个Event对象需要被生产者获取往里面填充数据。第二个是在消费时，获取这个Event对象用于消费 *</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>异常处理类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Interface ExceptionHandler</td>\n<td>事件处理周期中未捕获异常的回调处理程序的接口</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerWrapper(com.lmax.disruptor.dsl)</td>\n<td>异常处理的包装类</td>\n</tr>\n<tr>\n<td>Class IgnoreExceptionHandler</td>\n<td>INFO的异常处理程序的便捷实现</td>\n</tr>\n<tr>\n<td>Class FatalExceptionHandler</td>\n<td>SEVERE(严重)的异常处理程序的便捷实</td>\n</tr>\n<tr>\n<td>Class InsufficientCapacityException</td>\n<td>如果在没有包装消耗序列的情况下，无法将值插入RingBuffer，则抛出异常</td>\n</tr>\n<tr>\n<td>Class ExceptionHandlerSetting（com.lmax.disruptor.dsl）</td>\n<td>为特定消费者设置异常处理程序的支持类</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>事件类 所在包com.imax.disruptor</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Inetface EventSink</td>\n<td>这个类主要是提供发布事件(就是往队列上放数据)的功能</td>\n</tr>\n<tr>\n<td>Interface EventFactory</td>\n<td>由RingBuffer调用，以预先调用所有事件以填充RingBuffer</td>\n</tr>\n<tr>\n<td>Interface EventHandler</td>\n<td>回调接口，用于处理RingBuffer中可用的事件</td>\n</tr>\n<tr>\n<td>Class EventPoller</td>\n<td>用于Disruptor的基于轮询。 通过给定的数据提生产者控制序列来创建一个EventPoller</td>\n</tr>\n<tr>\n<td>Interface EventProcessor</td>\n<td>事件处理器会等待RingBuffer中的事件变为可用(可处理)，然后处理可用的事件</td>\n</tr>\n<tr>\n<td>Interface EventSequencer</td>\n<td>EventSequencer扩展了Sequenced，提供了一些序列功能；同时扩展了DataProvider，提供了按序列值来获取数据的功能。</td>\n</tr>\n<tr>\n<td>Interface EventTranslator</td>\n<td>在发布事件时需要传一个事件转换的接口，内部用这个接口做一下数据到事件的转换。</td>\n</tr>\n</tbody>\n</table>\n<p>时序图</p>\n<p><img src=\"/img/8ab72b98.png\" alt=\"img\"></p>\n<p>类图</p>\n<p><img src=\"/img/1618295759372.png\" alt=\"1618295759372\"></p>\n<p>参考资料：<a href=\"https://brokendreams.iteye.com/blog/2255720\">https://brokendreams.iteye.com/blog/2255720</a></p>\n<p><a href=\"http://www.ibigdata.io/?p=92\">http://www.ibigdata.io/?p=92</a></p>\n"},{"title":"ElasticSearch客户端","author":"郑天祺","date":"2020-07-15T03:10:00.000Z","_content":"\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","source":"_posts/ElasticSearch客户端.md","raw":"title: ElasticSearch客户端\nauthor: 郑天祺\ntags:\n  - es\ncategories:\n  - CICD\n  - ''\ndate: 2020-07-15 11:10:00\n---\n\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","slug":"ElasticSearch客户端","published":1,"updated":"2021-04-13T06:46:30.041Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjb0007l0t9fwvmckbb","content":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient &#123;\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() &#123;\n    &#125;\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() &#123;\n        return InternalClass.client;\n    &#125;\n\n    private static class InternalClass &#123;\n        private static RestHighLevelClient client;\n\n        static &#123;\n            try &#123;\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) &#123;\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                &#125;\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; &#123;\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                &#125;).setRequestConfigCallback(requestConfigBuilder -&gt; &#123;\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                &#125;);\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            &#125; catch (Exception e) &#123;\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            &#125;\n            if (null == client) &#123;\n                log.severe(&quot;创建ES连接失败!&quot;);\n            &#125;\n        &#125;\n    &#125;\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException &#123;\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) &#123;\n        try &#123;\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) &#123;\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            &#125;\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        &#125; catch (Exception e) &#123;\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        &#125;\n    &#125;\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try &#123;\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        &#125; catch (Throwable e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        &#125;\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try &#123;\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        &#125;\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) &#123;\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) &#123;\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        &#125;\n        return personList;\n    &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient &#123;\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() &#123;\n    &#125;\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() &#123;\n        return InternalClass.client;\n    &#125;\n\n    private static class InternalClass &#123;\n        private static RestHighLevelClient client;\n\n        static &#123;\n            try &#123;\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) &#123;\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                &#125;\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; &#123;\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                &#125;).setRequestConfigCallback(requestConfigBuilder -&gt; &#123;\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                &#125;);\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            &#125; catch (Exception e) &#123;\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            &#125;\n            if (null == client) &#123;\n                log.severe(&quot;创建ES连接失败!&quot;);\n            &#125;\n        &#125;\n    &#125;\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException &#123;\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) &#123;\n        try &#123;\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) &#123;\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            &#125;\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        &#125; catch (Exception e) &#123;\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        &#125;\n    &#125;\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try &#123;\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        &#125; catch (Throwable e) &#123;\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        &#125;\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try &#123;\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        &#125; catch (IOException e) &#123;\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        &#125;\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) &#123;\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) &#123;\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        &#125;\n        return personList;\n    &#125;\n</code></pre>\n"},{"title":"Docker入门","author":"郑天祺","date":"2020-12-14T05:13:00.000Z","_content":"\n\n\n## docker概念\t\t\n\ndocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；\n\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。\n\n![image-20201214131527522](/img/image-20201214131527522.png)\n\n## docker架构\n\n![image-20201214131543066](/img/image-20201214131543066.png)\n\n## docker的组成元素\n\n•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(http://hub.Docker.com)\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。\n\n​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；\n\n## docker安装\n\n安装 参考[docker官网](http://www.docker.com/products/docker)\n\n查看安装版本 docker version\n\n![image-20201214131734289](/img/image-20201214131734289.png)\n\n## 测试镜像库\n\n为docker 添加国内镜像 \n\n/etc/docker/daemon.json将: \n\n{ \"registry-mirrors\": [\" https://obou6wyb.mirror.aliyuncs.com\"]}\n\n替换为 { \"dns\" : [ \"192.168.101.2\" , \"8.8.8.8\" ], \"registry-mirrors\" : [ \"https://docker.mirrors.ustc.edu.cn\" ] } \n\n## 重启docker \n\nsystemctl start docker\n\n## 查看资源库有tomcat镜像\n\ndocker search tomcat\n\n![image-20201214131814678](/img/image-20201214131814678.png)\n\n## 从国内docker镜像库下载tomcat、centos\n\ndocker pull tomcat/centos/nginx\n\n## 查看有哪些镜像 \n\ndocker images\n\n![image-20201214131847691](/img/image-20201214131847691.png)\n\n## 启动基于tomcat,centos镜像启动容器 \n\n​    docker run -p 8081:8080 tomcat # 若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口\n\n​    docker run -dit -p 4000:4000 centos \n\n   -d 以守护态运行 \n\n   -p 宿主机端口映射容器端口 \n\n   -i 允许容器内标准输入 \n\n   -t 新容器内指定一个伪终端 \n\n浏览器查看访问容器tomcat实例http://192.168.6.71:8081/\n\n![image-20201214131948506](/img/image-20201214131948506.png)\n\n第一个容器服务部署成功了！\n\n## 进去伪终端查看 \n\n   docker登录容器 docker exec -it hardcore_edison  \"/bin/bash\"\n\n![image-20201214132014480](/img/image-20201214132014480.png)\n\n## 本地文件复制容器中\n\ndocker cp localFile containerID:targetAddress \n\n 命令： docker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n\n将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：\n\n![image-20201214132059091](/img/image-20201214132059091.png)\n\n以上就是docker的简单入门操作；\n\n 构建一个docker镜像需要写一个叫做Dockerfile的文件\n 先查看下本地镜像有哪些？\n\n![image-20201214132127621](/img/image-20201214132127621.png)\n\n在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：\nmkdir dockerDemo && cd dockerDemo && touch Dockerfile\n\n接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）\n  vim Dockerfile\n\n```java\n#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module && make && make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n```\n\n这里以编译nginx提供web服务来构建新的镜像\n\n下载nginx源码包到docker_demo这个目录下\n\nwget -c https://nginx.org/download/nginx-1.12.2.tar.gz\n\n![image-20201214132215125](/img/image-20201214132215125.png)\n\n## 构建nginx:v1版本镜像\n\n```java\ndocker build -t centos_nginx:v1 .\n```\n\n## 观察日志\n\n每一个步骤都成功\n\n![image-20201214132254636](/img/image-20201214132254636.png)\n\n## 构建步骤\n\n成功构建centos_nginx:v1\n\n![image-20201214132309723](/img/image-20201214132309723.png)\n\ndocker images\n\n![image-20201214132326043](/img/image-20201214132326043.png)\n\n## 启动容器\n\ndocker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g \"daemon off;\"\n\n![image-20201214132343909](/img/image-20201214132343909.png)\n\n## 查看镜像对外暴露端口号\n\ndocker port containerID\n\n![image-20201214132401207](/img/image-20201214132401207.png)\n\n## 浏览器查看nginx启动状态\n\n![image-20201214132416329](/img/image-20201214132416329.png)\n\n已经完成第一个nginx的镜像构建以及容器启动；","source":"_posts/Docker入门.md","raw":"title: Docker入门\nauthor: 郑天祺\ntags:\n  - docker\ncategories:\n  - 软件管理\ndate: 2020-12-14 13:13:00\n\n---\n\n\n\n## docker概念\t\t\n\ndocker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；\n\t\tdocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。\n\n![image-20201214131527522](/img/image-20201214131527522.png)\n\n## docker架构\n\n![image-20201214131543066](/img/image-20201214131543066.png)\n\n## docker的组成元素\n\n•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(http://hub.Docker.com)\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。\n\n​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；\n\n## docker安装\n\n安装 参考[docker官网](http://www.docker.com/products/docker)\n\n查看安装版本 docker version\n\n![image-20201214131734289](/img/image-20201214131734289.png)\n\n## 测试镜像库\n\n为docker 添加国内镜像 \n\n/etc/docker/daemon.json将: \n\n{ \"registry-mirrors\": [\" https://obou6wyb.mirror.aliyuncs.com\"]}\n\n替换为 { \"dns\" : [ \"192.168.101.2\" , \"8.8.8.8\" ], \"registry-mirrors\" : [ \"https://docker.mirrors.ustc.edu.cn\" ] } \n\n## 重启docker \n\nsystemctl start docker\n\n## 查看资源库有tomcat镜像\n\ndocker search tomcat\n\n![image-20201214131814678](/img/image-20201214131814678.png)\n\n## 从国内docker镜像库下载tomcat、centos\n\ndocker pull tomcat/centos/nginx\n\n## 查看有哪些镜像 \n\ndocker images\n\n![image-20201214131847691](/img/image-20201214131847691.png)\n\n## 启动基于tomcat,centos镜像启动容器 \n\n​    docker run -p 8081:8080 tomcat # 若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口\n\n​    docker run -dit -p 4000:4000 centos \n\n   -d 以守护态运行 \n\n   -p 宿主机端口映射容器端口 \n\n   -i 允许容器内标准输入 \n\n   -t 新容器内指定一个伪终端 \n\n浏览器查看访问容器tomcat实例http://192.168.6.71:8081/\n\n![image-20201214131948506](/img/image-20201214131948506.png)\n\n第一个容器服务部署成功了！\n\n## 进去伪终端查看 \n\n   docker登录容器 docker exec -it hardcore_edison  \"/bin/bash\"\n\n![image-20201214132014480](/img/image-20201214132014480.png)\n\n## 本地文件复制容器中\n\ndocker cp localFile containerID:targetAddress \n\n 命令： docker cp gag-material.war [b5e1e6975083:/usr/local/tomcat/webapps](http://b5e1e6975083/usr/local/tomcat/webapps) \n\n将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：\n\n![image-20201214132059091](/img/image-20201214132059091.png)\n\n以上就是docker的简单入门操作；\n\n 构建一个docker镜像需要写一个叫做Dockerfile的文件\n 先查看下本地镜像有哪些？\n\n![image-20201214132127621](/img/image-20201214132127621.png)\n\n在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：\nmkdir dockerDemo && cd dockerDemo && touch Dockerfile\n\n接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）\n  vim Dockerfile\n\n```java\n#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module && make && make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n```\n\n这里以编译nginx提供web服务来构建新的镜像\n\n下载nginx源码包到docker_demo这个目录下\n\nwget -c https://nginx.org/download/nginx-1.12.2.tar.gz\n\n![image-20201214132215125](/img/image-20201214132215125.png)\n\n## 构建nginx:v1版本镜像\n\n```java\ndocker build -t centos_nginx:v1 .\n```\n\n## 观察日志\n\n每一个步骤都成功\n\n![image-20201214132254636](/img/image-20201214132254636.png)\n\n## 构建步骤\n\n成功构建centos_nginx:v1\n\n![image-20201214132309723](/img/image-20201214132309723.png)\n\ndocker images\n\n![image-20201214132326043](/img/image-20201214132326043.png)\n\n## 启动容器\n\ndocker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g \"daemon off;\"\n\n![image-20201214132343909](/img/image-20201214132343909.png)\n\n## 查看镜像对外暴露端口号\n\ndocker port containerID\n\n![image-20201214132401207](/img/image-20201214132401207.png)\n\n## 浏览器查看nginx启动状态\n\n![image-20201214132416329](/img/image-20201214132416329.png)\n\n已经完成第一个nginx的镜像构建以及容器启动；","slug":"Docker入门","published":1,"updated":"2020-12-14T05:28:53.336Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjc000bl0t95osoe99j","content":"<h2 id=\"docker概念\">docker概念</h2>\n<p>docker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；<br>\ndocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。</p>\n<p><img src=\"/img/image-20201214131527522.png\" alt=\"image-20201214131527522\"></p>\n<h2 id=\"docker架构\">docker架构</h2>\n<p><img src=\"/img/image-20201214131543066.png\" alt=\"image-20201214131543066\"></p>\n<h2 id=\"docker的组成元素\">docker的组成元素</h2>\n<p>•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。<br>\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。<br>\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。<br>\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(<a href=\"http://hub.Docker.com\">http://hub.Docker.com</a>)<br>\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。</p>\n<p>​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；</p>\n<h2 id=\"docker安装\">docker安装</h2>\n<p>安装 参考<a href=\"http://www.docker.com/products/docker\">docker官网</a></p>\n<p>查看安装版本 docker version</p>\n<p><img src=\"/img/image-20201214131734289.png\" alt=\"image-20201214131734289\"></p>\n<h2 id=\"测试镜像库\">测试镜像库</h2>\n<p>为docker 添加国内镜像</p>\n<p>/etc/docker/daemon.json将:</p>\n<p>{ “registry-mirrors”: [&quot; <a href=\"https://obou6wyb.mirror.aliyuncs.com\">https://obou6wyb.mirror.aliyuncs.com</a>&quot;]}</p>\n<p>替换为 { “dns” : [ “192.168.101.2” , “8.8.8.8” ], “registry-mirrors” : [ “<a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a>” ] }</p>\n<h2 id=\"重启docker\">重启docker</h2>\n<p>systemctl start docker</p>\n<h2 id=\"查看资源库有tomcat镜像\">查看资源库有tomcat镜像</h2>\n<p>docker search tomcat</p>\n<p><img src=\"/img/image-20201214131814678.png\" alt=\"image-20201214131814678\"></p>\n<h2 id=\"从国内docker镜像库下载tomcat、centos\">从国内docker镜像库下载tomcat、centos</h2>\n<p>docker pull tomcat/centos/nginx</p>\n<h2 id=\"查看有哪些镜像\">查看有哪些镜像</h2>\n<p>docker images</p>\n<p><img src=\"/img/image-20201214131847691.png\" alt=\"image-20201214131847691\"></p>\n<h2 id=\"启动基于tomcat-centos镜像启动容器\">启动基于tomcat,centos镜像启动容器</h2>\n<p>​    docker run -p 8081:8080 tomcat # 若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口</p>\n<p>​    docker run -dit -p 4000:4000 centos</p>\n<p>-d 以守护态运行</p>\n<p>-p 宿主机端口映射容器端口</p>\n<p>-i 允许容器内标准输入</p>\n<p>-t 新容器内指定一个伪终端</p>\n<p>浏览器查看访问容器tomcat实例http://192.168.6.71:8081/</p>\n<p><img src=\"/img/image-20201214131948506.png\" alt=\"image-20201214131948506\"></p>\n<p>第一个容器服务部署成功了！</p>\n<h2 id=\"进去伪终端查看\">进去伪终端查看</h2>\n<p>docker登录容器 docker exec -it hardcore_edison  “/bin/bash”</p>\n<p><img src=\"/img/image-20201214132014480.png\" alt=\"image-20201214132014480\"></p>\n<h2 id=\"本地文件复制容器中\">本地文件复制容器中</h2>\n<p>docker cp localFile containerID:targetAddress</p>\n<p>命令： docker cp gag-material.war <a href=\"http://b5e1e6975083/usr/local/tomcat/webapps\">b5e1e6975083:/usr/local/tomcat/webapps</a></p>\n<p>将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：</p>\n<p><img src=\"/img/image-20201214132059091.png\" alt=\"image-20201214132059091\"></p>\n<p>以上就是docker的简单入门操作；</p>\n<p>构建一个docker镜像需要写一个叫做Dockerfile的文件<br>\n先查看下本地镜像有哪些？</p>\n<p><img src=\"/img/image-20201214132127621.png\" alt=\"image-20201214132127621\"></p>\n<p>在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：<br>\nmkdir dockerDemo &amp;&amp; cd dockerDemo &amp;&amp; touch Dockerfile</p>\n<p>接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）<br>\nvim Dockerfile</p>\n<pre><code class=\"language-java\">#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module &amp;&amp; make &amp;&amp; make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n</code></pre>\n<p>这里以编译nginx提供web服务来构建新的镜像</p>\n<p>下载nginx源码包到docker_demo这个目录下</p>\n<p>wget -c <a href=\"https://nginx.org/download/nginx-1.12.2.tar.gz\">https://nginx.org/download/nginx-1.12.2.tar.gz</a></p>\n<p><img src=\"/img/image-20201214132215125.png\" alt=\"image-20201214132215125\"></p>\n<h2 id=\"构建nginx-v1版本镜像\">构建nginx:v1版本镜像</h2>\n<pre><code class=\"language-java\">docker build -t centos_nginx:v1 .\n</code></pre>\n<h2 id=\"观察日志\">观察日志</h2>\n<p>每一个步骤都成功</p>\n<p><img src=\"/img/image-20201214132254636.png\" alt=\"image-20201214132254636\"></p>\n<h2 id=\"构建步骤\">构建步骤</h2>\n<p>成功构建centos_nginx:v1</p>\n<p><img src=\"/img/image-20201214132309723.png\" alt=\"image-20201214132309723\"></p>\n<p>docker images</p>\n<p><img src=\"/img/image-20201214132326043.png\" alt=\"image-20201214132326043\"></p>\n<h2 id=\"启动容器\">启动容器</h2>\n<p>docker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g “daemon off;”</p>\n<p><img src=\"/img/image-20201214132343909.png\" alt=\"image-20201214132343909\"></p>\n<h2 id=\"查看镜像对外暴露端口号\">查看镜像对外暴露端口号</h2>\n<p>docker port containerID</p>\n<p><img src=\"/img/image-20201214132401207.png\" alt=\"image-20201214132401207\"></p>\n<h2 id=\"浏览器查看nginx启动状态\">浏览器查看nginx启动状态</h2>\n<p><img src=\"/img/image-20201214132416329.png\" alt=\"image-20201214132416329\"></p>\n<p>已经完成第一个nginx的镜像构建以及容器启动；</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"docker概念\">docker概念</h2>\n<p>docker和虚拟机VM结构非常相似，但是docker并非虚拟机技术，容器除了运行其中的应用之外，基本不消耗额外的系统资源，虚拟机需要单独分配 独占内存、磁盘等资源；<br>\ndocker最初的设计优势，正是它比虚拟机更节省内存，启动更快。Docker不停地给大家宣传，”虚拟机需要数分钟启动，而Docker容器只需要50毫秒”。</p>\n<p><img src=\"/img/image-20201214131527522.png\" alt=\"image-20201214131527522\"></p>\n<h2 id=\"docker架构\">docker架构</h2>\n<p><img src=\"/img/image-20201214131543066.png\" alt=\"image-20201214131543066\"></p>\n<h2 id=\"docker的组成元素\">docker的组成元素</h2>\n<p>•\tDocker Client : Docker提供给用户的客户端。Docker Client提供给用户一个终端，用户输入Docker提供的命令来管理本地或者远程的服务器。<br>\n•\tDocker Daemon : Docker服务的守护进程。每台服务器（物理机或虚机）上只要安装了Docker的环境，基本上就跑了一个后台程序Docker Daemon，Docker Daemon会接收Docker Client发过来的指令,并对服务器的进行具体操作。<br>\n•\tDocker Images : 俗称Docker的镜像，这个可难懂了。你暂时可以认为这个就像我们要给电脑装系统用的系统CD盘，里面有操作系统的程序，并且还有一些CD盘在系统的基础上安装了必要的软件，做成的一张 “只读” 的CD。<br>\n•\tDocker Registry : 这个可认为是Docker Images的仓库，就像git的仓库一样，用来管理Docker镜像的，提供了Docker镜像的上传、下载和浏览等功能，并且提供安全的账号管理可以管理只有自己可见的私人image。就像git的仓库一样，docker也提供了官方的Registry，叫做Dock Hub(<a href=\"http://hub.Docker.com\">http://hub.Docker.com</a>)<br>\n•\tDocker Container : 俗称Docker的容器，这个是最关键的东西了。Docker Container是真正跑项目程序、消耗机器资源、提供服务的地方，Docker Container通过Docker Images启动，在Docker Images的基础上运行你需要的代码。你可以认为Docker Container提供了系统硬件环境，然后使用了Docker Images这些制作好的系统盘，再加上你的项目代码，跑起来就可以提供服务了。 听到这里，可能你会觉得是不是有点像一个VM利用保存的备份或者快照跑起来环境一样，其实是挺像的，但是实际上是有本质的区别，后面我会细说。</p>\n<p>​       (C/S) 架构模式， 使用远程API来管理和创建 Docker容器。Docker容器通过镜像来创建，容器与镜像的关系类 似于面向对象编程中的对象与类；</p>\n<h2 id=\"docker安装\">docker安装</h2>\n<p>安装 参考<a href=\"http://www.docker.com/products/docker\">docker官网</a></p>\n<p>查看安装版本 docker version</p>\n<p><img src=\"/img/image-20201214131734289.png\" alt=\"image-20201214131734289\"></p>\n<h2 id=\"测试镜像库\">测试镜像库</h2>\n<p>为docker 添加国内镜像</p>\n<p>/etc/docker/daemon.json将:</p>\n<p>{ “registry-mirrors”: [&quot; <a href=\"https://obou6wyb.mirror.aliyuncs.com\">https://obou6wyb.mirror.aliyuncs.com</a>&quot;]}</p>\n<p>替换为 { “dns” : [ “192.168.101.2” , “8.8.8.8” ], “registry-mirrors” : [ “<a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a>” ] }</p>\n<h2 id=\"重启docker\">重启docker</h2>\n<p>systemctl start docker</p>\n<h2 id=\"查看资源库有tomcat镜像\">查看资源库有tomcat镜像</h2>\n<p>docker search tomcat</p>\n<p><img src=\"/img/image-20201214131814678.png\" alt=\"image-20201214131814678\"></p>\n<h2 id=\"从国内docker镜像库下载tomcat、centos\">从国内docker镜像库下载tomcat、centos</h2>\n<p>docker pull tomcat/centos/nginx</p>\n<h2 id=\"查看有哪些镜像\">查看有哪些镜像</h2>\n<p>docker images</p>\n<p><img src=\"/img/image-20201214131847691.png\" alt=\"image-20201214131847691\"></p>\n<h2 id=\"启动基于tomcat-centos镜像启动容器\">启动基于tomcat,centos镜像启动容器</h2>\n<p>​    docker run -p 8081:8080 tomcat # 若端口被占用，可以指定容器和主机的映射端口 前者是外围访问端口：后者是容器内部端口</p>\n<p>​    docker run -dit -p 4000:4000 centos</p>\n<p>-d 以守护态运行</p>\n<p>-p 宿主机端口映射容器端口</p>\n<p>-i 允许容器内标准输入</p>\n<p>-t 新容器内指定一个伪终端</p>\n<p>浏览器查看访问容器tomcat实例http://192.168.6.71:8081/</p>\n<p><img src=\"/img/image-20201214131948506.png\" alt=\"image-20201214131948506\"></p>\n<p>第一个容器服务部署成功了！</p>\n<h2 id=\"进去伪终端查看\">进去伪终端查看</h2>\n<p>docker登录容器 docker exec -it hardcore_edison  “/bin/bash”</p>\n<p><img src=\"/img/image-20201214132014480.png\" alt=\"image-20201214132014480\"></p>\n<h2 id=\"本地文件复制容器中\">本地文件复制容器中</h2>\n<p>docker cp localFile containerID:targetAddress</p>\n<p>命令： docker cp gag-material.war <a href=\"http://b5e1e6975083/usr/local/tomcat/webapps\">b5e1e6975083:/usr/local/tomcat/webapps</a></p>\n<p>将本地应用war包上传到tomcat容器的webapps下面，加载应用成功，浏览器显示：</p>\n<p><img src=\"/img/image-20201214132059091.png\" alt=\"image-20201214132059091\"></p>\n<p>以上就是docker的简单入门操作；</p>\n<p>构建一个docker镜像需要写一个叫做Dockerfile的文件<br>\n先查看下本地镜像有哪些？</p>\n<p><img src=\"/img/image-20201214132127621.png\" alt=\"image-20201214132127621\"></p>\n<p>在某一个目录下面创建一个专门存放此demo的目录，也就是Dockerfile所在的context：<br>\nmkdir dockerDemo &amp;&amp; cd dockerDemo &amp;&amp; touch Dockerfile</p>\n<p>接下来就开始编写Dockerfile文件了（注意Dockerfile的D需要大写）<br>\nvim Dockerfile</p>\n<pre><code class=\"language-java\">#############################################################  \n#base image\nFROM centos\n#MAINTAINER\nMAINTAINER [test@qq.com](mailto:test@qq.com)\n\n \n\n#put nginx into /usr/local/src and unpack nginx\n   ADD nginx-1.12.2.tar.gz /usr/local/src\n\n#running required command\n\n  RUN yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel \n   RUN yum install -y libxslt-devel -y gd gd-devel GeoIP GeoIP-devel pcre pcre-devel\n   RUN useradd -M -s /sbin/nologin nginx\n\n#change dir to /usr/local/src/nginx-1.12.2\n   WORKDIR /usr/local/src/nginx-1.12.2\n# execute command to compile nginx\n    RUN ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module --with-http_image_filter_module --with-http_geoip_module --with-http_sub_module --with-http_dav_module --with-    http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module &amp;&amp; make &amp;&amp; make install\n#对外暴露端口\nEXPOSE 80\n#############################################################  \n</code></pre>\n<p>这里以编译nginx提供web服务来构建新的镜像</p>\n<p>下载nginx源码包到docker_demo这个目录下</p>\n<p>wget -c <a href=\"https://nginx.org/download/nginx-1.12.2.tar.gz\">https://nginx.org/download/nginx-1.12.2.tar.gz</a></p>\n<p><img src=\"/img/image-20201214132215125.png\" alt=\"image-20201214132215125\"></p>\n<h2 id=\"构建nginx-v1版本镜像\">构建nginx:v1版本镜像</h2>\n<pre><code class=\"language-java\">docker build -t centos_nginx:v1 .\n</code></pre>\n<h2 id=\"观察日志\">观察日志</h2>\n<p>每一个步骤都成功</p>\n<p><img src=\"/img/image-20201214132254636.png\" alt=\"image-20201214132254636\"></p>\n<h2 id=\"构建步骤\">构建步骤</h2>\n<p>成功构建centos_nginx:v1</p>\n<p><img src=\"/img/image-20201214132309723.png\" alt=\"image-20201214132309723\"></p>\n<p>docker images</p>\n<p><img src=\"/img/image-20201214132326043.png\" alt=\"image-20201214132326043\"></p>\n<h2 id=\"启动容器\">启动容器</h2>\n<p>docker run -d -p80:80 centos_nginx:v1 /usr/local/nginx/sbin/nginx -g “daemon off;”</p>\n<p><img src=\"/img/image-20201214132343909.png\" alt=\"image-20201214132343909\"></p>\n<h2 id=\"查看镜像对外暴露端口号\">查看镜像对外暴露端口号</h2>\n<p>docker port containerID</p>\n<p><img src=\"/img/image-20201214132401207.png\" alt=\"image-20201214132401207\"></p>\n<h2 id=\"浏览器查看nginx启动状态\">浏览器查看nginx启动状态</h2>\n<p><img src=\"/img/image-20201214132416329.png\" alt=\"image-20201214132416329\"></p>\n<p>已经完成第一个nginx的镜像构建以及容器启动；</p>\n"},{"title":"ElasticSearch近实时性介绍","author":"郑天祺","date":"2020-07-15T02:51:00.000Z","_content":"\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","source":"_posts/ElasticSearch近实时性介绍.md","raw":"title: ElasticSearch近实时性介绍\nauthor: 郑天祺\ntags:\n  - es\ncategories:\n  - CICD\n  - ''\ndate: 2020-07-15 10:51:00\n---\n\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","slug":"ElasticSearch近实时性介绍","published":1,"updated":"2021-04-13T06:46:57.049Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjd000cl0t94jg72fm9","content":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n"},{"title":"GET与POST区别","author":"郑天祺","date":"2020-07-21T00:29:00.000Z","_content":"\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","source":"_posts/GET与POST区别.md","raw":"title: GET与POST区别\nauthor: 郑天祺\ntags:\n  - GET/POST\ncategories:\n  - 网络\ndate: 2020-07-21 08:29:00\n\n---\n\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","slug":"GET与POST区别","published":1,"updated":"2020-07-21T01:30:43.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpje000gl0t9995uhumq","content":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2 id=\"GET请求\">GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型<br>\nAccept-Language 浏览器支持的语言<br>\nAccept-Encoding 浏览器支持的压缩格式<br>\nHost 请求的主机<br>\nConnection keep-alive 这个是链接一小段时间</p>\n<h2 id=\"GET响应\">GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2 id=\"POST请求\">POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 – 加上128 – 转化为16进制 – 添加%</p>\n<h2 id=\"POST响应\">POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2 id=\"（1）表格对比：\">（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存<br>\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2 id=\"（2）交互对比\">（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2 id=\"（1）GET方法的参数写法是固定的吗？\">（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2 id=\"（2）GET-方法的长度限制是怎么回事？\">（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2 id=\"（3）POST-方法比-GET-方法安全？\">（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：<a href=\"http://www.javanx.cn/20190227/get-post/\">http://www.javanx.cn/20190227/get-post/</a></p>\n<h2 id=\"（4）POST-方法会产生两个-TCP-数据包？\">（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2 id=\"GET请求\">GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型<br>\nAccept-Language 浏览器支持的语言<br>\nAccept-Encoding 浏览器支持的压缩格式<br>\nHost 请求的主机<br>\nConnection keep-alive 这个是链接一小段时间</p>\n<h2 id=\"GET响应\">GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2 id=\"POST请求\">POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 – 加上128 – 转化为16进制 – 添加%</p>\n<h2 id=\"POST响应\">POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2 id=\"（1）表格对比：\">（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存<br>\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2 id=\"（2）交互对比\">（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2 id=\"（1）GET方法的参数写法是固定的吗？\">（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2 id=\"（2）GET-方法的长度限制是怎么回事？\">（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2 id=\"（3）POST-方法比-GET-方法安全？\">（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：<a href=\"http://www.javanx.cn/20190227/get-post/\">http://www.javanx.cn/20190227/get-post/</a></p>\n<h2 id=\"（4）POST-方法会产生两个-TCP-数据包？\">（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n"},{"title":"Git梳理","author":"郑天祺","date":"2019-08-29T02:28:00.000Z","_content":"## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n        $ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： $ git clone http:/zzzzzzzz.git\n\n查看远程仓库：$ git remote -v\n\n添加远程仓库：$ git remote add [name] [url]\n\n删除远程仓库：$ git remote rm [name]\n\n修改远程仓库：$ git remote set-url --push       [name] [newUrl]\n\n拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支\n\n查看远程分支：$ git branch -r\n\n创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n\n切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：$ git checkout -b       [name]\n\n删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n\n合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并\n\n创建远程分支(本地分支push到远程)：$ git push origin [name]\n\n删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n    $git symbolic-ref HEAD refs/heads/[name]\n    $rm .git/index\n    $git clean -fdx\n\n####         c）版本(tag)操作相关命令\n\n查看版本：$ git tag\n\n创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：$ git tag -d [name]\n\n查看远程版本：$ git tag -r\n\n创建远程版本(本地版本push到远程)：$ git push origin [name]\n\n删除远程版本：$ git push origin       :refs/tags/[name]\n\n合并远程仓库的tag到本地：$ git pull origin --tags\n\n上传本地tag到远程仓库：$ git push origin --tags\n\n创建带注释的tag：$       git tag -a [name] -m 'yourMessage' \n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：$ git submodule add [url]       [path]\n\n​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n\n初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行\n\n更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下\n\n删除子模块：（分4步走） \n\n```java\n\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n```\n\n\n\n####         e）补充\n\n更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n\n删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'\n\n查看历史日志：$ git log\n\n还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：$ git commit    //       当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\npush -u [url]         \n\n####         f）忽略一些文件、文件夹不提交\n\n​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n​        IDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected","source":"_posts/Git梳理.md","raw":"title: Git梳理\ntags:\n  - git\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-29 10:28:00\n---\n## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n        $ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： $ git clone http:/zzzzzzzz.git\n\n查看远程仓库：$ git remote -v\n\n添加远程仓库：$ git remote add [name] [url]\n\n删除远程仓库：$ git remote rm [name]\n\n修改远程仓库：$ git remote set-url --push       [name] [newUrl]\n\n拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支\n\n查看远程分支：$ git branch -r\n\n创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n\n切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：$ git checkout -b       [name]\n\n删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n\n合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并\n\n创建远程分支(本地分支push到远程)：$ git push origin [name]\n\n删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n    $git symbolic-ref HEAD refs/heads/[name]\n    $rm .git/index\n    $git clean -fdx\n\n####         c）版本(tag)操作相关命令\n\n查看版本：$ git tag\n\n创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：$ git tag -d [name]\n\n查看远程版本：$ git tag -r\n\n创建远程版本(本地版本push到远程)：$ git push origin [name]\n\n删除远程版本：$ git push origin       :refs/tags/[name]\n\n合并远程仓库的tag到本地：$ git pull origin --tags\n\n上传本地tag到远程仓库：$ git push origin --tags\n\n创建带注释的tag：$       git tag -a [name] -m 'yourMessage' \n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：$ git submodule add [url]       [path]\n\n​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n\n初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行\n\n更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下\n\n删除子模块：（分4步走） \n\n```java\n\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n```\n\n\n\n####         e）补充\n\n更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n\n删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'\n\n查看历史日志：$ git log\n\n还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：$ git commit    //       当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\npush -u [url]         \n\n####         f）忽略一些文件、文件夹不提交\n\n​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n​        IDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected","slug":"Git梳理","published":1,"updated":"2019-10-15T12:21:32.785Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjf000il0t99s7xhs04","content":"<h2 id=\"1、Git介绍：\">1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2 id=\"2、为什么用GitLab？\">2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确<br>\n​\t清晰的看到产品迭代，为产品研发提供参考<br>\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2 id=\"3、Git-工作区、暂存区和版本库\">3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2 id=\"4、Git工作流程\">4、Git工作流程</h2>\n<h3 id=\"1、一个分支\">1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3 id=\"2、多个分支\">2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h2 id=\"5、Git配置及使用：\">5、Git配置及使用：</h2>\n<h3 id=\"1）配置用户信息\">1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3 id=\"2）查看配置\">2）查看配置</h3>\n<pre><code class=\"language-java\">        $ git config --list \n</code></pre>\n<h3 id=\"3）创建仓库\">3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  <a href=\"http://zzzzz.git\">http://zzzzz.git</a>   // ‘<a href=\"http://zzzzzz.git\">http://zzzzzz.git</a>’  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用’git init’命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用’git remote add’命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3 id=\"4）基本操作\">4）基本操作</h3>\n<h4 id=\"a）远程仓库相关命令\">a）远程仓库相关命令</h4>\n<p>检出仓库： $ git clone http:/zzzzzzzz.git</p>\n<p>查看远程仓库：$ git remote -v</p>\n<p>添加远程仓库：$ git remote add [name] [url]</p>\n<p>删除远程仓库：$ git remote rm [name]</p>\n<p>修改远程仓库：$ git remote set-url --push       [name] [newUrl]</p>\n<p>拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如’git push origin’就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4 id=\"b）分支-branch-操作相关命令\">b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如’git branch       new_branch’会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch’就会强制删除叫做new_branch的分支</p>\n<p>查看远程分支：$ git branch -r</p>\n<p>创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支</p>\n<p>切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如’git checkout       new_branch’就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如’git checkout app/model/user.rb’就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：$ git checkout -b       [name]</p>\n<p>删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</p>\n<p>合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并</p>\n<p>创建远程分支(本地分支push到远程)：$ git push origin [name]</p>\n<p>删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name]</p>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)<br>\n$git symbolic-ref HEAD refs/heads/[name]<br>\n$rm .git/index<br>\n$git clean -fdx</p>\n<h4 id=\"c）版本-tag-操作相关命令\">c）版本(tag)操作相关命令</h4>\n<p>查看版本：$ git tag</p>\n<p>创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用’git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20’来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：$ git tag -d [name]</p>\n<p>查看远程版本：$ git tag -r</p>\n<p>创建远程版本(本地版本push到远程)：$ git push origin [name]</p>\n<p>删除远程版本：$ git push origin       :refs/tags/[name]</p>\n<p>合并远程仓库的tag到本地：$ git pull origin --tags</p>\n<p>上传本地tag到远程仓库：$ git push origin --tags</p>\n<p>创建带注释的tag：$       git tag -a [name] -m ‘yourMessage’</p>\n<h4 id=\"d）子模块-submodule-相关操作命令\">d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：$ git submodule add [url]       [path]</p>\n<p>​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>\n<p>初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行</p>\n<p>更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下</p>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n</code></pre>\n<h4 id=\"e）补充\">e）补充</h4>\n<p>更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如’git       add app/model/user.rb’就会增加app/model/user.rb文件到Git的索引中</p>\n<p>删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’</p>\n<p>查看历史日志：$ git log</p>\n<p>还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值</p>\n<p>提交：$ git commit    //       当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<p>push -u [url]</p>\n<h4 id=\"f）忽略一些文件、文件夹不提交\">f）忽略一些文件、文件夹不提交</h4>\n<p>​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3 id=\"5-解决冲突\">5)解决冲突</h3>\n<p>​        IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、Git介绍：\">1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2 id=\"2、为什么用GitLab？\">2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确<br>\n​\t清晰的看到产品迭代，为产品研发提供参考<br>\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2 id=\"3、Git-工作区、暂存区和版本库\">3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2 id=\"4、Git工作流程\">4、Git工作流程</h2>\n<h3 id=\"1、一个分支\">1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3 id=\"2、多个分支\">2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h2 id=\"5、Git配置及使用：\">5、Git配置及使用：</h2>\n<h3 id=\"1）配置用户信息\">1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3 id=\"2）查看配置\">2）查看配置</h3>\n<pre><code class=\"language-java\">        $ git config --list \n</code></pre>\n<h3 id=\"3）创建仓库\">3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  <a href=\"http://zzzzz.git\">http://zzzzz.git</a>   // ‘<a href=\"http://zzzzzz.git\">http://zzzzzz.git</a>’  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用’git init’命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用’git remote add’命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3 id=\"4）基本操作\">4）基本操作</h3>\n<h4 id=\"a）远程仓库相关命令\">a）远程仓库相关命令</h4>\n<p>检出仓库： $ git clone http:/zzzzzzzz.git</p>\n<p>查看远程仓库：$ git remote -v</p>\n<p>添加远程仓库：$ git remote add [name] [url]</p>\n<p>删除远程仓库：$ git remote rm [name]</p>\n<p>修改远程仓库：$ git remote set-url --push       [name] [newUrl]</p>\n<p>拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如’git push origin’就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4 id=\"b）分支-branch-操作相关命令\">b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如’git branch       new_branch’会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch’就会强制删除叫做new_branch的分支</p>\n<p>查看远程分支：$ git branch -r</p>\n<p>创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支</p>\n<p>切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如’git checkout       new_branch’就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如’git checkout app/model/user.rb’就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：$ git checkout -b       [name]</p>\n<p>删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</p>\n<p>合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并</p>\n<p>创建远程分支(本地分支push到远程)：$ git push origin [name]</p>\n<p>删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name]</p>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)<br>\n$git symbolic-ref HEAD refs/heads/[name]<br>\n$rm .git/index<br>\n$git clean -fdx</p>\n<h4 id=\"c）版本-tag-操作相关命令\">c）版本(tag)操作相关命令</h4>\n<p>查看版本：$ git tag</p>\n<p>创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用’git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20’来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：$ git tag -d [name]</p>\n<p>查看远程版本：$ git tag -r</p>\n<p>创建远程版本(本地版本push到远程)：$ git push origin [name]</p>\n<p>删除远程版本：$ git push origin       :refs/tags/[name]</p>\n<p>合并远程仓库的tag到本地：$ git pull origin --tags</p>\n<p>上传本地tag到远程仓库：$ git push origin --tags</p>\n<p>创建带注释的tag：$       git tag -a [name] -m ‘yourMessage’</p>\n<h4 id=\"d）子模块-submodule-相关操作命令\">d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：$ git submodule add [url]       [path]</p>\n<p>​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>\n<p>初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行</p>\n<p>更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下</p>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n</code></pre>\n<h4 id=\"e）补充\">e）补充</h4>\n<p>更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如’git       add app/model/user.rb’就会增加app/model/user.rb文件到Git的索引中</p>\n<p>删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’</p>\n<p>查看历史日志：$ git log</p>\n<p>还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值</p>\n<p>提交：$ git commit    //       当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<p>push -u [url]</p>\n<h4 id=\"f）忽略一些文件、文件夹不提交\">f）忽略一些文件、文件夹不提交</h4>\n<p>​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3 id=\"5-解决冲突\">5)解决冲突</h3>\n<p>​        IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected</p>\n"},{"title":"HBASE shell操作（2）","author":"郑天祺","date":"2020-12-06T04:02:00.000Z","_content":"\n\n\na.创建学生成绩表，结果如下。\n\nRowkey：id\n\n列族：info和course，course包括3个版本数据\n\nb.插入数据\n\n数据包括\n\n| 学生学号 | Info | course |      |              |         |      |         |\n| -------- | ---- | ------ | ---- | ------------ | ------- | ---- | ------- |\n|          | name | age    | sex  | address      | Chinese | math | english |\n| 95001    | Jom  | 20     | 男   | 山东省济南市 | 80      | 85   | 89      |\n| 95002    | Tom  | 19     | 男   | 山东省济南市 | 55，60  | 80   | 71      |\n| 95003    | Lily | 20     | 女   | 北京市       |         | 65   |         |\n\nc.查询数据\n\n- 查找95001的相关数据\n- 查找95002 行、course 列族中 math 列的值\n- 查找成绩为80-90之间的相关数据\n- 查找名字为Jom的相关数据\n- 查找学生地址是山东省的相关数据\n\nd.删除学生95003的相关数据\n\n2、使用HBase Java API（选做）\n\na.查询所有表\n\nb.创建表test，包括列族f1和f2\n\nc.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135\n\nd.插入数据，rk002 ，f1中列name为lisi\n\ne.查看rk001的数据\n\n\n\n创建学生成绩表 \n\ncreate 'student','pratice','info',{NAME=>'course',VERSIONS=>3} \n\n插入数据 \n\nput 'student','95001','info:name','Jom' \n\nput 'student','95001','info:age','20' \n\nput 'student','95001','info:sex','男' \n\nput 'student','95001','info:address','山东省济南市' \n\nput 'student','95001','course:chinese','80' \n\nput 'student','95001','course:math','85' \n\nput 'student','95001','course:english','89' \n\n \n\nput 'student','95002','info:name','Tom' \n\nput 'student','95002','info:age','19' \n\nput 'student','95002','info:sex','男' \n\nput 'student','95002','info:address','山东省济南市' \n\nput 'student','95002','course:chinese','55,60' \n\nput 'student','95002','course:math','80' \n\nput 'student','95002','course:english','71' \n\n \n\nput 'student','95003','info:name','Lily' \n\nput 'student','95003','info:age','20' \n\nput 'student','95003','info:sex','女' \n\nput 'student','95003','info:address','北京市' \n\nput 'student','95003','course:chinese','' \n\nput 'student','95003','course:math','65' \n\nput 'student','95003','course:english','' \n\n\n\n查找95001的相关数据\n\nget 'student','95001' \n\n\n\n查找95002 行、course 列族中 math 列的值 \n\nget 'student','95002',{COLUMN=>'course:math'} \n\n\n\n查找成绩为80-90之间的相关数据 \n\nscan 'student',{COLUMN=>'course', FILTER=>\"ValueFilter(>=,'binary:80') AND ValueFilter(<=,'binary:90')\"}\n\n\n\n查找名字为Jom的相关数据 \n\nscan 'student',{FILTER=>\"ValueFilter(=,'binary:Jom')\"} \n\n\n\n查找学生地址是山东省的相关数据\n\nscan 'student',{FILTER=>\"ValueFilter(=,'substring:山东省')\"} \n\n\n\n删除学生95003的相关数据\n\ndeleteall 'student','95003' ","source":"_posts/HDFS-shell操作（2）.md","raw":"title: HBASE shell操作（2）\nauthor: 郑天祺\ntags:\n\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 12:02:00\n\n---\n\n\n\na.创建学生成绩表，结果如下。\n\nRowkey：id\n\n列族：info和course，course包括3个版本数据\n\nb.插入数据\n\n数据包括\n\n| 学生学号 | Info | course |      |              |         |      |         |\n| -------- | ---- | ------ | ---- | ------------ | ------- | ---- | ------- |\n|          | name | age    | sex  | address      | Chinese | math | english |\n| 95001    | Jom  | 20     | 男   | 山东省济南市 | 80      | 85   | 89      |\n| 95002    | Tom  | 19     | 男   | 山东省济南市 | 55，60  | 80   | 71      |\n| 95003    | Lily | 20     | 女   | 北京市       |         | 65   |         |\n\nc.查询数据\n\n- 查找95001的相关数据\n- 查找95002 行、course 列族中 math 列的值\n- 查找成绩为80-90之间的相关数据\n- 查找名字为Jom的相关数据\n- 查找学生地址是山东省的相关数据\n\nd.删除学生95003的相关数据\n\n2、使用HBase Java API（选做）\n\na.查询所有表\n\nb.创建表test，包括列族f1和f2\n\nc.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135\n\nd.插入数据，rk002 ，f1中列name为lisi\n\ne.查看rk001的数据\n\n\n\n创建学生成绩表 \n\ncreate 'student','pratice','info',{NAME=>'course',VERSIONS=>3} \n\n插入数据 \n\nput 'student','95001','info:name','Jom' \n\nput 'student','95001','info:age','20' \n\nput 'student','95001','info:sex','男' \n\nput 'student','95001','info:address','山东省济南市' \n\nput 'student','95001','course:chinese','80' \n\nput 'student','95001','course:math','85' \n\nput 'student','95001','course:english','89' \n\n \n\nput 'student','95002','info:name','Tom' \n\nput 'student','95002','info:age','19' \n\nput 'student','95002','info:sex','男' \n\nput 'student','95002','info:address','山东省济南市' \n\nput 'student','95002','course:chinese','55,60' \n\nput 'student','95002','course:math','80' \n\nput 'student','95002','course:english','71' \n\n \n\nput 'student','95003','info:name','Lily' \n\nput 'student','95003','info:age','20' \n\nput 'student','95003','info:sex','女' \n\nput 'student','95003','info:address','北京市' \n\nput 'student','95003','course:chinese','' \n\nput 'student','95003','course:math','65' \n\nput 'student','95003','course:english','' \n\n\n\n查找95001的相关数据\n\nget 'student','95001' \n\n\n\n查找95002 行、course 列族中 math 列的值 \n\nget 'student','95002',{COLUMN=>'course:math'} \n\n\n\n查找成绩为80-90之间的相关数据 \n\nscan 'student',{COLUMN=>'course', FILTER=>\"ValueFilter(>=,'binary:80') AND ValueFilter(<=,'binary:90')\"}\n\n\n\n查找名字为Jom的相关数据 \n\nscan 'student',{FILTER=>\"ValueFilter(=,'binary:Jom')\"} \n\n\n\n查找学生地址是山东省的相关数据\n\nscan 'student',{FILTER=>\"ValueFilter(=,'substring:山东省')\"} \n\n\n\n删除学生95003的相关数据\n\ndeleteall 'student','95003' ","slug":"HDFS-shell操作（2）","published":1,"updated":"2020-12-06T04:10:39.580Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjh000ml0t94s9pfq48","content":"<p>a.创建学生成绩表，结果如下。</p>\n<p>Rowkey：id</p>\n<p>列族：info和course，course包括3个版本数据</p>\n<p>b.插入数据</p>\n<p>数据包括</p>\n<table>\n<thead>\n<tr>\n<th>学生学号</th>\n<th>Info</th>\n<th>course</th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>name</td>\n<td>age</td>\n<td>sex</td>\n<td>address</td>\n<td>Chinese</td>\n<td>math</td>\n<td>english</td>\n</tr>\n<tr>\n<td>95001</td>\n<td>Jom</td>\n<td>20</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>80</td>\n<td>85</td>\n<td>89</td>\n</tr>\n<tr>\n<td>95002</td>\n<td>Tom</td>\n<td>19</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>55，60</td>\n<td>80</td>\n<td>71</td>\n</tr>\n<tr>\n<td>95003</td>\n<td>Lily</td>\n<td>20</td>\n<td>女</td>\n<td>北京市</td>\n<td></td>\n<td>65</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>c.查询数据</p>\n<ul>\n<li>查找95001的相关数据</li>\n<li>查找95002 行、course 列族中 math 列的值</li>\n<li>查找成绩为80-90之间的相关数据</li>\n<li>查找名字为Jom的相关数据</li>\n<li>查找学生地址是山东省的相关数据</li>\n</ul>\n<p>d.删除学生95003的相关数据</p>\n<p>2、使用HBase Java API（选做）</p>\n<p>a.查询所有表</p>\n<p>b.创建表test，包括列族f1和f2</p>\n<p>c.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135</p>\n<p>d.插入数据，rk002 ，f1中列name为lisi</p>\n<p>e.查看rk001的数据</p>\n<p>创建学生成绩表</p>\n<p>create ‘student’,‘pratice’,‘info’,{NAME=&gt;‘course’,VERSIONS=&gt;3}</p>\n<p>插入数据</p>\n<p>put ‘student’,‘95001’,‘info:name’,‘Jom’</p>\n<p>put ‘student’,‘95001’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95001’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95001’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95001’,‘course:chinese’,‘80’</p>\n<p>put ‘student’,‘95001’,‘course:math’,‘85’</p>\n<p>put ‘student’,‘95001’,‘course:english’,‘89’</p>\n<p>put ‘student’,‘95002’,‘info:name’,‘Tom’</p>\n<p>put ‘student’,‘95002’,‘info:age’,‘19’</p>\n<p>put ‘student’,‘95002’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95002’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95002’,‘course:chinese’,‘55,60’</p>\n<p>put ‘student’,‘95002’,‘course:math’,‘80’</p>\n<p>put ‘student’,‘95002’,‘course:english’,‘71’</p>\n<p>put ‘student’,‘95003’,‘info:name’,‘Lily’</p>\n<p>put ‘student’,‘95003’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95003’,‘info:sex’,‘女’</p>\n<p>put ‘student’,‘95003’,‘info:address’,‘北京市’</p>\n<p>put ‘student’,‘95003’,‘course:chinese’,‘’</p>\n<p>put ‘student’,‘95003’,‘course:math’,‘65’</p>\n<p>put ‘student’,‘95003’,‘course:english’,‘’</p>\n<p>查找95001的相关数据</p>\n<p>get ‘student’,‘95001’</p>\n<p>查找95002 行、course 列族中 math 列的值</p>\n<p>get ‘student’,‘95002’,{COLUMN=&gt;‘course:math’}</p>\n<p>查找成绩为80-90之间的相关数据</p>\n<p>scan ‘student’,{COLUMN=&gt;‘course’, FILTER=&gt;“ValueFilter(&gt;=,‘binary:80’) AND ValueFilter(&lt;=,‘binary:90’)”}</p>\n<p>查找名字为Jom的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘binary:Jom’)”}</p>\n<p>查找学生地址是山东省的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘substring:山东省’)”}</p>\n<p>删除学生95003的相关数据</p>\n<p>deleteall ‘student’,‘95003’</p>\n","site":{"data":{}},"excerpt":"","more":"<p>a.创建学生成绩表，结果如下。</p>\n<p>Rowkey：id</p>\n<p>列族：info和course，course包括3个版本数据</p>\n<p>b.插入数据</p>\n<p>数据包括</p>\n<table>\n<thead>\n<tr>\n<th>学生学号</th>\n<th>Info</th>\n<th>course</th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>name</td>\n<td>age</td>\n<td>sex</td>\n<td>address</td>\n<td>Chinese</td>\n<td>math</td>\n<td>english</td>\n</tr>\n<tr>\n<td>95001</td>\n<td>Jom</td>\n<td>20</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>80</td>\n<td>85</td>\n<td>89</td>\n</tr>\n<tr>\n<td>95002</td>\n<td>Tom</td>\n<td>19</td>\n<td>男</td>\n<td>山东省济南市</td>\n<td>55，60</td>\n<td>80</td>\n<td>71</td>\n</tr>\n<tr>\n<td>95003</td>\n<td>Lily</td>\n<td>20</td>\n<td>女</td>\n<td>北京市</td>\n<td></td>\n<td>65</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>c.查询数据</p>\n<ul>\n<li>查找95001的相关数据</li>\n<li>查找95002 行、course 列族中 math 列的值</li>\n<li>查找成绩为80-90之间的相关数据</li>\n<li>查找名字为Jom的相关数据</li>\n<li>查找学生地址是山东省的相关数据</li>\n</ul>\n<p>d.删除学生95003的相关数据</p>\n<p>2、使用HBase Java API（选做）</p>\n<p>a.查询所有表</p>\n<p>b.创建表test，包括列族f1和f2</p>\n<p>c.插入数据，rk001 ，f1中列name为zhangsan，f2中列number为135</p>\n<p>d.插入数据，rk002 ，f1中列name为lisi</p>\n<p>e.查看rk001的数据</p>\n<p>创建学生成绩表</p>\n<p>create ‘student’,‘pratice’,‘info’,{NAME=&gt;‘course’,VERSIONS=&gt;3}</p>\n<p>插入数据</p>\n<p>put ‘student’,‘95001’,‘info:name’,‘Jom’</p>\n<p>put ‘student’,‘95001’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95001’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95001’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95001’,‘course:chinese’,‘80’</p>\n<p>put ‘student’,‘95001’,‘course:math’,‘85’</p>\n<p>put ‘student’,‘95001’,‘course:english’,‘89’</p>\n<p>put ‘student’,‘95002’,‘info:name’,‘Tom’</p>\n<p>put ‘student’,‘95002’,‘info:age’,‘19’</p>\n<p>put ‘student’,‘95002’,‘info:sex’,‘男’</p>\n<p>put ‘student’,‘95002’,‘info:address’,‘山东省济南市’</p>\n<p>put ‘student’,‘95002’,‘course:chinese’,‘55,60’</p>\n<p>put ‘student’,‘95002’,‘course:math’,‘80’</p>\n<p>put ‘student’,‘95002’,‘course:english’,‘71’</p>\n<p>put ‘student’,‘95003’,‘info:name’,‘Lily’</p>\n<p>put ‘student’,‘95003’,‘info:age’,‘20’</p>\n<p>put ‘student’,‘95003’,‘info:sex’,‘女’</p>\n<p>put ‘student’,‘95003’,‘info:address’,‘北京市’</p>\n<p>put ‘student’,‘95003’,‘course:chinese’,‘’</p>\n<p>put ‘student’,‘95003’,‘course:math’,‘65’</p>\n<p>put ‘student’,‘95003’,‘course:english’,‘’</p>\n<p>查找95001的相关数据</p>\n<p>get ‘student’,‘95001’</p>\n<p>查找95002 行、course 列族中 math 列的值</p>\n<p>get ‘student’,‘95002’,{COLUMN=&gt;‘course:math’}</p>\n<p>查找成绩为80-90之间的相关数据</p>\n<p>scan ‘student’,{COLUMN=&gt;‘course’, FILTER=&gt;“ValueFilter(&gt;=,‘binary:80’) AND ValueFilter(&lt;=,‘binary:90’)”}</p>\n<p>查找名字为Jom的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘binary:Jom’)”}</p>\n<p>查找学生地址是山东省的相关数据</p>\n<p>scan ‘student’,{FILTER=&gt;“ValueFilter(=,‘substring:山东省’)”}</p>\n<p>删除学生95003的相关数据</p>\n<p>deleteall ‘student’,‘95003’</p>\n"},{"title":"HDFS shell操作（1）","author":"郑天祺","date":"2020-12-06T03:50:00.000Z","_content":"\n## 1、创建一个HDFS目录\n\n命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115218466](/img/hdfs创建文件夹.png)\n\n## 2、本地文件上传到HDFS\n\n本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir\n\n命令：echo “I am student” > a.txt\n\n​       echo “I am teacher” > c.txt\n\n​       echo “I like hadoop” > b.txt\n\n​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115351845](/img/HDFS上传.png)\n\n## 3、查看/usr/local/hadoop/data/txtdir目录结构\n\n命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir\n\n截图：\n\n![image-20201206115459406](/img/image-20201206115459406.png)\n\n## 4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容\n\n命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt\n\n截图：![image-20201206115600801](/img/image-20201206115600801.png)\n\n## 5、合并本地多个小文件上传到HDFS\n\n命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt\n\n截图：![image-20201206115653987](/img/image-20201206115653987.png)\n\n## 6、 下载a.txt到本地文件系统\n\n命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt\n\n截图：![image-20201206115738831](/img/image-20201206115738831.png)\n\n## 7、删除HDFS上的/usr/local/hadoop/data/txtdir\n\n命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/\n\n截图：![image-20201206115832025](/img/image-20201206115832025.png)","source":"_posts/HDFS-shell操作（1）.md","raw":"title: HDFS shell操作（1）\nauthor: 郑天祺\ntags:\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 11:50:00\n---\n\n## 1、创建一个HDFS目录\n\n命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115218466](/img/hdfs创建文件夹.png)\n\n## 2、本地文件上传到HDFS\n\n本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir\n\n命令：echo “I am student” > a.txt\n\n​       echo “I am teacher” > c.txt\n\n​       echo “I like hadoop” > b.txt\n\n​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir\n\n​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir\n\n截图：![image-20201206115351845](/img/HDFS上传.png)\n\n## 3、查看/usr/local/hadoop/data/txtdir目录结构\n\n命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir\n\n截图：\n\n![image-20201206115459406](/img/image-20201206115459406.png)\n\n## 4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容\n\n命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt\n\n​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt\n\n截图：![image-20201206115600801](/img/image-20201206115600801.png)\n\n## 5、合并本地多个小文件上传到HDFS\n\n命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt\n\n截图：![image-20201206115653987](/img/image-20201206115653987.png)\n\n## 6、 下载a.txt到本地文件系统\n\n命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt\n\n截图：![image-20201206115738831](/img/image-20201206115738831.png)\n\n## 7、删除HDFS上的/usr/local/hadoop/data/txtdir\n\n命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/\n\n截图：![image-20201206115832025](/img/image-20201206115832025.png)","slug":"HDFS-shell操作（1）","published":1,"updated":"2020-12-06T04:01:35.062Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpji000ol0t97zxj815u","content":"<h2 id=\"1、创建一个HDFS目录\">1、创建一个HDFS目录</h2>\n<p>命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/hdfs%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9.png\" alt=\"image-20201206115218466\"></p>\n<h2 id=\"2、本地文件上传到HDFS\">2、本地文件上传到HDFS</h2>\n<p>本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir</p>\n<p>命令：echo “I am student” &gt; a.txt</p>\n<p>​       echo “I am teacher” &gt; c.txt</p>\n<p>​       echo “I like hadoop” &gt; b.txt</p>\n<p>​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/HDFS%E4%B8%8A%E4%BC%A0.png\" alt=\"image-20201206115351845\"></p>\n<h2 id=\"3、查看-usr-local-hadoop-data-txtdir目录结构\">3、查看/usr/local/hadoop/data/txtdir目录结构</h2>\n<p>命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir</p>\n<p>截图：</p>\n<p><img src=\"/img/image-20201206115459406.png\" alt=\"image-20201206115459406\"></p>\n<h2 id=\"4、查看HDFS上-usr-local-hadoop-data-txtdir下的文件内容\">4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容</h2>\n<p>命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt</p>\n<p>截图：<img src=\"/img/image-20201206115600801.png\" alt=\"image-20201206115600801\"></p>\n<h2 id=\"5、合并本地多个小文件上传到HDFS\">5、合并本地多个小文件上传到HDFS</h2>\n<p>命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt</p>\n<p>截图：<img src=\"/img/image-20201206115653987.png\" alt=\"image-20201206115653987\"></p>\n<h2 id=\"6、-下载a-txt到本地文件系统\">6、 下载a.txt到本地文件系统</h2>\n<p>命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>截图：<img src=\"/img/image-20201206115738831.png\" alt=\"image-20201206115738831\"></p>\n<h2 id=\"7、删除HDFS上的-usr-local-hadoop-data-txtdir\">7、删除HDFS上的/usr/local/hadoop/data/txtdir</h2>\n<p>命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/</p>\n<p>截图：<img src=\"/img/image-20201206115832025.png\" alt=\"image-20201206115832025\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、创建一个HDFS目录\">1、创建一个HDFS目录</h2>\n<p>命令：hdfs dfs -mkdir -p /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/hdfs%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9.png\" alt=\"image-20201206115218466\"></p>\n<h2 id=\"2、本地文件上传到HDFS\">2、本地文件上传到HDFS</h2>\n<p>本地创建文件a.txt,b.txt,c.txt上传到HDFS /usr/local/hadoop/data/txtdir</p>\n<p>命令：echo “I am student” &gt; a.txt</p>\n<p>​       echo “I am teacher” &gt; c.txt</p>\n<p>​       echo “I like hadoop” &gt; b.txt</p>\n<p>​       hdfs dfs -put a.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -copyFromLocal b.txt /usr/local/hadoop/data/txtdir</p>\n<p>​       hdfs dfs -moveFromLocal c.txt /usr/local/hadoop/data/txtdir</p>\n<p>截图：<img src=\"/img/HDFS%E4%B8%8A%E4%BC%A0.png\" alt=\"image-20201206115351845\"></p>\n<h2 id=\"3、查看-usr-local-hadoop-data-txtdir目录结构\">3、查看/usr/local/hadoop/data/txtdir目录结构</h2>\n<p>命令：hdfs dfs -ls -R /usr/local/hadoop/data/txtdir</p>\n<p>截图：</p>\n<p><img src=\"/img/image-20201206115459406.png\" alt=\"image-20201206115459406\"></p>\n<h2 id=\"4、查看HDFS上-usr-local-hadoop-data-txtdir下的文件内容\">4、查看HDFS上/usr/local/hadoop/data/txtdir下的文件内容</h2>\n<p>命令：hdfs dfs -cat /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/b.txt</p>\n<p>​       hdfs dfs -cat /usr/local/hadoop/data/txtdir/c.txt</p>\n<p>截图：<img src=\"/img/image-20201206115600801.png\" alt=\"image-20201206115600801\"></p>\n<h2 id=\"5、合并本地多个小文件上传到HDFS\">5、合并本地多个小文件上传到HDFS</h2>\n<p>命令：hdfs dfs -appendToFile a.txt b.txt /usr/local/hadoop/data/txtdir/merges.txt</p>\n<p>截图：<img src=\"/img/image-20201206115653987.png\" alt=\"image-20201206115653987\"></p>\n<h2 id=\"6、-下载a-txt到本地文件系统\">6、 下载a.txt到本地文件系统</h2>\n<p>命令：hdfs dfs -get /usr/local/hadoop/data/txtdir/a.txt</p>\n<p>截图：<img src=\"/img/image-20201206115738831.png\" alt=\"image-20201206115738831\"></p>\n<h2 id=\"7、删除HDFS上的-usr-local-hadoop-data-txtdir\">7、删除HDFS上的/usr/local/hadoop/data/txtdir</h2>\n<p>命令：hdfs dfs -rm -r /usr/local/hadoop/data/txtdir/</p>\n<p>截图：<img src=\"/img/image-20201206115832025.png\" alt=\"image-20201206115832025\"></p>\n"},{"title":"HDFS概述","author":"郑天祺","date":"2019-12-16T02:10:00.000Z","_content":"\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","source":"_posts/HDFS概述.md","raw":"title: HDFS概述\nauthor: 郑天祺\ntags:\n\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 10:10:00\n\n---\n\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","slug":"HDFS概述","published":1,"updated":"2019-12-16T07:47:23.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjk000sl0t9bohzgex9","content":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2 id=\"1-NameNode\">(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2 id=\"2-DataNode\">(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2 id=\"（3）元信息的持久化\">（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2 id=\"（4）SecondaryNameNode\">（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt=\"\"></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2 id=\"1-NameNode\">(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2 id=\"2-DataNode\">(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2 id=\"（3）元信息的持久化\">（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2 id=\"（4）SecondaryNameNode\">（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt=\"\"></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n"},{"title":"HDFS文件操作","author":"郑天祺","date":"2019-12-16T07:47:00.000Z","_content":"\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","source":"_posts/HDFS文件操作.md","raw":"title: HDFS文件操作\nauthor: 郑天祺\ntags:\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 15:47:00\n\n---\n\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","slug":"HDFS文件操作","published":1,"updated":"2019-12-16T09:04:21.726Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjl000vl0t97wlg0qyk","content":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n"},{"title":"Hash解决冲突的方法","author":"郑天祺","date":"2020-09-18T05:02:00.000Z","_content":"\n# 1、什么是hash表\n\n​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。\n\n​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。\n\n​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。\n\n# 2、hash冲突\n\n​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。\n\n​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。\n\n# 3、常用的hash函数\n\n## \t1、直接定址法\n\n​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b\n\n## \t2、除留余数法\n\n​\t\t假设数组长度为l，value = key % l\n\n​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。\n\n## \t3、数字分析法\n\n​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算\n\n## \t4、平方区中法\n\n取关键字平方后中间几位作为哈希地址\n\n\n\n# 4、处理hash冲突的方法\n\n## \t1、开放定址法\n\n​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 \n\n​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,......,m-1)\n\n## \t2、再哈希法\n\n​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，...... ,等哈希函数\n\n​\t\t计算地址，直到无冲突。\n\n​\t（不易发生聚集，但是增加计算时间）\n\n## \t3、链地址法\n\n​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。\n\n​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。\n\n## \t4、建立公共溢出区\n\n​\t\t将 哈希表 分为 基本表 和 溢出表两部分\n\n​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。\n\n​\t\t","source":"_posts/Hash解决冲突的方法.md","raw":"title: Hash解决冲突的方法\nauthor: 郑天祺\ntags:\n\n  - 哈希表\ncategories:\n  - 面试\ndate: 2020-09-18 13:02:00\n\n---\n\n# 1、什么是hash表\n\n​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。\n\n​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。\n\n​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。\n\n# 2、hash冲突\n\n​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。\n\n​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。\n\n# 3、常用的hash函数\n\n## \t1、直接定址法\n\n​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b\n\n## \t2、除留余数法\n\n​\t\t假设数组长度为l，value = key % l\n\n​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。\n\n## \t3、数字分析法\n\n​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算\n\n## \t4、平方区中法\n\n取关键字平方后中间几位作为哈希地址\n\n\n\n# 4、处理hash冲突的方法\n\n## \t1、开放定址法\n\n​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 \n\n​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,......,m-1)\n\n## \t2、再哈希法\n\n​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，...... ,等哈希函数\n\n​\t\t计算地址，直到无冲突。\n\n​\t（不易发生聚集，但是增加计算时间）\n\n## \t3、链地址法\n\n​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。\n\n​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。\n\n## \t4、建立公共溢出区\n\n​\t\t将 哈希表 分为 基本表 和 溢出表两部分\n\n​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。\n\n​\t\t","slug":"Hash解决冲突的方法","published":1,"updated":"2020-09-18T06:04:38.475Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjm000zl0t980zz1su7","content":"<h1>1、什么是hash表</h1>\n<p>​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。</p>\n<p>​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。</p>\n<p>​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。</p>\n<h1>2、hash冲突</h1>\n<p>​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。</p>\n<p>​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。</p>\n<h1>3、常用的hash函数</h1>\n<h2 id=\"1、直接定址法\">1、直接定址法</h2>\n<p>​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b</p>\n<h2 id=\"2、除留余数法\">2、除留余数法</h2>\n<p>​\t\t假设数组长度为l，value = key % l</p>\n<p>​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。</p>\n<h2 id=\"3、数字分析法\">3、数字分析法</h2>\n<p>​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算</p>\n<h2 id=\"4、平方区中法\">4、平方区中法</h2>\n<p>取关键字平方后中间几位作为哈希地址</p>\n<h1>4、处理hash冲突的方法</h1>\n<h2 id=\"1、开放定址法\">1、开放定址法</h2>\n<p>​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</p>\n<p>​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,…,m-1)</p>\n<h2 id=\"2、再哈希法\">2、再哈希法</h2>\n<p>​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，… ,等哈希函数</p>\n<p>​\t\t计算地址，直到无冲突。</p>\n<p>​\t（不易发生聚集，但是增加计算时间）</p>\n<h2 id=\"3、链地址法\">3、链地址法</h2>\n<p>​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。</p>\n<p>​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。</p>\n<h2 id=\"4、建立公共溢出区\">4、建立公共溢出区</h2>\n<p>​\t\t将 哈希表 分为 基本表 和 溢出表两部分</p>\n<p>​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。</p>\n<p>​</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、什么是hash表</h1>\n<p>​\t\t散列表（hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。</p>\n<p>​\t\t也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫 散列函数，存放记录的数组叫做散列表。</p>\n<p>​\t\t给定表M，存在函数f（key），对于任意给定的关键字值 key，代入函数后若能得到包含改关键字的记录在表中的地址，则称表M为哈希（hash）表，函数 f（key）为哈希（hash）函数。</p>\n<h1>2、hash冲突</h1>\n<p>​\t\t对应不同的关键字可能获得相同的 hash 地址，即 key1 ≠ key2，但是f（key1） = f（key2）。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。</p>\n<p>​\t\t因为哈希函数是从关键字集合和地址集合的映像，通畅关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。</p>\n<h1>3、常用的hash函数</h1>\n<h2 id=\"1、直接定址法\">1、直接定址法</h2>\n<p>​\t\t取 key 的线性函数值作为 hash 值，value = a * key + b</p>\n<h2 id=\"2、除留余数法\">2、除留余数法</h2>\n<p>​\t\t假设数组长度为l，value = key % l</p>\n<p>​\t\t这一种散列码实现简单，运用比较多，但是如果输入的元素集合不具有一定的规律，比较容易产生冲突。数组的长度最好是质数，被除数为质数在一定程度上可以缓解数据堆积的问题。</p>\n<h2 id=\"3、数字分析法\">3、数字分析法</h2>\n<p>​\t\t对关键字进行分析，取关键字的若干位进行或者组合进行hash计算</p>\n<h2 id=\"4、平方区中法\">4、平方区中法</h2>\n<p>取关键字平方后中间几位作为哈希地址</p>\n<h1>4、处理hash冲突的方法</h1>\n<h2 id=\"1、开放定址法\">1、开放定址法</h2>\n<p>​\t\t所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</p>\n<p>​\t\tfi(key) = ( f(key) + di) MOD m  (di = 1,2,3,…,m-1)</p>\n<h2 id=\"2、再哈希法\">2、再哈希法</h2>\n<p>​\t\t再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，… ,等哈希函数</p>\n<p>​\t\t计算地址，直到无冲突。</p>\n<p>​\t（不易发生聚集，但是增加计算时间）</p>\n<h2 id=\"3、链地址法\">3、链地址法</h2>\n<p>​\t\t每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。</p>\n<p>​\t\t键值对k2，v2与键值对k1，v1通过计算后的索引值都为2，这时及时产生冲突，但是可以通到next 指将 k2，k1所在的节点连接起来，这样就解决了哈希的冲突问题。</p>\n<h2 id=\"4、建立公共溢出区\">4、建立公共溢出区</h2>\n<p>​\t\t将 哈希表 分为 基本表 和 溢出表两部分</p>\n<p>​\t\t凡是和基本表发生冲突的元素，依赖包填入溢出表。</p>\n<p>​</p>\n"},{"title":"HiveQL视图","author":"郑天祺","date":"2020-01-20T08:27:00.000Z","_content":"\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","source":"_posts/HiveQL视图.md","raw":"title: HiveQL视图\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 16:27:00\n\n---\n\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","slug":"HiveQL视图","published":1,"updated":"2020-01-21T02:58:33.416Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjm0011l0t9ac1z25pv","content":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n"},{"title":"Hive数据定义","author":"郑天祺","date":"2020-01-17T06:18:00.000Z","_content":"\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","source":"_posts/Hive数据定义.md","raw":"title: Hive数据定义\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 14:18:00\n\n---\n\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","slug":"Hive数据定义","published":1,"updated":"2020-01-17T07:53:05.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjn0013l0t9dcfb4n3s","content":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n"},{"title":"Hive数据操作（2）","author":"郑天祺","date":"2020-01-19T07:50:00.000Z","_content":"\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","source":"_posts/Hive数据操作（2）.md","raw":"title: Hive数据操作（2）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-19 15:50:00\n\n---\n\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","slug":"Hive数据操作（2）","published":1,"updated":"2020-01-20T06:46:13.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjo0017l0t98xzy0u6q","content":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN … EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN … EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n"},{"title":"Hive数据操作（1）","author":"郑天祺","date":"2020-01-17T08:11:00.000Z","_content":"\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","source":"_posts/Hive数据操作.md","raw":"title: Hive数据操作（1）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 16:11:00\n\n---\n\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","slug":"Hive数据操作","published":1,"updated":"2020-01-19T07:49:41.742Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjp0019l0t94qz425mz","content":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '$&#123;env:HOME&#125;/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE … WHEN … THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '$&#123;env:HOME&#125;/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE … WHEN … THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n"},{"title":"Hive数据操作（3）","author":"郑天祺","date":"2020-01-20T06:27:00.000Z","_content":"\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","source":"_posts/Hive数据操作（3）.md","raw":"title: Hive数据操作（3）\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 14:27:00\n---\n\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","slug":"Hive数据操作（3）","published":1,"updated":"2020-03-28T01:17:37.098Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjq001el0t93w5vfp5f","content":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n"},{"title":"Hive数据类型和文件格式","author":"郑天祺","date":"2020-01-17T05:41:00.000Z","_content":"\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","source":"_posts/Hive数据类型和文件格式.md","raw":"title: Hive数据类型和文件格式\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 13:41:00\n\n---\n\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","slug":"Hive数据类型和文件格式","published":1,"updated":"2020-01-17T06:12:51.383Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjr001fl0t91lisds2l","content":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：… cast(s AS INT) … ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  ‘\\t’ (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：… cast(s AS INT) … ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  ‘\\t’ (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n"},{"title":"Hive模式设计","author":"郑天祺","date":"2020-01-21T06:09:00.000Z","_content":"\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","source":"_posts/Hive模式设计.md","raw":"title: Hive模式设计\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 14:09:00\n\n---\n\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","slug":"Hive模式设计","published":1,"updated":"2020-01-21T08:48:03.705Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpju001jl0t95xbrfz31","content":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2 id=\"二、同一份数据多种处理\">二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=$&#123;dt&#125;)\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION($&#123;hiveconf:dt&#125;)\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='$&#123;hiveconf:dt&#125;')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2 id=\"二、同一份数据多种处理\">二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=$&#123;dt&#125;)\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='$&#123;hiveconf:dt&#125;'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION($&#123;hiveconf:dt&#125;)\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='$&#123;hiveconf:dt&#125;')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n"},{"title":"Hive索引","author":"郑天祺","date":"2020-01-21T02:59:00.000Z","_content":"\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","source":"_posts/Hive索引.md","raw":"title: Hive索引\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 10:59:00\n\n---\n\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","slug":"Hive索引","published":1,"updated":"2020-01-21T05:50:03.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpju001ll0t938gxdoht","content":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n"},{"title":"Hive调优","author":"郑天祺","date":"2020-01-21T08:48:00.000Z","_content":"\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","source":"_posts/Hive调优.md","raw":"title: Hive调优\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 16:48:00\n\n---\n\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","slug":"Hive调优","published":1,"updated":"2020-01-21T09:46:53.242Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjv001pl0t90sbac8dl","content":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=$&#123;hiveconf.mapred.job.tracker&#125;;\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=$&#123;oldjobtracker&#125;;\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=$&#123;hiveconf.mapred.job.tracker&#125;;\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=$&#123;oldjobtracker&#125;;\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n"},{"title":"HttpClient","author":"郑天祺","date":"2020-07-15T09:12:00.000Z","_content":"\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","source":"_posts/HttpClient.md","raw":"title: HttpClient\nauthor: 郑天祺\ntags:\n\n  - httpclient\ncategories:\n  - 网络\ndate: 2020-07-15 17:12:00\n\n---\n\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","slug":"HttpClient","published":1,"updated":"2020-07-15T14:39:17.273Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjw001rl0t9gqvkh45l","content":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2 id=\"（1）引入依赖\">（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2 id=\"（2）返回实体\">（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable &#123;\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) &#123;\n        this.code = code;\n        this.content = content;\n    &#125;\n\n    public HttpClientResult(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public int getCode() &#123;\n        return code;\n    &#125;\n\n    public void setCode(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public String getContent() &#123;\n        return content;\n    &#125;\n\n    public void setContent(String content) &#123;\n        this.content = content;\n    &#125;\n\n    @Override\n    public String toString() &#123;\n        return &quot;HttpClientResult&#123;&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '&#125;';\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"（3）工具类\">（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils &#123;\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception &#123;\n        return doGet(url, null, null);\n    &#125;\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doGet(url, null, params);\n    &#125;\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception &#123;\n        return doPost(url, null, null);\n    &#125;\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doPost(url, null, params);\n    &#125;\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception &#123;\n        return doPut(url);\n    &#125;\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        if (params == null) &#123;\n            params = new HashMap&lt;String, String&gt;();\n        &#125;\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    &#125;\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) &#123;\n        // 封装请求头\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException &#123;\n        // 封装请求参数\n        if (params != null) &#123;\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            &#125;\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        &#125;\n    &#125;\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception &#123;\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) &#123;\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) &#123;\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            &#125;\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        &#125;\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    &#125;\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException &#123;\n        // 释放资源\n        if (httpResponse != null) &#123;\n            httpResponse.close();\n        &#125;\n        if (httpClient != null) &#123;\n            httpClient.close();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2 id=\"（1）引入依赖\">（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;httpclient.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2 id=\"（2）返回实体\">（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable &#123;\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) &#123;\n        this.code = code;\n        this.content = content;\n    &#125;\n\n    public HttpClientResult(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public int getCode() &#123;\n        return code;\n    &#125;\n\n    public void setCode(int code) &#123;\n        this.code = code;\n    &#125;\n\n    public String getContent() &#123;\n        return content;\n    &#125;\n\n    public void setContent(String content) &#123;\n        this.content = content;\n    &#125;\n\n    @Override\n    public String toString() &#123;\n        return &quot;HttpClientResult&#123;&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '&#125;';\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"（3）工具类\">（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils &#123;\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception &#123;\n        return doGet(url, null, null);\n    &#125;\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doGet(url, null, params);\n    &#125;\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception &#123;\n        return doPost(url, null, null);\n    &#125;\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        return doPost(url, null, params);\n    &#125;\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception &#123;\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        &#125; finally &#123;\n            // 释放资源\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception &#123;\n        return doPut(url);\n    &#125;\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception &#123;\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try &#123;\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        &#125; finally &#123;\n            release(httpResponse, httpClient);\n        &#125;\n    &#125;\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception &#123;\n        if (params == null) &#123;\n            params = new HashMap&lt;String, String&gt;();\n        &#125;\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    &#125;\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) &#123;\n        // 封装请求头\n        if (params != null) &#123;\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException &#123;\n        // 封装请求参数\n        if (params != null) &#123;\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) &#123;\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            &#125;\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        &#125;\n    &#125;\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception &#123;\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) &#123;\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) &#123;\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            &#125;\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        &#125;\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    &#125;\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException &#123;\n        // 释放资源\n        if (httpResponse != null) &#123;\n            httpResponse.close();\n        &#125;\n        if (httpClient != null) &#123;\n            httpClient.close();\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n"},{"title":"Http和Https的区别","author":"郑天祺","date":"2020-09-11T03:11:00.000Z","_content":"\n# 一、基本概念\n\n## 1、HTTP\n\n​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。\n\n​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。\n\n​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。\n\n## 2、HTTPS\n\n​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。\n\n## 3、HTTPS和HTTP的区别\n\n（1）安全性\n\nHTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。\n\n（2）费用\n\n使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。\n\n（3）响应速度\n\nHTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。\n\n（4）端口\n\nhttp 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。\n\n（5）空间消耗\n\nHTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。\n\n# 二、HTTPS的工作流程\n\n![img](/img/https-intro.png)\n\n\n\n1、客户端发起 HTTPS 请求\n\n这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。\n\n2、服务端的配置\n\n采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。\n\n这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。\n\n3、传送证书\n\n这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。\n\n4、客户端解析证书\n\n这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。\n\n如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。\n\n5、传送加密信息\n\n这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。\n\n6、服务端解密信息\n\n服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。\n\n7、传输加密后的信息\n\n这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。\n\n8、客户端解密信息\n\n客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。","source":"_posts/Http和Https的区别.md","raw":"title: Http和Https的区别\nauthor: 郑天祺\ntags:\n  - https\ncategories:\n  - 网络\ndate: 2020-09-11 11:11:00\n\n---\n\n# 一、基本概念\n\n## 1、HTTP\n\n​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。\n\n​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。\n\n​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。\n\n## 2、HTTPS\n\n​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。\n\n## 3、HTTPS和HTTP的区别\n\n（1）安全性\n\nHTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。\n\n（2）费用\n\n使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。\n\n（3）响应速度\n\nHTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。\n\n（4）端口\n\nhttp 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。\n\n（5）空间消耗\n\nHTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。\n\n# 二、HTTPS的工作流程\n\n![img](/img/https-intro.png)\n\n\n\n1、客户端发起 HTTPS 请求\n\n这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。\n\n2、服务端的配置\n\n采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。\n\n这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。\n\n3、传送证书\n\n这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。\n\n4、客户端解析证书\n\n这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。\n\n如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。\n\n5、传送加密信息\n\n这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。\n\n6、服务端解密信息\n\n服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。\n\n7、传输加密后的信息\n\n这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。\n\n8、客户端解密信息\n\n客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。","slug":"Http和Https的区别","published":1,"updated":"2020-12-06T03:16:32.003Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjx001vl0t9c10e4syg","content":"<h1>一、基本概念</h1>\n<h2 id=\"1、HTTP\">1、HTTP</h2>\n<p>​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。</p>\n<p>​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。</p>\n<p>​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。</p>\n<h2 id=\"2、HTTPS\">2、HTTPS</h2>\n<p>​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。</p>\n<h2 id=\"3、HTTPS和HTTP的区别\">3、HTTPS和HTTP的区别</h2>\n<p>（1）安全性</p>\n<p>HTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。</p>\n<p>（2）费用</p>\n<p>使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。</p>\n<p>（3）响应速度</p>\n<p>HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。</p>\n<p>（4）端口</p>\n<p>http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。</p>\n<p>（5）空间消耗</p>\n<p>HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。</p>\n<h1>二、HTTPS的工作流程</h1>\n<p><img src=\"/img/https-intro.png\" alt=\"img\"></p>\n<p>1、客户端发起 HTTPS 请求</p>\n<p>这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。</p>\n<p>2、服务端的配置</p>\n<p>采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。</p>\n<p>这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。</p>\n<p>3、传送证书</p>\n<p>这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。</p>\n<p>4、客户端解析证书</p>\n<p>这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。</p>\n<p>如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。</p>\n<p>5、传送加密信息</p>\n<p>这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。</p>\n<p>6、服务端解密信息</p>\n<p>服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。</p>\n<p>7、传输加密后的信息</p>\n<p>这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。</p>\n<p>8、客户端解密信息</p>\n<p>客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本概念</h1>\n<h2 id=\"1、HTTP\">1、HTTP</h2>\n<p>​    HyperText Transfer Protocol：超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。</p>\n<p>​    HTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。</p>\n<p>​    HTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。</p>\n<h2 id=\"2、HTTPS\">2、HTTPS</h2>\n<p>​    Hypertext Transfer Protocol Secure：超文本传输安全协议是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。HTTPS 默认工作在 TCP 协议443端口。</p>\n<h2 id=\"3、HTTPS和HTTP的区别\">3、HTTPS和HTTP的区别</h2>\n<p>（1）安全性</p>\n<p>HTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。</p>\n<p>（2）费用</p>\n<p>使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。</p>\n<p>（3）响应速度</p>\n<p>HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。</p>\n<p>（4）端口</p>\n<p>http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。</p>\n<p>（5）空间消耗</p>\n<p>HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。</p>\n<h1>二、HTTPS的工作流程</h1>\n<p><img src=\"/img/https-intro.png\" alt=\"img\"></p>\n<p>1、客户端发起 HTTPS 请求</p>\n<p>这个没什么好说的，就是用户在浏览器里输入一个 https 网址，然后连接到 server 的 443 端口。</p>\n<p>2、服务端的配置</p>\n<p>采用 HTTPS 协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl 就是个不错的选择，有 1 年的免费服务)。</p>\n<p>这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。</p>\n<p>3、传送证书</p>\n<p>这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。</p>\n<p>4、客户端解析证书</p>\n<p>这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。</p>\n<p>如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。</p>\n<p>5、传送加密信息</p>\n<p>这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。</p>\n<p>6、服务端解密信息</p>\n<p>服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。</p>\n<p>7、传输加密后的信息</p>\n<p>这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。</p>\n<p>8、客户端解密信息</p>\n<p>客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。</p>\n"},{"title":"IOC中的基本反射步骤","author":"郑天祺","date":"2020-09-13T01:22:00.000Z","_content":"\n# 1、controller\n\n```java\npackage cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController {\n    \n    private UserService userService;\n\n    public UserService getUserService() {\n        return userService;\n    }\n\n    public void setUserService(UserService userService) {\n        this.userService = userService;\n    }\n}\n```\n\n# 2、service\n\n```java\npackage cn.edu.bjut.base.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n```\n\n# 3、TestReflect\n\n```java\npackage cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect {\n    public static void main(String[] args) throws Exception {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(\"userService\");\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = \"set\" + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    }\n}\n```\n\n","source":"_posts/IOC中的基本反射步骤.md","raw":"title: IOC中的基本反射步骤\nauthor: 郑天祺\ntags:\n\n  - spring\ncategories:\n  - spring\ndate: 2020-09-13 09:22:00\n\n---\n\n# 1、controller\n\n```java\npackage cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController {\n    \n    private UserService userService;\n\n    public UserService getUserService() {\n        return userService;\n    }\n\n    public void setUserService(UserService userService) {\n        this.userService = userService;\n    }\n}\n```\n\n# 2、service\n\n```java\npackage cn.edu.bjut.base.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n```\n\n# 3、TestReflect\n\n```java\npackage cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect {\n    public static void main(String[] args) throws Exception {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(\"userService\");\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = \"set\" + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    }\n}\n```\n\n","slug":"IOC中的基本反射步骤","published":1,"updated":"2020-09-13T01:25:24.128Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjy001yl0t9crgqbdrx","content":"<h1>1、controller</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController &#123;\n    \n    private UserService userService;\n\n    public UserService getUserService() &#123;\n        return userService;\n    &#125;\n\n    public void setUserService(UserService userService) &#123;\n        this.userService = userService;\n    &#125;\n&#125;\n</code></pre>\n<h1>2、service</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、TestReflect</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect &#123;\n    public static void main(String[] args) throws Exception &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(&quot;userService&quot;);\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = &quot;set&quot; + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、controller</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.controller;\n\nimport cn.edu.bjut.base.spring.Autowired;\nimport cn.edu.bjut.base.spring.service.UserService;\n\npublic class UserController &#123;\n    \n    private UserService userService;\n\n    public UserService getUserService() &#123;\n        return userService;\n    &#125;\n\n    public void setUserService(UserService userService) &#123;\n        this.userService = userService;\n    &#125;\n&#125;\n</code></pre>\n<h1>2、service</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、TestReflect</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.base.spring;\n\nimport cn.edu.bjut.base.spring.controller.UserController;\nimport cn.edu.bjut.base.spring.service.UserService;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class TestReflect &#123;\n    public static void main(String[] args) throws Exception &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 创建对象\n        UserService userService = new UserService();\n        System.out.println(userService);\n        // 获取所有的属性\n        Field serviceField = clazz.getDeclaredField(&quot;userService&quot;);\n        serviceField.setAccessible(true);\n        // 只有通过方法才能够设置具体的属性值\n        String name = serviceField.getName();\n        // 拼接方法的名称\n        name = name.substring(0, 1).toUpperCase() + name.substring(1);\n        String setMethodName = &quot;set&quot; + name;\n        // 通过方法注入属性的对象\n        Method method = clazz.getMethod(setMethodName, UserService.class);\n        // 反射\n        method.invoke(userController, userService);\n        System.out.println(userController.getUserService());\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"JAVA数据类型易混概念","author":"郑天祺","date":"2020-01-06T06:30:00.000Z","_content":"\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","source":"_posts/JAVA数据类型.md","raw":"title: JAVA数据类型易混概念\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-01-06 14:30:00\n\n---\n\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","slug":"JAVA数据类型","published":1,"updated":"2020-01-06T09:53:20.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpjz0022l0t92xw7frxo","content":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    &#125;\n&#125;\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。<br>\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。<br>\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。<br>\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：<br>\n​\t\t\tdouble x * 9.997 ;<br>\n​\t\t\tint nx = ( int ) x ;<br>\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。<br>\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b<br>\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    &#125;\n&#125;\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。<br>\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。<br>\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。<br>\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：<br>\n​\t\t\tdouble x * 9.997 ;<br>\n​\t\t\tint nx = ( int ) x ;<br>\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。<br>\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b<br>\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n"},{"title":"JVM内存结构","author":"郑天祺","date":"2020-09-27T03:55:00.000Z","_content":"\n![image-20200927115654479](/img/JVM Memory.png)\n\n# 1、方法区\n\n用于存储虚拟机加载的 类信息，常量，静态变量等数据。\n\n# 2、堆\n\n存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。\n\n# 3、栈\n\nJava 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。\n\n生命周期与线程相同。\n\n# 4、本地方法栈\n\n作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。\n\n# 5、程序计数器\n\n当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。","source":"_posts/JVM内存结构.md","raw":"title: JVM内存结构\nauthor: 郑天祺\ntags:\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-27 11:55:00\n\n---\n\n![image-20200927115654479](/img/JVM Memory.png)\n\n# 1、方法区\n\n用于存储虚拟机加载的 类信息，常量，静态变量等数据。\n\n# 2、堆\n\n存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。\n\n# 3、栈\n\nJava 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。\n\n生命周期与线程相同。\n\n# 4、本地方法栈\n\n作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。\n\n# 5、程序计数器\n\n当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。","slug":"JVM内存结构","published":1,"updated":"2020-09-27T04:14:16.575Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk00025l0t99nvucxc9","content":"<p>![image-20200927115654479](/img/JVM Memory.png)</p>\n<h1>1、方法区</h1>\n<p>用于存储虚拟机加载的 类信息，常量，静态变量等数据。</p>\n<h1>2、堆</h1>\n<p>存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。</p>\n<h1>3、栈</h1>\n<p>Java 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。</p>\n<p>生命周期与线程相同。</p>\n<h1>4、本地方法栈</h1>\n<p>作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。</p>\n<h1>5、程序计数器</h1>\n<p>当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>![image-20200927115654479](/img/JVM Memory.png)</p>\n<h1>1、方法区</h1>\n<p>用于存储虚拟机加载的 类信息，常量，静态变量等数据。</p>\n<h1>2、堆</h1>\n<p>存放对象实例，所有的对象和数组都要在堆上分配，是 JVM 所管理的内存最大的一块区域。</p>\n<h1>3、栈</h1>\n<p>Java 方法执行的内存模型：存储局部变量表、操作数栈、动态链接、方法出口灯信息。</p>\n<p>生命周期与线程相同。</p>\n<h1>4、本地方法栈</h1>\n<p>作用与虚拟机栈类似，不同点本地方法栈为 native 方法执行服务，虚拟机栈为虚拟机执行的 java方法服务。</p>\n<h1>5、程序计数器</h1>\n<p>当前线程所执行的行号指示器。是 JVM 内存区域最小的一块区域。执行字节码工作时就是利用程序计数器来选取下一条需要执行的字节码指令。</p>\n"},{"title":"JVM垃圾回收算法","author":"郑天祺","date":"2020-09-18T01:18:00.000Z","_content":"\n# 1、标记 - 清除 算法\n\n​\t标记无用对象，然后进行清除回收。\n\n​\t缺点：效率不高，无法清除垃圾碎片。\n\n![image-20200918092639589](/img/jvm1.png)\n\n# 2、复制 - 清除 算法\n\n​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n​\t缺点：内存使用率不高，只有原来的一半\n\n![image-20200918092729361](/img/jvm2.png)\n\n# 3、标记 - 整理 - 清除 算法\n\n​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存\n\n![image-20200918092801239](/img/jvm3.png)\n\n# 4、分代 - 收集算法\n\n​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；\n\n​\t\t新生代基本采用复制算法;\n\n​\t\t老年代采用标记整理算法。\n\n# 5、分代 - 收集算法详解\n\n​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。\n\n​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。\n\n​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。\n\n![image-20200918094003026](/img/jvmHeapStructure.png)\n\n·································································\n\n堆大小 = 新生代 + 老年代\n\n新生代与老年代的比例 = 1：2\n\n","source":"_posts/JVM垃圾回收算法.md","raw":"title: JVM垃圾回收算法\nauthor: 郑天祺\ntags:\n\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-18 09:18:00\n\n---\n\n# 1、标记 - 清除 算法\n\n​\t标记无用对象，然后进行清除回收。\n\n​\t缺点：效率不高，无法清除垃圾碎片。\n\n![image-20200918092639589](/img/jvm1.png)\n\n# 2、复制 - 清除 算法\n\n​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n​\t缺点：内存使用率不高，只有原来的一半\n\n![image-20200918092729361](/img/jvm2.png)\n\n# 3、标记 - 整理 - 清除 算法\n\n​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存\n\n![image-20200918092801239](/img/jvm3.png)\n\n# 4、分代 - 收集算法\n\n​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；\n\n​\t\t新生代基本采用复制算法;\n\n​\t\t老年代采用标记整理算法。\n\n# 5、分代 - 收集算法详解\n\n​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。\n\n​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。\n\n​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。\n\n![image-20200918094003026](/img/jvmHeapStructure.png)\n\n·································································\n\n堆大小 = 新生代 + 老年代\n\n新生代与老年代的比例 = 1：2\n\n","slug":"JVM垃圾回收算法","published":1,"updated":"2020-09-18T04:55:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk10028l0t96ke502ap","content":"<h1>1、标记 - 清除 算法</h1>\n<p>​\t标记无用对象，然后进行清除回收。</p>\n<p>​\t缺点：效率不高，无法清除垃圾碎片。</p>\n<p><img src=\"/img/jvm1.png\" alt=\"image-20200918092639589\"></p>\n<h1>2、复制 - 清除 算法</h1>\n<p>​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p>\n<p>​\t缺点：内存使用率不高，只有原来的一半</p>\n<p><img src=\"/img/jvm2.png\" alt=\"image-20200918092729361\"></p>\n<h1>3、标记 - 整理 - 清除 算法</h1>\n<p>​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</p>\n<p><img src=\"/img/jvm3.png\" alt=\"image-20200918092801239\"></p>\n<h1>4、分代 - 收集算法</h1>\n<p>​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；</p>\n<p>​\t\t新生代基本采用复制算法;</p>\n<p>​\t\t老年代采用标记整理算法。</p>\n<h1>5、分代 - 收集算法详解</h1>\n<p>​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。</p>\n<p>​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。</p>\n<p>​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。</p>\n<p><img src=\"/img/jvmHeapStructure.png\" alt=\"image-20200918094003026\"></p>\n<p>·································································</p>\n<p>堆大小 = 新生代 + 老年代</p>\n<p>新生代与老年代的比例 = 1：2</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、标记 - 清除 算法</h1>\n<p>​\t标记无用对象，然后进行清除回收。</p>\n<p>​\t缺点：效率不高，无法清除垃圾碎片。</p>\n<p><img src=\"/img/jvm1.png\" alt=\"image-20200918092639589\"></p>\n<h1>2、复制 - 清除 算法</h1>\n<p>​\t按照容量划分二个大小相等的内存区域，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p>\n<p>​\t缺点：内存使用率不高，只有原来的一半</p>\n<p><img src=\"/img/jvm2.png\" alt=\"image-20200918092729361\"></p>\n<h1>3、标记 - 整理 - 清除 算法</h1>\n<p>​\t标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</p>\n<p><img src=\"/img/jvm3.png\" alt=\"image-20200918092801239\"></p>\n<h1>4、分代 - 收集算法</h1>\n<p>​\t\t根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代；</p>\n<p>​\t\t新生代基本采用复制算法;</p>\n<p>​\t\t老年代采用标记整理算法。</p>\n<h1>5、分代 - 收集算法详解</h1>\n<p>​\t新生代：朝生夕灭的对象（例如：方法的局部变量引用的对象等）。</p>\n<p>​    老年代：存活得比较久，但还是要死的对象（例如：缓存对象、单例对象等）。</p>\n<p>​    永久代：对象生成后几乎不灭的对象（例如：加载过的类信息）。</p>\n<p><img src=\"/img/jvmHeapStructure.png\" alt=\"image-20200918094003026\"></p>\n<p>·································································</p>\n<p>堆大小 = 新生代 + 老年代</p>\n<p>新生代与老年代的比例 = 1：2</p>\n"},{"title":"JVM类加载过程","author":"郑天祺","date":"2020-09-27T03:08:00.000Z","_content":"\n# 0、图解\n\n![image-20200927113941669](/img/JVM类加载过程.png)\n\n# 1、加载\n\n类的加载，分为三步：\n\n（1）通过一个类的全限定名获取该类的二进制流\n\n（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构\n\n（3）在内存中生成该类的Class对象，作为该类的数据访问入口\n\n# 2、验证\n\n验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：\n\n## （1）文件格式验证：\n\n验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。\n\n## （2）元数据验证：\n\n对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。\n\n## （3）字节码验证：\n\n是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。\n\n## （4）符号引用验证：\n\n这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n# 3、准备\n\n准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。\n\n```java\npublic static int value = 123; \n```\n\n在准备阶段初始值是0，在初始化阶段才会变成123\n\n# 4、解析\n\n该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n# 5、初始化\n\n初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码\n\n# 6、总结\n\nJava语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。\n\n这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。\n\n任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。\n\n此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。\n\n","source":"_posts/JVM类加载过程.md","raw":"title: JVM类加载过程\nauthor: 郑天祺\ntags:\n  - JVM\ncategories:\n  - 面试\ndate: 2020-09-27 11:08:00\n\n---\n\n# 0、图解\n\n![image-20200927113941669](/img/JVM类加载过程.png)\n\n# 1、加载\n\n类的加载，分为三步：\n\n（1）通过一个类的全限定名获取该类的二进制流\n\n（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构\n\n（3）在内存中生成该类的Class对象，作为该类的数据访问入口\n\n# 2、验证\n\n验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：\n\n## （1）文件格式验证：\n\n验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。\n\n## （2）元数据验证：\n\n对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。\n\n## （3）字节码验证：\n\n是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。\n\n## （4）符号引用验证：\n\n这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n# 3、准备\n\n准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。\n\n```java\npublic static int value = 123; \n```\n\n在准备阶段初始值是0，在初始化阶段才会变成123\n\n# 4、解析\n\n该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n# 5、初始化\n\n初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码\n\n# 6、总结\n\nJava语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。\n\n这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。\n\n任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。\n\n此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。\n\n","slug":"JVM类加载过程","published":1,"updated":"2020-09-27T03:54:17.818Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk2002bl0t9gt4cem9h","content":"<h1>0、图解</h1>\n<p><img src=\"/img/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B.png\" alt=\"image-20200927113941669\"></p>\n<h1>1、加载</h1>\n<p>类的加载，分为三步：</p>\n<p>（1）通过一个类的全限定名获取该类的二进制流</p>\n<p>（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构</p>\n<p>（3）在内存中生成该类的Class对象，作为该类的数据访问入口</p>\n<h1>2、验证</h1>\n<p>验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：</p>\n<h2 id=\"（1）文件格式验证：\">（1）文件格式验证：</h2>\n<p>验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。</p>\n<h2 id=\"（2）元数据验证：\">（2）元数据验证：</h2>\n<p>对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。</p>\n<h2 id=\"（3）字节码验证：\">（3）字节码验证：</h2>\n<p>是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<h2 id=\"（4）符号引用验证：\">（4）符号引用验证：</h2>\n<p>这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<h1>3、准备</h1>\n<p>准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。</p>\n<pre><code class=\"language-java\">public static int value = 123; \n</code></pre>\n<p>在准备阶段初始值是0，在初始化阶段才会变成123</p>\n<h1>4、解析</h1>\n<p>该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<h1>5、初始化</h1>\n<p>初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码</p>\n<h1>6、总结</h1>\n<p>Java语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。</p>\n<p>这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。</p>\n<p>任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。</p>\n<p>此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>0、图解</h1>\n<p><img src=\"/img/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B.png\" alt=\"image-20200927113941669\"></p>\n<h1>1、加载</h1>\n<p>类的加载，分为三步：</p>\n<p>（1）通过一个类的全限定名获取该类的二进制流</p>\n<p>（2）将该二进制流中的静态存储结构转化为方法去运行时数据结构</p>\n<p>（3）在内存中生成该类的Class对象，作为该类的数据访问入口</p>\n<h1>2、验证</h1>\n<p>验证的目的是为了确保Class文件的字节流的信息不会危害到虚拟机，分为四步：</p>\n<h2 id=\"（1）文件格式验证：\">（1）文件格式验证：</h2>\n<p>验证字节流是否符合 Class 文件的规范，如：主次版本号是否在当前虚拟机范围内，常量池中收到常量是否有不被支持的类型。</p>\n<h2 id=\"（2）元数据验证：\">（2）元数据验证：</h2>\n<p>对字节码描述的信息进行语义分析，如果这个类是否有父类，是否集成了不被集成的类。</p>\n<h2 id=\"（3）字节码验证：\">（3）字节码验证：</h2>\n<p>是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<h2 id=\"（4）符号引用验证：\">（4）符号引用验证：</h2>\n<p>这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<h1>3、准备</h1>\n<p>准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。</p>\n<pre><code class=\"language-java\">public static int value = 123; \n</code></pre>\n<p>在准备阶段初始值是0，在初始化阶段才会变成123</p>\n<h1>4、解析</h1>\n<p>该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<h1>5、初始化</h1>\n<p>初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码</p>\n<h1>6、总结</h1>\n<p>Java语言是一种具有动态性的解释型语言，类（Class）只有被加载到 JVM 后才能运行。当运行指定程序时，JVM 会将编译生成的 .class 文件按照需求和一定的规则加载到内存中，并组成成为一个完整的 Java 应用程序。</p>\n<p>这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的，类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用 new 等方式创建对象时，会隐式地调用类的加载器把对应的类 加载到 JVM 中。显示加载指的是通过直接调用 class.forName() 方法来把所需的类加载到 JVM 中。</p>\n<p>任何一个工程项目都是由许多类组成的，当程序启动时，只把需要的类加载到 JVM 中，其他类只有被使用到的时候才会被加载，采用这种方法一方面可以加快加载速度，另一方面可以节约程序运行时对内存的开销。</p>\n<p>此外，在 java 语言中，每个类或接口都对应一个 .class文件，这些文件可以被看成是一个个可以被动态加载的单元，因此当只有部分类被修改时，只需要重新编译变化的类即可，而不需要重新编译所有文件，因此加快了编译速度。</p>\n"},{"title":"Kafka的简单使用","author":"郑天祺","date":"2020-12-14T04:59:00.000Z","_content":"\n# 1、安装kafka\n\nhttp://kafka.apache.org/quickstart  （linux版）\n\nwindows版：\n首先cmd到kafka的bin下\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties \n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties \n\n集群的设置: http://kafka.apache.org/quickstart#quickstart_multibroker\n\n# 2、参数\n\n建立KafkaProperties 类，编写连接kafka所需要的参数\n\n```java\npublic class KafkaProperties {\n    private static final String IP = \"127.0.0.1:9092\";\n    public static final String TOPIC = \"topic_test\";\n    public static Properties initConfig() {\n        Properties properties = new Properties();\n        properties.setProperty(\"bootstrap.servers\", IP);\n        properties.put(\"group.id\", \"group-1\");\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(\"session.timeout.ms\", \"30000\");\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(\"enable.auto.commit\", \"false\");\n        properties.put(\"auto.commit.interval.ms\", \"1000\");\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(\"auto.offset.reset\", \"earliest\");\n        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    }\n}\n```\n\n# 3、消费者\n\n```java\npublic class KafkaConsumerConnection {\n    private static KafkaConsumer<String, String> consumer = null;\n    private KafkaConsumerConnection() {\n    }\n    public static KafkaConsumer<String, String> getConsumer() {\n        if (consumer == null) {\n            consumer = new KafkaConsumer<>(KafkaProperties.initConfig());\n        }\n        return consumer;\n    }\n}\n\n```\n\n# 4、生产者\n\n```java\npublic class KafkaProducerConnection {\n    private static KafkaProducer<String, String> producer = null;\n    private KafkaProducerConnection() {\n    }\n    public static KafkaProducer<String, String> getProducer(){\n        if(producer == null){\n            producer = new KafkaProducer<>(KafkaProperties.initConfig());\n        }\n        return producer;\n    }\n}\n```\n\n# 5、编写生产者main函数\n\n```java\npublic class ProducerTest {\n    public static void main(String[] args) {\n        Producer<String, String> producer = KafkaProducerConnection.getProducer();\n        try {\n            while (true) {\n                String msg = \"Hello,\" + new Random().nextInt(100);\n                ProducerRecord<String, String> record = new ProducerRecord<>(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(\"消息发送成功:\" + msg);\n                Thread.sleep(500);\n            }\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } finally {\n            producer.close();\n        }\n    }\n}\n```\n\n# 6、编写消费者main函数\n\n```java\npublic class ConsumerTest {\n    public static void main(String[] args) {\n        Consumer<String, String> consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(duration);\n            for (ConsumerRecord<String, String> record : records) {\n                System.out.println(String.format(\"topic:%s,offset:%d,消息:%s\", record.topic(), record.offset(), record.value()));\n            }\n        }\n    }\n}\n```\n\n# 7、先执行消费者后执行生产者。","source":"_posts/Kafka的简单使用.md","raw":"title: Kafka的简单使用\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:59:00\n---\n\n# 1、安装kafka\n\nhttp://kafka.apache.org/quickstart  （linux版）\n\nwindows版：\n首先cmd到kafka的bin下\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties \n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties \n\n集群的设置: http://kafka.apache.org/quickstart#quickstart_multibroker\n\n# 2、参数\n\n建立KafkaProperties 类，编写连接kafka所需要的参数\n\n```java\npublic class KafkaProperties {\n    private static final String IP = \"127.0.0.1:9092\";\n    public static final String TOPIC = \"topic_test\";\n    public static Properties initConfig() {\n        Properties properties = new Properties();\n        properties.setProperty(\"bootstrap.servers\", IP);\n        properties.put(\"group.id\", \"group-1\");\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(\"session.timeout.ms\", \"30000\");\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(\"enable.auto.commit\", \"false\");\n        properties.put(\"auto.commit.interval.ms\", \"1000\");\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(\"auto.offset.reset\", \"earliest\");\n        properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    }\n}\n```\n\n# 3、消费者\n\n```java\npublic class KafkaConsumerConnection {\n    private static KafkaConsumer<String, String> consumer = null;\n    private KafkaConsumerConnection() {\n    }\n    public static KafkaConsumer<String, String> getConsumer() {\n        if (consumer == null) {\n            consumer = new KafkaConsumer<>(KafkaProperties.initConfig());\n        }\n        return consumer;\n    }\n}\n\n```\n\n# 4、生产者\n\n```java\npublic class KafkaProducerConnection {\n    private static KafkaProducer<String, String> producer = null;\n    private KafkaProducerConnection() {\n    }\n    public static KafkaProducer<String, String> getProducer(){\n        if(producer == null){\n            producer = new KafkaProducer<>(KafkaProperties.initConfig());\n        }\n        return producer;\n    }\n}\n```\n\n# 5、编写生产者main函数\n\n```java\npublic class ProducerTest {\n    public static void main(String[] args) {\n        Producer<String, String> producer = KafkaProducerConnection.getProducer();\n        try {\n            while (true) {\n                String msg = \"Hello,\" + new Random().nextInt(100);\n                ProducerRecord<String, String> record = new ProducerRecord<>(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(\"消息发送成功:\" + msg);\n                Thread.sleep(500);\n            }\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } finally {\n            producer.close();\n        }\n    }\n}\n```\n\n# 6、编写消费者main函数\n\n```java\npublic class ConsumerTest {\n    public static void main(String[] args) {\n        Consumer<String, String> consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(duration);\n            for (ConsumerRecord<String, String> record : records) {\n                System.out.println(String.format(\"topic:%s,offset:%d,消息:%s\", record.topic(), record.offset(), record.value()));\n            }\n        }\n    }\n}\n```\n\n# 7、先执行消费者后执行生产者。","slug":"Kafka的简单使用","published":1,"updated":"2021-04-13T07:11:45.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk3002el0t99x5pbyxp","content":"<h1>1、安装kafka</h1>\n<p><a href=\"http://kafka.apache.org/quickstart\">http://kafka.apache.org/quickstart</a>  （linux版）</p>\n<p>windows版：<br>\n首先cmd到kafka的bin下<br>\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties<br>\n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties</p>\n<p>集群的设置: <a href=\"http://kafka.apache.org/quickstart#quickstart_multibroker\">http://kafka.apache.org/quickstart#quickstart_multibroker</a></p>\n<h1>2、参数</h1>\n<p>建立KafkaProperties 类，编写连接kafka所需要的参数</p>\n<pre><code class=\"language-java\">public class KafkaProperties &#123;\n    private static final String IP = &quot;127.0.0.1:9092&quot;;\n    public static final String TOPIC = &quot;topic_test&quot;;\n    public static Properties initConfig() &#123;\n        Properties properties = new Properties();\n        properties.setProperty(&quot;bootstrap.servers&quot;, IP);\n        properties.put(&quot;group.id&quot;, &quot;group-1&quot;);\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);\n        properties.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;);\n        properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、消费者</h1>\n<pre><code class=\"language-java\">public class KafkaConsumerConnection &#123;\n    private static KafkaConsumer&lt;String, String&gt; consumer = null;\n    private KafkaConsumerConnection() &#123;\n    &#125;\n    public static KafkaConsumer&lt;String, String&gt; getConsumer() &#123;\n        if (consumer == null) &#123;\n            consumer = new KafkaConsumer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return consumer;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、生产者</h1>\n<pre><code class=\"language-java\">public class KafkaProducerConnection &#123;\n    private static KafkaProducer&lt;String, String&gt; producer = null;\n    private KafkaProducerConnection() &#123;\n    &#125;\n    public static KafkaProducer&lt;String, String&gt; getProducer()&#123;\n        if(producer == null)&#123;\n            producer = new KafkaProducer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return producer;\n    &#125;\n&#125;\n</code></pre>\n<h1>5、编写生产者main函数</h1>\n<pre><code class=\"language-java\">public class ProducerTest &#123;\n    public static void main(String[] args) &#123;\n        Producer&lt;String, String&gt; producer = KafkaProducerConnection.getProducer();\n        try &#123;\n            while (true) &#123;\n                String msg = &quot;Hello,&quot; + new Random().nextInt(100);\n                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(&quot;消息发送成功:&quot; + msg);\n                Thread.sleep(500);\n            &#125;\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            producer.close();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、编写消费者main函数</h1>\n<pre><code class=\"language-java\">public class ConsumerTest &#123;\n    public static void main(String[] args) &#123;\n        Consumer&lt;String, String&gt; consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) &#123;\n            ConsumerRecords&lt;String, String&gt; records = consumer.poll(duration);\n            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;\n                System.out.println(String.format(&quot;topic:%s,offset:%d,消息:%s&quot;, record.topic(), record.offset(), record.value()));\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>7、先执行消费者后执行生产者。</h1>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、安装kafka</h1>\n<p><a href=\"http://kafka.apache.org/quickstart\">http://kafka.apache.org/quickstart</a>  （linux版）</p>\n<p>windows版：<br>\n首先cmd到kafka的bin下<br>\n其中启动内置的zk用：zookeeper-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\zookeeper.properties<br>\n启动Kafka用：kafka-server-start.bat D:\\environment\\kafka_2.12-2.3.0\\config\\server.properties</p>\n<p>集群的设置: <a href=\"http://kafka.apache.org/quickstart#quickstart_multibroker\">http://kafka.apache.org/quickstart#quickstart_multibroker</a></p>\n<h1>2、参数</h1>\n<p>建立KafkaProperties 类，编写连接kafka所需要的参数</p>\n<pre><code class=\"language-java\">public class KafkaProperties &#123;\n    private static final String IP = &quot;127.0.0.1:9092&quot;;\n    public static final String TOPIC = &quot;topic_test&quot;;\n    public static Properties initConfig() &#123;\n        Properties properties = new Properties();\n        properties.setProperty(&quot;bootstrap.servers&quot;, IP);\n        properties.put(&quot;group.id&quot;, &quot;group-1&quot;);\n        // session.timeout.ms：消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s 。\n        properties.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);\n        // 消费者是否自动提交偏移量，默认值是true,避免出现重复数据和数据丢失，可以把它设为 false。\n        properties.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);\n        properties.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);\n        // auto.offset.reset:消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理\n        properties.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;);\n        properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);\n        // earliest：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。\n        properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // latest：在偏移量无效的情况下，消费者将从最新位置读取分区的记录\n        properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);\n        // max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数\n        // fetch.max.wait.ms:消费者等待时间，默认是500。\n        // fetch.min.bytes:消费者从服务器获取记录的最小字节数。\n        // client.id：该参数可以是任意的字符串，服务器会用它来识别消息的来源。\n        // max.poll.records:用于控制单次调用 call() 方住能够返回的记录数量\n        // receive.buffer.bytes和send.buffer.bytes：指定了 TCP socket 接收和发送数据包的缓冲区大小，默认值为-1\n        return properties;\n    &#125;\n&#125;\n</code></pre>\n<h1>3、消费者</h1>\n<pre><code class=\"language-java\">public class KafkaConsumerConnection &#123;\n    private static KafkaConsumer&lt;String, String&gt; consumer = null;\n    private KafkaConsumerConnection() &#123;\n    &#125;\n    public static KafkaConsumer&lt;String, String&gt; getConsumer() &#123;\n        if (consumer == null) &#123;\n            consumer = new KafkaConsumer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return consumer;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、生产者</h1>\n<pre><code class=\"language-java\">public class KafkaProducerConnection &#123;\n    private static KafkaProducer&lt;String, String&gt; producer = null;\n    private KafkaProducerConnection() &#123;\n    &#125;\n    public static KafkaProducer&lt;String, String&gt; getProducer()&#123;\n        if(producer == null)&#123;\n            producer = new KafkaProducer&lt;&gt;(KafkaProperties.initConfig());\n        &#125;\n        return producer;\n    &#125;\n&#125;\n</code></pre>\n<h1>5、编写生产者main函数</h1>\n<pre><code class=\"language-java\">public class ProducerTest &#123;\n    public static void main(String[] args) &#123;\n        Producer&lt;String, String&gt; producer = KafkaProducerConnection.getProducer();\n        try &#123;\n            while (true) &#123;\n                String msg = &quot;Hello,&quot; + new Random().nextInt(100);\n                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(KafkaProperties.TOPIC, msg);\n                producer.send(record);\n                System.out.println(&quot;消息发送成功:&quot; + msg);\n                Thread.sleep(500);\n            &#125;\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            producer.close();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、编写消费者main函数</h1>\n<pre><code class=\"language-java\">public class ConsumerTest &#123;\n    public static void main(String[] args) &#123;\n        Consumer&lt;String, String&gt; consumer = KafkaConsumerConnection.getConsumer();\n        consumer.subscribe(Collections.singletonList(KafkaProperties.TOPIC));\n        Duration duration = Duration.ofMillis(100);\n        while (true) &#123;\n            ConsumerRecords&lt;String, String&gt; records = consumer.poll(duration);\n            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;\n                System.out.println(String.format(&quot;topic:%s,offset:%d,消息:%s&quot;, record.topic(), record.offset(), record.value()));\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>7、先执行消费者后执行生产者。</h1>\n"},{"title":"JVM性能优化整理","author":"郑天祺","date":"2020-11-17T07:14:00.000Z","_content":"\n1、类加载过程\n\n​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。\n\n​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。\n\n7个阶段：加载-验证-准备-解析-初始化-使用-卸载\n\n（1）加载\n\n加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:\n\n​\t\t通过一个类的全限定名获取该类的二进制流。\n\n​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。\n\n​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。\n\n（2）验证\n\n验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:\n\t\t文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.\n\n​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。\n\n​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n（3）解析\n\n​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n（4）初始化\n\n​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。\n\n（6）使用\n\n（7）卸载\n\n2、GC对象的判定方法\n\n（1）引用计数法\n\n​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。\n\n（2）可达性算法（引用链法）\n\n该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）\n\n在Java中可以作为GC Roots 的对象有以下几种:\n\n虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象\n\n3、java内存泄露\n\n​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。\n\n​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。\n\n​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。\n\n​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。\n\n​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。\n\n​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。\n\n4、深拷贝和浅拷贝\n\n​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。\n\n5、类加载器\n\n实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:\n\t\t（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。\n\t\t（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。\n\n​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。\n\n6、类加载器双亲委派模型\n\n​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。","source":"_posts/JVM性能优化整理.md","raw":"title: JVM性能优化整理\nauthor: 郑天祺\ntags: []\ncategories:\n\n  - 面试\ndate: 2020-11-17 15:14:00\n\n---\n\n1、类加载过程\n\n​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。\n\n​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。\n\n​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。\n\n7个阶段：加载-验证-准备-解析-初始化-使用-卸载\n\n（1）加载\n\n加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:\n\n​\t\t通过一个类的全限定名获取该类的二进制流。\n\n​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。\n\n​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。\n\n（2）验证\n\n验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:\n\t\t文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.\n\n​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。\n\n​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。\n\n（3）解析\n\n​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。\n\n（4）初始化\n\n​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。\n\n（6）使用\n\n（7）卸载\n\n2、GC对象的判定方法\n\n（1）引用计数法\n\n​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。\n\n（2）可达性算法（引用链法）\n\n该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）\n\n在Java中可以作为GC Roots 的对象有以下几种:\n\n虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象\n\n3、java内存泄露\n\n​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。\n\n​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。\n\n​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。\n\n​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。\n\n​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。\n\n​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。\n\n4、深拷贝和浅拷贝\n\n​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。\n\n5、类加载器\n\n实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:\n\t\t（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。\n\t\t（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。\n\n​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。\n\n6、类加载器双亲委派模型\n\n​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。","slug":"JVM性能优化整理","published":1,"updated":"2020-11-17T08:56:51.613Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk4002hl0t9cn3g00z4","content":"<p>1、类加载过程</p>\n<p>​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。</p>\n<p>​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。</p>\n<p>7个阶段：加载-验证-准备-解析-初始化-使用-卸载</p>\n<p>（1）加载</p>\n<p>加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:</p>\n<p>​\t\t通过一个类的全限定名获取该类的二进制流。</p>\n<p>​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。</p>\n<p>​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。</p>\n<p>（2）验证</p>\n<p>验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:<br>\n文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.</p>\n<p>​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。<br>\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<p>​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<p>（3）解析</p>\n<p>​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<p>（4）初始化</p>\n<p>​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。</p>\n<p>（6）使用</p>\n<p>（7）卸载</p>\n<p>2、GC对象的判定方法</p>\n<p>（1）引用计数法</p>\n<p>​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。</p>\n<p>（2）可达性算法（引用链法）</p>\n<p>该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）</p>\n<p>在Java中可以作为GC Roots 的对象有以下几种:</p>\n<p>虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象</p>\n<p>3、java内存泄露</p>\n<p>​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。</p>\n<p>​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。</p>\n<p>​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。</p>\n<p>​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。</p>\n<p>​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。</p>\n<p>​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。</p>\n<p>4、深拷贝和浅拷贝</p>\n<p>​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。<br>\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。</p>\n<p>5、类加载器</p>\n<p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:<br>\n（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。<br>\n（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。</p>\n<p>​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径<br>\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。<br>\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</p>\n<p>6、类加载器双亲委派模型</p>\n<p>​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、类加载过程</p>\n<p>​\t\tJava语言是一种具有动态性的解释型语言，类(Class)只有被加载到JVM后才能运行。当运行指定程序时，JVM会将编译生成的.class文件按照需求和一定的规则加载到内存中，并组织成为一个完整的Java应用程序。</p>\n<p>​\t\t这个加载过程是由类加载器完成，具体来说，就是由ClassLoader和它的子类来实现的。类加载器本身也是一个类，其实质是把类文件从硬盘读取到内存中。</p>\n<p>​\t\t类的加载方式分为隐式加载和显示加载。隐式加载指的是程序在使用new等方式创建对象时，会隐式地调用类的加载器把对应的类加载到JVM中。显示加载指的是通过直接调用class.forName()方法来把所需的类加载到JVM中。</p>\n<p>7个阶段：加载-验证-准备-解析-初始化-使用-卸载</p>\n<p>（1）加载</p>\n<p>加载是类加载的第一个过程，在这个阶段，将完成一下三件事情:</p>\n<p>​\t\t通过一个类的全限定名获取该类的二进制流。</p>\n<p>​\t\t将该二进制流中的静态存储结构转化为方法去运行时数据结构。</p>\n<p>​\t\t在内存中生成该类的Class 对象，作为该类的数据访问入口。</p>\n<p>（2）验证</p>\n<p>验证的目的是为了确保Class 文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:<br>\n文件格式验证∶验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.</p>\n<p>​\t\t元数据验证∶对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。<br>\n​\t\t字节码验证∶是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如∶方法中的类型转换是否正确，跳转指令是否正确等。</p>\n<p>​\t\t符号引用验证∶这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。</p>\n<p>（3）解析</p>\n<p>​\t\t该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。</p>\n<p>（4）初始化</p>\n<p>​\t\t初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中的定义的java程序代码。</p>\n<p>（6）使用</p>\n<p>（7）卸载</p>\n<p>2、GC对象的判定方法</p>\n<p>（1）引用计数法</p>\n<p>​\t\t引用对象时，计数器+1；引用失效时，计数器-1。无法解决循环引用问题。</p>\n<p>（2）可达性算法（引用链法）</p>\n<p>该算法的思想是: 从一个被称为GC Roots的对象开始向下搜索，如果一个对象到GC Roots没有任何引用链相连时，则说明此对象不可用。（有向图进行管理）</p>\n<p>在Java中可以作为GC Roots 的对象有以下几种:</p>\n<p>虚拟机栈中引用的对象、方法区类静态属性引用的对象、方法区常量池引用的对象、本地方法栈JNI引用的对象</p>\n<p>3、java内存泄露</p>\n<p>​\t\t所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java中有垃圾回收机制，它可以保证一对象不再被引用的时候，即对象变成了孤儿的时候，对象将自动被垃圾回收器从内存中清除掉。</p>\n<p>​\t\t由于Java使用有向图的方式进行垃圾回收管理,可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的。</p>\n<p>​\t\tJava 中的内存泄露的情况:长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。</p>\n<p>​\t\t通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中可能出现内存泄露的情况，例如，缓存系统，我们加载了一个对象放在缓存中(例如放在一个全局map对象中)，然后一直不再使用它，这个对象一直被缓存引用，但却不再被使用。。</p>\n<p>​\t\t检查Java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。</p>\n<p>​\t如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存?不会，在下一个垃圾回收周期中，这个对象将是可被回收的。</p>\n<p>4、深拷贝和浅拷贝</p>\n<p>​\t\t浅拷贝就是对对象中的数据成员进行简单赋值，如果存在动态成员或者指针就会报错。<br>\n​\t\t深拷贝就是对对象中存在的动态成员或指针重新开辟内存空间。</p>\n<p>5、类加载器</p>\n<p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。主要有一下四种类加载器:<br>\n（1）启动类加载器(BootstrapClassLoader)用来加载Java核心类库，无法被Java程序直接引用。<br>\n（2）扩展类加载器(extensions class loader):它用来加载Java 的扩展库。Java虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载Java类。</p>\n<p>​\t\t（3）系统类加载器(system class loader):它根据Java应用的类路径<br>\n(CLASSPATH)来加载Java类。一般来说,Java应用的类都是由它来完成加载的。可以通过ClassLoader.getSystemClassLoader()来获取它。<br>\n​\t\t（4）用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</p>\n<p>6、类加载器双亲委派模型</p>\n<p>​\t\t当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p>\n"},{"title":"MapReduce平均数计算","author":"郑天祺","date":"2020-12-06T04:14:00.000Z","_content":"\n## 1、建立三个文档\n\n![image-20201206121627187](/img/image-20201206121627187.png)\n\n![image-20201206121650739](/img/image-20201206121650739.png)\n\n![image-20201206121711716](/img/image-20201206121711716.png)\n\n上传 到hdfs\n\n![image-20201206121829515](/img/image-20201206121829515.png)\n\n\n\n## 2、代码\n\n```java\npackage cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore {  \n    public static class Map extends Mapper<LongWritable, Text, Text, IntWritable> {  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(\" \");  \n            List<String> list = new ArrayList<String>(2);  \n            for (String nameOrScore : nameAndScore) {  \n                if (!\"\".equals(nameOrScore)) {  \n                    list.add(nameOrScore);  \n                }  \n            }  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        }  \n    }  \n    public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {  \n        @Override  \n        protected void reduce(Text key, Iterable<IntWritable> values,  \n                Reducer<Text, IntWritable, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) {  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            }  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        }  \n    }  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  \n        Configuration conf = new Configuration();  \n        conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\");  \n        String[] othersArgs = new String[] {\"/mapreduce/inputavg\",\"/mapreduce/outputavg\"};  \n        if(othersArgs.length != 2) {  \n            System.err.println(\"Usage:Avgscore<int><out>\");  \n            System.exit(2);  \n        }  \n        Job job = Job.getInstance(conf, \"Avgscore\");  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    }  \n}  \n```\n\n3、查看结果\n\n![image-20201206122126739](/img/image-20201206122126739.png)","source":"_posts/MapReduce平均数计算.md","raw":"title: MapReduce平均数计算\nauthor: 郑天祺\ntags:\n\n  - hadoop\ncategories:\n  - 大数据\ndate: 2020-12-06 12:14:00\n\n---\n\n## 1、建立三个文档\n\n![image-20201206121627187](/img/image-20201206121627187.png)\n\n![image-20201206121650739](/img/image-20201206121650739.png)\n\n![image-20201206121711716](/img/image-20201206121711716.png)\n\n上传 到hdfs\n\n![image-20201206121829515](/img/image-20201206121829515.png)\n\n\n\n## 2、代码\n\n```java\npackage cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore {  \n    public static class Map extends Mapper<LongWritable, Text, Text, IntWritable> {  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(\" \");  \n            List<String> list = new ArrayList<String>(2);  \n            for (String nameOrScore : nameAndScore) {  \n                if (!\"\".equals(nameOrScore)) {  \n                    list.add(nameOrScore);  \n                }  \n            }  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        }  \n    }  \n    public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {  \n        @Override  \n        protected void reduce(Text key, Iterable<IntWritable> values,  \n                Reducer<Text, IntWritable, Text, IntWritable>.Context context)  \n                throws IOException, InterruptedException {  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) {  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            }  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        }  \n    }  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  \n        Configuration conf = new Configuration();  \n        conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\");  \n        String[] othersArgs = new String[] {\"/mapreduce/inputavg\",\"/mapreduce/outputavg\"};  \n        if(othersArgs.length != 2) {  \n            System.err.println(\"Usage:Avgscore<int><out>\");  \n            System.exit(2);  \n        }  \n        Job job = Job.getInstance(conf, \"Avgscore\");  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    }  \n}  \n```\n\n3、查看结果\n\n![image-20201206122126739](/img/image-20201206122126739.png)","slug":"MapReduce平均数计算","published":1,"updated":"2020-12-06T04:21:44.982Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk5002ll0t97vx9e1u7","content":"<h2 id=\"1、建立三个文档\">1、建立三个文档</h2>\n<p><img src=\"/img/image-20201206121627187.png\" alt=\"image-20201206121627187\"></p>\n<p><img src=\"/img/image-20201206121650739.png\" alt=\"image-20201206121650739\"></p>\n<p><img src=\"/img/image-20201206121711716.png\" alt=\"image-20201206121711716\"></p>\n<p>上传 到hdfs</p>\n<p><img src=\"/img/image-20201206121829515.png\" alt=\"image-20201206121829515\"></p>\n<h2 id=\"2、代码\">2、代码</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore &#123;  \n    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(&quot; &quot;);  \n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(2);  \n            for (String nameOrScore : nameAndScore) &#123;  \n                if (!&quot;&quot;.equals(nameOrScore)) &#123;  \n                    list.add(nameOrScore);  \n                &#125;  \n            &#125;  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        &#125;  \n    &#125;  \n    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  \n                Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) &#123;  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            &#125;  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        &#125;  \n    &#125;  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;  \n        Configuration conf = new Configuration();  \n        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);  \n        String[] othersArgs = new String[] &#123;&quot;/mapreduce/inputavg&quot;,&quot;/mapreduce/outputavg&quot;&#125;;  \n        if(othersArgs.length != 2) &#123;  \n            System.err.println(&quot;Usage:Avgscore&lt;int&gt;&lt;out&gt;&quot;);  \n            System.exit(2);  \n        &#125;  \n        Job job = Job.getInstance(conf, &quot;Avgscore&quot;);  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    &#125;  \n&#125;  \n</code></pre>\n<p>3、查看结果</p>\n<p><img src=\"/img/image-20201206122126739.png\" alt=\"image-20201206122126739\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、建立三个文档\">1、建立三个文档</h2>\n<p><img src=\"/img/image-20201206121627187.png\" alt=\"image-20201206121627187\"></p>\n<p><img src=\"/img/image-20201206121650739.png\" alt=\"image-20201206121650739\"></p>\n<p><img src=\"/img/image-20201206121711716.png\" alt=\"image-20201206121711716\"></p>\n<p>上传 到hdfs</p>\n<p><img src=\"/img/image-20201206121829515.png\" alt=\"image-20201206121829515\"></p>\n<h2 id=\"2、代码\">2、代码</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut;  \n\nimport org.apache.hadoop.io.LongWritable;  \nimport org.apache.hadoop.io.Text;  \nimport org.apache.hadoop.mapreduce.lib.input.*;  \nimport org.apache.hadoop.mapreduce.lib.output.*;  \nimport org.apache.hadoop.mapreduce.Job;  \nimport org.apache.hadoop.mapreduce.Mapper;  \nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;  \nimport java.util.ArrayList;  \nimport java.util.List;  \nimport org.apache.hadoop.conf.Configuration;  \nimport org.apache.hadoop.fs.Path;  \nimport org.apache.hadoop.io.IntWritable;  \n\npublic class AvgScore &#123;  \n    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            String line = value.toString();  \n            String[] nameAndScore = line.split(&quot; &quot;);  \n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(2);  \n            for (String nameOrScore : nameAndScore) &#123;  \n                if (!&quot;&quot;.equals(nameOrScore)) &#123;  \n                    list.add(nameOrScore);  \n                &#125;  \n            &#125;  \n            context.write(new Text(list.get(0)), new IntWritable(Integer.parseInt(list.get(1))));  \n        &#125;  \n    &#125;  \n    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;  \n        @Override  \n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  \n                Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)  \n                throws IOException, InterruptedException &#123;  \n            int sum = 0;  \n            int count = 0;  \n            for (IntWritable value : values) &#123;  \n                sum += Integer.parseInt(value.toString());  \n                count++;  \n            &#125;  \n            int average = sum / count;  \n            context.write(key, new IntWritable(average));  \n        &#125;  \n    &#125;  \n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;  \n        Configuration conf = new Configuration();  \n        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);  \n        String[] othersArgs = new String[] &#123;&quot;/mapreduce/inputavg&quot;,&quot;/mapreduce/outputavg&quot;&#125;;  \n        if(othersArgs.length != 2) &#123;  \n            System.err.println(&quot;Usage:Avgscore&lt;int&gt;&lt;out&gt;&quot;);  \n            System.exit(2);  \n        &#125;  \n        Job job = Job.getInstance(conf, &quot;Avgscore&quot;);  \n        job.setJarByClass(AvgScore.class);  \n        job.setMapperClass(Map.class);  \n        job.setReducerClass(Reduce.class);  \n        job.setOutputKeyClass(Text.class);  \n        job.setOutputValueClass(IntWritable.class);  \n        job.setInputFormatClass(TextInputFormat.class);  \n        job.setOutputFormatClass(TextOutputFormat.class);  \n        FileInputFormat.addInputPath(job, new Path(othersArgs[0]));  \n        FileOutputFormat.setOutputPath(job, new Path(othersArgs[1]));  \n        System.exit(job.waitForCompletion(true) ? 0 : 1);  \n    &#125;  \n&#125;  \n</code></pre>\n<p>3、查看结果</p>\n<p><img src=\"/img/image-20201206122126739.png\" alt=\"image-20201206122126739\"></p>\n"},{"title":"MapReduce概述","author":"郑天祺","date":"2019-12-16T09:13:00.000Z","_content":"\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","source":"_posts/MapReduce概述.md","raw":"title: MapReduce概述\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 17:13:00\n\n---\n\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","slug":"MapReduce概述","published":1,"updated":"2019-12-16T10:07:22.822Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk6002pl0t91v7fbiw1","content":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2 id=\"（1）拆分输入数据\">（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2 id=\"（2）执行Map方法\">（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2 id=\"（3）排序与合并处理\">（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2 id=\"（4）Reduce-阶段的排序与合并\">（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3 id=\"（A）Map\">（A）Map</h3>\n<h4 id=\"（a）Read：\">（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4 id=\"（b）Map：\">（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4 id=\"（c）Collect：\">（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4 id=\"（d）Spill：\">（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4 id=\"（e）Combine：\">（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3 id=\"（B）Reduce\">（B）Reduce</h3>\n<h4 id=\"（a）Shuffle：\">（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4 id=\"（b）Merge：\">（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4 id=\"（c）Sort：\">（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4 id=\"（d）Reduce：\">（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4 id=\"（e）Write：\">（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2 id=\"（1）拆分输入数据\">（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2 id=\"（2）执行Map方法\">（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2 id=\"（3）排序与合并处理\">（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2 id=\"（4）Reduce-阶段的排序与合并\">（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3 id=\"（A）Map\">（A）Map</h3>\n<h4 id=\"（a）Read：\">（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4 id=\"（b）Map：\">（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4 id=\"（c）Collect：\">（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4 id=\"（d）Spill：\">（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4 id=\"（e）Combine：\">（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3 id=\"（B）Reduce\">（B）Reduce</h3>\n<h4 id=\"（a）Shuffle：\">（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4 id=\"（b）Merge：\">（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4 id=\"（c）Sort：\">（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4 id=\"（d）Reduce：\">（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4 id=\"（e）Write：\">（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n"},{"title":"Nacos配置中心使用","author":"郑天祺","date":"2019-11-25T08:38:00.000Z","_content":"\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","source":"_posts/Nacos配置中心使用.md","raw":"title: Nacos配置中心使用\nauthor: 郑天祺\ntags:\n  - SpringCloud\n  - nacos-config\ncategories:\n  - spring\n  - ''\ndate: 2019-11-25 16:38:00\n---\n\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","slug":"Nacos配置中心使用","published":1,"updated":"2021-04-13T07:12:12.904Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk7002tl0t97bpf7gwd","content":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants &#123;\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n&#125;\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils &#123;\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static &#123;\n        try &#123;\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        &#125;\n    &#125;\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) &#123;\n        Properties properties = null;\n        try &#123;\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (Exception e) &#123;\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId, e);\n        &#125;\n        if (null == properties) &#123;\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId);\n        &#125;\n        return properties;\n    &#125;\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) &#123;\n        Properties properties = null;\n        try &#123;\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() &#123;\n                @Override\n                public void receiveConfigInfo(String configInfo) &#123;\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                &#125;\n\n                @Override\n                public Executor getExecutor() &#123;\n                    return null;\n                &#125;\n            &#125;);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        &#125; catch (IOException e) &#123;\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        &#125;\n        return properties;\n    &#125;\n\n&#125;\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    &#125;\n\n&#125;\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-greenwich.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants &#123;\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n&#125;\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils &#123;\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static &#123;\n        try &#123;\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        &#125;\n    &#125;\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) &#123;\n        Properties properties = null;\n        try &#123;\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (Exception e) &#123;\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId, e);\n        &#125;\n        if (null == properties) &#123;\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group=&#123;&#125;,dataId=&#123;&#125;&quot;, group, dataId);\n        &#125;\n        return properties;\n    &#125;\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) &#123;\n        Properties properties = null;\n        try &#123;\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() &#123;\n                @Override\n                public void receiveConfigInfo(String configInfo) &#123;\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                &#125;\n\n                @Override\n                public Executor getExecutor() &#123;\n                    return null;\n                &#125;\n            &#125;);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        &#125; catch (NacosException e) &#123;\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        &#125; catch (IOException e) &#123;\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        &#125;\n        return properties;\n    &#125;\n\n&#125;\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    &#125;\n\n&#125;\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-greenwich.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n"},{"title":"SimpleDateFormat引发的线程安全问题","author":"郑天祺","date":"2019-10-12T10:42:00.000Z","_content":"\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","source":"_posts/SimpleDateFormat引发的线程安全问题.md","raw":"title: SimpleDateFormat引发的线程安全问题\nauthor: 郑天祺\ntags:\n  - 并发\n  - 线程安全\ncategories:\n  - java基础\ndate: 2019-10-12 18:42:00\n\n---\n\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","slug":"SimpleDateFormat引发的线程安全问题","published":1,"updated":"2019-10-12T12:08:31.315Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk8002xl0t91xe96n4x","content":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans &#123;\n\n    public static void main(String[] args) &#123;\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try &#123;\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        &#125; catch (ParseException e) &#123;\n            System.out.println(&quot;时间转换出错，出错信息为 =&#123;&#125;&quot; + e);\n        &#125;\n\n    &#125;\n&#125;\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n&#125;\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：<a href=\"https://blog.csdn.net/lululove19870526/article/details/83684568\">https://blog.csdn.net/lululove19870526/article/details/83684568</a></p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\t<a href=\"https://www.jianshu.com/p/3c5d7f09dfbd\">https://www.jianshu.com/p/3c5d7f09dfbd</a></p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans &#123;\n\n    public static void main(String[] args) &#123;\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try &#123;\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        &#125; catch (ParseException e) &#123;\n            System.out.println(&quot;时间转换出错，出错信息为 =&#123;&#125;&quot; + e);\n        &#125;\n\n    &#125;\n&#125;\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n&#125;\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：<a href=\"https://blog.csdn.net/lululove19870526/article/details/83684568\">https://blog.csdn.net/lululove19870526/article/details/83684568</a></p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\t<a href=\"https://www.jianshu.com/p/3c5d7f09dfbd\">https://www.jianshu.com/p/3c5d7f09dfbd</a></p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils &#123;\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n"},{"title":"Spark相关概述","author":"郑天祺","date":"2019-12-18T05:41:00.000Z","_content":"\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","source":"_posts/Spark相关概述.md","raw":"title: Spark相关概述\nauthor: 郑天祺\ntags:\n\n  - Spark\ncategories:\n  - 大数据\ndate: 2019-12-18 13:41:00\n\n---\n\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","slug":"Spark相关概述","published":1,"updated":"2019-12-18T09:12:52.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpk90031l0t9hfdofkvs","content":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n"},{"title":"SpringCloud-Alibaba整合Nacos服务注册发现","author":"郑天祺","date":"2019-12-03T07:18:00.000Z","_content":"\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","source":"_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","raw":"title: SpringCloud-Alibaba整合Nacos服务注册发现\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2019-12-03 15:18:00\n---\n\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","slug":"SpringCloud-Alibaba整合Nacos服务注册发现","published":1,"updated":"2021-04-13T07:12:54.786Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpka0035l0t95slr89if","content":"<h1>一、服务注册</h1>\n<h2 id=\"1、引入依赖\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置application-yml\">2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2 id=\"3、springboot启动类\">3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、确认注册成功\">4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2 id=\"1、引入依赖-2\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置文件配置\">2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2 id=\"3、开启服务发现、负载均衡、熔断器功能\">3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.sy&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.sy&quot;&#125;)\npublic class NacosDiscoveryExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"4、创建服务代理类\">4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：<br>\n<a href=\"http://1.name\">1.name</a>：服务提供者注册在服务注册中心的名称；<br>\n2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；<br>\n3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n&#125;\n</code></pre>\n<h2 id=\"5、创建Hystrix的断路器工厂类\">5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; &#123;\n    @Override\n    public ConsumerService create(Throwable cause) &#123;\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() &#123;\n            @Override\n            public String hello() &#123;\n                return &quot;请求失败&quot;;\n            &#125;\n\n            @Override\n            public String hello(String string) &#123;\n                return &quot;请求失败. string=&quot; + string;\n            &#125;\n        &#125;;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"6、通用代理类的实例进行服务调用，与本地调用无异。如下：\">6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController &#123;\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return consumerService.hello(string);\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、服务注册</h1>\n<h2 id=\"1、引入依赖\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置application-yml\">2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2 id=\"3、springboot启动类\">3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、确认注册成功\">4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2 id=\"1、引入依赖-2\">1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2 id=\"2、配置文件配置\">2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2 id=\"3、开启服务发现、负载均衡、熔断器功能\">3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.sy&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.sy&quot;&#125;)\npublic class NacosDiscoveryExampleApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"4、创建服务代理类\">4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：<br>\n<a href=\"http://1.name\">1.name</a>：服务提供者注册在服务注册中心的名称；<br>\n2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；<br>\n3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n&#125;\n</code></pre>\n<h2 id=\"5、创建Hystrix的断路器工厂类\">5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; &#123;\n    @Override\n    public ConsumerService create(Throwable cause) &#123;\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() &#123;\n            @Override\n            public String hello() &#123;\n                return &quot;请求失败&quot;;\n            &#125;\n\n            @Override\n            public String hello(String string) &#123;\n                return &quot;请求失败. string=&quot; + string;\n            &#125;\n        &#125;;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"6、通用代理类的实例进行服务调用，与本地调用无异。如下：\">6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController &#123;\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return consumerService.hello(string);\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"Spring Bean作用域与生命周期","author":"郑天祺","date":"2019-11-14T08:52:00.000Z","_content":"\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","source":"_posts/Spring-Bean生命周期.md","raw":"title: Spring Bean作用域与生命周期\nauthor: 郑天祺\ntags:\n\n  - spring\ncategories:\n  - java基础\ndate: 2019-11-14 16:52:00\n\n---\n\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","slug":"Spring-Bean生命周期","published":1,"updated":"2019-11-14T09:19:21.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpka0039l0t91tb3aa5o","content":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2 id=\"1、使用-Profile\">1、使用@Profile</h2>\n<h4 id=\"1）假设存在dev-spring-boot-和-test-spring-boot两个数据库，使用注解-Profile定义两个Bean\">1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4 id=\"2）在-Java-启动项目中，我们只需要如下配置就能启动Profile机制：\">2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4 id=\"3）按照-springboot-的规则\">3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2 id=\"2、使用-Spring-EL\">2、使用 Spring EL</h2>\n<h4 id=\"1）读取属性文件的值，如：\">1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// $&#123;......&#125; 代表占位符\n@Value(&quot;$&#123;database.driverName&#125;&quot;)   \nString driver\n</code></pre>\n<h4 id=\"2）记录一个Bean初始化事件，如：\">2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #&#123;......&#125; 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#&#123;T(System).currentTimeMillis()&#125;&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#&#123;‘使用 Spring EL 赋值字符串’&#125;&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#&#123;9.3E3&#125;&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#&#123;beanName.str&#125;&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2 id=\"1、使用-Profile\">1、使用@Profile</h2>\n<h4 id=\"1）假设存在dev-spring-boot-和-test-spring-boot两个数据库，使用注解-Profile定义两个Bean\">1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4 id=\"2）在-Java-启动项目中，我们只需要如下配置就能启动Profile机制：\">2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4 id=\"3）按照-springboot-的规则\">3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2 id=\"2、使用-Spring-EL\">2、使用 Spring EL</h2>\n<h4 id=\"1）读取属性文件的值，如：\">1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// $&#123;......&#125; 代表占位符\n@Value(&quot;$&#123;database.driverName&#125;&quot;)   \nString driver\n</code></pre>\n<h4 id=\"2）记录一个Bean初始化事件，如：\">2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #&#123;......&#125; 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#&#123;T(System).currentTimeMillis()&#125;&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#&#123;‘使用 Spring EL 赋值字符串’&#125;&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#&#123;9.3E3&#125;&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#&#123;beanName.str&#125;&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n"},{"title":"SpringCloud Hystrix参数配置","author":"郑天祺","date":"2020-12-14T03:33:00.000Z","_content":"\nHystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。\n\n# 1、方法一：注解参数注入\n\n```java\n    @RequestMapping(value = \"/helloHystrixA/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = \"testFallback\", // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = {// 针对单个方法的配置\n            @HystrixProperty(name = \"circuitBreaker.enabled\", value = \"true\"), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"50\"), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"10\"), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = \"circuitBreaker.sleepWindowInMilliseconds\", value = \"10000\"),// 开启熔断器后过10秒再尝试访问\n        })\n    public String helloHystirxA(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n\n```\n\n# 2、方法二：配置文件\n\n分两步，首先在代码里配置commandKey：\n\n```java\n@RequestMapping(value = \"/helloHystrixB/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(commandKey = \"testCommand\", // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = \"testFallback\"// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n    }\n\n```\n\n然后在application.yml里配置 commandKey = \"testCommand\" 对应的配置项：\n\n```java\nhystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n```\n\n注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。","source":"_posts/SpringCloud-Hystrix参数配置.md","raw":"title: SpringCloud Hystrix参数配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 11:33:00\n---\n\nHystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。\n\n# 1、方法一：注解参数注入\n\n```java\n    @RequestMapping(value = \"/helloHystrixA/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = \"testFallback\", // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = {// 针对单个方法的配置\n            @HystrixProperty(name = \"circuitBreaker.enabled\", value = \"true\"), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"50\"), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"10\"), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = \"circuitBreaker.sleepWindowInMilliseconds\", value = \"10000\"),// 开启熔断器后过10秒再尝试访问\n        })\n    public String helloHystirxA(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n\n```\n\n# 2、方法二：配置文件\n\n分两步，首先在代码里配置commandKey：\n\n```java\n@RequestMapping(value = \"/helloHystrixB/{string}\", method = RequestMethod.GET)\n    @HystrixCommand(commandKey = \"testCommand\", // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = \"testFallback\"// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) {\n        return \"Nacos服务发现：远端调用成功！ result=\"\n            + restTemplate.getForObject(\"http://nacos.provider.demo/hello/\" + string, String.class);\n    }\n\n```\n\n然后在application.yml里配置 commandKey = \"testCommand\" 对应的配置项：\n\n```java\nhystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n```\n\n注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。","slug":"SpringCloud-Hystrix参数配置","published":1,"updated":"2021-04-13T07:13:13.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkb003cl0t98jojf437","content":"<p>Hystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。</p>\n<h1>1、方法一：注解参数注入</h1>\n<pre><code class=\"language-java\">    @RequestMapping(value = &quot;/helloHystrixA/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = &quot;testFallback&quot;, // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = &#123;// 针对单个方法的配置\n            @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;50&quot;), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;),// 开启熔断器后过10秒再尝试访问\n        &#125;)\n    public String helloHystirxA(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n\n</code></pre>\n<h1>2、方法二：配置文件</h1>\n<p>分两步，首先在代码里配置commandKey：</p>\n<pre><code class=\"language-java\">@RequestMapping(value = &quot;/helloHystrixB/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(commandKey = &quot;testCommand&quot;, // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = &quot;testFallback&quot;// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n    &#125;\n\n</code></pre>\n<p>然后在application.yml里配置 commandKey = “testCommand” 对应的配置项：</p>\n<pre><code class=\"language-java\">hystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n</code></pre>\n<p>注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hystrix修改默认配置有两种方式，注解参数注入，和application.yml配置文件配置。</p>\n<h1>1、方法一：注解参数注入</h1>\n<pre><code class=\"language-java\">    @RequestMapping(value = &quot;/helloHystrixA/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(fallbackMethod = &quot;testFallback&quot;, // 请求失败降级回调方法，值为方法名，不需要括号\n        commandProperties = &#123;// 针对单个方法的配置\n            @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;), // 开启熔断器，可不加默认为true\n            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;50&quot;), // 请求错误超过50%，开启熔断器\n            @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), // 一个周期(十秒)内超过10个请求才进行进行容错率判断\n            @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;),// 开启熔断器后过10秒再尝试访问\n        &#125;)\n    public String helloHystirxA(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n\n</code></pre>\n<h1>2、方法二：配置文件</h1>\n<p>分两步，首先在代码里配置commandKey：</p>\n<pre><code class=\"language-java\">@RequestMapping(value = &quot;/helloHystrixB/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    @HystrixCommand(commandKey = &quot;testCommand&quot;, // 为修饰的方法定义一个 commandKey，不设置默认取方法名为commandKey\n        fallbackMethod = &quot;testFallback&quot;// 请求失败降级回调方法，值为方法名，不需要括号\n    )\n    public String helloHystirxB(@PathVariable String string) &#123;\n        return &quot;Nacos服务发现：远端调用成功！ result=&quot;\n            + restTemplate.getForObject(&quot;http://nacos.provider.demo/hello/&quot; + string, String.class);\n    &#125;\n\n</code></pre>\n<p>然后在application.yml里配置 commandKey = “testCommand” 对应的配置项：</p>\n<pre><code class=\"language-java\">hystrix:\n    command:\n        testCommand: #commandKey，配置作用于指定的commandKey\n            # ============  常用的熔断器配置  =============\n            circuitBreaker:\n                enabled: true #默认为true，可不用配置\n                errorThresholdPercentage: 50 #一个监测周期（默认10s），请求失败率超过50%开启熔断器\n                requestVolumeThreshold: 10 #一个监测周期内，超过10个请求才进行进行容错率判断\n                sleepWindowInMilliseconds: 10000 #开启熔断器后过10s再尝试访问，默认5s\n            metrics:\n                rollingStats:\n                    timeInMilliseconds: 10000 #监测周期时长（单位 ms）,默认10000，即10秒\n                    numBuckets: 10 #监测周期切分为10个buckets\n                         #结合上面的参数就是10秒监测周期 分为10个buckets，每个buckets 1秒；每1秒进行1次监测计算\n                         #注意 timeInMilliseconds % numBuckets 必须为0 否则会触发异常\n            #============  常用的资源隔离配置 ============\n            execution:\n                isolation:\n                    strategy: THREAD # THREAD：线程隔离， SEMAPHORE：信号量隔离；默认线程隔离\n                    thread:\n                        timeoutInMilliseconds: 400 #占用线程调用接口的超时时间\n                        interruptOnTimeout: true #占用线程超时 是否中断线程的执行\n                    timeout:\n                        enabled: true #开启超时限制\n                    semaphore:\n                        maxConcurrentRequests: 20 #信号量隔离下才有效，最大的信号量值，可以理解为 最大支持的并发数\n            fallback:\n                isolation:\n                    semaphore:\n                        maxConcurrentRequests: 20 #降级回调方法允许的最大调用\n\n</code></pre>\n<p>注：如果将application.yml中的commandKey设置为default，则会作为全局默认配置，覆盖Hystrix自身的默认配置。</p>\n"},{"title":"SpringCloud Ribbon参数配置","author":"郑天祺","date":"2020-12-14T03:33:00.000Z","_content":"\n# Ribbon策略类型\n\n![image-20201214123508379](/img/image-20201214123508379.png)\n\nRibbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。\n\n# 方法一：创建配置类\n\n```java\n @Configuration  \npublic class MyRibbonConfig {\n    @Bean\n    public IRule ribbonRule() {\n        //随机策略\n        return new RandomRule();\n    }\n}\n然后在启动类上加注解：\n@RibbonClient(name = \"nacos.provider.demo\", configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n```\n\n# 方法二：配置文件\n\n无须任何配置类和代码，只需要在在application.yml中添加配置：\n\n```java\n#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n```\n\n","source":"_posts/SpringCloud-Ribbon参数配置.md","raw":"title: SpringCloud Ribbon参数配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 11:33:00\n---\n\n# Ribbon策略类型\n\n![image-20201214123508379](/img/image-20201214123508379.png)\n\nRibbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。\n\n# 方法一：创建配置类\n\n```java\n @Configuration  \npublic class MyRibbonConfig {\n    @Bean\n    public IRule ribbonRule() {\n        //随机策略\n        return new RandomRule();\n    }\n}\n然后在启动类上加注解：\n@RibbonClient(name = \"nacos.provider.demo\", configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n```\n\n# 方法二：配置文件\n\n无须任何配置类和代码，只需要在在application.yml中添加配置：\n\n```java\n#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n```\n\n","slug":"SpringCloud-Ribbon参数配置","published":1,"updated":"2021-04-13T07:13:31.814Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkd003gl0t94h26hgdk","content":"<h1>Ribbon策略类型</h1>\n<p><img src=\"/img/image-20201214123508379.png\" alt=\"image-20201214123508379\"></p>\n<p>Ribbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。</p>\n<h1>方法一：创建配置类</h1>\n<pre><code class=\"language-java\"> @Configuration  \npublic class MyRibbonConfig &#123;\n    @Bean\n    public IRule ribbonRule() &#123;\n        //随机策略\n        return new RandomRule();\n    &#125;\n&#125;\n然后在启动类上加注解：\n@RibbonClient(name = &quot;nacos.provider.demo&quot;, configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n</code></pre>\n<h1>方法二：配置文件</h1>\n<p>无须任何配置类和代码，只需要在在application.yml中添加配置：</p>\n<pre><code class=\"language-java\">#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>Ribbon策略类型</h1>\n<p><img src=\"/img/image-20201214123508379.png\" alt=\"image-20201214123508379\"></p>\n<p>Ribbon负载均衡策略为轮询，如果要修改默认策略 ，有两种方法，分别是创建配置类，和配置application.yml。</p>\n<h1>方法一：创建配置类</h1>\n<pre><code class=\"language-java\"> @Configuration  \npublic class MyRibbonConfig &#123;\n    @Bean\n    public IRule ribbonRule() &#123;\n        //随机策略\n        return new RandomRule();\n    &#125;\n&#125;\n然后在启动类上加注解：\n@RibbonClient(name = &quot;nacos.provider.demo&quot;, configuration = MyRibbonConfig.class)  //name为服务提供者名称\n\n</code></pre>\n<h1>方法二：配置文件</h1>\n<p>无须任何配置类和代码，只需要在在application.yml中添加配置：</p>\n<pre><code class=\"language-java\">#针对单个服务配置路由规则，注意 配置的值 需要类全名（包名+类名）；\nnacos.config.demo: #目标服务提供名称\n    ribbon:\n        ConnectionTimeout: 400 #链接超时\n        ReadTimeout: 400 #读取超时\n        MaxAutoRetries: 1 #重试当前实例的次数\n        MaxAutoRetriesNextServer: 1 #服务实例切换重试次数\n        ServerListRefreshInterval: 30000 #刷新所服务列表间隔时间\n        NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule #配置对应的规则，其他ribbon自带的规则 可查看IRule接口的实现类\n</code></pre>\n"},{"title":"SpringCloud client配置","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config -->\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n```\n\n# 2、配置文件\n\n创建配置文件  bootstrap.properties ，并添加基础配置信息：\n\n```java\nbootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n```\n\n# 3、加载配置\n\n使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。 \n\n示例代码\n\n```java\n@RestController\n@RefreshScope\npublic class HelloController {\n \n    @Value(\"${user.code:默认值}\")\n    private String userCode;\n \n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello, this is a nacos-config-demo. userCode=\" + userCode;\n    }\n}\n```\n\n","source":"_posts/SpringCloud-client配置.md","raw":"title: SpringCloud client配置\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:32:00\n---\n\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config -->\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>\n</dependency>\n```\n\n# 2、配置文件\n\n创建配置文件  bootstrap.properties ，并添加基础配置信息：\n\n```java\nbootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n```\n\n# 3、加载配置\n\n使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。 \n\n示例代码\n\n```java\n@RestController\n@RefreshScope\npublic class HelloController {\n \n    @Value(\"${user.code:默认值}\")\n    private String userCode;\n \n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello, this is a nacos-config-demo. userCode=\" + userCode;\n    }\n}\n```\n\n","slug":"SpringCloud-client配置","published":1,"updated":"2021-04-13T07:13:48.225Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkd003jl0t94djea7cn","content":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config --&gt;\n&lt;dependency&gt;\n\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置文件</h1>\n<p>创建配置文件  bootstrap.properties ，并添加基础配置信息：</p>\n<pre><code class=\"language-java\">bootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n</code></pre>\n<h1>3、加载配置</h1>\n<p>使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。</p>\n<p>示例代码</p>\n<pre><code class=\"language-java\">@RestController\n@RefreshScope\npublic class HelloController &#123;\n \n    @Value(&quot;$&#123;user.code:默认值&#125;&quot;)\n    private String userCode;\n \n    @RequestMapping(&quot;/hello&quot;)\n    public String hello() &#123;\n        return &quot;Hello, this is a nacos-config-demo. userCode=&quot; + userCode;\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-config --&gt;\n&lt;dependency&gt;\n\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置文件</h1>\n<p>创建配置文件  bootstrap.properties ，并添加基础配置信息：</p>\n<pre><code class=\"language-java\">bootstrap.properties \n#配置中心地址\nspring.cloud.nacos.config.server-addr=nacos.xt.com\n#配置项 dataId 前缀\nspring.cloud.nacos.config.prefix=supervision.web-platform.kafka.consumer\n#配置格式，建议使用properties或yaml，同时为dataId的后缀\nspring.cloud.nacos.config.file-extension=properties\n#配置组id\nspring.cloud.nacos.config.group=supervision:web-platform\n</code></pre>\n<h1>3、加载配置</h1>\n<p>使用 Spring 的注解  @Value  设置属性值，使用 Spring Cloud 原生注解  @RefreshScope  实现配置自动更新。</p>\n<p>示例代码</p>\n<pre><code class=\"language-java\">@RestController\n@RefreshScope\npublic class HelloController &#123;\n \n    @Value(&quot;$&#123;user.code:默认值&#125;&quot;)\n    private String userCode;\n \n    @RequestMapping(&quot;/hello&quot;)\n    public String hello() &#123;\n        return &quot;Hello, this is a nacos-config-demo. userCode=&quot; + userCode;\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"SpringCloud使用Feign+Ribbon+Hystrix","author":"郑天祺","date":"2020-12-14T04:39:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、增加FeignClient\n\n```java\n@FeignClient(name = \"discovery-provider\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n\n    @LoadBalanced\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 4、修改默认的负载均衡规则\n\n```java\n@Configuration\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    }\n}\n```\n\n# 5、增加对应方法的Hystrix配置\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<RemoteClient> {\n\n    @Override\n    public RemoteClient create(Throwable throwable) {\n       return (name)-> \"请求失败. name=\" + name;\n    }\n}\n```\n\n# 6、使用效果如下\n\n## 1）.启动两个服务端，端口分别为8080，8090\n\n![image-20201214125402694](/img/image-20201214125402694.png)\n\n## 2）.启动服务消费端，多次调用对应的服务\n\n![image-20201214125453643](/img/image-20201214125453643.png)\n\n![image-20201214125504552](/img/image-20201214125504552.png)\n\n由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。\n\n## 3）. 关掉两个服务端实例\n\n![image-20201214125529023](/img/image-20201214125529023.png)\n\n再次访问时返回了对应fallback中的返回值。","source":"_posts/SpringCloud使用Feign-Ribbon-Hystrix.md","raw":"title: SpringCloud使用Feign+Ribbon+Hystrix\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 12:39:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、增加FeignClient\n\n```java\n@FeignClient(name = \"discovery-provider\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n\n    @LoadBalanced\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 4、修改默认的负载均衡规则\n\n```java\n@Configuration\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    }\n}\n```\n\n# 5、增加对应方法的Hystrix配置\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<RemoteClient> {\n\n    @Override\n    public RemoteClient create(Throwable throwable) {\n       return (name)-> \"请求失败. name=\" + name;\n    }\n}\n```\n\n# 6、使用效果如下\n\n## 1）.启动两个服务端，端口分别为8080，8090\n\n![image-20201214125402694](/img/image-20201214125402694.png)\n\n## 2）.启动服务消费端，多次调用对应的服务\n\n![image-20201214125453643](/img/image-20201214125453643.png)\n\n![image-20201214125504552](/img/image-20201214125504552.png)\n\n由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。\n\n## 3）. 关掉两个服务端实例\n\n![image-20201214125529023](/img/image-20201214125529023.png)\n\n再次访问时返回了对应fallback中的返回值。","slug":"SpringCloud使用Feign-Ribbon-Hystrix","published":1,"updated":"2021-04-13T07:10:11.594Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkf003nl0t96cwocymp","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、增加FeignClient</h1>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;discovery-provider&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>4、修改默认的负载均衡规则</h1>\n<pre><code class=\"language-java\">@Configuration\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>5、增加对应方法的Hystrix配置</h1>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;RemoteClient&gt; &#123;\n\n    @Override\n    public RemoteClient create(Throwable throwable) &#123;\n       return (name)-&gt; &quot;请求失败. name=&quot; + name;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、使用效果如下</h1>\n<h2 id=\"1）-启动两个服务端，端口分别为8080，8090\">1）.启动两个服务端，端口分别为8080，8090</h2>\n<p><img src=\"/img/image-20201214125402694.png\" alt=\"image-20201214125402694\"></p>\n<h2 id=\"2）-启动服务消费端，多次调用对应的服务\">2）.启动服务消费端，多次调用对应的服务</h2>\n<p><img src=\"/img/image-20201214125453643.png\" alt=\"image-20201214125453643\"></p>\n<p><img src=\"/img/image-20201214125504552.png\" alt=\"image-20201214125504552\"></p>\n<p>由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。</p>\n<h2 id=\"3）-关掉两个服务端实例\">3）. 关掉两个服务端实例</h2>\n<p><img src=\"/img/image-20201214125529023.png\" alt=\"image-20201214125529023\"></p>\n<p>再次访问时返回了对应fallback中的返回值。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: server-consumer-feign-hystrix #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n  main:\n    allow-bean-definition-overriding: true\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、增加FeignClient</h1>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;discovery-provider&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>4、修改默认的负载均衡规则</h1>\n<pre><code class=\"language-java\">@Configuration\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n       //此处的RoundRobinRule()为轮询方式的负载均衡\n       return new RoundRobinRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>5、增加对应方法的Hystrix配置</h1>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;RemoteClient&gt; &#123;\n\n    @Override\n    public RemoteClient create(Throwable throwable) &#123;\n       return (name)-&gt; &quot;请求失败. name=&quot; + name;\n    &#125;\n&#125;\n</code></pre>\n<h1>6、使用效果如下</h1>\n<h2 id=\"1）-启动两个服务端，端口分别为8080，8090\">1）.启动两个服务端，端口分别为8080，8090</h2>\n<p><img src=\"/img/image-20201214125402694.png\" alt=\"image-20201214125402694\"></p>\n<h2 id=\"2）-启动服务消费端，多次调用对应的服务\">2）.启动服务消费端，多次调用对应的服务</h2>\n<p><img src=\"/img/image-20201214125453643.png\" alt=\"image-20201214125453643\"></p>\n<p><img src=\"/img/image-20201214125504552.png\" alt=\"image-20201214125504552\"></p>\n<p>由上方两张图可见，在每次调用时，均会路由到不同端口的实例上。</p>\n<h2 id=\"3）-关掉两个服务端实例\">3）. 关掉两个服务端实例</h2>\n<p><img src=\"/img/image-20201214125529023.png\" alt=\"image-20201214125529023\"></p>\n<p>再次访问时返回了对应fallback中的返回值。</p>\n"},{"title":"SpringCloud使用Feign+Ribbon","author":"郑天祺","date":"2020-12-14T04:39:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、远程调用\n\n基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡\n\n```java\n/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate(){\n    return new RestTemplate();\n}\n```\n\n# 4、定义Feign接口\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 5、可选的负载均衡策略\n\n```java\n//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n        return new BestAvailableRule();\n    }\n}\n```\n\n# 6、主类增加相关注解\n\n```java\n/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = {@RibbonClient(name = \"discovery-provider\",configuration = AppointRibbonMetric.class)})\n@ComponentScan(excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = {AvoidScan.class})})\npublic class ConsumerRibbonApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 7、测试调用\n\n多次点击调用成功，并路由到不同端口的实例上。\n\n![image-20201214125020584](/img/image-20201214125020584.png)\n\n![image-20201214125032862](/img/image-20201214125032862.png)","source":"_posts/SpringCloud使用Feign-Ribbon.md","raw":"title: SpringCloud使用Feign+Ribbon\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:39:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n```\n\n# 2、增加相关配置\n\n```\nspring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n```\n\n# 3、远程调用\n\n基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡\n\n```java\n/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate(){\n    return new RestTemplate();\n}\n```\n\n# 4、定义Feign接口\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n}\n```\n\n# 5、可选的负载均衡策略\n\n```java\n//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric {\n    @Bean\n    public IRule ribbonRule(){\n        return new BestAvailableRule();\n    }\n}\n```\n\n# 6、主类增加相关注解\n\n```java\n/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = {@RibbonClient(name = \"discovery-provider\",configuration = AppointRibbonMetric.class)})\n@ComponentScan(excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = {AvoidScan.class})})\npublic class ConsumerRibbonApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 7、测试调用\n\n多次点击调用成功，并路由到不同端口的实例上。\n\n![image-20201214125020584](/img/image-20201214125020584.png)\n\n![image-20201214125032862](/img/image-20201214125032862.png)","slug":"SpringCloud使用Feign-Ribbon","published":1,"updated":"2021-04-13T07:09:54.331Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkg003ql0t9h31x6c1u","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、远程调用</h1>\n<p>基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡</p>\n<pre><code class=\"language-java\">/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate()&#123;\n    return new RestTemplate();\n&#125;\n</code></pre>\n<h1>4、定义Feign接口</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>5、可选的负载均衡策略</h1>\n<pre><code class=\"language-java\">//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n        return new BestAvailableRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>6、主类增加相关注解</h1>\n<pre><code class=\"language-java\">/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = &#123;@RibbonClient(name = &quot;discovery-provider&quot;,configuration = AppointRibbonMetric.class)&#125;)\n@ComponentScan(excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = &#123;AvoidScan.class&#125;)&#125;)\npublic class ConsumerRibbonApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>7、测试调用</h1>\n<p>多次点击调用成功，并路由到不同端口的实例上。</p>\n<p><img src=\"/img/image-20201214125020584.png\" alt=\"image-20201214125020584\"></p>\n<p><img src=\"/img/image-20201214125032862.png\" alt=\"image-20201214125032862\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、增加相关配置</h1>\n<pre><code>spring:\n  application:\n    name: discovery-consumer-feign-ribbon #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer  #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8086\n</code></pre>\n<h1>3、远程调用</h1>\n<p>基础使用方式为给RestTemplate增加@LoadBlanced注解实现负载均衡</p>\n<pre><code class=\"language-java\">/**\n * 最简单的ribbon负载均衡实现，定义一个RestTemplate的Bean，\n * 添加@LoadBalanced注解，在调用时注入该Bean即可实现客户端负载均衡\n * */\n@Bean\n@LoadBalanced\npublic RestTemplate restTemplate()&#123;\n    return new RestTemplate();\n&#125;\n</code></pre>\n<h1>4、定义Feign接口</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n&#125;\n</code></pre>\n<h1>5、可选的负载均衡策略</h1>\n<pre><code class=\"language-java\">//创建一个配置类\n//自定义了一个标记注解，@AvoidScan避免该配置类成为全局的负载均衡策略。\n@Configuration\n@AvoidScan\npublic class AppointRibbonMetric &#123;\n    @Bean\n    public IRule ribbonRule()&#123;\n        return new BestAvailableRule();\n    &#125;\n&#125;\n</code></pre>\n<h1>6、主类增加相关注解</h1>\n<pre><code class=\"language-java\">/**\n * 使用@RibbonClient可以对具体的服务调用指定特定的负载均衡策略。\n * 此处@ComponentScan中的属性配置用于在spring进行扫描时，不将@AvoidScan注解修饰的策略设为全局默认策略\n * 可以在@RibbonClients配置多个@RibbonClient\n */\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients\n@RibbonClients(value = &#123;@RibbonClient(name = &quot;discovery-provider&quot;,configuration = AppointRibbonMetric.class)&#125;)\n@ComponentScan(excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION\n,value = &#123;AvoidScan.class&#125;)&#125;)\npublic class ConsumerRibbonApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerRibbonApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>7、测试调用</h1>\n<p>多次点击调用成功，并路由到不同端口的实例上。</p>\n<p><img src=\"/img/image-20201214125020584.png\" alt=\"image-20201214125020584\"></p>\n<p><img src=\"/img/image-20201214125032862.png\" alt=\"image-20201214125032862\"></p>\n"},{"title":"SpringCloud使用Feign","author":"郑天祺","date":"2020-12-14T04:38:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、添加配置\n\n此处主要为nacos服务端地址配置\n\n```java\nspring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n```\n\n# 3、主类添加相关注解\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、定义FeignClient,用与服务调用\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n\n}\n```\n\n# 5.注入4中的FeignClient\n\n调用其hello方法 即可调用远程服务。\n\n![image-20201214124603810](/img/image-20201214124603810.png)","source":"_posts/SpringCloud使用Feign.md","raw":"title: SpringCloud使用Feign\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:38:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、添加配置\n\n此处主要为nacos服务端地址配置\n\n```java\nspring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \"*\"  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n```\n\n# 3、主类添加相关注解\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、定义FeignClient,用与服务调用\n\n```java\n//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = \"discovery-provider\")\npublic interface RemoteClient {\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = \"/echo/{name}\")\n    String hello(@PathVariable(\"name\") String name);\n\n}\n```\n\n# 5.注入4中的FeignClient\n\n调用其hello方法 即可调用远程服务。\n\n![image-20201214124603810](/img/image-20201214124603810.png)","slug":"SpringCloud使用Feign","published":1,"updated":"2021-04-13T07:10:19.992Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkh003ul0t9g2kw0bb5","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置</h1>\n<p>此处主要为nacos服务端地址配置</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n</code></pre>\n<h1>3、主类添加相关注解</h1>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、定义FeignClient,用与服务调用</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n\n&#125;\n</code></pre>\n<h1>5.注入4中的FeignClient</h1>\n<p>调用其hello方法 即可调用远程服务。</p>\n<p><img src=\"/img/image-20201214124603810.png\" alt=\"image-20201214124603810\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置</h1>\n<p>此处主要为nacos服务端地址配置</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: discovery-consumer-feign #修改此处为您的应用程序名称\n    group: test #部门\n    developer: developer #&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.goo.com    #Nacos服务地址\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: &quot;*&quot;  #打开所有端点，默认是info,health\n  endpoint:\n    health:\n      show-details: always #显示health的明细内容，默认是never\nserver:\n  port: 8081\n\n</code></pre>\n<h1>3、主类添加相关注解</h1>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\n@EnableFeignClients    //启用feign调用，该注解会扫描@FeignClient注解\npublic class ConsumerFeignApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(ConsumerFeignApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、定义FeignClient,用与服务调用</h1>\n<pre><code class=\"language-java\">//定义接口，增加FeignClient注解，在注解中使用name属性指定调用的具体服务名\n@FeignClient(name = &quot;discovery-provider&quot;)\npublic interface RemoteClient &#123;\n    \n    //此处的请求方式同服务端提供的访问方式相同\n    @GetMapping(value = &quot;/echo/&#123;name&#125;&quot;)\n    String hello(@PathVariable(&quot;name&quot;) String name);\n\n&#125;\n</code></pre>\n<h1>5.注入4中的FeignClient</h1>\n<p>调用其hello方法 即可调用远程服务。</p>\n<p><img src=\"/img/image-20201214124603810.png\" alt=\"image-20201214124603810\"></p>\n"},{"title":"SpringCloud使用RestTemplate","author":"郑天祺","date":"2020-12-14T04:37:00.000Z","_content":"\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、配置\n\n```java\nspring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n```\n\n3、使用RestTemplate进行服务调用\n\n```java\n@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = \"/hello/{name}\", method = RequestMethod.GET)\npublic String echo(@PathVariable String name) {\n    try {\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(\"server-provider\", true);\n        String sendUrl = \"http://\" + instance.getIp() + \":\" + instance.getPort() + \"/echo/\" + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    } catch (NacosException e) {\n        e.printStackTrace();\n    }\n    return null;\n}\n\n```\n\n# 4、调用\n\n调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法\n\n![image-20201214124300675](/img/image-20201214124300675.png)","source":"_posts/SpringCloud使用RestTemplate.md","raw":"title: SpringCloud使用RestTemplate\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\n  - ''\ndate: 2020-12-14 12:37:00\n---\n\n# 1、引入依赖\n\n```java\n<dependency>\n    <groupId>com.alibaba.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n# 2、配置\n\n```java\nspring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#<负责人姓名>\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n```\n\n3、使用RestTemplate进行服务调用\n\n```java\n@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = \"/hello/{name}\", method = RequestMethod.GET)\npublic String echo(@PathVariable String name) {\n    try {\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(\"server-provider\", true);\n        String sendUrl = \"http://\" + instance.getIp() + \":\" + instance.getPort() + \"/echo/\" + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    } catch (NacosException e) {\n        e.printStackTrace();\n    }\n    return null;\n}\n\n```\n\n# 4、调用\n\n调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法\n\n![image-20201214124300675](/img/image-20201214124300675.png)","slug":"SpringCloud使用RestTemplate","published":1,"updated":"2021-04-13T07:10:27.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpki003xl0t9asf35drm","content":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置</h1>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n</code></pre>\n<p>3、使用RestTemplate进行服务调用</p>\n<pre><code class=\"language-java\">@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = &quot;/hello/&#123;name&#125;&quot;, method = RequestMethod.GET)\npublic String echo(@PathVariable String name) &#123;\n    try &#123;\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(&quot;server-provider&quot;, true);\n        String sendUrl = &quot;http://&quot; + instance.getIp() + &quot;:&quot; + instance.getPort() + &quot;/echo/&quot; + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    &#125; catch (NacosException e) &#123;\n        e.printStackTrace();\n    &#125;\n    return null;\n&#125;\n\n</code></pre>\n<h1>4、调用</h1>\n<p>调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法</p>\n<p><img src=\"/img/image-20201214124300675.png\" alt=\"image-20201214124300675\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、引入依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、配置</h1>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: server-discovery-consumer #修改此处为您的应用程序名称\n    group: test #部门\n    developer:  developer#&lt;负责人姓名&gt;\n  cloud:\n    nacos:\n      discovery:\n        server-addr: nacos.com    #Nacos服务地址\n\nserver:\n  port: 8081\n\n</code></pre>\n<p>3、使用RestTemplate进行服务调用</p>\n<pre><code class=\"language-java\">@Autowired\nprivate NamingService namingService;\n\n@Autowired\nprivate RestTemplate restTemplate;\n \n/**\n * 在不使用feign组件时，使用nacos的NamingService配合RestTemplate的实现服务的发现及调用\n */\n@RequestMapping(value = &quot;/hello/&#123;name&#125;&quot;, method = RequestMethod.GET)\npublic String echo(@PathVariable String name) &#123;\n    try &#123;\n        //获取服务实例列表 参数分别为实例名、是否为健康实例\n        Instance instance = namingService.selectOneHealthyInstance(&quot;server-provider&quot;, true);\n        String sendUrl = &quot;http://&quot; + instance.getIp() + &quot;:&quot; + instance.getPort() + &quot;/echo/&quot; + name;\n        String result = restTemplate.getForObject(sendUrl, String.class);\n        //打印log\n        return result;\n    &#125; catch (NacosException e) &#123;\n        e.printStackTrace();\n    &#125;\n    return null;\n&#125;\n\n</code></pre>\n<h1>4、调用</h1>\n<p>调用3中的echo方法即可远程调用 discovery-provider 服务的echo方法</p>\n<p><img src=\"/img/image-20201214124300675.png\" alt=\"image-20201214124300675\"></p>\n"},{"title":"SpringCloud健康检查","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n# 服务健康检查：\n\n基于Spring Cloud体系，可以使用spring cloud  actuator组件\n\n# 1、POM依赖\n\n```java\n<dependency>\n     <groupId>org.springframework.boot</groupId>\n     <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n# 2、Application.yml配置\n\n```java\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n```\n\n# 3、访问路径： \n\nhttp://服务地址/actuator/health\n\n\n\n# 4、响应报文内容\n\n```java\n{\n    \"status\": \"UP\", \n    \"details\": {\n        \"diskSpace\": {\n            \"status\": \"UP\", \n            \"details\": {\n                \"total\": 499963174912, \n                \"free\": 200715714560, \n                \"threshold\": 10485760\n            }\n        }\n    }\n}\n```\n\n其中status状态含义如下\n\n![image-20201214114013294](/img/image-20201214114013294.png)\n\n\n\n# 5、服务自检主动通知\n\n```java\n{\n\t\"status\": \"当前状态（以整数形式字符串表示）\"，\n\t\"msg\": \"说明信息\"，\n}\n```\n\nstatus状态值描述：\n\n0：服务正常\n\n1：处理能力紧张，需要扩容\n\n2：服务内部错误 ，需要重启或版本回滚\n\n3：服务获取依赖资源失败，需要人工干预","source":"_posts/SpringCloud健康检查.md","raw":"title: SpringCloud健康检查\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n# 服务健康检查：\n\n基于Spring Cloud体系，可以使用spring cloud  actuator组件\n\n# 1、POM依赖\n\n```java\n<dependency>\n     <groupId>org.springframework.boot</groupId>\n     <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n# 2、Application.yml配置\n\n```java\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n```\n\n# 3、访问路径： \n\nhttp://服务地址/actuator/health\n\n\n\n# 4、响应报文内容\n\n```java\n{\n    \"status\": \"UP\", \n    \"details\": {\n        \"diskSpace\": {\n            \"status\": \"UP\", \n            \"details\": {\n                \"total\": 499963174912, \n                \"free\": 200715714560, \n                \"threshold\": 10485760\n            }\n        }\n    }\n}\n```\n\n其中status状态含义如下\n\n![image-20201214114013294](/img/image-20201214114013294.png)\n\n\n\n# 5、服务自检主动通知\n\n```java\n{\n\t\"status\": \"当前状态（以整数形式字符串表示）\"，\n\t\"msg\": \"说明信息\"，\n}\n```\n\nstatus状态值描述：\n\n0：服务正常\n\n1：处理能力紧张，需要扩容\n\n2：服务内部错误 ，需要重启或版本回滚\n\n3：服务获取依赖资源失败，需要人工干预","slug":"SpringCloud健康检查","published":1,"updated":"2021-04-13T07:10:58.521Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkj0041l0t92h1i2n2z","content":"<h1>服务健康检查：</h1>\n<p>基于Spring Cloud体系，可以使用spring cloud  actuator组件</p>\n<h1>1、POM依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n     &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、Application.yml配置</h1>\n<pre><code class=\"language-java\">management:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n</code></pre>\n<h1>3、访问路径：</h1>\n<p><a href=\"http://xn--zfry9hnb732h/actuator/health\">http://服务地址/actuator/health</a></p>\n<h1>4、响应报文内容</h1>\n<pre><code class=\"language-java\">&#123;\n    &quot;status&quot;: &quot;UP&quot;, \n    &quot;details&quot;: &#123;\n        &quot;diskSpace&quot;: &#123;\n            &quot;status&quot;: &quot;UP&quot;, \n            &quot;details&quot;: &#123;\n                &quot;total&quot;: 499963174912, \n                &quot;free&quot;: 200715714560, \n                &quot;threshold&quot;: 10485760\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<p>其中status状态含义如下</p>\n<p><img src=\"/img/image-20201214114013294.png\" alt=\"image-20201214114013294\"></p>\n<h1>5、服务自检主动通知</h1>\n<pre><code class=\"language-java\">&#123;\n\t&quot;status&quot;: &quot;当前状态（以整数形式字符串表示）&quot;，\n\t&quot;msg&quot;: &quot;说明信息&quot;，\n&#125;\n</code></pre>\n<p>status状态值描述：</p>\n<p>0：服务正常</p>\n<p>1：处理能力紧张，需要扩容</p>\n<p>2：服务内部错误 ，需要重启或版本回滚</p>\n<p>3：服务获取依赖资源失败，需要人工干预</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>服务健康检查：</h1>\n<p>基于Spring Cloud体系，可以使用spring cloud  actuator组件</p>\n<h1>1、POM依赖</h1>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n     &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h1>2、Application.yml配置</h1>\n<pre><code class=\"language-java\">management:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n\n</code></pre>\n<h1>3、访问路径：</h1>\n<p><a href=\"http://xn--zfry9hnb732h/actuator/health\">http://服务地址/actuator/health</a></p>\n<h1>4、响应报文内容</h1>\n<pre><code class=\"language-java\">&#123;\n    &quot;status&quot;: &quot;UP&quot;, \n    &quot;details&quot;: &#123;\n        &quot;diskSpace&quot;: &#123;\n            &quot;status&quot;: &quot;UP&quot;, \n            &quot;details&quot;: &#123;\n                &quot;total&quot;: 499963174912, \n                &quot;free&quot;: 200715714560, \n                &quot;threshold&quot;: 10485760\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<p>其中status状态含义如下</p>\n<p><img src=\"/img/image-20201214114013294.png\" alt=\"image-20201214114013294\"></p>\n<h1>5、服务自检主动通知</h1>\n<pre><code class=\"language-java\">&#123;\n\t&quot;status&quot;: &quot;当前状态（以整数形式字符串表示）&quot;，\n\t&quot;msg&quot;: &quot;说明信息&quot;，\n&#125;\n</code></pre>\n<p>status状态值描述：</p>\n<p>0：服务正常</p>\n<p>1：处理能力紧张，需要扩容</p>\n<p>2：服务内部错误 ，需要重启或版本回滚</p>\n<p>3：服务获取依赖资源失败，需要人工干预</p>\n"},{"title":"SpringCloud服务构建","author":"郑天祺","date":"2020-12-14T03:30:00.000Z","_content":"\n# 1、添加依赖\n\n建议使用官方提供的在线地址进行工程的初始化创建：https://start.spring.io\n\n添加Spring Cloud和Spring Cloud Alibaba依赖管理：\n\t\n\n```java\n<properties>\n\t\t<java.version>1.8</java.version>\n\t\t<spring-cloud.version>Greenwich.SR3</spring-cloud.version>\n\t\t<spring-cloud-alibaba.version>0.9.0.RELEASE</spring-cloud-alibaba.version>\n</properties>\n\t<dependencyManagement>\n\t\t<dependencies>\n\t\t\t<!--Spring Cloud -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t\t<!--Spring Cloud Alibaba -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-alibaba-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud-alibaba.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t</dependencies>\n\t</dependencyManagement>\n```\n\n注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。\n\n# 2、application.yml配置\n\n注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。\n\n```java\n# 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #<负责人姓名>\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n```\n\n","source":"_posts/SpringCloud服务构建.md","raw":"title: SpringCloud服务构建\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:30:00\n---\n\n# 1、添加依赖\n\n建议使用官方提供的在线地址进行工程的初始化创建：https://start.spring.io\n\n添加Spring Cloud和Spring Cloud Alibaba依赖管理：\n\t\n\n```java\n<properties>\n\t\t<java.version>1.8</java.version>\n\t\t<spring-cloud.version>Greenwich.SR3</spring-cloud.version>\n\t\t<spring-cloud-alibaba.version>0.9.0.RELEASE</spring-cloud-alibaba.version>\n</properties>\n\t<dependencyManagement>\n\t\t<dependencies>\n\t\t\t<!--Spring Cloud -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t\t<!--Spring Cloud Alibaba -->\n\t\t\t<dependency>\n\t\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t\t<artifactId>spring-cloud-alibaba-dependencies</artifactId>\n\t\t\t\t<version>${spring-cloud-alibaba.version}</version>\n\t\t\t\t<type>pom</type>\n\t\t\t\t<scope>import</scope>\n\t\t\t</dependency>\n\t\t</dependencies>\n\t</dependencyManagement>\n```\n\n注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。\n\n# 2、application.yml配置\n\n注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。\n\n```java\n# 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #<负责人姓名>\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: \"*\"  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n```\n\n","slug":"SpringCloud服务构建","published":1,"updated":"2021-04-13T07:10:49.104Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkk0044l0t9famt8rri","content":"<h1>1、添加依赖</h1>\n<p>建议使用官方提供的在线地址进行工程的初始化创建：<a href=\"https://start.spring.io\">https://start.spring.io</a></p>\n<p>添加Spring Cloud和Spring Cloud Alibaba依赖管理：</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;spring-cloud.version&gt;Greenwich.SR3&lt;/spring-cloud.version&gt;\n\t\t&lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n&lt;/properties&gt;\n\t&lt;dependencyManagement&gt;\n\t\t&lt;dependencies&gt;\n\t\t\t&lt;!--Spring Cloud --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t\t&lt;!--Spring Cloud Alibaba --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t&lt;/dependencies&gt;\n\t&lt;/dependencyManagement&gt;\n</code></pre>\n<p>注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。</p>\n<h1>2、application.yml配置</h1>\n<p>注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。</p>\n<pre><code class=\"language-java\"># 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #&lt;负责人姓名&gt;\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、添加依赖</h1>\n<p>建议使用官方提供的在线地址进行工程的初始化创建：<a href=\"https://start.spring.io\">https://start.spring.io</a></p>\n<p>添加Spring Cloud和Spring Cloud Alibaba依赖管理：</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;spring-cloud.version&gt;Greenwich.SR3&lt;/spring-cloud.version&gt;\n\t\t&lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n&lt;/properties&gt;\n\t&lt;dependencyManagement&gt;\n\t\t&lt;dependencies&gt;\n\t\t\t&lt;!--Spring Cloud --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t\t&lt;!--Spring Cloud Alibaba --&gt;\n\t\t\t&lt;dependency&gt;\n\t\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt;\n\t\t\t\t&lt;type&gt;pom&lt;/type&gt;\n\t\t\t\t&lt;scope&gt;import&lt;/scope&gt;\n\t\t\t&lt;/dependency&gt;\n\t\t&lt;/dependencies&gt;\n\t&lt;/dependencyManagement&gt;\n</code></pre>\n<p>注意：spring-cloud-alibaba的0.2.x.RELEASE及0.9.0.RELEASE对应于Spring Boot 2.x版本，不能使用0.1.x.RELEASE版本。</p>\n<h1>2、application.yml配置</h1>\n<p>注意：每一级退格必须为两个或四个空格，同一级节点需要左对齐，且不能使用制表符TAB。</p>\n<pre><code class=\"language-java\"># 服务端口配置（必填）\nserver:  \n    port: 8080\n# 应用基本信息配置（必填）\nspring:\n    application:\n        name: app #修改此处为您的应用程序名称\n        group: base #部门\n        developer:  developer #&lt;负责人姓名&gt;\n# 健康检查通用配置（必填）\nmanagement:\n    port: 8080  #actuator端口，保持与tomcat端口一致\n    endpoints:\n        web:\n          exposure:\n            include: &quot;*&quot;  #打开所有端点，默认是never\n    endpoint:\n        health:\n            show-details: always #显示health的明细内容，默认是never\n# 应用定制信息（选填），以info起始，后面的路径和内容能开发人员完全自定义，该信息可由actuator/info请求获取\ninfo:\n    interface:\n        list: \n            - hello\n            - actuator/health\n            - actuator/info\n    app:\n        desc: 这是一条描述信息\n</code></pre>\n"},{"title":"SpringCloud异常配置","date":"2020-12-14T03:34:00.000Z","_content":"\n---\ntitle: SpringCloud异常配置\nauthor: 郑天祺\ntags:\n\n  - SpringCloud\ncategories:\n  - SpringCloud\ndate: 2020-12-14 11:33:00\n\n\n1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。\n 说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。\n正例:if (obj != null) {...}\n反例:try { obj.method(); } catch (NullPointerException e) {...}\n\n2.【强制】异常不要用来做流程控制，条件控制。\n 说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。\n\n3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n 说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。\n\n4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n\n5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。\n\n6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。\n\n7.【强制】不要在 finally 块中使用 return。\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n\n8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。\n\n10. 【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。\n      反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。\n2)  数据库的查询结果可能为null。\n3)  集合里的元素即使isNotEmpty，取出的数据元素也可能为null。\n4)  远程调用返回对象时，一律要求进行空指针判断，防止NPE。\n5)  对于Session中获取的数据，建议NPE检查，避免空指针。\n6)  级联调用obj.getA().getB().getC();一连串调用，易产生NPE。\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。\n\n\n11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。\n\n12. 【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。\n\n13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 \n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {...}","source":"_posts/SpringCloud异常配置.md","raw":"title: SpringCloud异常配置\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:34:00\n---\n\n---\ntitle: SpringCloud异常配置\nauthor: 郑天祺\ntags:\n\n  - SpringCloud\ncategories:\n  - SpringCloud\ndate: 2020-12-14 11:33:00\n\n\n1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。\n 说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。\n正例:if (obj != null) {...}\n反例:try { obj.method(); } catch (NullPointerException e) {...}\n\n2.【强制】异常不要用来做流程控制，条件控制。\n 说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。\n\n3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n 说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。\n\n4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n\n5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。\n\n6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。\n\n7.【强制】不要在 finally 块中使用 return。\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n\n8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。\n\n10. 【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。\n      反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。\n2)  数据库的查询结果可能为null。\n3)  集合里的元素即使isNotEmpty，取出的数据元素也可能为null。\n4)  远程调用返回对象时，一律要求进行空指针判断，防止NPE。\n5)  对于Session中获取的数据，建议NPE检查，避免空指针。\n6)  级联调用obj.getA().getB().getC();一连串调用，易产生NPE。\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。\n\n\n11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。\n\n12. 【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。\n\n13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 \n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {...}","slug":"SpringCloud异常配置","published":1,"updated":"2021-04-13T07:10:38.522Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkl0048l0t9erry0mu1","content":"<hr>\n<p>title: SpringCloud异常配置<br>\nauthor: 郑天祺<br>\ntags:</p>\n<ul>\n<li>SpringCloud<br>\ncategories:</li>\n<li>SpringCloud<br>\ndate: 2020-12-14 11:33:00</li>\n</ul>\n<p>1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。<br>\n说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。<br>\n正例:if (obj != null) {…}<br>\n反例:try { obj.method(); } catch (NullPointerException e) {…}</p>\n<p>2.【强制】异常不要用来做流程控制，条件控制。<br>\n说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</p>\n<p>3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<p>说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。</p>\n<p>4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</p>\n<p>5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。</p>\n<p>6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。</p>\n<p>7.【强制】不要在 finally 块中使用 return。<br>\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</p>\n<p>8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<p>9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。<br>\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。</p>\n<ol start=\"10\">\n<li>【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:<br>\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。<br>\n反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。</li>\n</ol>\n<ol start=\"2\">\n<li>数据库的查询结果可能为null。</li>\n<li>集合里的元素即使isNotEmpty，取出的数据元素也可能为null。</li>\n<li>远程调用返回对象时，一律要求进行空指针判断，防止NPE。</li>\n<li>对于Session中获取的数据，建议NPE检查，避免空指针。</li>\n<li>级联调用obj.getA().getB().getC();一连串调用，易产生NPE。<br>\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。</li>\n</ol>\n<p>11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。</p>\n<ol start=\"12\">\n<li>【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。<br>\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:<br>\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。</li>\n</ol>\n<p>13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。<br>\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。<br>\n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {…}</p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<p>title: SpringCloud异常配置<br>\nauthor: 郑天祺<br>\ntags:</p>\n<ul>\n<li>SpringCloud<br>\ncategories:</li>\n<li>SpringCloud<br>\ndate: 2020-12-14 11:33:00</li>\n</ul>\n<p>1.【强制】Java 类库中定义的可以通过预检查方式规避的 RuntimeException 异常不应该通过catch 的方式来处理，比如:NullPointerException，IndexOutOfBoundsException 等等。<br>\n说明:无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过 catch NumberFormatException 来实现。<br>\n正例:if (obj != null) {…}<br>\n反例:try { obj.method(); } catch (NullPointerException e) {…}</p>\n<p>2.【强制】异常不要用来做流程控制，条件控制。<br>\n说明:异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</p>\n<p>3.【强制】catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。 对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<p>说明:对大段代码进行 try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利 于定位问题，这是一种不负责任的表现。正例:用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于 简单，在程序上作出分门别类的判断，并提示给用户。</p>\n<p>4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请 将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</p>\n<p>5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回 滚事务。</p>\n<p>6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明:如果 JDK7 及以上，可以使用 try-with-resources 方式。</p>\n<p>7.【强制】不要在 finally 块中使用 return。<br>\n说明:finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</p>\n<p>8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明:如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<p>9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分 说明什么情况下会返回 null 值。<br>\n说明:本手册明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回 null 的情况。</p>\n<ol start=\"10\">\n<li>【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景:<br>\n1)返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。<br>\n反例:public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。</li>\n</ol>\n<ol start=\"2\">\n<li>数据库的查询结果可能为null。</li>\n<li>集合里的元素即使isNotEmpty，取出的数据元素也可能为null。</li>\n<li>远程调用返回对象时，一律要求进行空指针判断，防止NPE。</li>\n<li>对于Session中获取的数据，建议NPE检查，避免空指针。</li>\n<li>级联调用obj.getA().getB().getC();一连串调用，易产生NPE。<br>\n正例:使用 JDK8 的 Optional 类来防止 NPE 问题。</li>\n</ol>\n<p>11.【推荐】定义时区分unchecked/checked 异常，避免直接抛出newRuntimeException()， 更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义 过的自定义异常，如:DAOException / ServiceException等。</p>\n<ol start=\"12\">\n<li>【参考】对于公司外的 http/api 开放接口必须使用“错误码”;而应用内部推荐异常抛出;跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess()方法、“错误码”、“错误简 短信息”。<br>\n说明:关于 RPC 方法返回方式使用 Result 方式的理由:<br>\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用 端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输 的性能损耗也是问题。</li>\n</ol>\n<p>13.【参考】避免出现重复的代码(Don’t Repeat Yourself)，即DRY原则。<br>\n说明:随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副 本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。<br>\n正例:一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取:private boolean checkParam(DTO dto) {…}</p>\n"},{"title":"SpringCloud服务注册","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery -->\n\t\t<dependency>\t\n            <groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n\n```\n\n# 2、配置文件添加注册服务器地址\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n```\n\n# 3、开启服务发现功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：\n\n```\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n\n}\n```\n\n# 4、配置服务接口\n\n```java\n@RestController\npublic class EchoController {\n\n    @RequestMapping(value = \"/echo/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return \"Hello Nacos Discovery \" + string;\n    }\n\n}\n```\n\n# 5、确认服务注册结果\n\n运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。\n\n![image-20201214122255720](/img/image-20201214122255720.png)","source":"_posts/SpringCloud服务注册.md","raw":"title: SpringCloud服务注册\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:32:00\n---\n\n# 1、pom.xml添加starter依赖\n\n```java\n<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery -->\n\t\t<dependency>\t\n            <groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n\n```\n\n# 2、配置文件添加注册服务器地址\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n```\n\n# 3、开启服务发现功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：\n\n```\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n\n}\n```\n\n# 4、配置服务接口\n\n```java\n@RestController\npublic class EchoController {\n\n    @RequestMapping(value = \"/echo/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return \"Hello Nacos Discovery \" + string;\n    }\n\n}\n```\n\n# 5、确认服务注册结果\n\n运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。\n\n![image-20201214122255720](/img/image-20201214122255720.png)","slug":"SpringCloud服务注册","published":1,"updated":"2021-04-13T07:11:28.615Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkm004bl0t9bofs3nx7","content":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery --&gt;\n\t\t&lt;dependency&gt;\t\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\n</code></pre>\n<h1>2、配置文件添加注册服务器地址</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code>#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n</code></pre>\n<h1>3、开启服务发现功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：</p>\n<pre><code>@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n\n&#125;\n</code></pre>\n<h1>4、配置服务接口</h1>\n<pre><code class=\"language-java\">@RestController\npublic class EchoController &#123;\n\n    @RequestMapping(value = &quot;/echo/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return &quot;Hello Nacos Discovery &quot; + string;\n    &#125;\n\n&#125;\n</code></pre>\n<h1>5、确认服务注册结果</h1>\n<p>运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。</p>\n<p><img src=\"/img/image-20201214122255720.png\" alt=\"image-20201214122255720\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-alibaba-nacos-discovery --&gt;\n\t\t&lt;dependency&gt;\t\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\n</code></pre>\n<h1>2、配置文件添加注册服务器地址</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code>#应用基本信息配置\nspring:\n    application:\n        name: nacos-provider-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.xt.com    #Nacos服务地址\n\n</code></pre>\n<h1>3、开启服务发现功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能：</p>\n<pre><code>@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    &#125;\n\n&#125;\n</code></pre>\n<h1>4、配置服务接口</h1>\n<pre><code class=\"language-java\">@RestController\npublic class EchoController &#123;\n\n    @RequestMapping(value = &quot;/echo/&#123;string&#125;&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) &#123;\n        return &quot;Hello Nacos Discovery &quot; + string;\n    &#125;\n\n&#125;\n</code></pre>\n<h1>5、确认服务注册结果</h1>\n<p>运行Nacos-provider-demo，打开Nacos管理服务，可以看到nacos-prodiver-demo已经成功注册。</p>\n<p><img src=\"/img/image-20201214122255720.png\" alt=\"image-20201214122255720\"></p>\n"},{"title":"SpringCloud服务消费","author":"郑天祺","date":"2020-12-14T03:32:00.000Z","_content":"\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）\n\n# 1、pom.xml添加starter依赖\n\n```java\n\t\t<!-- Nacos服务发现 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n        <!-- 声明式调用 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n\t\t<!-- 负载均衡 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n\t\t</dependency>\n\t\t<!-- 熔断器 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n\t\t</dependency>\n```\n\n# 2、添加配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n```\n\n# 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.example\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.example\"})\npublic class NacosFeignDemoApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n1.\tname：服务提供者注册在服务注册中心的名称；\n2.\tfallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n3.\tfallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@FeignClient(name = \"nacos-provider-demo\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n\n```\n\n","source":"_posts/SpringCloud服务消费.md","raw":"title: SpringCloud服务消费\nauthor: 郑天祺\ntags:\n\n  - SpringCloud\ncategories: []\ndate: 2020-12-14 11:32:00\n\n---\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）\n\n# 1、pom.xml添加starter依赖\n\n```java\n\t\t<!-- Nacos服务发现 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n\t\t</dependency>\n        <!-- 声明式调用 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n\t\t<!-- 负载均衡 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n\t\t</dependency>\n\t\t<!-- 熔断器 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n\t\t</dependency>\n```\n\n# 2、添加配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\n#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #<负责人姓名>\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n```\n\n# 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.example\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.example\"})\npublic class NacosFeignDemoApplication {\n\n    public static void main(String[] args) {\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(\"spring.application.name\"));\n    }\n}\n```\n\n# 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n1.\tname：服务提供者注册在服务注册中心的名称；\n2.\tfallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n3.\tfallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@FeignClient(name = \"nacos-provider-demo\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n\n```\n\n","slug":"SpringCloud服务消费","published":1,"updated":"2020-12-14T04:26:25.577Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkn004fl0t9h9t3bljr","content":"<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）</p>\n<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">\t\t&lt;!-- Nacos服务发现 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n        &lt;!-- 声明式调用 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\t\t&lt;!-- 负载均衡 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\t\t&lt;!-- 熔断器 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置文件配置</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n</code></pre>\n<h1>3、开启服务发现、负载均衡、熔断器功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.example&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.example&quot;&#125;)\npublic class NacosFeignDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、创建服务代理类</h1>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：</p>\n<ol>\n<li>name：服务提供者注册在服务注册中心的名称；</li>\n<li>fallback：使用者提供的断路器实现，必须是当前代理类的实现类；</li>\n<li>fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。<br>\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</li>\n</ol>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;nacos-provider-demo&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡）</p>\n<h1>1、pom.xml添加starter依赖</h1>\n<pre><code class=\"language-java\">\t\t&lt;!-- Nacos服务发现 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n        &lt;!-- 声明式调用 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\t\t&lt;!-- 负载均衡 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n\t\t&lt;!-- 熔断器 --&gt;\n\t\t&lt;dependency&gt;\n\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n\t\t&lt;/dependency&gt;\n</code></pre>\n<h1>2、添加配置文件配置</h1>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">#应用基本信息配置\nspring:\n    application:\n        name: nacos-consumer-demo  #修改此处为您的应用程序名称\n        group: test #部门\n        developer:  developer #&lt;负责人姓名&gt;\n    cloud:\n        nacos:\n            discovery:\n                server-addr: nacos.com    #Nacos服务地址\n#允许feign开启熔断器，默认未开启\nfeign:\n    hystrix:\n        enabled: true\n\n</code></pre>\n<h1>3、开启服务发现、负载均衡、熔断器功能</h1>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = &#123;&quot;com.example&quot;&#125;)   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = &#123;&quot;com.example&quot;&#125;)\npublic class NacosFeignDemoApplication &#123;\n\n    public static void main(String[] args) &#123;\n        ConfigurableApplicationContext context = SpringApplication.run(NacosFeignDemoApplication.class, args);\n        System.out.println(context.getEnvironment().getProperty(&quot;spring.application.name&quot;));\n    &#125;\n&#125;\n</code></pre>\n<h1>4、创建服务代理类</h1>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：</p>\n<ol>\n<li>name：服务提供者注册在服务注册中心的名称；</li>\n<li>fallback：使用者提供的断路器实现，必须是当前代理类的实现类；</li>\n<li>fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。<br>\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</li>\n</ol>\n<pre><code class=\"language-java\">@FeignClient(name = &quot;nacos-provider-demo&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface RemoteClient &#123;\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/&#123;string&#125;&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n\n</code></pre>\n"},{"title":"SpringCloud管理配置页面","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n# 1、配置中心管理服务\n\n1、配置 ID：Data ID\n\n![image-20201214121332594](/img/image-20201214121332594.png)\n\n```java\n在 Nacos Spring Cloud 中，dataId 的完整格式如下：${prefix}-${spring.profile.active}.${file-extension}\n```\n\n```java\n${prefix} 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n${file-extension} 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n${spring.profile.active} 为为当前环境对应的 profile，如为空，则变为${prefix}-${spring.profile.active}.${file-extension}形式。\n```\n\n建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。\n全部字符小写。只允许英文字符和 4 种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过 256 字节。\n\n\n\n2、配置分组：group\n\n一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过128字节。\n\n![image-20201214121613146](/img/image-20201214121613146.png)\n\n3、配置格式\n\n可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。\n\n![image-20201214121645728](/img/image-20201214121645728.png)\n\n4、配置内容\n\n配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。\n\n![image-20201214121721100](/img/image-20201214121721100.png)","source":"_posts/SpringCloud管理配置页面.md","raw":"title: SpringCloud管理配置页面\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n# 1、配置中心管理服务\n\n1、配置 ID：Data ID\n\n![image-20201214121332594](/img/image-20201214121332594.png)\n\n```java\n在 Nacos Spring Cloud 中，dataId 的完整格式如下：${prefix}-${spring.profile.active}.${file-extension}\n```\n\n```java\n${prefix} 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n${file-extension} 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n${spring.profile.active} 为为当前环境对应的 profile，如为空，则变为${prefix}-${spring.profile.active}.${file-extension}形式。\n```\n\n建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。\n全部字符小写。只允许英文字符和 4 种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过 256 字节。\n\n\n\n2、配置分组：group\n\n一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（\".\"、\":\"、\"-\"、\"_\"），不超过128字节。\n\n![image-20201214121613146](/img/image-20201214121613146.png)\n\n3、配置格式\n\n可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。\n\n![image-20201214121645728](/img/image-20201214121645728.png)\n\n4、配置内容\n\n配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。\n\n![image-20201214121721100](/img/image-20201214121721100.png)","slug":"SpringCloud管理配置页面","published":1,"updated":"2021-04-13T07:11:06.850Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpko004il0t9ekzh3rx3","content":"<h1>1、配置中心管理服务</h1>\n<p>1、配置 ID：Data ID</p>\n<p><img src=\"/img/image-20201214121332594.png\" alt=\"image-20201214121332594\"></p>\n<pre><code class=\"language-java\">在 Nacos Spring Cloud 中，dataId 的完整格式如下：$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;\n</code></pre>\n<pre><code class=\"language-java\">$&#123;prefix&#125; 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n$&#123;file-extension&#125; 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n$&#123;spring.profile.active&#125; 为为当前环境对应的 profile，如为空，则变为$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;形式。\n</code></pre>\n<p>建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。<br>\n全部字符小写。只允许英文字符和 4 种特殊字符（“.”、“:”、“-”、“_”），不超过 256 字节。</p>\n<p>2、配置分组：group</p>\n<p>一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（“.”、“:”、“-”、“_”），不超过128字节。</p>\n<p><img src=\"/img/image-20201214121613146.png\" alt=\"image-20201214121613146\"></p>\n<p>3、配置格式</p>\n<p>可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。</p>\n<p><img src=\"/img/image-20201214121645728.png\" alt=\"image-20201214121645728\"></p>\n<p>4、配置内容</p>\n<p>配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。</p>\n<p><img src=\"/img/image-20201214121721100.png\" alt=\"image-20201214121721100\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、配置中心管理服务</h1>\n<p>1、配置 ID：Data ID</p>\n<p><img src=\"/img/image-20201214121332594.png\" alt=\"image-20201214121332594\"></p>\n<pre><code class=\"language-java\">在 Nacos Spring Cloud 中，dataId 的完整格式如下：$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;\n</code></pre>\n<pre><code class=\"language-java\">$&#123;prefix&#125; 为dataId的前缀，对应于Client端配置 spring.cloud.nacos.config.prefix 的值，如未配置，则默认对应Client端 spring.application.name 配置项的值。\n$&#123;file-extension&#125; 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。\n$&#123;spring.profile.active&#125; 为为当前环境对应的 profile，如为空，则变为$&#123;prefix&#125;-$&#123;spring.profile.active&#125;.$&#123;file-extension&#125;形式。\n</code></pre>\n<p>建议${prefix}采用类似 package.class的命名规则保证全局唯一性，class 部分建议是配置的业务含义。<br>\n全部字符小写。只允许英文字符和 4 种特殊字符（“.”、“:”、“-”、“_”），不超过 256 字节。</p>\n<p>2、配置分组：group</p>\n<p>一组相关配置的集合，建议以产品分组，ID建议填写产品名:项目/模块名（如：supervision:web-platform）保证唯一性，只允许英文字符和4种特殊字符（“.”、“:”、“-”、“_”），不超过128字节。</p>\n<p><img src=\"/img/image-20201214121613146.png\" alt=\"image-20201214121613146\"></p>\n<p>3、配置格式</p>\n<p>可选配置格式，Nacos 会帮助您做格式校验。建议使用properties和yaml。</p>\n<p><img src=\"/img/image-20201214121645728.png\" alt=\"image-20201214121645728\"></p>\n<p>4、配置内容</p>\n<p>配置的内容，建议不超过 10 KB，最大不超过 100 KB。内容格式应当与【配置格式】的设置一致。</p>\n<p><img src=\"/img/image-20201214121721100.png\" alt=\"image-20201214121721100\"></p>\n"},{"title":"SpringCloud运维接口","author":"郑天祺","date":"2020-12-14T03:31:00.000Z","_content":"\n注意：默认端点 path 前面有一级 /actuator ，例如：http://服务地址/actuator/info\n\n```java\nEndpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/{name}\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/{name}\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n```\n\n","source":"_posts/SpringCloud运维接口.md","raw":"title: SpringCloud运维接口\nauthor: 郑天祺\ntags:\n  - SpringCloud\ncategories:\n  - spring\ndate: 2020-12-14 11:31:00\n---\n\n注意：默认端点 path 前面有一级 /actuator ，例如：http://服务地址/actuator/info\n\n```java\nEndpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/{name}\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/{name}\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n```\n\n","slug":"SpringCloud运维接口","published":1,"updated":"2021-04-13T07:11:17.368Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkp004ml0t9anjk3n5e","content":"<p>注意：默认端点 path 前面有一级 /actuator ，例如：<a href=\"http://xn--zfry9hnb732h/actuator/info\">http://服务地址/actuator/info</a></p>\n<pre><code class=\"language-java\">Endpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/&#123;name&#125;\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/&#123;name&#125;\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>注意：默认端点 path 前面有一级 /actuator ，例如：<a href=\"http://xn--zfry9hnb732h/actuator/info\">http://服务地址/actuator/info</a></p>\n<pre><code class=\"language-java\">Endpoint ID\tDescription\nauditevents\t显示应用暴露的审计事件 (比如认证进入、订单失败)\ninfo\t显示应用的基本信息\nhealth\t显示应用的健康状态\nmetrics\t显示应用的度量信息\nmetrics/&#123;name&#125;\t显示应用指定名称的度量信息，例如：http://localhost:8080/actuator/metrics/system.cpu.count\nloggers\t显示和修改配置的loggers\nlogfile\t返回log file中的内容(如果logging.file或者logging.path被设置)\nhttptrace\t显示HTTP足迹，最近100个HTTP request/repsponse\nenv\t显示当前的环境特性\nenv/&#123;name&#125;\t显示指定名称的环境信息，例如：http://localhost:8080/actuator/spring.application.name\nflyway\t显示数据库迁移路径的详细信息\nliquidbase\t显示Liquibase 数据库迁移的纤细信息\nshutdown\t让你逐步关闭应用\nmappings\t显示所有的@RequestMapping路径\nscheduledtasks\t显示应用中的调度任务\nthreaddump\t执行一个线程dump\nheapdump\t返回一个GZip压缩的JVM堆dump\n</code></pre>\n"},{"title":"TCP与UDP的区别","author":"郑天祺","date":"2020-07-21T00:00:00.000Z","_content":"\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","source":"_posts/TCP与UDP的区别.md","raw":"title: TCP与UDP的区别\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\n  - UDP\ncategories:\n  - 网络\ndate: 2020-07-21 08:00:00\n\n---\n\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","slug":"TCP与UDP的区别","published":1,"updated":"2020-07-21T00:22:47.635Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkq004pl0t9fk4u9mij","content":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2 id=\"（1）面向无连接\">（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了<br>\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2 id=\"（2）单播、多播、广播\">（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2 id=\"（3）不可靠性\">（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2 id=\"（4）头部开销小、传输高效\">（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口<br>\n​\t\t整个数据报文的长度<br>\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误<br>\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2 id=\"（1）TCP连接\">（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2 id=\"（2）特点\">（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：<br>\nTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：<br>\nTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2 id=\"4、对比\">4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为<br>\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2 id=\"（1）面向无连接\">（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了<br>\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2 id=\"（2）单播、多播、广播\">（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2 id=\"（3）不可靠性\">（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2 id=\"（4）头部开销小、传输高效\">（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口<br>\n​\t\t整个数据报文的长度<br>\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误<br>\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2 id=\"（1）TCP连接\">（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2 id=\"（2）特点\">（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：<br>\nTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：<br>\nTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2 id=\"4、对比\">4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为<br>\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n"},{"title":"TCP IP四层网络模型","author":"郑天祺","date":"2019-08-30T07:12:00.000Z","_content":"\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","source":"_posts/TCP-IP四层网络模型.md","raw":"title: TCP IP四层网络模型\nauthor: 郑天祺\ntags:\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:12:00\n\n---\n\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","slug":"TCP-IP四层网络模型","published":1,"updated":"2019-10-15T12:19:39.272Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkr004tl0t9d254ek1e","content":"<h2 id=\"1、用户发送请求\">1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"2、服务器接收请求\">2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"3、网络连接模型\">3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"4、使用协议进行通讯\">4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：<br>\n​\t1.流套接字（stream socket）-TCP<br>\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt=\"\"></p>\n<h2 id=\"5、Socket通讯模型\">5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、用户发送请求\">1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"2、服务器接收请求\">2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt=\"\"></p>\n<h2 id=\"3、网络连接模型\">3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"4、使用协议进行通讯\">4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：<br>\n​\t1.流套接字（stream socket）-TCP<br>\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt=\"\"></p>\n<h2 id=\"5、Socket通讯模型\">5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt=\"\"></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt=\"\"></p>\n"},{"title":"TCP握手、挥手协议","author":"郑天祺","date":"2019-08-30T07:58:00.000Z","_content":"\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","source":"_posts/TCP握手、挥手协议.md","raw":"title: TCP握手、挥手协议\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:58:00\n\n---\n\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","slug":"TCP握手、挥手协议","published":1,"updated":"2020-07-20T23:18:04.754Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpks004wl0t98srfdt7j","content":"<h2 id=\"1、TCP三次握手协议（打开连接）\">1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt=\"\"></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<h2 id=\"2、TCP四次挥手协议（关闭连接）\">2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt=\"\"></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3 id=\"（2）四次挥手\">（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、TCP三次握手协议（打开连接）\">1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt=\"\"></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<h2 id=\"2、TCP四次挥手协议（关闭连接）\">2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt=\"\"></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3 id=\"（2）四次挥手\">（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt=\"\"></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n"},{"title":"ThreadLocal","author":"郑天祺","date":"2020-03-28T01:59:00.000Z","_content":"\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","source":"_posts/ThreadLocal.md","raw":"title: ThreadLocal\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-03-28 09:59:00\n\n---\n\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","slug":"ThreadLocal","published":1,"updated":"2020-03-31T13:12:37.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkt004zl0t9fzd82gwh","content":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取<br>\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest &#123;\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) &#123;\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal2&quot;).start();\n    &#125;\n&#125;\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： <a href=\"http://www.jiangxinlingdu.com/\">http://www.jiangxinlingdu.com/</a> 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n","site":{"data":{}},"excerpt":"","more":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取<br>\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest &#123;\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) &#123;\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; &#123;\n            try &#123;\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try &#123;\n                        Thread.sleep(200);\n                    &#125; catch (InterruptedException e) &#123;\n                        e.printStackTrace();\n                    &#125;\n                &#125;\n            &#125; finally &#123;\n                threadLocal.remove();\n            &#125;\n        &#125;, &quot;threadlocal2&quot;).start();\n    &#125;\n&#125;\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： <a href=\"http://www.jiangxinlingdu.com/\">http://www.jiangxinlingdu.com/</a> 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n"},{"title":"Tomcat性能优化整理","author":"郑天祺","date":"2020-11-17T04:34:00.000Z","_content":"\n# 1、JVM参数调优\n\n```java\n-Xms<size>\n```\n\n表示JVM初始化堆的大小\n\n```java\n-Xmx<size>\n```\n\n表示JVM堆的最大值\n\n​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。\n\n​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。\n\n# 2、禁用DNS查询\n\n​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。\n\n​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。\n\n​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。\n\n# 3、调整线程数\n\n​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。\n\n​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。\n\n​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。\n\n​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。\n\n​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。\n\n​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。\n\n​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。\n\n在Tomcat5对这些参数进行了调整，请看下面属性:\n\n​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。\n\n​\t\tminSpareThreadsTomcat初始化时创建的线程数。\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。\n\n​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。\n\n# 4、加大Tomcat内存\n\n​\t\t首先检查程序有没有限入死循环\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。\n\n​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。\n\n跟据网上的答案大致有这两种解决方法:\n\n（1）设置环境变量\n\t\t解决方法:手动设置Heap size\n\t\t修改TOMCAT_HOME/bin/catalina.sh\n\n​\t\tset JAVA_OPTS=-Xms32m-Xmx512m\n\n​\t\t可以根据自己机器的内存进行更改。\n\n（2）java-Xms32m-Xmx800m className\n\t\t就是在执行JAVA类文件时加上这个参数，其中className\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。\n\n​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse->run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。\n\n（3）java.lang.OutOfMemoryError: PermGen space\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。\n解决方法:手动设置MaxPermSize大小修\n\n（4）java.lang.OutOfMemoryError:Java heap space Heap size设置\n\t\tJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。\n\n​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。\n解决方法:手动设置Heap size\n修改 TOMCAT_HOME/bin/catalina.sh\n在“echo\"Using CATALINA_BASE:$CATALINA_BASE\"\"上面加入以下行:JAVA_OPTS=\"-server-Xms800m-Xmx800m-XX:MaxNewSize=256m\"\n\n\n\n\n\n\n\n5、","source":"_posts/Tomcat性能优化整理.md","raw":"title: Tomcat性能优化整理\nauthor: 郑天祺\ntags:\n\n  - tomcat\ncategories:\n  - 面试\ndate: 2020-11-17 12:34:00\n\n---\n\n# 1、JVM参数调优\n\n```java\n-Xms<size>\n```\n\n表示JVM初始化堆的大小\n\n```java\n-Xmx<size>\n```\n\n表示JVM堆的最大值\n\n​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。\n\n​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。\n\n# 2、禁用DNS查询\n\n​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。\n\n​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。\n\n​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。\n\n# 3、调整线程数\n\n​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。\n\n​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。\n\n​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。\n\n​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。\n\n​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。\n\n​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。\n\n​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。\n\n在Tomcat5对这些参数进行了调整，请看下面属性:\n\n​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。\n\n​\t\tminSpareThreadsTomcat初始化时创建的线程数。\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。\n\n​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。\n\n# 4、加大Tomcat内存\n\n​\t\t首先检查程序有没有限入死循环\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。\n\n​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。\n\n跟据网上的答案大致有这两种解决方法:\n\n（1）设置环境变量\n\t\t解决方法:手动设置Heap size\n\t\t修改TOMCAT_HOME/bin/catalina.sh\n\n​\t\tset JAVA_OPTS=-Xms32m-Xmx512m\n\n​\t\t可以根据自己机器的内存进行更改。\n\n（2）java-Xms32m-Xmx800m className\n\t\t就是在执行JAVA类文件时加上这个参数，其中className\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。\n\n​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse->run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。\n\n（3）java.lang.OutOfMemoryError: PermGen space\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。\n解决方法:手动设置MaxPermSize大小修\n\n（4）java.lang.OutOfMemoryError:Java heap space Heap size设置\n\t\tJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。\n\n​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。\n解决方法:手动设置Heap size\n修改 TOMCAT_HOME/bin/catalina.sh\n在“echo\"Using CATALINA_BASE:$CATALINA_BASE\"\"上面加入以下行:JAVA_OPTS=\"-server-Xms800m-Xmx800m-XX:MaxNewSize=256m\"\n\n\n\n\n\n\n\n5、","slug":"Tomcat性能优化整理","published":1,"updated":"2020-11-17T06:58:13.216Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkt0051l0t9cyafetl6","content":"<h1>1、JVM参数调优</h1>\n<pre><code class=\"language-java\">-Xms&lt;size&gt;\n</code></pre>\n<p>表示JVM初始化堆的大小</p>\n<pre><code class=\"language-java\">-Xmx&lt;size&gt;\n</code></pre>\n<p>表示JVM堆的最大值</p>\n<p>​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。</p>\n<p>​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。</p>\n<h1>2、禁用DNS查询</h1>\n<p>​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。</p>\n<p>​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。</p>\n<p>​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。</p>\n<h1>3、调整线程数</h1>\n<p>​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。</p>\n<p>​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。</p>\n<p>​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。</p>\n<p>​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。</p>\n<p>​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。</p>\n<p>​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。</p>\n<p>​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。</p>\n<p>在Tomcat5对这些参数进行了调整，请看下面属性:</p>\n<p>​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。<br>\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。<br>\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。</p>\n<p>​\t\tminSpareThreadsTomcat初始化时创建的线程数。<br>\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。</p>\n<p>​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。</p>\n<h1>4、加大Tomcat内存</h1>\n<p>​\t\t首先检查程序有没有限入死循环<br>\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。</p>\n<p>​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。</p>\n<p>跟据网上的答案大致有这两种解决方法:</p>\n<p>（1）设置环境变量<br>\n解决方法:手动设置Heap size<br>\n修改TOMCAT_HOME/bin/catalina.sh</p>\n<p>​\t\tset JAVA_OPTS=-Xms32m-Xmx512m</p>\n<p>​\t\t可以根据自己机器的内存进行更改。</p>\n<p>（2）java-Xms32m-Xmx800m className<br>\n就是在执行JAVA类文件时加上这个参数，其中className<br>\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。</p>\n<p>​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse-&gt;run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。</p>\n<p>（3）java.lang.OutOfMemoryError: PermGen space<br>\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。<br>\n解决方法:手动设置MaxPermSize大小修</p>\n<p>（4）java.lang.OutOfMemoryError:Java heap space Heap size设置<br>\nJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。</p>\n<p>​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。<br>\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。<br>\n解决方法:手动设置Heap size<br>\n修改 TOMCAT_HOME/bin/catalina.sh<br>\n在“echo&quot;Using CATALINA_BASE:$CATALINA_BASE&quot;“上面加入以下行:JAVA_OPTS=”-server-Xms800m-Xmx800m-XX:MaxNewSize=256m&quot;</p>\n<p>5、</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、JVM参数调优</h1>\n<pre><code class=\"language-java\">-Xms&lt;size&gt;\n</code></pre>\n<p>表示JVM初始化堆的大小</p>\n<pre><code class=\"language-java\">-Xmx&lt;size&gt;\n</code></pre>\n<p>表示JVM堆的最大值</p>\n<p>​\t\t这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。</p>\n<p>​\t\t在catalina.bat中，设置JAVA_OPTS='-xms256m-Xmx512M‘，表示初始化内存为256 MB，可以使用的最大内存为512 MB。</p>\n<h1>2、禁用DNS查询</h1>\n<p>​\t\t当web应用程序向要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。</p>\n<p>​\t\tDNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。</p>\n<p>​\t\t为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。</p>\n<h1>3、调整线程数</h1>\n<p>​\t\t通过应用程序的连接器(Connector)进行性能控制的的参数是创建的处理请求的线程数。Tomcat 使用线程池加速响应速度来处理请求。</p>\n<p>​\t\t在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。</p>\n<p>​\t\t多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接更多的请求。</p>\n<p>​\t\tTomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。</p>\n<p>​\t\tminProcessors 服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。</p>\n<p>​\t\tmaxProcessors的值也是有上限的，应防止流量不可控制(或者恶意的服务攻击)，从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。</p>\n<p>​\t\tWeb server 允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。</p>\n<p>在Tomcat5对这些参数进行了调整，请看下面属性:</p>\n<p>​\t\tmaxThreads Tomcat 使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。<br>\n​\t\tacceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。<br>\n​\t\tconnnection Timeout网络连接超时，单位:毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。</p>\n<p>​\t\tminSpareThreadsTomcat初始化时创建的线程数。<br>\n​\t\tmaxSpareThreads一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket 线程。</p>\n<p>​\t\t最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。</p>\n<h1>4、加大Tomcat内存</h1>\n<p>​\t\t首先检查程序有没有限入死循环<br>\n​\t\t这个问题主要还是由这个问题 java.lang.0utOfMemoryError:Javaheap space引起的。</p>\n<p>​\t\t第一次出现这样的的问题以后，引发了其他的问题。在网上—查可能是JAVA的堆栈设置太小的原因。</p>\n<p>跟据网上的答案大致有这两种解决方法:</p>\n<p>（1）设置环境变量<br>\n解决方法:手动设置Heap size<br>\n修改TOMCAT_HOME/bin/catalina.sh</p>\n<p>​\t\tset JAVA_OPTS=-Xms32m-Xmx512m</p>\n<p>​\t\t可以根据自己机器的内存进行更改。</p>\n<p>（2）java-Xms32m-Xmx800m className<br>\n就是在执行JAVA类文件时加上这个参数，其中className<br>\n是需要执行的确类名。(包括包名)这个解决问题了。而且执行的速度比没有设置的时候快很多。</p>\n<p>​\t\t如果在测试的时候可能会用Eclispe这时候就需要在Eclipse-&gt;run-arguments中的VM arguments中输入-Xms32m-Xmx800m这个参数就可以了。</p>\n<p>（3）java.lang.OutOfMemoryError: PermGen space<br>\nPermGen space的全称是Permanent Generation space ,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中,它和存放类实例(Instance)的Heap区域不同,GC(GarbageCollection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很多CLASS的话，就很可能出现PermGen space 错误。<br>\n解决方法:手动设置MaxPermSize大小修</p>\n<p>（4）java.lang.OutOfMemoryError:Java heap space Heap size设置<br>\nJVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn-Xms-Xmx等选项可进行设置。Heap size的大小是Young Generation和TenuredGeneraion之和。</p>\n<p>​\t\t提示:在JVM中如果98%的时间是用于GC且可用的Heap size不足2%的时候将抛出此异常信息。<br>\n​\t\t提示:Heap Size最大不要超过可用物理内存的80%，一般的要将-Xms 和-Xmx选项设置为相同，而-Xms为1/4的-Xmx值。<br>\n解决方法:手动设置Heap size<br>\n修改 TOMCAT_HOME/bin/catalina.sh<br>\n在“echo&quot;Using CATALINA_BASE:$CATALINA_BASE&quot;“上面加入以下行:JAVA_OPTS=”-server-Xms800m-Xmx800m-XX:MaxNewSize=256m&quot;</p>\n<p>5、</p>\n"},{"title":"WordCount简析","author":"郑天祺","date":"2019-12-18T03:57:00.000Z","_content":"\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","source":"_posts/WordCount简析.md","raw":"title: WordCount简析\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 11:57:00\n\n---\n\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","slug":"WordCount简析","published":1,"updated":"2019-12-18T03:59:10.021Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpku0055l0t94m2i6ioc","content":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount &#123;\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException &#123;\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) &#123;\n                word.set(itr.nextToken());\n                context.write(word, one);\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException &#123;\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) &#123;\n                sum += val.get();\n            &#125;\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        &#125;\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) &#123;\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        &#125;\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        &#125;\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    &#125;\n&#125;\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount &#123;\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException &#123;\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) &#123;\n                word.set(itr.nextToken());\n                context.write(word, one);\n            &#125;\n        &#125;\n    &#125;\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException &#123;\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) &#123;\n                sum += val.get();\n            &#125;\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        &#125;\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) &#123;\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        &#125;\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        &#125;\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    &#125;\n&#125;\n\n</code></pre>\n"},{"title":"java8新特性","author":"郑天祺","date":"2019-11-14T06:18:00.000Z","_content":"​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","source":"_posts/java8新特性.md","raw":"title: java8新特性\nauthor: 郑天祺\ntags:\n\n  - JDK1.8新特性\ncategories:\n  - java基础\ndate: 2019-11-14 14:18:00\n---\n​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","slug":"java8新特性","published":1,"updated":"2019-11-14T08:30:37.235Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkw0057l0t9dft21pht","content":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest &#123;\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() &#123;\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    &#125;\n\n    @Test\n    public void forEach() &#123;\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void map() &#123;\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    &#125;\n\n    @Test\n    public void sorted() &#123;\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void parallel() &#123;\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    &#125;\n\n    @Test\n    public void join() &#123;\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    &#125;\n\n    @Test\n    public void statistics()&#123;\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    &#125;\n\n&#125;\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;&#123;\n    T get();\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car &#123;\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) &#123;\n        return supplier.get();\n    &#125;\n\n    public static void collide(final Car car) &#123;\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    &#125;\n\n    public void follow(final Car another) &#123;\n        System.out.println(&quot;Following the&quot; + another.toString());\n    &#125;\n\n    public void repair() &#123;\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest &#123;\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() &#123;\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    &#125;\n\n    @Test\n    public void quoteExample()&#123;\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface &#123;\n    default void print()&#123;\n        System.out.println(&quot;默认方法&quot;);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface&#123;\n\n    @Test\n    public void test()&#123;\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    &#125;\n&#125;\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest &#123;\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() &#123;\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    &#125;\n\n    @Test\n    public void testStructDateTime() &#123;\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    &#125;\n\n    @Test\n    public void testZonedDateTime() &#123;\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest &#123;\n\n    @Test\n    public void testOptional() &#123;\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    &#125;\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) &#123;\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    &#125;\n&#125;\n\n</code></pre>\n<p>参考文献：<a href=\"https://www.runoob.com/java/java8-new-features.html\">https://www.runoob.com/java/java8-new-features.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest &#123;\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() &#123;\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    &#125;\n\n    @Test\n    public void forEach() &#123;\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void map() &#123;\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    &#125;\n\n    @Test\n    public void sorted() &#123;\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    &#125;\n\n    @Test\n    public void parallel() &#123;\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    &#125;\n\n    @Test\n    public void join() &#123;\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    &#125;\n\n    @Test\n    public void statistics()&#123;\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    &#125;\n\n&#125;\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;&#123;\n    T get();\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car &#123;\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) &#123;\n        return supplier.get();\n    &#125;\n\n    public static void collide(final Car car) &#123;\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    &#125;\n\n    public void follow(final Car another) &#123;\n        System.out.println(&quot;Following the&quot; + another.toString());\n    &#125;\n\n    public void repair() &#123;\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest &#123;\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() &#123;\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    &#125;\n\n    @Test\n    public void quoteExample()&#123;\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface &#123;\n    default void print()&#123;\n        System.out.println(&quot;默认方法&quot;);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface&#123;\n\n    @Test\n    public void test()&#123;\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    &#125;\n&#125;\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest &#123;\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() &#123;\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    &#125;\n\n    @Test\n    public void testStructDateTime() &#123;\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    &#125;\n\n    @Test\n    public void testZonedDateTime() &#123;\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    &#125;\n&#125;\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest &#123;\n\n    @Test\n    public void testOptional() &#123;\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    &#125;\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) &#123;\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    &#125;\n&#125;\n\n</code></pre>\n<p>参考文献：<a href=\"https://www.runoob.com/java/java8-new-features.html\">https://www.runoob.com/java/java8-new-features.html</a></p>\n"},{"title":"Yarn概述","author":"郑天祺","date":"2019-12-18T01:12:00.000Z","_content":"\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","source":"_posts/Yarn概述.md","raw":"title: Yarn概述\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 09:12:00\n\n---\n\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","slug":"Yarn概述","published":1,"updated":"2019-12-18T02:22:59.255Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkx005bl0t90ezb92ve","content":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2 id=\"（1）Resource-Manager\">（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2 id=\"（2）Application-Master\">（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2 id=\"（3）Node-Manager\">（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2 id=\"（4）Resource-Request-和-Container\">（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2 id=\"（1）Resource-Manager\">（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2 id=\"（2）Application-Master\">（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2 id=\"（3）Node-Manager\">（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2 id=\"（4）Resource-Request-和-Container\">（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n"},{"title":"java中的Queue队列","author":"郑天祺","date":"2020-12-14T05:04:00.000Z","_content":"\n# 1、介绍\n\n​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。\n\n# 2、Queue的实现：\n\n 一个是以ConcurrentLinkedQueue为代表的高性能队列； \n 一个是以BlockingQueue接口为代表的阻塞队列； \n\n## （1）没有实现的阻塞接口队列\n\n​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue\n\n- PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。 \n- PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位\n\n- ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，\n- ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。\n\n## （2）实现阻塞接口的队列\n\njava.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。\n五个队列所提供的各有不同：\n\n　　* ArrayBlockingQueue ：一个由数组支持的有界队列。\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。\n\n![image-20201214130757812](/img/image-20201214130757812.png)\n\n# 3、ConcurrentLinkedQueue\n\n```java\n/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo {\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) {\n        q.offer(\"张三\");\n        q.offer(\"李四\");\n        q.offer(\"王五\");\n        q.offer(\"赵六\");\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    }\n}\n\n```\n\n# 4、BlockingQueue\n\n 定义： \n\t\t阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是： \n\t\t1、在队列为空时，获取元素的线程会等待队列变为非空。 \n\t\t2、当队列满时，存储元素的线程会等待队列可用。 \n阻塞队列是线程安全的。 \n用途： \n\t\t阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。\n\n​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n\n## 1）ArrayBlockingQueue\n\n```java\n/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(\"张三\");\n        array.add(\"李四\");\n        array.add(\"王五\");\n        try {\n            // 添加阻塞队列\n            boolean a = array.offer(\"赵六\", 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n## 2）LinkedBlockingQueue\n\n```java\n/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(\"张三\");\n        lbq.add(\"李四\");\n        lbq.add(\"李四\");\n        // 运行结果：3\n        System.out.println(lbq.size());\n    }\n}\n```\n\n## 3）PriorityBlockingQueue\n\n```java\n/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * <p>\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * <p>\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo {\n    public static void main(String[] args) {\n        PriorityBlockingQueue<Task> q = new PriorityBlockingQueue<>();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(\"id为2\"); t2.setName(\"id为3\"); t3.setName(\"id为1\");\n        q.add(t1); q.add(t2); q.add(t3);\n        try {\n            System.out.println(\"容器：\" + q);\n            System.out.println(q.take().getId());\n            System.out.println(\"容器：\" + q);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    public static class Task implements Comparable<Task> {\n        private int id;\n        private String name;\n        public int getId() {\n            return id;\n        }\n        public void setId(int id) {\n            this.id = id;\n        }\n        public String getName() {\n            return name;\n        }\n        public void setName(String name) {\n            this.name = name;\n        }\n        @Override\n        public int compareTo(Task task) {\n            return this.id > task.id ? 1 : (this.id < task.id ? -1 : 0);\n        }\n        @Override\n        public String toString() {\n            return this.id + \",\" + this.name;\n        }\n    }\n}\n\n```\n\n## 4）SynchronousQueue\n\nhttps://blog.51cto.com/14220760/2416470?source=dra","source":"_posts/java中的Queue队列.md","raw":"title: java中的Queue队列\nauthor: 郑天祺\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2020-12-14 13:04:00\n\n---\n\n# 1、介绍\n\n​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。\n\n# 2、Queue的实现：\n\n 一个是以ConcurrentLinkedQueue为代表的高性能队列； \n 一个是以BlockingQueue接口为代表的阻塞队列； \n\n## （1）没有实现的阻塞接口队列\n\n​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue\n\n- PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。 \n- PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位\n\n- ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，\n- ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。\n\n## （2）实现阻塞接口的队列\n\njava.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。\n五个队列所提供的各有不同：\n\n　　* ArrayBlockingQueue ：一个由数组支持的有界队列。\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。\n\n![image-20201214130757812](/img/image-20201214130757812.png)\n\n# 3、ConcurrentLinkedQueue\n\n```java\n/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo {\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) {\n        q.offer(\"张三\");\n        q.offer(\"李四\");\n        q.offer(\"王五\");\n        q.offer(\"赵六\");\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    }\n}\n\n```\n\n# 4、BlockingQueue\n\n 定义： \n\t\t阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是： \n\t\t1、在队列为空时，获取元素的线程会等待队列变为非空。 \n\t\t2、当队列满时，存储元素的线程会等待队列可用。 \n阻塞队列是线程安全的。 \n用途： \n\t\t阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。\n\n​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n\n## 1）ArrayBlockingQueue\n\n```java\n/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(\"张三\");\n        array.add(\"李四\");\n        array.add(\"王五\");\n        try {\n            // 添加阻塞队列\n            boolean a = array.offer(\"赵六\", 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n## 2）LinkedBlockingQueue\n\n```java\n/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo {\n    public static void main(String[] args) {\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(\"张三\");\n        lbq.add(\"李四\");\n        lbq.add(\"李四\");\n        // 运行结果：3\n        System.out.println(lbq.size());\n    }\n}\n```\n\n## 3）PriorityBlockingQueue\n\n```java\n/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * <p>\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * <p>\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo {\n    public static void main(String[] args) {\n        PriorityBlockingQueue<Task> q = new PriorityBlockingQueue<>();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(\"id为2\"); t2.setName(\"id为3\"); t3.setName(\"id为1\");\n        q.add(t1); q.add(t2); q.add(t3);\n        try {\n            System.out.println(\"容器：\" + q);\n            System.out.println(q.take().getId());\n            System.out.println(\"容器：\" + q);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    public static class Task implements Comparable<Task> {\n        private int id;\n        private String name;\n        public int getId() {\n            return id;\n        }\n        public void setId(int id) {\n            this.id = id;\n        }\n        public String getName() {\n            return name;\n        }\n        public void setName(String name) {\n            this.name = name;\n        }\n        @Override\n        public int compareTo(Task task) {\n            return this.id > task.id ? 1 : (this.id < task.id ? -1 : 0);\n        }\n        @Override\n        public String toString() {\n            return this.id + \",\" + this.name;\n        }\n    }\n}\n\n```\n\n## 4）SynchronousQueue\n\nhttps://blog.51cto.com/14220760/2416470?source=dra","slug":"java中的Queue队列","published":1,"updated":"2020-12-14T05:11:35.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpky005el0t97nxka69g","content":"<h1>1、介绍</h1>\n<p>​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构<br>\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。</p>\n<h1>2、Queue的实现：</h1>\n<p>一个是以ConcurrentLinkedQueue为代表的高性能队列；<br>\n一个是以BlockingQueue接口为代表的阻塞队列；</p>\n<h2 id=\"（1）没有实现的阻塞接口队列\">（1）没有实现的阻塞接口队列</h2>\n<p>​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口<br>\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue</p>\n<ul>\n<li>\n<p>PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。</p>\n</li>\n<li>\n<p>PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。</p>\n</li>\n</ul>\n<h2 id=\"（2）实现阻塞接口的队列\">（2）实现阻塞接口的队列</h2>\n<p>java.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。<br>\n五个队列所提供的各有不同：</p>\n<p>* ArrayBlockingQueue ：一个由数组支持的有界队列。<br>\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。<br>\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。<br>\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。<br>\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。</p>\n<p><img src=\"/img/image-20201214130757812.png\" alt=\"image-20201214130757812\"></p>\n<h1>3、ConcurrentLinkedQueue</h1>\n<pre><code class=\"language-java\">/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo &#123;\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) &#123;\n        q.offer(&quot;张三&quot;);\n        q.offer(&quot;李四&quot;);\n        q.offer(&quot;王五&quot;);\n        q.offer(&quot;赵六&quot;);\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、BlockingQueue</h1>\n<p>定义：<br>\n阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<br>\n1、在队列为空时，获取元素的线程会等待队列变为非空。<br>\n2、当队列满时，存储元素的线程会等待队列可用。<br>\n阻塞队列是线程安全的。<br>\n用途：<br>\n阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。</p>\n<p>​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>\n<h2 id=\"1）ArrayBlockingQueue\">1）ArrayBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(&quot;张三&quot;);\n        array.add(&quot;李四&quot;);\n        array.add(&quot;王五&quot;);\n        try &#123;\n            // 添加阻塞队列\n            boolean a = array.offer(&quot;赵六&quot;, 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"2）LinkedBlockingQueue\">2）LinkedBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(&quot;张三&quot;);\n        lbq.add(&quot;李四&quot;);\n        lbq.add(&quot;李四&quot;);\n        // 运行结果：3\n        System.out.println(lbq.size());\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"3）PriorityBlockingQueue\">3）PriorityBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * &lt;p&gt;\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * &lt;p&gt;\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        PriorityBlockingQueue&lt;Task&gt; q = new PriorityBlockingQueue&lt;&gt;();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(&quot;id为2&quot;); t2.setName(&quot;id为3&quot;); t3.setName(&quot;id为1&quot;);\n        q.add(t1); q.add(t2); q.add(t3);\n        try &#123;\n            System.out.println(&quot;容器：&quot; + q);\n            System.out.println(q.take().getId());\n            System.out.println(&quot;容器：&quot; + q);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n    public static class Task implements Comparable&lt;Task&gt; &#123;\n        private int id;\n        private String name;\n        public int getId() &#123;\n            return id;\n        &#125;\n        public void setId(int id) &#123;\n            this.id = id;\n        &#125;\n        public String getName() &#123;\n            return name;\n        &#125;\n        public void setName(String name) &#123;\n            this.name = name;\n        &#125;\n        @Override\n        public int compareTo(Task task) &#123;\n            return this.id &gt; task.id ? 1 : (this.id &lt; task.id ? -1 : 0);\n        &#125;\n        @Override\n        public String toString() &#123;\n            return this.id + &quot;,&quot; + this.name;\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4）SynchronousQueue\">4）SynchronousQueue</h2>\n<p><a href=\"https://blog.51cto.com/14220760/2416470?source=dra\">https://blog.51cto.com/14220760/2416470?source=dra</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​        Queue： 基本上，一个队列就是一个先入先出（FIFO）的数据结构<br>\n​        Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Deque接 口。</p>\n<h1>2、Queue的实现：</h1>\n<p>一个是以ConcurrentLinkedQueue为代表的高性能队列；<br>\n一个是以BlockingQueue接口为代表的阻塞队列；</p>\n<h2 id=\"（1）没有实现的阻塞接口队列\">（1）没有实现的阻塞接口队列</h2>\n<p>​\t\t没有实现的阻塞接口的LinkedList： 实现了java.util.Queue接口和java.util.AbstractQueue接口<br>\n　　内置的两个不阻塞队列： PriorityQueue 和 ConcurrentLinkedQueue</p>\n<ul>\n<li>\n<p>PriorityQueue     和 ConcurrentLinkedQueue 类在     Collection Framework 中加入两个具体集合实现。</p>\n</li>\n<li>\n<p>PriorityQueue     类实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的     java.util.Comparator 实现来定位</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     是基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，</p>\n</li>\n<li>\n<p>ConcurrentLinkedQueue     对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。</p>\n</li>\n</ul>\n<h2 id=\"（2）实现阻塞接口的队列\">（2）实现阻塞接口的队列</h2>\n<p>java.util.concurrent 中加入了 BlockingQueue 接口和五个阻塞队列类。它实质上就是一种带有一点扭曲的 FIFO 数据结构。不是立即从队列中添加或者删除元素，线程执行操作阻塞，直到有空间或者元素可用。<br>\n五个队列所提供的各有不同：</p>\n<p>* ArrayBlockingQueue ：一个由数组支持的有界队列。<br>\n　　* LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。<br>\n　　* PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。<br>\n　　* DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。<br>\n　　* SynchronousQueue ：一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。</p>\n<p><img src=\"/img/image-20201214130757812.png\" alt=\"image-20201214130757812\"></p>\n<h1>3、ConcurrentLinkedQueue</h1>\n<pre><code class=\"language-java\">/**\n * ConcurrentLinkedQueue : 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue。\n * 它是一个基于链接节点的无界线程安全队列。该队列的元素遵循先进先出的原则。\n * 头是最先加入的，尾是最近加入的，该队列不允许null元素。\n *\n */\npublic class ConcurrentLinkedQueueDemo &#123;\n    private static ConcurrentLinkedQueue q = new ConcurrentLinkedQueue();\n    public static void main(String[] args) &#123;\n        q.offer(&quot;张三&quot;);\n        q.offer(&quot;李四&quot;);\n        q.offer(&quot;王五&quot;);\n        q.offer(&quot;赵六&quot;);\n        // 从头获取元素,删除该元素\n        System.out.println(q.poll());\n        // 从头获取元素,不刪除该元素\n        System.out.println(q.peek());\n        // 获取总长度\n        System.out.println(q.size());\n    &#125;\n&#125;\n\n</code></pre>\n<h1>4、BlockingQueue</h1>\n<p>定义：<br>\n阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<br>\n1、在队列为空时，获取元素的线程会等待队列变为非空。<br>\n2、当队列满时，存储元素的线程会等待队列可用。<br>\n阻塞队列是线程安全的。<br>\n用途：<br>\n阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。</p>\n<p>​\t\t阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>\n<h2 id=\"1）ArrayBlockingQueue\">1）ArrayBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * ArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。\n * 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。\n * ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class ArrayBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化3个队列\n        ArrayBlockingQueue array = new ArrayBlockingQueue(3);\n        array.add(&quot;张三&quot;);\n        array.add(&quot;李四&quot;);\n        array.add(&quot;王五&quot;);\n        try &#123;\n            // 添加阻塞队列\n            boolean a = array.offer(&quot;赵六&quot;, 1, TimeUnit.SECONDS);\n            System.out.println(a);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"2）LinkedBlockingQueue\">2）LinkedBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * LinkedBlockingQueue阻塞队列大小的配置是可选的，\n * 如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。\n * 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量。它的内部实现是一个链表。\n * 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。\n *\n */\npublic class LinkedBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化\n        LinkedBlockingQueue lbq = new LinkedBlockingQueue(3);\n        lbq.add(&quot;张三&quot;);\n        lbq.add(&quot;李四&quot;);\n        lbq.add(&quot;李四&quot;);\n        // 运行结果：3\n        System.out.println(lbq.size());\n    &#125;\n&#125;\n</code></pre>\n<h2 id=\"3）PriorityBlockingQueue\">3）PriorityBlockingQueue</h2>\n<pre><code class=\"language-java\">/**\n * 实现原理：PriorityBlockingQueue通过使用堆这种数据结构实现将队列中的元素按照某种排序规则进行排序，从而改变先进先出的队列顺序\n * &lt;p&gt;\n * PriorityBlockingQueue是一个没有边界的队列，它的排序规则和 java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。\n * 所有插入PriorityBlockingQueue的对象必须实现 java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。\n * 另外，我们可以从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。\n * &lt;p&gt;\n * add方法添加元素时，是自下而上的调整堆，取出元素时，是自上而下的调整堆顺序；\n *\n * @Author: zhengtianqi\n * @Date: 2019/7/8 15:54\n */\npublic class PriorityBlockingQueueDemo &#123;\n    public static void main(String[] args) &#123;\n        PriorityBlockingQueue&lt;Task&gt; q = new PriorityBlockingQueue&lt;&gt;();\n        Task t1 = new Task(); Task t2 = new Task(); Task t3 = new Task();\n        t1.setId(2); t2.setId(3); t3.setId(1);\n        t1.setName(&quot;id为2&quot;); t2.setName(&quot;id为3&quot;); t3.setName(&quot;id为1&quot;);\n        q.add(t1); q.add(t2); q.add(t3);\n        try &#123;\n            System.out.println(&quot;容器：&quot; + q);\n            System.out.println(q.take().getId());\n            System.out.println(&quot;容器：&quot; + q);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n    public static class Task implements Comparable&lt;Task&gt; &#123;\n        private int id;\n        private String name;\n        public int getId() &#123;\n            return id;\n        &#125;\n        public void setId(int id) &#123;\n            this.id = id;\n        &#125;\n        public String getName() &#123;\n            return name;\n        &#125;\n        public void setName(String name) &#123;\n            this.name = name;\n        &#125;\n        @Override\n        public int compareTo(Task task) &#123;\n            return this.id &gt; task.id ? 1 : (this.id &lt; task.id ? -1 : 0);\n        &#125;\n        @Override\n        public String toString() &#123;\n            return this.id + &quot;,&quot; + this.name;\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4）SynchronousQueue\">4）SynchronousQueue</h2>\n<p><a href=\"https://blog.51cto.com/14220760/2416470?source=dra\">https://blog.51cto.com/14220760/2416470?source=dra</a></p>\n"},{"title":"k8s构建ELK日志平台","author":"ztq","date":"2021-04-13T06:40:00.000Z","_content":"\n\n\n# k8s构建ELK日志平台\n\n## Pod中附加专用日志收集的容器\n\n# 一、概述\n\n目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；\n\n日志收集方案设计图\n\n![img](/img/70db7f87.jpg)\n\n# 二、优缺点\n\n每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。\n\n![img](/img/743e6cec.jpg)\n\n优点：低耦合。\n\n缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。\n\n \n\n# 三、部署ELK日志平台\n\nELK官网：\n\nhttps://www.elastic.co/cn/\n\n配置yum源参考：\n\nhttps://www.elastic.co/guide/en/logstash/current/installing-logstash.html\n\n## 3.1 安装JDK\n\n```java\n$ yum install -y java-1.8.0-openjdk \n```\n\n\n\n## 3.2 配置yum源\n\n```java\n[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n```\n\n\n\n## 3.3 安装ELK\n\n```java\n[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n```\n\n## 3.4 ELK相关配置\n\n这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。\n\n### 配置ES：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n```\n\n注意，ES需要优化一些内核参数：\n\n```java\n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n```\n\n### 运行：\n\n```java\n [root@qixiao1v zhengtianqi]# sysctl –p\n```\n\n生效配置。\n\n### 配置kibana：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: \"10.16.13.52\" \nserver.name: \"kibana\" \nelasticsearch.hosts: [\"http://10.16.13.52:9200\"] \nkibana.index: \".kibana\" \ni18n.locale: \"zh-CN\" \n```\n\n## 3.5 启动ES和Kibana\n\n```java\n[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n```\n\n访问kibana：[http://10.16.13.52:5601](http://10.16.13.52:5601/)\n\n \n\n# 四、采集k8s应用日志部署\n\n采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。\n\n## 4.1 部署Filebeat日志收集客户端\n\n### 4.1.1 编写Filebeat配置文件\n\n采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。\n\n```java\n[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          \"-c\", \"/etc/filebeat.yml\",\n          \"-e\",\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n```\n\n\n\n### 4.1.2 上传Filebeat配置文件\n\n```\n[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 <none> 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n```\n\n\n\n### 4.1.3 上传Filebeat配置文件是否成功\n\n检测配置的日志目录是否有挂载到Pod中：\n\n```java\n[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n```\n\n4.1.3过程也可以进入k8s管理页面 -> 命名空间选择 -> 配置与存储修改配置文件\n\n\n\n### 4.1.4 创建/修改pod，更新项目配置文件\n\n```java\n[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n```\n\n\n\n4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -> 工作负载 -> 部署 -> qixiao –> 右侧三个点 –> 查看 /编辑YAML -> 复制出来修改 -> 修改完点击更新\n\n## 4.2 配置Logstash接收日志\n\n### 4.2.1 配置logstash配置文件\n\n```java\n[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput {\n  beats {\n     port => 5044\n  }\n}\n\nfilter {\n}\n\noutput {\n    if [type] == \"module\" {\n        elasticsearch {\n            hosts => [\"http://127.0.0.1:9200\"]\n            index => \"k8s-log-%{+YYYY.MM.dd}\"\n        }\n    }\n    stdout { codec=> rubydebug }\n}\n```\n\n上述配置文件中的[type] == \"pipeline\"为filebeat-configmap.yaml中的fields:\n\ntype:\n\n### 4.2.2 启动/重启logstash\n\n```java\n[root@]# systemctl start logstash\n```\n\n注意：每次修改需要重启logstash\n\n```java\n[root@]# systemctl restart logstash\n```\n\n \n\n### 4.2.3 logstash部署是否成功\n\n```java\n# 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n```\n\n \n\n# 五、展示ELK日志\n\n## 5.1 配置Kibana展示日志\n\n左侧导航栏 -> 点击Management –> 点击Stack Management –> Kibana 索引模式 -> 创建索引模式 ）-> 时间字段@timestamp -> 创建成功\n\n![img](/img/1337b059.png)\n\n \n\n\n\n## 5.2 查看kibana日志\n\n左侧导航栏 -> Kibana -> Discover\n\n![img](/img/fb523dd6.png)\n\n![img](/img/e18c9709.jpg)\n\n \n\n \n\n## 5.3 绘制kibana图表\n\n左侧导航栏 -> Kibana -> dashboards –> 创建 仪表板 -> 新建\n\n \n\n以TSVB为例：\n\n面板选择 -> 索引模式-> 时间字段@timestamp\n\n![img](/img/480c47d9.png)\n\n![img](/img/8fb0cd0c.png)\n\n![img](/img/ea179a09.png)\n\n![img](/img/5a824e2f.png)\n\n参考文档：https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html","source":"_posts/k8s构建ELK日志平台.md","raw":"title: k8s构建ELK日志平台\nauthor: ztq\ntags:\n  - k8s\n  - elk\ncategories:\n  - CICD\n  - ''\ndate: 2021-04-13 14:40:00\n---\n\n\n\n# k8s构建ELK日志平台\n\n## Pod中附加专用日志收集的容器\n\n# 一、概述\n\n目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；\n\n日志收集方案设计图\n\n![img](/img/70db7f87.jpg)\n\n# 二、优缺点\n\n每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。\n\n![img](/img/743e6cec.jpg)\n\n优点：低耦合。\n\n缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。\n\n \n\n# 三、部署ELK日志平台\n\nELK官网：\n\nhttps://www.elastic.co/cn/\n\n配置yum源参考：\n\nhttps://www.elastic.co/guide/en/logstash/current/installing-logstash.html\n\n## 3.1 安装JDK\n\n```java\n$ yum install -y java-1.8.0-openjdk \n```\n\n\n\n## 3.2 配置yum源\n\n```java\n[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n```\n\n\n\n## 3.3 安装ELK\n\n```java\n[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n```\n\n## 3.4 ELK相关配置\n\n这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。\n\n### 配置ES：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n```\n\n注意，ES需要优化一些内核参数：\n\n```java\n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n```\n\n### 运行：\n\n```java\n [root@qixiao1v zhengtianqi]# sysctl –p\n```\n\n生效配置。\n\n### 配置kibana：\n\n```java\n[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: \"10.16.13.52\" \nserver.name: \"kibana\" \nelasticsearch.hosts: [\"http://10.16.13.52:9200\"] \nkibana.index: \".kibana\" \ni18n.locale: \"zh-CN\" \n```\n\n## 3.5 启动ES和Kibana\n\n```java\n[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n```\n\n访问kibana：[http://10.16.13.52:5601](http://10.16.13.52:5601/)\n\n \n\n# 四、采集k8s应用日志部署\n\n采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。\n\n## 4.1 部署Filebeat日志收集客户端\n\n### 4.1.1 编写Filebeat配置文件\n\n采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。\n\n```java\n[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          \"-c\", \"/etc/filebeat.yml\",\n          \"-e\",\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n```\n\n\n\n### 4.1.2 上传Filebeat配置文件\n\n```\n[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 <none> 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n```\n\n\n\n### 4.1.3 上传Filebeat配置文件是否成功\n\n检测配置的日志目录是否有挂载到Pod中：\n\n```java\n[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n```\n\n4.1.3过程也可以进入k8s管理页面 -> 命名空间选择 -> 配置与存储修改配置文件\n\n\n\n### 4.1.4 创建/修改pod，更新项目配置文件\n\n```java\n[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n```\n\n\n\n4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -> 工作负载 -> 部署 -> qixiao –> 右侧三个点 –> 查看 /编辑YAML -> 复制出来修改 -> 修改完点击更新\n\n## 4.2 配置Logstash接收日志\n\n### 4.2.1 配置logstash配置文件\n\n```java\n[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput {\n  beats {\n     port => 5044\n  }\n}\n\nfilter {\n}\n\noutput {\n    if [type] == \"module\" {\n        elasticsearch {\n            hosts => [\"http://127.0.0.1:9200\"]\n            index => \"k8s-log-%{+YYYY.MM.dd}\"\n        }\n    }\n    stdout { codec=> rubydebug }\n}\n```\n\n上述配置文件中的[type] == \"pipeline\"为filebeat-configmap.yaml中的fields:\n\ntype:\n\n### 4.2.2 启动/重启logstash\n\n```java\n[root@]# systemctl start logstash\n```\n\n注意：每次修改需要重启logstash\n\n```java\n[root@]# systemctl restart logstash\n```\n\n \n\n### 4.2.3 logstash部署是否成功\n\n```java\n# 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n```\n\n \n\n# 五、展示ELK日志\n\n## 5.1 配置Kibana展示日志\n\n左侧导航栏 -> 点击Management –> 点击Stack Management –> Kibana 索引模式 -> 创建索引模式 ）-> 时间字段@timestamp -> 创建成功\n\n![img](/img/1337b059.png)\n\n \n\n\n\n## 5.2 查看kibana日志\n\n左侧导航栏 -> Kibana -> Discover\n\n![img](/img/fb523dd6.png)\n\n![img](/img/e18c9709.jpg)\n\n \n\n \n\n## 5.3 绘制kibana图表\n\n左侧导航栏 -> Kibana -> dashboards –> 创建 仪表板 -> 新建\n\n \n\n以TSVB为例：\n\n面板选择 -> 索引模式-> 时间字段@timestamp\n\n![img](/img/480c47d9.png)\n\n![img](/img/8fb0cd0c.png)\n\n![img](/img/ea179a09.png)\n\n![img](/img/5a824e2f.png)\n\n参考文档：https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html","slug":"k8s构建ELK日志平台","published":1,"updated":"2021-04-13T07:09:01.143Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpkz005il0t9ed8ia0gm","content":"<h1>k8s构建ELK日志平台</h1>\n<h2 id=\"Pod中附加专用日志收集的容器\">Pod中附加专用日志收集的容器</h2>\n<h1>一、概述</h1>\n<p>目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；</p>\n<p>日志收集方案设计图</p>\n<p><img src=\"/img/70db7f87.jpg\" alt=\"img\"></p>\n<h1>二、优缺点</h1>\n<p>每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。</p>\n<p><img src=\"/img/743e6cec.jpg\" alt=\"img\"></p>\n<p>优点：低耦合。</p>\n<p>缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。</p>\n<h1>三、部署ELK日志平台</h1>\n<p>ELK官网：</p>\n<p><a href=\"https://www.elastic.co/cn/\">https://www.elastic.co/cn/</a></p>\n<p>配置yum源参考：</p>\n<p><a href=\"https://www.elastic.co/guide/en/logstash/current/installing-logstash.html\">https://www.elastic.co/guide/en/logstash/current/installing-logstash.html</a></p>\n<h2 id=\"3-1-安装JDK\">3.1 安装JDK</h2>\n<pre><code class=\"language-java\">$ yum install -y java-1.8.0-openjdk \n</code></pre>\n<h2 id=\"3-2-配置yum源\">3.2 配置yum源</h2>\n<pre><code class=\"language-java\">[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n</code></pre>\n<h2 id=\"3-3-安装ELK\">3.3 安装ELK</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n</code></pre>\n<h2 id=\"3-4-ELK相关配置\">3.4 ELK相关配置</h2>\n<p>这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。</p>\n<h3 id=\"配置ES：\">配置ES：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n</code></pre>\n<p>注意，ES需要优化一些内核参数：</p>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n</code></pre>\n<h3 id=\"运行：\">运行：</h3>\n<pre><code class=\"language-java\"> [root@qixiao1v zhengtianqi]# sysctl –p\n</code></pre>\n<p>生效配置。</p>\n<h3 id=\"配置kibana：\">配置kibana：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: &quot;10.16.13.52&quot; \nserver.name: &quot;kibana&quot; \nelasticsearch.hosts: [&quot;http://10.16.13.52:9200&quot;] \nkibana.index: &quot;.kibana&quot; \ni18n.locale: &quot;zh-CN&quot; \n</code></pre>\n<h2 id=\"3-5-启动ES和Kibana\">3.5 启动ES和Kibana</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n</code></pre>\n<p>访问kibana：<a href=\"http://10.16.13.52:5601/\">http://10.16.13.52:5601</a></p>\n<h1>四、采集k8s应用日志部署</h1>\n<p>采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。</p>\n<h2 id=\"4-1-部署Filebeat日志收集客户端\">4.1 部署Filebeat日志收集客户端</h2>\n<h3 id=\"4-1-1-编写Filebeat配置文件\">4.1.1 编写Filebeat配置文件</h3>\n<p>采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,\n          &quot;-e&quot;,\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n</code></pre>\n<h3 id=\"4-1-2-上传Filebeat配置文件\">4.1.2 上传Filebeat配置文件</h3>\n<pre><code>[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 &lt;none&gt; 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n</code></pre>\n<h3 id=\"4-1-3-上传Filebeat配置文件是否成功\">4.1.3 上传Filebeat配置文件是否成功</h3>\n<p>检测配置的日志目录是否有挂载到Pod中：</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n</code></pre>\n<p>4.1.3过程也可以进入k8s管理页面 -&gt; 命名空间选择 -&gt; 配置与存储修改配置文件</p>\n<h3 id=\"4-1-4-创建-修改pod，更新项目配置文件\">4.1.4 创建/修改pod，更新项目配置文件</h3>\n<pre><code class=\"language-java\">[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n</code></pre>\n<p>4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -&gt; 工作负载 -&gt; 部署 -&gt; qixiao –&gt; 右侧三个点 –&gt; 查看 /编辑YAML -&gt; 复制出来修改 -&gt; 修改完点击更新</p>\n<h2 id=\"4-2-配置Logstash接收日志\">4.2 配置Logstash接收日志</h2>\n<h3 id=\"4-2-1-配置logstash配置文件\">4.2.1 配置logstash配置文件</h3>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput &#123;\n  beats &#123;\n     port =&gt; 5044\n  &#125;\n&#125;\n\nfilter &#123;\n&#125;\n\noutput &#123;\n    if [type] == &quot;module&quot; &#123;\n        elasticsearch &#123;\n            hosts =&gt; [&quot;http://127.0.0.1:9200&quot;]\n            index =&gt; &quot;k8s-log-%&#123;+YYYY.MM.dd&#125;&quot;\n        &#125;\n    &#125;\n    stdout &#123; codec=&gt; rubydebug &#125;\n&#125;\n</code></pre>\n<p>上述配置文件中的[type] == &quot;pipeline&quot;为filebeat-configmap.yaml中的fields:</p>\n<p>type:</p>\n<h3 id=\"4-2-2-启动-重启logstash\">4.2.2 启动/重启logstash</h3>\n<pre><code class=\"language-java\">[root@]# systemctl start logstash\n</code></pre>\n<p>注意：每次修改需要重启logstash</p>\n<pre><code class=\"language-java\">[root@]# systemctl restart logstash\n</code></pre>\n<h3 id=\"4-2-3-logstash部署是否成功\">4.2.3 logstash部署是否成功</h3>\n<pre><code class=\"language-java\"># 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n</code></pre>\n<h1>五、展示ELK日志</h1>\n<h2 id=\"5-1-配置Kibana展示日志\">5.1 配置Kibana展示日志</h2>\n<p>左侧导航栏 -&gt; 点击Management –&gt; 点击Stack Management –&gt; Kibana 索引模式 -&gt; 创建索引模式 ）-&gt; 时间字段@timestamp -&gt; 创建成功</p>\n<p><img src=\"/img/1337b059.png\" alt=\"img\"></p>\n<h2 id=\"5-2-查看kibana日志\">5.2 查看kibana日志</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; Discover</p>\n<p><img src=\"/img/fb523dd6.png\" alt=\"img\"></p>\n<p><img src=\"/img/e18c9709.jpg\" alt=\"img\"></p>\n<h2 id=\"5-3-绘制kibana图表\">5.3 绘制kibana图表</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; dashboards –&gt; 创建 仪表板 -&gt; 新建</p>\n<p>以TSVB为例：</p>\n<p>面板选择 -&gt; 索引模式-&gt; 时间字段@timestamp</p>\n<p><img src=\"/img/480c47d9.png\" alt=\"img\"></p>\n<p><img src=\"/img/8fb0cd0c.png\" alt=\"img\"></p>\n<p><img src=\"/img/ea179a09.png\" alt=\"img\"></p>\n<p><img src=\"/img/5a824e2f.png\" alt=\"img\"></p>\n<p>参考文档：<a href=\"https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html\">https://nicksors.cc/2019/07/11/kubernetes系列之《k8s构建ELK日志平台》.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>k8s构建ELK日志平台</h1>\n<h2 id=\"Pod中附加专用日志收集的容器\">Pod中附加专用日志收集的容器</h2>\n<h1>一、概述</h1>\n<p>目前主流日志收集系统为：Filebeat + ELK，本文尝试使用该系统对k8s里部署的Pod进行日志收集并加以图形可视化展示；</p>\n<p>日志收集方案设计图</p>\n<p><img src=\"/img/70db7f87.jpg\" alt=\"img\"></p>\n<h1>二、优缺点</h1>\n<p>每个预应用程序的Pod中增加一个日志收集容器，使用emptyDir共享日志目录，让日志收集程序能够读取到。</p>\n<p><img src=\"/img/743e6cec.jpg\" alt=\"img\"></p>\n<p>优点：低耦合。</p>\n<p>缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本。</p>\n<h1>三、部署ELK日志平台</h1>\n<p>ELK官网：</p>\n<p><a href=\"https://www.elastic.co/cn/\">https://www.elastic.co/cn/</a></p>\n<p>配置yum源参考：</p>\n<p><a href=\"https://www.elastic.co/guide/en/logstash/current/installing-logstash.html\">https://www.elastic.co/guide/en/logstash/current/installing-logstash.html</a></p>\n<h2 id=\"3-1-安装JDK\">3.1 安装JDK</h2>\n<pre><code class=\"language-java\">$ yum install -y java-1.8.0-openjdk \n</code></pre>\n<h2 id=\"3-2-配置yum源\">3.2 配置yum源</h2>\n<pre><code class=\"language-java\">[zhengtianqi@root ~]# vim /etc/yum.repos.d/elk.repo \n[logstash-7.x]\nname=Elastic repository for 7.x packages \nbaseurl=https://artifacts.elastic.co/packages/7.x/yum \ngpgcheck=1 \ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch \nenabled=1 \nautorefresh=1 \ntype=rpm-md \n</code></pre>\n<h2 id=\"3-3-安装ELK\">3.3 安装ELK</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# sudo su root\n[root@qixiao1v zhengtianqi]# yum install -y logstash elasticsearch kibana \n</code></pre>\n<h2 id=\"3-4-ELK相关配置\">3.4 ELK相关配置</h2>\n<p>这里主要对ES和kibana的主配置文件进行配置，指定相关IP和端口等。</p>\n<h3 id=\"配置ES：\">配置ES：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/elasticsearch/elasticsearch.yml \n\npath.data: /var/lib/elasticsearch \n\npath.logs: /var/log/elasticsearch \n\nbootstrap.memory_lock: false \n\nnetwork.host: 0.0.0.0 \n\nhttp.port: 9200 \n\ndiscovery.type: single-node # 如果启动单节点，则需要添加此参数 \n</code></pre>\n<p>注意，ES需要优化一些内核参数：</p>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# vim /etc/security/limits.conf \n# End of file \n* soft nofile 60000 \n* hard nofile 65535 \n* soft nproc 65535 \n* hard nproc 65535 \n[root@qixiao1v zhengtianqi]# vim /etc/security/limits.d/20-nproc.conf \n* soft nproc 65535 \nroot soft nproc 65535 \n</code></pre>\n<h3 id=\"运行：\">运行：</h3>\n<pre><code class=\"language-java\"> [root@qixiao1v zhengtianqi]# sysctl –p\n</code></pre>\n<p>生效配置。</p>\n<h3 id=\"配置kibana：\">配置kibana：</h3>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# grep -Ev '^#|^$' /etc/kibana/kibana.yml \nserver.port: 5601 \nserver.host: &quot;10.16.13.52&quot; \nserver.name: &quot;kibana&quot; \nelasticsearch.hosts: [&quot;http://10.16.13.52:9200&quot;] \nkibana.index: &quot;.kibana&quot; \ni18n.locale: &quot;zh-CN&quot; \n</code></pre>\n<h2 id=\"3-5-启动ES和Kibana\">3.5 启动ES和Kibana</h2>\n<pre><code class=\"language-java\">[root@qixiao1v zhengtianqi]# systemctl start elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl enable elasticsearch \n[root@qixiao1v zhengtianqi]# systemctl start kibana \n[root@qixiao1v zhengtianqi]# systemctl enable kibana \n[root@qixiao1v zhengtianqi]# netstat -lntup|grep java \ntcp  0  0 0.0.0.0:9200  0.0.0.0:*  LISTEN  31645/java \ntcp  0  0 0.0.0.0:9300  0.0.0.0:*  LISTEN  31645/java \n</code></pre>\n<p>访问kibana：<a href=\"http://10.16.13.52:5601/\">http://10.16.13.52:5601</a></p>\n<h1>四、采集k8s应用日志部署</h1>\n<p>采集日志客户端采用Filebeat来进行采集日志，使用ConfigMap的形式来存储Filebeat的配置，采用ConfigMap形式部署Filebeat，然后将配置文件和日志挂载到Filebeat的Pod中，利用Filebeat采集k8s的集群日志。</p>\n<h2 id=\"4-1-部署Filebeat日志收集客户端\">4.1 部署Filebeat日志收集客户端</h2>\n<h3 id=\"4-1-1-编写Filebeat配置文件\">4.1.1 编写Filebeat配置文件</h3>\n<p>采用ConfigMap来保存Filebeat的配置文件，然后启动Pod时挂载到Pod里的容器里。</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat k8s-logs.yaml\napiVersion: v1\nkind: ConfigMap # 保存Filebeat的配置信息\nmetadata:\n  name: k8s-logs-filebeat-config\n  namespace: kube-system\n\ndata:\n  filebeat.yml: |-\n    filebeat.prospectors:\n      - type: log\n        paths:\n          - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n        fields:\n          app: k8s\n          type: module\n        fields_under_root: true\n\n    output.logstash:\n      hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n\n---\n\napiVersion: apps/v1\nkind: DaemonSet # 使用DaemonSet方式将Filebeat部署到集群每个节点上\nmetadata:\n  name: k8s-logs # Pod名称\n  namespace: kube-system # 指定运行命名空间\nspec:\n  selector:\n    matchLabels:\n      project: k8s\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        project: k8s\n        app: filebeat\n    spec:\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.4.2\n        args: [\n          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,\n          &quot;-e&quot;,\n        ]\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: filebeat-config\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: k8s-logs\n          mountPath: /opt/kubernetes/logs\n      volumes:\n      - name: k8s-logs\n        hostPath:\n          path: /opt/kubernetes/logs # 将主机上的目录挂载到Pod容器里\n      - name: filebeat-config\n        configMap:\n          name: k8s-logs-filebeat-config # 指定configmap挂载到Pod容器里\n</code></pre>\n<h3 id=\"4-1-2-上传Filebeat配置文件\">4.1.2 上传Filebeat配置文件</h3>\n<pre><code>[root@k8s-master-128 elk]# kubectl create -f k8s-logs.yaml\nconfigmap/k8s-logs-filebeat-config created\ndaemonset.apps/k8s-logs created\n[root@k8s-master-128 elk]# kubectl get -f k8s-logs.yaml\nNAME DATA AGE\nconfigmap/k8s-logs-filebeat-config 1 6s\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\ndaemonset.apps/k8s-logs 2 2 0 2 0 &lt;none&gt; 5s\n\n[root@k8s-master-128 elk]# kubectl get pod -A|grep k8s-log\nkube-system k8s-logs-7wwlx 1/1 Running 0 5m\nkube-system k8s-logs-pd8m2 1/1 Running 0 5m\n</code></pre>\n<h3 id=\"4-1-3-上传Filebeat配置文件是否成功\">4.1.3 上传Filebeat配置文件是否成功</h3>\n<p>检测配置的日志目录是否有挂载到Pod中：</p>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# kubectl exec -it -n kube-system k8s-logs-7wwlx bash\n[root@k8s-logs-7wwlx filebeat]# ls -lh /opt/kubernetes/logs/ -d\ndrwxr-xr-x 2 root root 8.0K Jun 3 06:44 /opt/kubernetes/logs/\n[root@k8s-logs-7wwlx filebeat]# cat /etc/filebeat.yml\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /opt/kubernetes/logs/* # 指定k8s集群的采集日志目录，星号匹配所有该目录下的文件\n    fields:\n      app: k8s\n      type: module\n    fields_under_root: true\n\noutput.logstash:\n  hosts: ['172.16.194.128:5044'] # 这里写logstash启动的监听地址和端口\n</code></pre>\n<p>4.1.3过程也可以进入k8s管理页面 -&gt; 命名空间选择 -&gt; 配置与存储修改配置文件</p>\n<h3 id=\"4-1-4-创建-修改pod，更新项目配置文件\">4.1.4 创建/修改pod，更新项目配置文件</h3>\n<pre><code class=\"language-java\">[root@]# kubectl get pod,deploy -n root \nNAME                 READY  STATUS  RESTARTS  AGE \npod/qixiao-569bf65846-bdrd4     3/3   Running  0     3d22h \npod/qixiao-569bf65846-swwlj     3/3   Running  0     3d22h \npod/qixiao-569bf65846-xr9v7     3/3   Running  0     3d22h \npod/qixiao-socket-7d7dfcff76-6kqwt  1/1   Running  0     25d \nNAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \ndeployment.extensions/root  3  3  3  3  592d \n[root@]# kubectl edit deployment.extensions/root -n root\n</code></pre>\n<p>4.1.4过程也可以进入k8s管理页面：命名空间选择qixiao -&gt; 工作负载 -&gt; 部署 -&gt; qixiao –&gt; 右侧三个点 –&gt; 查看 /编辑YAML -&gt; 复制出来修改 -&gt; 修改完点击更新</p>\n<h2 id=\"4-2-配置Logstash接收日志\">4.2 配置Logstash接收日志</h2>\n<h3 id=\"4-2-1-配置logstash配置文件\">4.2.1 配置logstash配置文件</h3>\n<pre><code class=\"language-java\">[root@k8s-master-128 elk]# cat filebeat-to-logstash.conf\ninput &#123;\n  beats &#123;\n     port =&gt; 5044\n  &#125;\n&#125;\n\nfilter &#123;\n&#125;\n\noutput &#123;\n    if [type] == &quot;module&quot; &#123;\n        elasticsearch &#123;\n            hosts =&gt; [&quot;http://127.0.0.1:9200&quot;]\n            index =&gt; &quot;k8s-log-%&#123;+YYYY.MM.dd&#125;&quot;\n        &#125;\n    &#125;\n    stdout &#123; codec=&gt; rubydebug &#125;\n&#125;\n</code></pre>\n<p>上述配置文件中的[type] == &quot;pipeline&quot;为filebeat-configmap.yaml中的fields:</p>\n<p>type:</p>\n<h3 id=\"4-2-2-启动-重启logstash\">4.2.2 启动/重启logstash</h3>\n<pre><code class=\"language-java\">[root@]# systemctl start logstash\n</code></pre>\n<p>注意：每次修改需要重启logstash</p>\n<pre><code class=\"language-java\">[root@]# systemctl restart logstash\n</code></pre>\n<h3 id=\"4-2-3-logstash部署是否成功\">4.2.3 logstash部署是否成功</h3>\n<pre><code class=\"language-java\"># 调试启动\n[root@k8s-master-128 elk]# /usr/share/logstash/bin/logstash -f filebeat-to-logstash.conf\n\n# 守护程序启动：需要编辑配置文件，去掉stdout配置\n[root@k8s-master-128 elk]# cp filebeat-to-logstash.conf /etc/logstash/conf.d/logstash.conf\n[root@k8s-master-128 elk]# systemctl start logstash\n\n[root@k8s-master-128 ~]# netstat -lntup|grep java\ntcp6 0 0 :::9200 :::* LISTEN 119229/java\ntcp6 0 0 :::5044 :::* LISTEN 710/java\ntcp6 0 0 :::9300 :::* LISTEN 119229/java\ntcp6 0 0 127.0.0.1:9600 :::* LISTEN 710/java\n</code></pre>\n<h1>五、展示ELK日志</h1>\n<h2 id=\"5-1-配置Kibana展示日志\">5.1 配置Kibana展示日志</h2>\n<p>左侧导航栏 -&gt; 点击Management –&gt; 点击Stack Management –&gt; Kibana 索引模式 -&gt; 创建索引模式 ）-&gt; 时间字段@timestamp -&gt; 创建成功</p>\n<p><img src=\"/img/1337b059.png\" alt=\"img\"></p>\n<h2 id=\"5-2-查看kibana日志\">5.2 查看kibana日志</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; Discover</p>\n<p><img src=\"/img/fb523dd6.png\" alt=\"img\"></p>\n<p><img src=\"/img/e18c9709.jpg\" alt=\"img\"></p>\n<h2 id=\"5-3-绘制kibana图表\">5.3 绘制kibana图表</h2>\n<p>左侧导航栏 -&gt; Kibana -&gt; dashboards –&gt; 创建 仪表板 -&gt; 新建</p>\n<p>以TSVB为例：</p>\n<p>面板选择 -&gt; 索引模式-&gt; 时间字段@timestamp</p>\n<p><img src=\"/img/480c47d9.png\" alt=\"img\"></p>\n<p><img src=\"/img/8fb0cd0c.png\" alt=\"img\"></p>\n<p><img src=\"/img/ea179a09.png\" alt=\"img\"></p>\n<p><img src=\"/img/5a824e2f.png\" alt=\"img\"></p>\n<p>参考文档：<a href=\"https://nicksors.cc/2019/07/11/kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8Ak8s%E6%9E%84%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E3%80%8B.html\">https://nicksors.cc/2019/07/11/kubernetes系列之《k8s构建ELK日志平台》.html</a></p>\n"},{"title":"String、 StringBuilder、StringBuffer区别","author":"郑天祺","date":"2020-07-21T07:43:00.000Z","_content":"\n# 1、介绍\n\n## （1）运行速度\n\n​\t\tStringBuilder > StringBuffer > String\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。\n\n​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。\n\n![image-20200721154908228](/img/StringUpdate.png)\n\n​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。\n\n## （2）线程安全\n\n　　StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的\n\n## （3）继承关系\n\n![image-20200721155019047](/img/StringStringBuilderStringBuffer.png)\n\n# 2、对比\n\n![image-20200721154514018](/img/StringBuilder.png)\n\n\n\n参考：https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303","source":"_posts/l-String、-StringBuilder、StringBuffer区别.md","raw":"title: String、 StringBuilder、StringBuffer区别\nauthor: 郑天祺\ntags:\n  - String\n  - StringBuilder\n  - StringBuffer\ncategories:\n  - java基础\ndate: 2020-07-21 15:43:00\n\n---\n\n# 1、介绍\n\n## （1）运行速度\n\n​\t\tStringBuilder > StringBuffer > String\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。\n\n​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。\n\n![image-20200721154908228](/img/StringUpdate.png)\n\n​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。\n\n## （2）线程安全\n\n　　StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的\n\n## （3）继承关系\n\n![image-20200721155019047](/img/StringStringBuilderStringBuffer.png)\n\n# 2、对比\n\n![image-20200721154514018](/img/StringBuilder.png)\n\n\n\n参考：https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303","slug":"l-String、-StringBuilder、StringBuffer区别","published":1,"updated":"2020-07-21T07:51:47.729Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl0005ll0t9feli2ora","content":"<h1>1、介绍</h1>\n<h2 id=\"（1）运行速度\">（1）运行速度</h2>\n<p>​\t\tStringBuilder &gt; StringBuffer &gt; String<br>\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。</p>\n<p>​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。</p>\n<p><img src=\"/img/StringUpdate.png\" alt=\"image-20200721154908228\"></p>\n<p>​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。</p>\n<h2 id=\"（2）线程安全\">（2）线程安全</h2>\n<p>StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的</p>\n<h2 id=\"（3）继承关系\">（3）继承关系</h2>\n<p><img src=\"/img/StringStringBuilderStringBuffer.png\" alt=\"image-20200721155019047\"></p>\n<h1>2、对比</h1>\n<p><img src=\"/img/StringBuilder.png\" alt=\"image-20200721154514018\"></p>\n<p>参考：<a href=\"https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303\">https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<h2 id=\"（1）运行速度\">（1）运行速度</h2>\n<p>​\t\tStringBuilder &gt; StringBuffer &gt; String<br>\n​\t\tString为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。</p>\n<p>​\t\t因为String修改其实是new了一个新对象，原来的String被JVM的垃圾回收机制（GC）给回收掉了。</p>\n<p><img src=\"/img/StringUpdate.png\" alt=\"image-20200721154908228\"></p>\n<p>​\t\tJava中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。</p>\n<h2 id=\"（2）线程安全\">（2）线程安全</h2>\n<p>StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的</p>\n<h2 id=\"（3）继承关系\">（3）继承关系</h2>\n<p><img src=\"/img/StringStringBuilderStringBuffer.png\" alt=\"image-20200721155019047\"></p>\n<h1>2、对比</h1>\n<p><img src=\"/img/StringBuilder.png\" alt=\"image-20200721154514018\"></p>\n<p>参考：<a href=\"https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303\">https://blog.csdn.net/itchuxuezhe_yang/article/details/89966303</a></p>\n"},{"title":"maven梳理","author":"郑天祺","date":"2019-08-28T09:01:00.000Z","_content":"# Maven使用\n\n## maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n## 1、Maven的简介  \n\n### 1.1 构建（build）\n\n除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。\n\n### 1.2 Maven的用途\n\n自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。\n\n## 2、Maven的安装和配置 \n\n### 2.1 Maven怎么升级：\n\n解压新的maven到一个目录，只需更新系统变量指向它。\n\n### 2.2 Maven目录介绍：\n\n（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）\n\n1. Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。\n\n2. Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。\n\n3. Conf目录：settings.xml可以在机器上定义全局的行为。\n\n4. Lib类库\n\n默认情况下：\n\n大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml\n\n### 2.3 设置HTTP代理\n\n有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。\n\n### 2.4 设置MAVEN_OPTS环境变量\n\n目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。\n\n###    2.5 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​ Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t2.6 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n\n## 3、使用入门\n\n### 3.1 编写pom.xml\n\nMaven的核心是pom.xml\n\n![1618244002479](/img/1618244002479.png)\n\n第三方工具可以快速构建pom.xml的头\n\nProject是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。\n\n最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。\n\nGroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。\n\nArtifactId定义了当前Maven项目在组中唯一的ID，子模块\n\nVersion指定了版本\n\n### 3.2编写主代码，项目打包过程\n\n此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install\n\n#### 3.2.1使用maven编译项目\n\n当我们编写一个main调用sayHello()打印helloworld字符串时。\n\n该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。\n\n当编码完毕，使用Maven进行编译，\n\n在项目根目录下运行命令mvn clean compile，\n\nmvn clean compile运行步骤：\n\n![1618244073306](/img/1618244073306.png)\n\n（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；\n\n（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；\n\n（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录\n\n####  3.2.2 使用maven编译测试类\n\n编写完测试用例运行命令mvn clean test\n\n需要maven-compiler-plugin插件\n\n![1618244110010](/img/1618244110010.png)\n\nmvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少  \n\n![1618244137853](/img/1618244137853.png)\n\n#### 3.2.3 使用maven将项目打包和运行\n\nmvn clean package进行打包，可以看到target下生成jar，\n\n它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。\n\n![1618244166891](/img/1618244166891.png)\n\n#### 3.2.4 使用maven运行带main方法的类\n\n目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置\n\n​         maven-shade-plug \n\n![1618244193984](/img/1618244193984.png)\n\n#### 3.2.5 将项目打包放到本地maven仓库\n\nmvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址\n\n![1618244251013](/img/1618244251013.png)\n\n## 4、坐标和依赖\n\n### 4.1 坐标详解\n\nMaven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier\n\n![1618244289269](/img/1618244289269.png)\n\n#### 4.1.1 groupId定义到项目\n\n与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应\n\n#### 4.1.2 artifactId定义到项目其中的一个模块\n\n​    为了方便区分，便于寻找实际构件，用nexus作为前缀\n\n#### 4.1.3 version为版本\n\n#### 4.1.4 packaging为该项目的打包方式，默认jar（可选）\n\n#### 4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）\n\n## 5、依赖\n\n### 5.1 依赖范围\n\n例：Junit依赖的测试范围test， 测试范围用元素scope表示\n\nMaven在编译项目主代码时候使用一套classpath\n\n在编译和执行测试的时候回使用另一套classpath\n\n依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：\n\n  Compile：编译依赖范围，三种classpath都有效(默认)\n\n  Test：测试依赖范围\n\n  Provided：已提供依赖范围（编译和测试）\n\n  Runtime：运行时依赖规范 \n\n  System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）\n\n![1618244361109](/img/1618244361109.png)\n\n### 5.2 传递性依赖\n\n有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。\n\nA依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期\n\n![1618244387386](/img/1618244387386.png)\n\n### 5.3 排除依赖\n\n#### 5.3.1引入相同版本的依赖\n\n像framework定义version\n\n```java\n<properties>\n\t <springframework.version>5.2.2</springframework.version>\n</properties>\n```\n\n#### 5.3.2 依赖优化\n\nmvn dependency:list 查看当前项目已解析的依赖\n\nmvn dependency:tree 查看当前项目的依赖树\n\nmvn dependency:analyze 帮助分析当前项目的依赖\n\n## 6、仓库\n\n### 6.1概念\n\n任何一个依赖，插件或者项目构建的输出，都可以称为构件\n\n### 6.2 仓库的布局\n\n仓库布局的源码，是基于简单的文件系统\n\n### 6.3 仓库的分类  \n\n仓库的配置，中央仓库、远程仓库\n\n分类：\n\n![1618244497271](/img/1618244497271.png)\n\n### 6.4 仓库的配置\n\n#### 6.4.1设置仓库\n\n（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。\n\n![1618244522945](/img/1618244522945.png)\n\n（2）配置maven 更新频率和检查文件策略\n\n![1618244544994](/img/1618244544994.png)\n\n（3）Maven的认证\n\n![1618244566899](/img/1618244566899.png)\n\n#### 6.4.2 上传构件到私有仓库\n\n（1）首先配置好distributionManagement配置\n\ndistributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。\n\n![1618244592782](/img/1618244592782.png)\n\n（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。\n\n#### 6.4.3 Maven版本号机制  \n\n![1618244623527](/img/1618244623527.png)\n\n版本号(version number)是版本的标识号。\n\n1.版本命名规范\n\n软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号\n\n2.软件版本阶段说明\n\n3.版本号修改规则\n\n（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。\n\n（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。\n\n（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。\n\n（4）日期版本号：发版当天的日期，需要包括年份。如：20160617 ","source":"_posts/maven梳理.md","raw":"title: maven梳理\ntags:\n\n  - maven\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-28 17:01:00\n---\n# Maven使用\n\n## maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n## 1、Maven的简介  \n\n### 1.1 构建（build）\n\n除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。\n\n### 1.2 Maven的用途\n\n自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。\n\n## 2、Maven的安装和配置 \n\n### 2.1 Maven怎么升级：\n\n解压新的maven到一个目录，只需更新系统变量指向它。\n\n### 2.2 Maven目录介绍：\n\n（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）\n\n1. Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。\n\n2. Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。\n\n3. Conf目录：settings.xml可以在机器上定义全局的行为。\n\n4. Lib类库\n\n默认情况下：\n\n大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml\n\n### 2.3 设置HTTP代理\n\n有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。\n\n### 2.4 设置MAVEN_OPTS环境变量\n\n目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。\n\n###    2.5 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​ Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t2.6 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n\n## 3、使用入门\n\n### 3.1 编写pom.xml\n\nMaven的核心是pom.xml\n\n![1618244002479](/img/1618244002479.png)\n\n第三方工具可以快速构建pom.xml的头\n\nProject是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。\n\n最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。\n\nGroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。\n\nArtifactId定义了当前Maven项目在组中唯一的ID，子模块\n\nVersion指定了版本\n\n### 3.2编写主代码，项目打包过程\n\n此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install\n\n#### 3.2.1使用maven编译项目\n\n当我们编写一个main调用sayHello()打印helloworld字符串时。\n\n该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。\n\n当编码完毕，使用Maven进行编译，\n\n在项目根目录下运行命令mvn clean compile，\n\nmvn clean compile运行步骤：\n\n![1618244073306](/img/1618244073306.png)\n\n（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；\n\n（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；\n\n（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录\n\n####  3.2.2 使用maven编译测试类\n\n编写完测试用例运行命令mvn clean test\n\n需要maven-compiler-plugin插件\n\n![1618244110010](/img/1618244110010.png)\n\nmvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少  \n\n![1618244137853](/img/1618244137853.png)\n\n#### 3.2.3 使用maven将项目打包和运行\n\nmvn clean package进行打包，可以看到target下生成jar，\n\n它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。\n\n![1618244166891](/img/1618244166891.png)\n\n#### 3.2.4 使用maven运行带main方法的类\n\n目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置\n\n​         maven-shade-plug \n\n![1618244193984](/img/1618244193984.png)\n\n#### 3.2.5 将项目打包放到本地maven仓库\n\nmvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址\n\n![1618244251013](/img/1618244251013.png)\n\n## 4、坐标和依赖\n\n### 4.1 坐标详解\n\nMaven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier\n\n![1618244289269](/img/1618244289269.png)\n\n#### 4.1.1 groupId定义到项目\n\n与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应\n\n#### 4.1.2 artifactId定义到项目其中的一个模块\n\n​    为了方便区分，便于寻找实际构件，用nexus作为前缀\n\n#### 4.1.3 version为版本\n\n#### 4.1.4 packaging为该项目的打包方式，默认jar（可选）\n\n#### 4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）\n\n## 5、依赖\n\n### 5.1 依赖范围\n\n例：Junit依赖的测试范围test， 测试范围用元素scope表示\n\nMaven在编译项目主代码时候使用一套classpath\n\n在编译和执行测试的时候回使用另一套classpath\n\n依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：\n\n  Compile：编译依赖范围，三种classpath都有效(默认)\n\n  Test：测试依赖范围\n\n  Provided：已提供依赖范围（编译和测试）\n\n  Runtime：运行时依赖规范 \n\n  System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）\n\n![1618244361109](/img/1618244361109.png)\n\n### 5.2 传递性依赖\n\n有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。\n\nA依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期\n\n![1618244387386](/img/1618244387386.png)\n\n### 5.3 排除依赖\n\n#### 5.3.1引入相同版本的依赖\n\n像framework定义version\n\n```java\n<properties>\n\t <springframework.version>5.2.2</springframework.version>\n</properties>\n```\n\n#### 5.3.2 依赖优化\n\nmvn dependency:list 查看当前项目已解析的依赖\n\nmvn dependency:tree 查看当前项目的依赖树\n\nmvn dependency:analyze 帮助分析当前项目的依赖\n\n## 6、仓库\n\n### 6.1概念\n\n任何一个依赖，插件或者项目构建的输出，都可以称为构件\n\n### 6.2 仓库的布局\n\n仓库布局的源码，是基于简单的文件系统\n\n### 6.3 仓库的分类  \n\n仓库的配置，中央仓库、远程仓库\n\n分类：\n\n![1618244497271](/img/1618244497271.png)\n\n### 6.4 仓库的配置\n\n#### 6.4.1设置仓库\n\n（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。\n\n![1618244522945](/img/1618244522945.png)\n\n（2）配置maven 更新频率和检查文件策略\n\n![1618244544994](/img/1618244544994.png)\n\n（3）Maven的认证\n\n![1618244566899](/img/1618244566899.png)\n\n#### 6.4.2 上传构件到私有仓库\n\n（1）首先配置好distributionManagement配置\n\ndistributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。\n\n![1618244592782](/img/1618244592782.png)\n\n（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。\n\n#### 6.4.3 Maven版本号机制  \n\n![1618244623527](/img/1618244623527.png)\n\n版本号(version number)是版本的标识号。\n\n1.版本命名规范\n\n软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号\n\n2.软件版本阶段说明\n\n3.版本号修改规则\n\n（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。\n\n（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。\n\n（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。\n\n（4）日期版本号：发版当天的日期，需要包括年份。如：20160617 ","slug":"maven梳理","published":1,"updated":"2021-04-12T16:25:36.944Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl1005ol0t9batc4ph2","content":"<h1>Maven使用</h1>\n<h2 id=\"maven的命令：\">maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n<h2 id=\"1、Maven的简介\">1、Maven的简介</h2>\n<h3 id=\"1-1-构建（build）\">1.1 构建（build）</h3>\n<p>除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。</p>\n<h3 id=\"1-2-Maven的用途\">1.2 Maven的用途</h3>\n<p>自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。<br>\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。</p>\n<h2 id=\"2、Maven的安装和配置\">2、Maven的安装和配置</h2>\n<h3 id=\"2-1-Maven怎么升级：\">2.1 Maven怎么升级：</h3>\n<p>解压新的maven到一个目录，只需更新系统变量指向它。</p>\n<h3 id=\"2-2-Maven目录介绍：\">2.2 Maven目录介绍：</h3>\n<p>（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）</p>\n<ol>\n<li>\n<p>Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。</p>\n</li>\n<li>\n<p>Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。</p>\n</li>\n<li>\n<p>Conf目录：settings.xml可以在机器上定义全局的行为。</p>\n</li>\n<li>\n<p>Lib类库</p>\n</li>\n</ol>\n<p>默认情况下：</p>\n<p>大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml</p>\n<h3 id=\"2-3-设置HTTP代理\">2.3 设置HTTP代理</h3>\n<p>有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。</p>\n<h3 id=\"2-4-设置MAVEN-OPTS环境变量\">2.4 设置MAVEN_OPTS环境变量</h3>\n<p>目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。</p>\n<h3 id=\"2-5-参数设置：\">2.5 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）<br>\n​ Windows：如下图<br>\n​\t\t<img src=\"/img/maven配置.png\"><br>\n​</p>\n<h3 id=\"2-6-用户配置：\">2.6 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2 id=\"3、使用入门\">3、使用入门</h2>\n<h3 id=\"3-1-编写pom-xml\">3.1 编写pom.xml</h3>\n<p>Maven的核心是pom.xml</p>\n<p><img src=\"/img/1618244002479.png\" alt=\"1618244002479\"></p>\n<p>第三方工具可以快速构建pom.xml的头</p>\n<p>Project是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。</p>\n<p>最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。</p>\n<p>GroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。</p>\n<p>ArtifactId定义了当前Maven项目在组中唯一的ID，子模块</p>\n<p>Version指定了版本</p>\n<h3 id=\"3-2编写主代码，项目打包过程\">3.2编写主代码，项目打包过程</h3>\n<p>此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install</p>\n<h4 id=\"3-2-1使用maven编译项目\">3.2.1使用maven编译项目</h4>\n<p>当我们编写一个main调用sayHello()打印helloworld字符串时。</p>\n<p>该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。</p>\n<p>当编码完毕，使用Maven进行编译，</p>\n<p>在项目根目录下运行命令mvn clean compile，</p>\n<p>mvn clean compile运行步骤：</p>\n<p><img src=\"/img/1618244073306.png\" alt=\"1618244073306\"></p>\n<p>（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；</p>\n<p>（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；</p>\n<p>（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录</p>\n<h4 id=\"3-2-2-使用maven编译测试类\">3.2.2 使用maven编译测试类</h4>\n<p>编写完测试用例运行命令mvn clean test</p>\n<p>需要maven-compiler-plugin插件</p>\n<p><img src=\"/img/1618244110010.png\" alt=\"1618244110010\"></p>\n<p>mvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少</p>\n<p><img src=\"/img/1618244137853.png\" alt=\"1618244137853\"></p>\n<h4 id=\"3-2-3-使用maven将项目打包和运行\">3.2.3 使用maven将项目打包和运行</h4>\n<p>mvn clean package进行打包，可以看到target下生成jar，</p>\n<p>它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。</p>\n<p><img src=\"/img/1618244166891.png\" alt=\"1618244166891\"></p>\n<h4 id=\"3-2-4-使用maven运行带main方法的类\">3.2.4 使用maven运行带main方法的类</h4>\n<p>目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置</p>\n<p>​         maven-shade-plug</p>\n<p><img src=\"/img/1618244193984.png\" alt=\"1618244193984\"></p>\n<h4 id=\"3-2-5-将项目打包放到本地maven仓库\">3.2.5 将项目打包放到本地maven仓库</h4>\n<p>mvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址</p>\n<p><img src=\"/img/1618244251013.png\" alt=\"1618244251013\"></p>\n<h2 id=\"4、坐标和依赖\">4、坐标和依赖</h2>\n<h3 id=\"4-1-坐标详解\">4.1 坐标详解</h3>\n<p>Maven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier</p>\n<p><img src=\"/img/1618244289269.png\" alt=\"1618244289269\"></p>\n<h4 id=\"4-1-1-groupId定义到项目\">4.1.1 groupId定义到项目</h4>\n<p>与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应</p>\n<h4 id=\"4-1-2-artifactId定义到项目其中的一个模块\">4.1.2 artifactId定义到项目其中的一个模块</h4>\n<p>​    为了方便区分，便于寻找实际构件，用nexus作为前缀</p>\n<h4 id=\"4-1-3-version为版本\">4.1.3 version为版本</h4>\n<h4 id=\"4-1-4-packaging为该项目的打包方式，默认jar（可选）\">4.1.4 packaging为该项目的打包方式，默认jar（可选）</h4>\n<h4 id=\"4-1-5-classifier为（可选，不能直接定义，由附加插件帮助生成）\">4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）</h4>\n<h2 id=\"5、依赖\">5、依赖</h2>\n<h3 id=\"5-1-依赖范围\">5.1 依赖范围</h3>\n<p>例：Junit依赖的测试范围test， 测试范围用元素scope表示</p>\n<p>Maven在编译项目主代码时候使用一套classpath</p>\n<p>在编译和执行测试的时候回使用另一套classpath</p>\n<p>依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：</p>\n<p>Compile：编译依赖范围，三种classpath都有效(默认)</p>\n<p>Test：测试依赖范围</p>\n<p>Provided：已提供依赖范围（编译和测试）</p>\n<p>Runtime：运行时依赖规范</p>\n<p>System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）</p>\n<p><img src=\"/img/1618244361109.png\" alt=\"1618244361109\"></p>\n<h3 id=\"5-2-传递性依赖\">5.2 传递性依赖</h3>\n<p>有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。</p>\n<p>A依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期</p>\n<p><img src=\"/img/1618244387386.png\" alt=\"1618244387386\"></p>\n<h3 id=\"5-3-排除依赖\">5.3 排除依赖</h3>\n<h4 id=\"5-3-1引入相同版本的依赖\">5.3.1引入相同版本的依赖</h4>\n<p>像framework定义version</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t &lt;springframework.version&gt;5.2.2&lt;/springframework.version&gt;\n&lt;/properties&gt;\n</code></pre>\n<h4 id=\"5-3-2-依赖优化\">5.3.2 依赖优化</h4>\n<p>mvn dependency:list 查看当前项目已解析的依赖</p>\n<p>mvn dependency:tree 查看当前项目的依赖树</p>\n<p>mvn dependency:analyze 帮助分析当前项目的依赖</p>\n<h2 id=\"6、仓库\">6、仓库</h2>\n<h3 id=\"6-1概念\">6.1概念</h3>\n<p>任何一个依赖，插件或者项目构建的输出，都可以称为构件</p>\n<h3 id=\"6-2-仓库的布局\">6.2 仓库的布局</h3>\n<p>仓库布局的源码，是基于简单的文件系统</p>\n<h3 id=\"6-3-仓库的分类\">6.3 仓库的分类</h3>\n<p>仓库的配置，中央仓库、远程仓库</p>\n<p>分类：</p>\n<p><img src=\"/img/1618244497271.png\" alt=\"1618244497271\"></p>\n<h3 id=\"6-4-仓库的配置\">6.4 仓库的配置</h3>\n<h4 id=\"6-4-1设置仓库\">6.4.1设置仓库</h4>\n<p>（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。</p>\n<p><img src=\"/img/1618244522945.png\" alt=\"1618244522945\"></p>\n<p>（2）配置maven 更新频率和检查文件策略</p>\n<p><img src=\"/img/1618244544994.png\" alt=\"1618244544994\"></p>\n<p>（3）Maven的认证</p>\n<p><img src=\"/img/1618244566899.png\" alt=\"1618244566899\"></p>\n<h4 id=\"6-4-2-上传构件到私有仓库\">6.4.2 上传构件到私有仓库</h4>\n<p>（1）首先配置好distributionManagement配置</p>\n<p>distributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。</p>\n<p><img src=\"/img/1618244592782.png\" alt=\"1618244592782\"></p>\n<p>（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。</p>\n<h4 id=\"6-4-3-Maven版本号机制\">6.4.3 Maven版本号机制</h4>\n<p><img src=\"/img/1618244623527.png\" alt=\"1618244623527\"></p>\n<p>版本号(version number)是版本的标识号。</p>\n<p>1.版本命名规范</p>\n<p>软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号</p>\n<p>2.软件版本阶段说明</p>\n<p>3.版本号修改规则</p>\n<p>（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。</p>\n<p>（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。</p>\n<p>（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。</p>\n<p>（4）日期版本号：发版当天的日期，需要包括年份。如：20160617</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Maven使用</h1>\n<h2 id=\"maven的命令：\">maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n<h2 id=\"1、Maven的简介\">1、Maven的简介</h2>\n<h3 id=\"1-1-构建（build）\">1.1 构建（build）</h3>\n<p>除了编写源代码，一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。于是有人使用使用软件只需简单的一条命令，就能自动完成。</p>\n<h3 id=\"1-2-Maven的用途\">1.2 Maven的用途</h3>\n<p>自动化构建过程、清理、编译、测试到生成报告，再到打包和部署。<br>\n依赖增加、版本不一致、版本冲突、依赖臃肿等问题：Maven通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个Java类库（如jar文件）。类似于经纬度定位。</p>\n<h2 id=\"2、Maven的安装和配置\">2、Maven的安装和配置</h2>\n<h3 id=\"2-1-Maven怎么升级：\">2.1 Maven怎么升级：</h3>\n<p>解压新的maven到一个目录，只需更新系统变量指向它。</p>\n<h3 id=\"2-2-Maven目录介绍：\">2.2 Maven目录介绍：</h3>\n<p>（bin boot conf lib LINCENSE. txt NOTICE. txt README.txt）</p>\n<ol>\n<li>\n<p>Bin目录：mvn运行脚本（mvn是unix的shell脚本，mvn.bat是windows版），这些脚本是用来配置java命令的。</p>\n</li>\n<li>\n<p>Boot目录： 里只有一个jar包，plexus-classworlds-2.5.2.jar是maven加载类库。</p>\n</li>\n<li>\n<p>Conf目录：settings.xml可以在机器上定义全局的行为。</p>\n</li>\n<li>\n<p>Lib类库</p>\n</li>\n</ol>\n<p>默认情况下：</p>\n<p>大多数人需要把M2_HOME/conf/settings.xml复制到~/.m2/settings.xml</p>\n<h3 id=\"2-3-设置HTTP代理\">2.3 设置HTTP代理</h3>\n<p>有些公司为了安全无法访问公共的Maven中央仓库，需要设置代理，必须保证代理服务器的通畅。</p>\n<h3 id=\"2-4-设置MAVEN-OPTS环境变量\">2.4 设置MAVEN_OPTS环境变量</h3>\n<p>目的是让maven构建是速度增加。由于Mvn命令实际是使用java命令，默认无法满足maven运行的需要，容易产生java.lang.OutOfMemeoryError，需要设置系统环境变量MAVEN_OPTS的值为-Xms128m -Xmx512m。</p>\n<h3 id=\"2-5-参数设置：\">2.5 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>（此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）<br>\n​ Windows：如下图<br>\n​\t\t<img src=\"/img/maven配置.png\"><br>\n​</p>\n<h3 id=\"2-6-用户配置：\">2.6 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2 id=\"3、使用入门\">3、使用入门</h2>\n<h3 id=\"3-1-编写pom-xml\">3.1 编写pom.xml</h3>\n<p>Maven的核心是pom.xml</p>\n<p><img src=\"/img/1618244002479.png\" alt=\"1618244002479\"></p>\n<p>第三方工具可以快速构建pom.xml的头</p>\n<p>Project是所有的pom.xml的根元素，其中第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2和Maven3来说，它只能是4.0.0。</p>\n<p>最重要的是groupId、artifactId和version三行。这三行元素定义了一个项目的基本坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。</p>\n<p>GroupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联。</p>\n<p>ArtifactId定义了当前Maven项目在组中唯一的ID，子模块</p>\n<p>Version指定了版本</p>\n<h3 id=\"3-2编写主代码，项目打包过程\">3.2编写主代码，项目打包过程</h3>\n<p>此处介绍mvn clean complie、mvn clean test、mvn clean package、mvn clean install</p>\n<h4 id=\"3-2-1使用maven编译项目\">3.2.1使用maven编译项目</h4>\n<p>当我们编写一个main调用sayHello()打印helloworld字符串时。</p>\n<p>该代码的（com.sy.sa.myapp.helloworld）与之前的POM中定义的groupId和artifactId相吻合。一般来说，项目中Java类的包都应该基于项目的group和artifactId，方便搜索。</p>\n<p>当编码完毕，使用Maven进行编译，</p>\n<p>在项目根目录下运行命令mvn clean compile，</p>\n<p>mvn clean compile运行步骤：</p>\n<p><img src=\"/img/1618244073306.png\" alt=\"1618244073306\"></p>\n<p>（1）Clean告诉Maven清理输出目录target/，compile告诉Maven编译项目主代码，从输出中看到Maven首先执行了clean：Clean任务，删除target/目录；默认情况下，Maven构建的所有输出都在target/目录中；</p>\n<p>（2）接着执行resources：resources任务（未定义项目资源，暂且略过）；</p>\n<p>（3）最后执行compiler：compile任务，将项目主代码编译至targert/classes目录</p>\n<h4 id=\"3-2-2-使用maven编译测试类\">3.2.2 使用maven编译测试类</h4>\n<p>编写完测试用例运行命令mvn clean test</p>\n<p>需要maven-compiler-plugin插件</p>\n<p><img src=\"/img/1618244110010.png\" alt=\"1618244110010\"></p>\n<p>mvn clean test运行的步骤中会提示测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少</p>\n<p><img src=\"/img/1618244137853.png\" alt=\"1618244137853\"></p>\n<h4 id=\"3-2-3-使用maven将项目打包和运行\">3.2.3 使用maven将项目打包和运行</h4>\n<p>mvn clean package进行打包，可以看到target下生成jar，</p>\n<p>它是根据artifact-version.jar规则进行命名的，还可以使用finalName来自定义该文件的名称。</p>\n<p><img src=\"/img/1618244166891.png\" alt=\"1618244166891\"></p>\n<h4 id=\"3-2-4-使用maven运行带main方法的类\">3.2.4 使用maven运行带main方法的类</h4>\n<p>目前我们打成的jar不能识别main方法，需要指定main方法的位置。使用这个插件，来制定main方法的位置</p>\n<p>​         maven-shade-plug</p>\n<p><img src=\"/img/1618244193984.png\" alt=\"1618244193984\"></p>\n<h4 id=\"3-2-5-将项目打包放到本地maven仓库\">3.2.5 将项目打包放到本地maven仓库</h4>\n<p>mvn clean install将此jar包放到maven指定的仓库，该仓库的地址是setting.xml的本地仓库的地址</p>\n<p><img src=\"/img/1618244251013.png\" alt=\"1618244251013\"></p>\n<h2 id=\"4、坐标和依赖\">4、坐标和依赖</h2>\n<h3 id=\"4-1-坐标详解\">4.1 坐标详解</h3>\n<p>Maven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。它们是groupId、artifactId、version、packaging、classifier</p>\n<p><img src=\"/img/1618244289269.png\" alt=\"1618244289269\"></p>\n<h4 id=\"4-1-1-groupId定义到项目\">4.1.1 groupId定义到项目</h4>\n<p>与java包名类似，通常是反向的域名。GroupId为org.sonatype.nexus，（org.sonatype是非营利组织sonatype、nexus是实际项目）。该groupId与域名nexus. Sonatype.org反向对应</p>\n<h4 id=\"4-1-2-artifactId定义到项目其中的一个模块\">4.1.2 artifactId定义到项目其中的一个模块</h4>\n<p>​    为了方便区分，便于寻找实际构件，用nexus作为前缀</p>\n<h4 id=\"4-1-3-version为版本\">4.1.3 version为版本</h4>\n<h4 id=\"4-1-4-packaging为该项目的打包方式，默认jar（可选）\">4.1.4 packaging为该项目的打包方式，默认jar（可选）</h4>\n<h4 id=\"4-1-5-classifier为（可选，不能直接定义，由附加插件帮助生成）\">4.1.5 classifier为（可选，不能直接定义，由附加插件帮助生成）</h4>\n<h2 id=\"5、依赖\">5、依赖</h2>\n<h3 id=\"5-1-依赖范围\">5.1 依赖范围</h3>\n<p>例：Junit依赖的测试范围test， 测试范围用元素scope表示</p>\n<p>Maven在编译项目主代码时候使用一套classpath</p>\n<p>在编译和执行测试的时候回使用另一套classpath</p>\n<p>依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath），Maven有以下几种依赖范围：</p>\n<p>Compile：编译依赖范围，三种classpath都有效(默认)</p>\n<p>Test：测试依赖范围</p>\n<p>Provided：已提供依赖范围（编译和测试）</p>\n<p>Runtime：运行时依赖规范</p>\n<p>System：系统依赖范围（编译和测试，必须显示的依赖文件的路径）</p>\n<p><img src=\"/img/1618244361109.png\" alt=\"1618244361109\"></p>\n<h3 id=\"5-2-传递性依赖\">5.2 传递性依赖</h3>\n<p>有了传递依赖机制，Maven会直接依赖POM，将那些必要的简介依赖，以传递性依赖的形式引入到当前的项目中。</p>\n<p>A依赖B、B依赖C：A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。传递依赖同样有生命周期</p>\n<p><img src=\"/img/1618244387386.png\" alt=\"1618244387386\"></p>\n<h3 id=\"5-3-排除依赖\">5.3 排除依赖</h3>\n<h4 id=\"5-3-1引入相同版本的依赖\">5.3.1引入相同版本的依赖</h4>\n<p>像framework定义version</p>\n<pre><code class=\"language-java\">&lt;properties&gt;\n\t &lt;springframework.version&gt;5.2.2&lt;/springframework.version&gt;\n&lt;/properties&gt;\n</code></pre>\n<h4 id=\"5-3-2-依赖优化\">5.3.2 依赖优化</h4>\n<p>mvn dependency:list 查看当前项目已解析的依赖</p>\n<p>mvn dependency:tree 查看当前项目的依赖树</p>\n<p>mvn dependency:analyze 帮助分析当前项目的依赖</p>\n<h2 id=\"6、仓库\">6、仓库</h2>\n<h3 id=\"6-1概念\">6.1概念</h3>\n<p>任何一个依赖，插件或者项目构建的输出，都可以称为构件</p>\n<h3 id=\"6-2-仓库的布局\">6.2 仓库的布局</h3>\n<p>仓库布局的源码，是基于简单的文件系统</p>\n<h3 id=\"6-3-仓库的分类\">6.3 仓库的分类</h3>\n<p>仓库的配置，中央仓库、远程仓库</p>\n<p>分类：</p>\n<p><img src=\"/img/1618244497271.png\" alt=\"1618244497271\"></p>\n<h3 id=\"6-4-仓库的配置\">6.4 仓库的配置</h3>\n<h4 id=\"6-4-1设置仓库\">6.4.1设置仓库</h4>\n<p>（1) 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。</p>\n<p><img src=\"/img/1618244522945.png\" alt=\"1618244522945\"></p>\n<p>（2）配置maven 更新频率和检查文件策略</p>\n<p><img src=\"/img/1618244544994.png\" alt=\"1618244544994\"></p>\n<p>（3）Maven的认证</p>\n<p><img src=\"/img/1618244566899.png\" alt=\"1618244566899\"></p>\n<h4 id=\"6-4-2-上传构件到私有仓库\">6.4.2 上传构件到私有仓库</h4>\n<p>（1）首先配置好distributionManagement配置</p>\n<p>distributionManagement是项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。</p>\n<p><img src=\"/img/1618244592782.png\" alt=\"1618244592782\"></p>\n<p>（2）配置成功后，可以用mvn clean deploy Maven就会将项目构建输出的构建部署到配置对应的远程仓库。如果项目当前是快照版本，则部署到快照仓库地址，否则就部署到发布版本仓库地址。</p>\n<h4 id=\"6-4-3-Maven版本号机制\">6.4.3 Maven版本号机制</h4>\n<p><img src=\"/img/1618244623527.png\" alt=\"1618244623527\"></p>\n<p>版本号(version number)是版本的标识号。</p>\n<p>1.版本命名规范</p>\n<p>软件版本号有四部分组成，第一部分为主版本号，第二部分为次版本号，第三部分为修订版本号，第四部分为日期版本号</p>\n<p>2.软件版本阶段说明</p>\n<p>3.版本号修改规则</p>\n<p>（1）主版本号：当整体框架结构发生变化时，此版本号增加。此版本号由项目决定是否修改。</p>\n<p>（2）次版本号：相对于主版本号而言，次版本号的升级对应的只是局部的变动，当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0。</p>\n<p>（3）修订版本号：当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变，修正版本号加 1。</p>\n<p>（4）日期版本号：发版当天的日期，需要包括年份。如：20160617</p>\n"},{"title":"mysql排序","author":"郑天祺","date":"2019-11-20T12:35:00.000Z","_content":"\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","source":"_posts/mysql排序.md","raw":"title: mysql排序\ntags:\n\n  - mysql\ncategories:\n  - 数据库\nauthor: 郑天祺\ndate: 2019-11-20 20:35:00\n---\n\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","slug":"mysql排序","published":1,"updated":"2019-11-26T07:31:53.949Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl1005rl0t96d5r2evn","content":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n"},{"title":"mysql事务","author":"郑天祺","date":"2020-07-21T01:42:00.000Z","_content":"\n# 1、介绍\n\n​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：\n\n​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。\n\n```java\n原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n```\n\n## 举例：\n\n​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：\n\n```java\n1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n```\n\n原子性：\n\t\t保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。\n\n一致性\n\t\t在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。\n\n隔离性\n\t\t在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。\n\n持久性\n\t\t一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）\n\n## 原子性与隔离行\n\n​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。\n\n因此，事务的原子性与一致性缺一不可。\n\n借鉴于：http://www.hollischuang.com/archives/898\n\n# 2、事务的隔离级别\n\n## （1）read uncommited\n\n​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。\n\n## （2）read commited\n\n​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。\n\n## （3）repeatable read\n\n​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。\n\n## （4）serializable\n\n​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读\n\n## （5）脏读、不可重复读、幻象\n\n​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）\n\n# 3、JAVA中的事务管理器\n\n​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：\n\n![image-20200721113313947](/img/transaction.png)","source":"_posts/mysql事务.md","raw":"title: mysql事务\nauthor: 郑天祺\ntags:\n\n  - mysql\ncategories:\n  - 数据库\ndate: 2020-07-21 09:42:00\n\n---\n\n# 1、介绍\n\n​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：\n\n​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。\n\n```java\n原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n```\n\n## 举例：\n\n​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：\n\n```java\n1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n```\n\n原子性：\n\t\t保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。\n\n一致性\n\t\t在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。\n\n隔离性\n\t\t在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。\n\n持久性\n\t\t一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）\n\n## 原子性与隔离行\n\n​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。\n\n因此，事务的原子性与一致性缺一不可。\n\n借鉴于：http://www.hollischuang.com/archives/898\n\n# 2、事务的隔离级别\n\n## （1）read uncommited\n\n​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。\n\n## （2）read commited\n\n​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。\n\n## （3）repeatable read\n\n​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。\n\n## （4）serializable\n\n​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读\n\n## （5）脏读、不可重复读、幻象\n\n​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）\n\n# 3、JAVA中的事务管理器\n\n​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：\n\n![image-20200721113313947](/img/transaction.png)","slug":"mysql事务","published":1,"updated":"2020-07-21T06:13:22.677Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl2005tl0t9327v6h8e","content":"<h1>1、介绍</h1>\n<p>​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：</p>\n<p>​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。<br>\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。</p>\n<p>​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。</p>\n<pre><code class=\"language-java\">原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n</code></pre>\n<h2 id=\"举例：\">举例：</h2>\n<p>​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：</p>\n<pre><code class=\"language-java\">1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n</code></pre>\n<p>原子性：<br>\n保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。</p>\n<p>一致性<br>\n在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。</p>\n<p>隔离性<br>\n在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。<br>\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。</p>\n<p>持久性<br>\n一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）</p>\n<h2 id=\"原子性与隔离行\">原子性与隔离行</h2>\n<p>​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。<br>\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。</p>\n<p>因此，事务的原子性与一致性缺一不可。</p>\n<p>借鉴于：<a href=\"http://www.hollischuang.com/archives/898\">http://www.hollischuang.com/archives/898</a></p>\n<h1>2、事务的隔离级别</h1>\n<h2 id=\"（1）read-uncommited\">（1）read uncommited</h2>\n<p>​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。</p>\n<h2 id=\"（2）read-commited\">（2）read commited</h2>\n<p>​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。</p>\n<h2 id=\"（3）repeatable-read\">（3）repeatable read</h2>\n<p>​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。</p>\n<h2 id=\"（4）serializable\">（4）serializable</h2>\n<p>​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读</p>\n<h2 id=\"（5）脏读、不可重复读、幻象\">（5）脏读、不可重复读、幻象</h2>\n<p>​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。<br>\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。<br>\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）</p>\n<h1>3、JAVA中的事务管理器</h1>\n<p>​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：</p>\n<p><img src=\"/img/transaction.png\" alt=\"image-20200721113313947\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t一个数据库事务通常包含对数据库进行读或写的一个操作序列：</p>\n<p>​\t\t（1）为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。<br>\n​\t\t（2）当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。</p>\n<p>​\t\t并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。</p>\n<pre><code class=\"language-java\">原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。\n一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。\n持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。\n</code></pre>\n<h2 id=\"举例：\">举例：</h2>\n<p>​\t\t用一个常用的“A账户向B账号汇钱”的例子来说明如何通过数据库事务保证数据的准确性和完整性。熟悉关系型数据库事务的都知道从帐号A到帐号B需要6个操作：</p>\n<pre><code class=\"language-java\">1、从A账号中把余额读出来（500）。\n2、对A账号做减法操作（500-100）。\n3、把结果写回A账号中（400）。\n4、从B账号中把余额读出来（500）。\n5、对B账号做加法操作（500+100）。\n6、把结果写回B账号中（600）。\n</code></pre>\n<p>原子性：<br>\n保证1-6所有过程要么都执行，要么都不执行。一旦在执行某一步骤的过程中发生问题，就需要执行回滚操作。 假如执行到第五步的时候，B账户突然不可用（比如被注销），那么之前的所有操作都应该回滚到执行事务之前的状态。</p>\n<p>一致性<br>\n在转账之前，A和B的账户中共有500+500=1000元钱。在转账之后，A和B的账户中共有400+600=1000元。也就是说，数据的状态在执行该事务操作之后从一个状态改变到了另外一个状态。同时一致性还能保证账户余额不会变成负数等。</p>\n<p>隔离性<br>\n在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。<br>\n如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱应该是A转给B的钱加上C转给B的钱再加上自己原有的钱。</p>\n<p>持久性<br>\n一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化（会把数据写入数据库做持久化保存）</p>\n<h2 id=\"原子性与隔离行\">原子性与隔离行</h2>\n<p>​\t\t一致性与原子性是密切相关的,原子性的破坏可能导致数据库的不一致，数据的一致性问题并不都和原子性有关。<br>\n比如刚刚的例子，在第五步的时候，对B账户做加法时只加了50元。那么该过程可以符合原子性，但是数据的一致性就出现了问题。</p>\n<p>因此，事务的原子性与一致性缺一不可。</p>\n<p>借鉴于：<a href=\"http://www.hollischuang.com/archives/898\">http://www.hollischuang.com/archives/898</a></p>\n<h1>2、事务的隔离级别</h1>\n<h2 id=\"（1）read-uncommited\">（1）read uncommited</h2>\n<p>​\t\t是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。</p>\n<h2 id=\"（2）read-commited\">（2）read commited</h2>\n<p>​\t\t保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。</p>\n<h2 id=\"（3）repeatable-read\">（3）repeatable read</h2>\n<p>​\t\t这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未提交的数据之外还避免了以下情况产生（不可重复读）。</p>\n<h2 id=\"（4）serializable\">（4）serializable</h2>\n<p>​\t\t这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻象读</p>\n<h2 id=\"（5）脏读、不可重复读、幻象\">（5）脏读、不可重复读、幻象</h2>\n<p>​\t\ta.脏读：指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的。<br>\n​\t\tb.不可重复读：指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。<br>\n​\t\tc.幻象读：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）</p>\n<h1>3、JAVA中的事务管理器</h1>\n<p>​\t\tSpring并不直接管理事务，而是提供了多种事务管理器。事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：</p>\n<p><img src=\"/img/transaction.png\" alt=\"image-20200721113313947\"></p>\n"},{"title":"mysql表设计及优化","author":"郑天祺","date":"2019-08-31T07:28:00.000Z","_content":"\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","source":"_posts/mysql表设计及优化.md","raw":"title: mysql表设计及优化\nauthor: 郑天祺\ntags:\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 15:28:00\n\n---\n\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","slug":"mysql表设计及优化","published":1,"updated":"2019-10-15T10:10:18.948Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl3005yl0t9edor7yus","content":"<h2 id=\"一、一些建议\">一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select…like ‘%**%’ 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？<a href=\"https://blog.csdn.net/n88Lpo/article/details/78099114\">https://blog.csdn.net/n88Lpo/article/details/78099114</a></p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2 id=\"二、算是面试题吧\">二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2 id=\"三、数据结构\">三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、一些建议\">一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select…like ‘%**%’ 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？<a href=\"https://blog.csdn.net/n88Lpo/article/details/78099114\">https://blog.csdn.net/n88Lpo/article/details/78099114</a></p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2 id=\"二、算是面试题吧\">二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2 id=\"三、数据结构\">三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n"},{"title":"互斥锁","author":"郑天祺","date":"2019-08-31T05:13:00.000Z","_content":"\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","source":"_posts/互斥锁.md","raw":"title: 互斥锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:13:00\n\n---\n\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","slug":"互斥锁","published":1,"updated":"2019-10-15T10:10:39.998Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl30060l0t9g171f34p","content":"<h2 id=\"1、关于“互斥”和“同步”的概念\">1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了<br>\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2 id=\"2、JAVA中synchronized和Lock是互斥锁\">2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象<br>\n修饰在实例方法上，锁对象是当前实例对象<br>\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2 id=\"3、synchronized的用法\">3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo &#123;\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1()&#123;\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     &#125;\n\n    public synchronized void s2() &#123;\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     &#125;\n\n    public void s3() &#123;\n         synchronized (LOCK) &#123;\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         &#125;\n     &#125;\n\n    public void s4() &#123;\n         synchronized (SynchronizedDemo.class) &#123;\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         &#125;\n     &#125;\n\n&#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、关于“互斥”和“同步”的概念\">1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了<br>\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2 id=\"2、JAVA中synchronized和Lock是互斥锁\">2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象<br>\n修饰在实例方法上，锁对象是当前实例对象<br>\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2 id=\"3、synchronized的用法\">3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo &#123;\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1()&#123;\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     &#125;\n\n    public synchronized void s2() &#123;\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     &#125;\n\n    public void s3() &#123;\n         synchronized (LOCK) &#123;\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         &#125;\n     &#125;\n\n    public void s4() &#123;\n         synchronized (SynchronizedDemo.class) &#123;\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         &#125;\n     &#125;\n\n&#125;\n\n \n</code></pre>\n"},{"title":"伪共享","author":"郑天祺","date":"2020-12-14T05:34:00.000Z","_content":"\n# 1、CPU缓存介绍\n\n​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。\n\n​\t\t图具有3级缓存的处理器\n\n![image-20201214133601256](/img/image-20201214133601256.png)\n\n![image-20201214133612299](/img/image-20201214133612299.png)\n\n图片来自： https://lwn.net/Articles/252125/\n\n​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。\n\n​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，\n\n​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。\n\n# 2、L1 L2 L3\n\nL1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。\n\n# 3、缓存命中或错过和延迟\n\n数据从RAM流到L3缓存，然后是L2，最后是L1。\n\n当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。\n\n然后它继续在L2中找到它，然后在L3中找到它。\n\n如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。\n\n# 4、缓存行（Cache Line）\n\n缓存，是由缓存行组成的。一般一行缓存行有64字节\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。\n\n# 5、伪共享的发生\n\n![image-20201214133830361](/img/image-20201214133830361.png)\n\n## 产生原因：\n\n数据X、Y、Z被加载到同一Cache Line中，\n线程A在Core1修改X，线程B在Core2上修改Y\n\n根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。\n\n## 解决方法：\n\n此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。\n\n图片来自：https://blog.csdn.net/qq_27680317/article/details/78486220\n\nM 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)\n\n# 6、伪共享的实例\n\n## 6.1、伪共享的产生\n\n​\t\t假如业务场景中，上述的类满足以下几个特点：\n\n​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。\n\n​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。\n\n![image-20201214133935704](/img/image-20201214133935704.png)\n\n​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。\n\n![image-20201214133955605](/img/image-20201214133955605.png)\n\n## 6.2、解决方法\n\n### （1）缓存行的填充\n\n![image-20201214134132982](/img/image-20201214134132982.png)\n\n​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。\n\n​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数\n\n​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求\n\n## 6.2、解决方法\n\n### （2）Contended注解方式\n\n![image-20201214134208507](/img/image-20201214134208507.png)\n\n​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能 \n\n采取上述措施图示：\n\n![image-20201214134235050](/img/image-20201214134235050.png)\n\n更多实例：ConcurrentHashMap、Thread 、Disruptor","source":"_posts/伪共享.md","raw":"title: 伪共享\nauthor: 郑天祺\ntags:\n  - 伪共享\ncategories:\n  - 操作系统\ndate: 2020-12-14 13:34:00\n\n---\n\n# 1、CPU缓存介绍\n\n​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。\n\n​\t\t图具有3级缓存的处理器\n\n![image-20201214133601256](/img/image-20201214133601256.png)\n\n![image-20201214133612299](/img/image-20201214133612299.png)\n\n图片来自： https://lwn.net/Articles/252125/\n\n​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。\n\n​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，\n\n​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。\n\n# 2、L1 L2 L3\n\nL1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。\n\n# 3、缓存命中或错过和延迟\n\n数据从RAM流到L3缓存，然后是L2，最后是L1。\n\n当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。\n\n然后它继续在L2中找到它，然后在L3中找到它。\n\n如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。\n\n# 4、缓存行（Cache Line）\n\n缓存，是由缓存行组成的。一般一行缓存行有64字节\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。\n\n# 5、伪共享的发生\n\n![image-20201214133830361](/img/image-20201214133830361.png)\n\n## 产生原因：\n\n数据X、Y、Z被加载到同一Cache Line中，\n线程A在Core1修改X，线程B在Core2上修改Y\n\n根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。\n\n## 解决方法：\n\n此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。\n\n图片来自：https://blog.csdn.net/qq_27680317/article/details/78486220\n\nM 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)\n\n# 6、伪共享的实例\n\n## 6.1、伪共享的产生\n\n​\t\t假如业务场景中，上述的类满足以下几个特点：\n\n​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。\n\n​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。\n\n![image-20201214133935704](/img/image-20201214133935704.png)\n\n​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。\n\n![image-20201214133955605](/img/image-20201214133955605.png)\n\n## 6.2、解决方法\n\n### （1）缓存行的填充\n\n![image-20201214134132982](/img/image-20201214134132982.png)\n\n​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。\n\n​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数\n\n​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求\n\n## 6.2、解决方法\n\n### （2）Contended注解方式\n\n![image-20201214134208507](/img/image-20201214134208507.png)\n\n​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能 \n\n采取上述措施图示：\n\n![image-20201214134235050](/img/image-20201214134235050.png)\n\n更多实例：ConcurrentHashMap、Thread 、Disruptor","slug":"伪共享","published":1,"updated":"2020-12-14T05:46:28.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl40064l0t904cj3r4v","content":"<h1>1、CPU缓存介绍</h1>\n<p>​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。</p>\n<p>​\t\t图具有3级缓存的处理器</p>\n<p><img src=\"/img/image-20201214133601256.png\" alt=\"image-20201214133601256\"></p>\n<p><img src=\"/img/image-20201214133612299.png\" alt=\"image-20201214133612299\"></p>\n<p>图片来自： <a href=\"https://lwn.net/Articles/252125/\">https://lwn.net/Articles/252125/</a></p>\n<p>​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。</p>\n<p>​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，</p>\n<p>​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。</p>\n<h1>2、L1 L2 L3</h1>\n<p>L1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。<br>\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。<br>\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。</p>\n<h1>3、缓存命中或错过和延迟</h1>\n<p>数据从RAM流到L3缓存，然后是L2，最后是L1。</p>\n<p>当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。</p>\n<p>然后它继续在L2中找到它，然后在L3中找到它。</p>\n<p>如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。</p>\n<h1>4、缓存行（Cache Line）</h1>\n<p>缓存，是由缓存行组成的。一般一行缓存行有64字节<br>\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。</p>\n<h1>5、伪共享的发生</h1>\n<p><img src=\"/img/image-20201214133830361.png\" alt=\"image-20201214133830361\"></p>\n<h2 id=\"产生原因：\">产生原因：</h2>\n<p>数据X、Y、Z被加载到同一Cache Line中，<br>\n线程A在Core1修改X，线程B在Core2上修改Y</p>\n<p>根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；<br>\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。</p>\n<h2 id=\"解决方法：\">解决方法：</h2>\n<p>此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。</p>\n<p>图片来自：<a href=\"https://blog.csdn.net/qq_27680317/article/details/78486220\">https://blog.csdn.net/qq_27680317/article/details/78486220</a></p>\n<p>M 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)</p>\n<h1>6、伪共享的实例</h1>\n<h2 id=\"6-1、伪共享的产生\">6.1、伪共享的产生</h2>\n<p>​\t\t假如业务场景中，上述的类满足以下几个特点：</p>\n<p>​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。</p>\n<p>​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。</p>\n<p><img src=\"/img/image-20201214133935704.png\" alt=\"image-20201214133935704\"></p>\n<p>​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。</p>\n<p><img src=\"/img/image-20201214133955605.png\" alt=\"image-20201214133955605\"></p>\n<h2 id=\"6-2、解决方法\">6.2、解决方法</h2>\n<h3 id=\"（1）缓存行的填充\">（1）缓存行的填充</h3>\n<p><img src=\"/img/image-20201214134132982.png\" alt=\"image-20201214134132982\"></p>\n<p>​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。</p>\n<p>​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数</p>\n<p>​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求</p>\n<h2 id=\"6-2、解决方法-2\">6.2、解决方法</h2>\n<h3 id=\"（2）Contended注解方式\">（2）Contended注解方式</h3>\n<p><img src=\"/img/image-20201214134208507.png\" alt=\"image-20201214134208507\"></p>\n<p>​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能</p>\n<p>采取上述措施图示：</p>\n<p><img src=\"/img/image-20201214134235050.png\" alt=\"image-20201214134235050\"></p>\n<p>更多实例：ConcurrentHashMap、Thread 、Disruptor</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、CPU缓存介绍</h1>\n<p>​\t\t以近代CPU的视角来说，它们的作用都是作为CPU与主内存之间的高速数据缓冲区，L1最靠近CPU核心；L2其次；L3再次。</p>\n<p>​\t\t图具有3级缓存的处理器</p>\n<p><img src=\"/img/image-20201214133601256.png\" alt=\"image-20201214133601256\"></p>\n<p><img src=\"/img/image-20201214133612299.png\" alt=\"image-20201214133612299\"></p>\n<p>图片来自： <a href=\"https://lwn.net/Articles/252125/\">https://lwn.net/Articles/252125/</a></p>\n<p>​\t\t早期，缓存设计过去常常在CPU外部安装L2和L3缓存，这会对延迟产生负面影响。</p>\n<p>​\t\t缓存设计总是在不断发展，特别是随着内存变得更便宜，更快，更密集。英特尔和AMD已经在缓存设计方面做了大量实验，</p>\n<p>​\t\t英特尔甚至尝试使用L4缓存。CPU市场正以前所未有的速度向前发展。</p>\n<h1>2、L1 L2 L3</h1>\n<p>L1（1级）高速缓存是计算机系统中存在的最快内存。在访问优先级方面，L1缓存具有CPU在完成特定任务时最可能需要的数据。L1缓存通常也有两种分割方式，分为指令缓存和数据缓存。指令高速缓存处理有关CPU必须执行的操作的信息，而数据高速缓存保存要在其上执行操作的数据。<br>\nL2（级别2）缓存比L1缓存慢，在大多数现代CPU中，L1和L2高速缓存存在于CPU内核本身，每个内核都有自己的高速缓存。<br>\nL3（Level 3）缓存是最大的缓存单元，也是最慢的缓存单元。它的范围在4MB到50MB之间。现代CPU在CPU裸片上有专用空间用于L3缓存，占用了大量空间。</p>\n<h1>3、缓存命中或错过和延迟</h1>\n<p>数据从RAM流到L3缓存，然后是L2，最后是L1。</p>\n<p>当处理器正在寻找执行操作的数据时，它首先尝试在L1高速缓存中找到它。如果CPU能够找到它，则该条件称为缓存命中。</p>\n<p>然后它继续在L2中找到它，然后在L3中找到它。</p>\n<p>如果找不到数据，它会尝试从主存储器访问它。这称为缓存未命中。</p>\n<h1>4、缓存行（Cache Line）</h1>\n<p>缓存，是由缓存行组成的。一般一行缓存行有64字节<br>\n所以使用缓存时，并不是一个一个字节使用，而是一行缓存行、一行缓存行这样使用；换句话说，CPU存取缓存都是按照一行，为最小单位操作的。</p>\n<h1>5、伪共享的发生</h1>\n<p><img src=\"/img/image-20201214133830361.png\" alt=\"image-20201214133830361\"></p>\n<h2 id=\"产生原因：\">产生原因：</h2>\n<p>数据X、Y、Z被加载到同一Cache Line中，<br>\n线程A在Core1修改X，线程B在Core2上修改Y</p>\n<p>根据MESI，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；<br>\n当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。</p>\n<h2 id=\"解决方法：\">解决方法：</h2>\n<p>此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。即 缓存行的填充。</p>\n<p>图片来自：<a href=\"https://blog.csdn.net/qq_27680317/article/details/78486220\">https://blog.csdn.net/qq_27680317/article/details/78486220</a></p>\n<p>M 修改 (Modified)  E 独享、互斥 (Exclusive)  S 共享 (Shared)  I 无效 (Invalid)</p>\n<h1>6、伪共享的实例</h1>\n<h2 id=\"6-1、伪共享的产生\">6.1、伪共享的产生</h2>\n<p>​\t\t假如业务场景中，上述的类满足以下几个特点：</p>\n<p>​\t\t当value变量改变时，modifyTime肯定会改变createTime变量和key变量在创建后，就不会再变化。flag也经常会变化，不过与modifyTime和value变量毫无关联。</p>\n<p>​\t\t当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。</p>\n<p><img src=\"/img/image-20201214133935704.png\" alt=\"image-20201214133935704\"></p>\n<p>​\t\t如图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。</p>\n<p><img src=\"/img/image-20201214133955605.png\" alt=\"image-20201214133955605\"></p>\n<h2 id=\"6-2、解决方法\">6.2、解决方法</h2>\n<h3 id=\"（1）缓存行的填充\">（1）缓存行的填充</h3>\n<p><img src=\"/img/image-20201214134132982.png\" alt=\"image-20201214134132982\"></p>\n<p>​\t\t在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。</p>\n<p>​\t\t通过填充变量，使不相关的变量分开。被操作的每一组属性占的字节数</p>\n<p>​\t\t加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求</p>\n<h2 id=\"6-2、解决方法-2\">6.2、解决方法</h2>\n<h3 id=\"（2）Contended注解方式\">（2）Contended注解方式</h3>\n<p><img src=\"/img/image-20201214134208507.png\" alt=\"image-20201214134208507\"></p>\n<p>​\t\t在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能</p>\n<p>采取上述措施图示：</p>\n<p><img src=\"/img/image-20201214134235050.png\" alt=\"image-20201214134235050\"></p>\n<p>更多实例：ConcurrentHashMap、Thread 、Disruptor</p>\n"},{"title":"信息系统项目管理师-信息化","author":"ztq","date":"2021-04-15T11:34:00.000Z","_content":"\n![image-20210415193556837](/img/image-20210415193556837.png)","source":"_posts/信息系统项目管理师-信息化-1.md","raw":"title: 信息系统项目管理师-信息化\nauthor: ztq\ntags:\n  - 信息系统项目管理师\ncategories:\n  - 软件管理\ndate: 2021-04-15 19:34:00\n\n---\n\n![image-20210415193556837](/img/image-20210415193556837.png)","slug":"信息系统项目管理师-信息化-1","published":1,"updated":"2021-04-17T10:15:18.587Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl60067l0t95fn4937q","content":"<p><img src=\"/img/image-20210415193556837.png\" alt=\"image-20210415193556837\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/img/image-20210415193556837.png\" alt=\"image-20210415193556837\"></p>\n"},{"title":"偏向锁","author":"郑天祺","date":"2019-08-31T05:22:00.000Z","_content":"\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","source":"_posts/偏向锁.md","raw":"title: 偏向锁\nauthor: 郑天祺\ntags:\n\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:22:00\n\n---\n\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","slug":"偏向锁","published":1,"updated":"2020-03-28T01:00:34.402Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl6006bl0t92b7w27aq","content":"<h2 id=\"0、从偏向锁到重量锁\">0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test()&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2 id=\"1、先了解一下对象在JVM内存中的布局，如下图\">1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2 id=\"2、锁的创建\">2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.<br>\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.<br>\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。<br>\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"0、从偏向锁到重量锁\">0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test()&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2 id=\"1、先了解一下对象在JVM内存中的布局，如下图\">1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2 id=\"2、锁的创建\">2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject)&#123;\n     //代码\n &#125;\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.<br>\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.<br>\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。<br>\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n"},{"title":"公平锁、非公平锁","author":"郑天祺","date":"2019-08-31T05:21:00.000Z","_content":"\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","source":"_posts/公平锁、非公平锁.md","raw":"title: 公平锁、非公平锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:21:00\n\n---\n\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","slug":"公平锁、非公平锁","published":1,"updated":"2019-10-15T10:06:08.660Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl7006el0t9dlelh4bo","content":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。<br>\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： <a href=\"https://www.cnblogs.com/little-fly/p/10365109.html\">https://www.cnblogs.com/little-fly/p/10365109.html</a></p>\n<p>​        <a href=\"https://www.jianshu.com/p/06340f8feb05\">https://www.jianshu.com/p/06340f8feb05</a></p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：<a href=\"https://www.jianshu.com/p/c7d17b5c6be3\">https://www.jianshu.com/p/c7d17b5c6be3</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。<br>\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： <a href=\"https://www.cnblogs.com/little-fly/p/10365109.html\">https://www.cnblogs.com/little-fly/p/10365109.html</a></p>\n<p>​        <a href=\"https://www.jianshu.com/p/06340f8feb05\">https://www.jianshu.com/p/06340f8feb05</a></p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：<a href=\"https://www.jianshu.com/p/c7d17b5c6be3\">https://www.jianshu.com/p/c7d17b5c6be3</a></p>\n"},{"title":"分布式CAP概念","author":"郑天祺","date":"2020-12-14T05:47:00.000Z","_content":"\n​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；\n\n## CAP原理概述\n\n​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个\n\n![image-20201214134902337](/img/image-20201214134902337.png)\n\n## Consistency 一致性\n\n​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。\n\n## 三种一致性策略\n\n​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。\n\n## Availability 可用性\n\n​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：\n\n![image-20201214135053884](/img/image-20201214135053884.png)\n\n​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)*365*24*60 = 5.256 min，这是一个极高的要求。\n\n​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。\n\n## Partition Tolerance分区容错性\n\n​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。\n\n## CA without P\n\n​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。\n\n## CP without A\n\n​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？\n\n## CPwithoutA示例说明\n\n ZooKeeper是个CP（一致性+分区容错性）\n\t\t即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。\n\n## HBase是强一致性系统\n\nHbase具有以下特点\n•\t每个值只出现在一个REGION\n•\t同一时间一个Region只分配给一个Region服务器\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。\n•\tput操作要么成功，要么完全失败。\n\n​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。\n\n​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。\n\n## CPwithoutA示例说明\n\n## AP wihtout C\n\n​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。\n\n## APwithoutC示例说明\n\n​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n\n## APwithoutC示例说明\n\n​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。","source":"_posts/分布式CAP概念.md","raw":"title: 分布式CAP概念\nauthor: 郑天祺\ntags:\n\n  - CAP\ncategories:\n  - 分布式\ndate: 2020-12-14 13:47:00\n\n---\n\n​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；\n\n## CAP原理概述\n\n​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个\n\n![image-20201214134902337](/img/image-20201214134902337.png)\n\n## Consistency 一致性\n\n​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。\n\n## 三种一致性策略\n\n​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。\n\n## Availability 可用性\n\n​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：\n\n![image-20201214135053884](/img/image-20201214135053884.png)\n\n​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)*365*24*60 = 5.256 min，这是一个极高的要求。\n\n​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。\n\n## Partition Tolerance分区容错性\n\n​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。\n\n## CA without P\n\n​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。\n\n## CP without A\n\n​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？\n\n## CPwithoutA示例说明\n\n ZooKeeper是个CP（一致性+分区容错性）\n\t\t即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。\n\n## HBase是强一致性系统\n\nHbase具有以下特点\n•\t每个值只出现在一个REGION\n•\t同一时间一个Region只分配给一个Region服务器\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。\n•\tput操作要么成功，要么完全失败。\n\n​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。\n\n​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。\n\n## CPwithoutA示例说明\n\n## AP wihtout C\n\n​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。\n\n## APwithoutC示例说明\n\n​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n\n## APwithoutC示例说明\n\n​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。","slug":"分布式CAP概念","published":1,"updated":"2020-12-14T05:56:36.424Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl8006hl0t99ulub7zb","content":"<p>​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；</p>\n<h2 id=\"CAP原理概述\">CAP原理概述</h2>\n<p>​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个</p>\n<p><img src=\"/img/image-20201214134902337.png\" alt=\"image-20201214134902337\"></p>\n<h2 id=\"Consistency-一致性\">Consistency 一致性</h2>\n<p>​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。<br>\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。<br>\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。</p>\n<h2 id=\"三种一致性策略\">三种一致性策略</h2>\n<p>​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。<br>\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。<br>\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。<br>\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。</p>\n<h2 id=\"Availability-可用性\">Availability 可用性</h2>\n<p>​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。<br>\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：</p>\n<p><img src=\"/img/image-20201214135053884.png\" alt=\"image-20201214135053884\"></p>\n<p>​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)<em>365</em>24*60 = 5.256 min，这是一个极高的要求。</p>\n<p>​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。</p>\n<h2 id=\"Partition-Tolerance分区容错性\">Partition Tolerance分区容错性</h2>\n<p>​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。<br>\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。<br>\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。</p>\n<h2 id=\"CA-without-P\">CA without P</h2>\n<p>​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。<br>\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。<br>\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：<br>\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。<br>\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。<br>\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。</p>\n<h2 id=\"CP-without-A\">CP without A</h2>\n<p>​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。<br>\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。<br>\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p>\n<h2 id=\"CPwithoutA示例说明\">CPwithoutA示例说明</h2>\n<p>ZooKeeper是个CP（一致性+分区容错性）<br>\n即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。</p>\n<h2 id=\"HBase是强一致性系统\">HBase是强一致性系统</h2>\n<p>Hbase具有以下特点<br>\n•\t每个值只出现在一个REGION<br>\n•\t同一时间一个Region只分配给一个Region服务器<br>\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。<br>\n•\tput操作要么成功，要么完全失败。</p>\n<p>​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。</p>\n<p>​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。</p>\n<h2 id=\"CPwithoutA示例说明-2\">CPwithoutA示例说明</h2>\n<h2 id=\"AP-wihtout-C\">AP wihtout C</h2>\n<p>​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。<br>\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p>\n<h2 id=\"APwithoutC示例说明\">APwithoutC示例说明</h2>\n<p>​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。<br>\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。<br>\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。<br>\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p>\n<h2 id=\"APwithoutC示例说明-2\">APwithoutC示例说明</h2>\n<p>​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。<br>\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。<br>\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\t2000年7月，加州大学伯克利分校的Eric Brewer教授ACM PODC会议上提出CAP猜想。两年后，麻省理工学院的seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认理论。(理论是有时间期限的，没有绝对意义上的公理，是相对于目前计算机科学水平)；</p>\n<h2 id=\"CAP原理概述\">CAP原理概述</h2>\n<p>​      一个分布式系统最多只能同时满足一致性(consistency)、可用性(Availability)、分区容错性(Partition tolerance)的两个</p>\n<p><img src=\"/img/image-20201214134902337.png\" alt=\"image-20201214134902337\"></p>\n<h2 id=\"Consistency-一致性\">Consistency 一致性</h2>\n<p>​\t\t一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。分布式的一致性。<br>\n​\t\t对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。<br>\n​\t\t一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。</p>\n<h2 id=\"三种一致性策略\">三种一致性策略</h2>\n<p>​\t\t对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。<br>\n​\t\t如果能容忍后续的部分或者全部访问不到，则是弱一致性。<br>\n​\t\t如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。<br>\n​\t\tCAP中说，不可能同时满足的这个一致性指的是强一致性。</p>\n<h2 id=\"Availability-可用性\">Availability 可用性</h2>\n<p>​\t\t可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。<br>\n​\t\t对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，在衡量一个系统的可用性的时候，都是通过停机时间来计算的，借鉴淘宝的标准如下：</p>\n<p><img src=\"/img/image-20201214135053884.png\" alt=\"image-20201214135053884\"></p>\n<p>​\t\t通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)<em>365</em>24*60 = 5.256 min，这是一个极高的要求。</p>\n<p>​\t\t好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。</p>\n<h2 id=\"Partition-Tolerance分区容错性\">Partition Tolerance分区容错性</h2>\n<p>​\t\t分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。<br>\n​\t\t分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。<br>\n​\t\t简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。</p>\n<h2 id=\"CA-without-P\">CA without P</h2>\n<p>​\t\t这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。<br>\n比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。<br>\n​\t\t其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到：<br>\n如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。<br>\n​\t\t从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。<br>\n​\t\t所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。</p>\n<h2 id=\"CP-without-A\">CP without A</h2>\n<p>​\t\t如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。<br>\n​\t\t设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。<br>\n​\t\t无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p>\n<h2 id=\"CPwithoutA示例说明\">CPwithoutA示例说明</h2>\n<p>ZooKeeper是个CP（一致性+分区容错性）<br>\n即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下(出现网络分区的情况下，需要重新选主节点，这个时候zookeeper是不能立即响应请求的)，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。</p>\n<h2 id=\"HBase是强一致性系统\">HBase是强一致性系统</h2>\n<p>Hbase具有以下特点<br>\n•\t每个值只出现在一个REGION<br>\n•\t同一时间一个Region只分配给一个Region服务器<br>\n•\t行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。<br>\n•\tput操作要么成功，要么完全失败。</p>\n<p>​\t\t联系上文提到的一致性特点，可以得出HBase是强一致性系统的结论。</p>\n<p>​\t\t当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。</p>\n<h2 id=\"CPwithoutA示例说明-2\">CPwithoutA示例说明</h2>\n<h2 id=\"AP-wihtout-C\">AP wihtout C</h2>\n<p>​\t\t要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。<br>\n​\t\t这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p>\n<h2 id=\"APwithoutC示例说明\">APwithoutC示例说明</h2>\n<p>​\t\t你在xx电商双十一购物的时候，同时下单的并发很高，如果先检查库存，再减库存，确定下单的话，效率会很低，减库存+下单的原子操作成为系统瓶颈，效应时间过长，用户体验就非常差了，为了提高用户体验，不用每一次下单都减库存，而是隔一段时间检查库存，这样会导致商家超卖，用户下单体验好了，超卖的那部分用户的收货时候就会出现库存不足，收货延迟的现象。<br>\n你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。<br>\n​      我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。<br>\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p>\n<h2 id=\"APwithoutC示例说明-2\">APwithoutC示例说明</h2>\n<p>​\t\t上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。<br>\n​\t\t对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。<br>\n​\t\t对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。这其实是分布式领域的另外一个理论——BASE理论(CAP的C变成最终一致性)。</p>\n"},{"title":"分布式全局唯一ID生成策略","author":"郑天祺","date":"2019-10-09T04:00:00.000Z","_content":"\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","source":"_posts/分布式全局唯一ID生成策略.md","raw":"title: 分布式全局唯一ID生成策略\nauthor: 郑天祺\ntags:\n\n  - 分布式\ncategories:\n  - 分布式\ndate: 2019-10-09 12:00:00\n\n---\n\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","slug":"分布式全局唯一ID生成策略","published":1,"updated":"2019-10-15T10:05:44.357Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpl9006ll0t94bg5b9lo","content":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2 id=\"1、雪花算法-snowflake\">1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3 id=\"（1）优点：\"><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3 id=\"（2）缺点：\"><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2 id=\"2、UUID\">2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3 id=\"1-优点：\">(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3 id=\"2-缺点：\">(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2 id=\"3、利用数据库\">3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/#description\">https://docs.mongodb.com/manual/reference/method/ObjectId/#description</a><br>\nLeaf——美团点评分布式ID生成系统: <a href=\"https://tech.meituan.com/2017/04/21/mt-leaf.html\">https://tech.meituan.com/2017/04/21/mt-leaf.html</a><br>\n分布式ID生成 - 雪花算法: <a href=\"https://blog.csdn.net/u012488504/article/details/82194495\">https://blog.csdn.net/u012488504/article/details/82194495</a><br>\n梅丽莎病毒: <a href=\"https://baike.baidu.com/item/%E6%A2%85%E4%B8%BD%E8%8E%8E%E7%97%85%E6%AF%92/9739231\">https://baike.baidu.com/item/梅丽莎病毒/9739231</a><br>\nmysql中InnoDB表为什么要建议用自增列做主键: <a href=\"https://www.cnblogs.com/moyand/p/9013663.html\">https://www.cnblogs.com/moyand/p/9013663.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2 id=\"1、雪花算法-snowflake\">1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3 id=\"（1）优点：\"><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3 id=\"（2）缺点：\"><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2 id=\"2、UUID\">2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3 id=\"1-优点：\">(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3 id=\"2-缺点：\">(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2 id=\"3、利用数据库\">3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/#description\">https://docs.mongodb.com/manual/reference/method/ObjectId/#description</a><br>\nLeaf——美团点评分布式ID生成系统: <a href=\"https://tech.meituan.com/2017/04/21/mt-leaf.html\">https://tech.meituan.com/2017/04/21/mt-leaf.html</a><br>\n分布式ID生成 - 雪花算法: <a href=\"https://blog.csdn.net/u012488504/article/details/82194495\">https://blog.csdn.net/u012488504/article/details/82194495</a><br>\n梅丽莎病毒: <a href=\"https://baike.baidu.com/item/%E6%A2%85%E4%B8%BD%E8%8E%8E%E7%97%85%E6%AF%92/9739231\">https://baike.baidu.com/item/梅丽莎病毒/9739231</a><br>\nmysql中InnoDB表为什么要建议用自增列做主键: <a href=\"https://www.cnblogs.com/moyand/p/9013663.html\">https://www.cnblogs.com/moyand/p/9013663.html</a></p>\n"},{"title":"加密解密","author":"郑天祺","date":"2019-09-02T05:37:00.000Z","_content":"\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","source":"_posts/加密解密.md","raw":"title: 加密解密\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-02 13:37:00\n\n---\n\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","slug":"加密解密","published":1,"updated":"2019-10-15T12:40:18.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpla006ol0t9ahzo64rq","content":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2 id=\"（1）对称加密算法\">（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（2）非对称加密算法\">（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（3）混合加密机制\">（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt=\"\"></p>\n<h3 id=\"加密过程\">加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3 id=\"解密过程\">解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2 id=\"（4）为什么需要混合加密机制？\">（4）为什么需要混合加密机制？</h2>\n<h3 id=\"安全？速度快？\">安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2 id=\"（1）对称加密算法\">（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（2）非对称加密算法\">（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"\"></p>\n<h2 id=\"（3）混合加密机制\">（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt=\"\"></p>\n<h3 id=\"加密过程\">加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3 id=\"解密过程\">解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2 id=\"（4）为什么需要混合加密机制？\">（4）为什么需要混合加密机制？</h2>\n<h3 id=\"安全？速度快？\">安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n"},{"title":"单例模式","author":"郑天祺","date":"2020-01-03T08:24:00.000Z","_content":"\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","source":"_posts/单例模式.md","raw":"title: 单例模式\nauthor: 郑天祺\ntags:\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 16:24:00\n\n---\n\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","slug":"单例模式","published":1,"updated":"2020-01-03T09:24:02.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplb006ql0t913xmd2i3","content":"<p>继之前的单例模式（<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215%EF%BC%89%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%EF%BC%8C%E8%B6%8A%E7%9C%8B%E8%B6%8A%E5%AE%B9%E6%98%93%E4%B8%8D%E6%98%8E%E7%99%BD%E4%BA%86%5B%E5%93%AD%E5%93%AD%5D%EF%BC%9A\">https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</a></p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。<br>\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（<a href=\"https://zhengtianqi.github.io/2019/09/01/%E6%B1%A0%E5%8C%96%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0/%EF%BC%89%E3%80%82\">https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</a></p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>继之前的单例模式（<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215%EF%BC%89%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%EF%BC%8C%E8%B6%8A%E7%9C%8B%E8%B6%8A%E5%AE%B9%E6%98%93%E4%B8%8D%E6%98%8E%E7%99%BD%E4%BA%86%5B%E5%93%AD%E5%93%AD%5D%EF%BC%9A\">https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</a></p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。<br>\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（<a href=\"https://zhengtianqi.github.io/2019/09/01/%E6%B1%A0%E5%8C%96%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0/%EF%BC%89%E3%80%82\">https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</a></p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n"},{"title":"可信与可信计算","author":"郑天祺","date":"2019-09-28T08:55:00.000Z","_content":"\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","source":"_posts/可信与可信计算.md","raw":"title: 可信与可信计算\nauthor: 郑天祺\ntags:\n  - 可信计算\ncategories:\n  - 可信\ndate: 2019-09-28 16:55:00\n\n---\n\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","slug":"可信与可信计算","published":1,"updated":"2019-10-15T10:07:21.402Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplc006ul0t96mii96qi","content":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n"},{"title":"原子性，有序性，可见性","author":"郑天祺","date":"2020-08-11T03:04:00.000Z","_content":"","source":"_posts/原子性，有序性，可见性.md","raw":"title: 原子性，有序性，可见性\nauthor: 郑天祺\ntags:\n  - 多线程\ncategories:\n  - java基础\ndate: 2020-08-11 11:04:00\n---\n","slug":"原子性，有序性，可见性","published":1,"updated":"2020-08-11T03:04:55.668Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpld006xl0t96kvg9wlx","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"可信基本概念","author":"郑天祺","date":"2019-09-01T06:46:00.000Z","_content":"\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","source":"_posts/可信基本概念.md","raw":"title: 可信基本概念\nauthor: 郑天祺\ntags:\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-01 14:46:00\n\n---\n\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","slug":"可信基本概念","published":1,"updated":"2019-10-15T10:10:52.269Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvple0071l0t94am09o5a","content":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2 id=\"（1）可信1-0（软件容错）\">（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2 id=\"（2）可信2-0（硬件可信）\">（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2 id=\"（3）可信3-0（主动防御体系）\">（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3 id=\"（a）TPCM\">（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3 id=\"（b）-可信基础支撑软件框架\">（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3 id=\"（c）三层三元对等的可信连接框架\">（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3 id=\"（d）加密算法均自主设计\">（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3 id=\"（f）主动免疫可信架构信任链传递示意图：\">（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt=\"\"></p>\n<h2 id=\"2、可信的应用\">2、可信的应用</h2>\n<h2 id=\"（1）基础架构图\">（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt=\"\"></p>\n<h2 id=\"（2）可信的安全保障机制\">（2）可信的安全保障机制</h2>\n<h3 id=\"（a）运行环境\">（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3 id=\"（b）监控技术\">（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3 id=\"（c）隔离技术\">（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3 id=\"（d）接入技术\">（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2 id=\"（1）可信1-0（软件容错）\">（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2 id=\"（2）可信2-0（硬件可信）\">（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2 id=\"（3）可信3-0（主动防御体系）\">（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3 id=\"（a）TPCM\">（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3 id=\"（b）-可信基础支撑软件框架\">（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3 id=\"（c）三层三元对等的可信连接框架\">（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3 id=\"（d）加密算法均自主设计\">（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3 id=\"（f）主动免疫可信架构信任链传递示意图：\">（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt=\"\"></p>\n<h2 id=\"2、可信的应用\">2、可信的应用</h2>\n<h2 id=\"（1）基础架构图\">（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt=\"\"></p>\n<h2 id=\"（2）可信的安全保障机制\">（2）可信的安全保障机制</h2>\n<h3 id=\"（a）运行环境\">（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3 id=\"（b）监控技术\">（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3 id=\"（c）隔离技术\">（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3 id=\"（d）接入技术\">（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n"},{"title":"可重入锁","author":"郑天祺","date":"2019-08-31T05:05:00.000Z","_content":"\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","source":"_posts/可重入锁.md","raw":"title: 可重入锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:05:00\n\n---\n\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","slug":"可重入锁","published":1,"updated":"2019-10-15T10:07:40.790Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvple0074l0t93njvhcbd","content":"<h2 id=\"1、可重入锁：\">1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。<br>\n​\t“独占”，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2 id=\"2、Synchronized和ReentrantLock\">2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable &#123;\n     public synchronized void get() &#123;\n         System.out.println(Thread.currentThread().getName());\n         set();\n     &#125;\n     public synchronized void set() &#123;\n         System.out.println(Thread.currentThread().getName());\n     &#125;\n     @Override\n     public void run() &#123;\n         get();\n     &#125;\n     public static void main(String[] args) &#123;\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     &#125;\n &#125;\n\n \n\npublic class ReentrantLockTest implements Runnable &#123;\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     &#125;\n\n    public void set() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     &#125;\n\n    @Override\n     public void run() &#123;\n         get();\n     &#125;\n\n    public static void main(String[] args) &#123;\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     &#125;\n &#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、可重入锁：\">1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。<br>\n​\t“独占”，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2 id=\"2、Synchronized和ReentrantLock\">2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable &#123;\n     public synchronized void get() &#123;\n         System.out.println(Thread.currentThread().getName());\n         set();\n     &#125;\n     public synchronized void set() &#123;\n         System.out.println(Thread.currentThread().getName());\n     &#125;\n     @Override\n     public void run() &#123;\n         get();\n     &#125;\n     public static void main(String[] args) &#123;\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     &#125;\n &#125;\n\n \n\npublic class ReentrantLockTest implements Runnable &#123;\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     &#125;\n\n    public void set() &#123;\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     &#125;\n\n    @Override\n     public void run() &#123;\n         get();\n     &#125;\n\n    public static void main(String[] args) &#123;\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     &#125;\n &#125;\n\n \n</code></pre>\n"},{"title":"可靠性和容错技术","author":"郑天祺","date":"2019-10-02T05:39:00.000Z","_content":"\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","source":"_posts/可靠性和容错技术.md","raw":"title: 可靠性和容错技术\nauthor: 郑天祺\ntags:\n  - 可靠\n  - 容错\ncategories:\n  - 可信\ndate: 2019-10-02 13:39:00\n\n---\n\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","slug":"可靠性和容错技术","published":1,"updated":"2019-10-02T06:48:35.455Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplf0076l0t92cxpf9qk","content":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n"},{"title":"图解公钥私钥","author":"郑天祺","date":"2019-09-24T13:32:00.000Z","_content":"\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","source":"_posts/图解公钥与私钥.md","raw":"title: 图解公钥私钥\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-24 21:32:00\n---\n\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","slug":"图解公钥与私钥","published":1,"updated":"2019-09-24T13:51:57.731Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplg007bl0t9amlfdmj9","content":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n"},{"title":"基于JavaAgent的全链路监控（1）","author":"郑天祺","date":"2020-07-17T05:11:00.000Z","_content":"\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（1）.md","raw":"title: 基于JavaAgent的全链路监控（1）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-17 13:11:00\n\n---\n\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","slug":"基于JavaAgent的全链路监控（1）","published":1,"updated":"2020-07-19T14:50:51.434Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplh007el0t9bt990bz3","content":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2 id=\"（1）建立maven的空java项目\">（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2 id=\"（2）MANIFEST-MF-文件\">（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"（3）写一个main函数\">（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n\n&#125;\n</code></pre>\n<h2 id=\"（4）打包\">（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2 id=\"（1）主程序运行前\">（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2 id=\"（2）主程序运行后加载\">（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;$&#123;java.home&#125;/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException &#123;\n        VirtualMachine virtualMachine = null;\n        try &#123;\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        &#125; finally &#123;\n            if (virtualMachine != null) &#123;\n                virtualMachine.detach();\n            &#125;\n        &#125;\n\n    &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2 id=\"（1）建立maven的空java项目\">（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2 id=\"（2）MANIFEST-MF-文件\">（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"（3）写一个main函数\">（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n\n&#125;\n</code></pre>\n<h2 id=\"（4）打包\">（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2 id=\"（1）主程序运行前\">（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2 id=\"（2）主程序运行后加载\">（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;$&#123;java.home&#125;/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException &#123;\n        VirtualMachine virtualMachine = null;\n        try &#123;\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        &#125; finally &#123;\n            if (virtualMachine != null) &#123;\n                virtualMachine.detach();\n            &#125;\n        &#125;\n\n    &#125;\n</code></pre>\n"},{"title":"基于JavaAgent的全链路监控（2）","author":"郑天祺","date":"2020-07-19T08:54:00.000Z","_content":"\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","source":"_posts/基于JavaAgent的全链路监控（2）.md","raw":"title: 基于JavaAgent的全链路监控（2）\nauthor: 郑天祺\ntags:\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 16:54:00\n\n---\n\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","slug":"基于JavaAgent的全链路监控（2）","published":1,"updated":"2020-07-19T14:50:44.890Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpli007hl0t9cr0w7yby","content":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2 id=\"1、介绍\">1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2 id=\"2、pom-xml\">2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;javassist.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2 id=\"3、MethodCostTime-java\">3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime &#123;\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception &#123;\n        long start = System.currentTimeMillis();\n        try &#123;\n            // 原有函数执行\n            return callable.call();\n        &#125; finally &#123;\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、MyAgent-java\">4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; &#123;\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        &#125;;\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() &#123;\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) &#123;\n\n            &#125;\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) &#123;\n\n            &#125;\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n        &#125;;\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"5、MANIFEST-MF\">5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"6、测试\">6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2 id=\"1、介绍\">1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2 id=\"2、pom-xml\">2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;javassist.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;$&#123;byte-buddy.version&#125;&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2 id=\"3、MethodCostTime-java\">3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime &#123;\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception &#123;\n        long start = System.currentTimeMillis();\n        try &#123;\n            // 原有函数执行\n            return callable.call();\n        &#125; finally &#123;\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        &#125;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"4、MyAgent-java\">4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; &#123;\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        &#125;;\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() &#123;\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) &#123;\n\n            &#125;\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) &#123;\n\n            &#125;\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) &#123;\n\n            &#125;\n\n        &#125;;\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n\n</code></pre>\n<h2 id=\"5、MANIFEST-MF\">5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2 id=\"6、测试\">6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n"},{"title":"基于JavaAgent的全链路监控（3）","author":"郑天祺","date":"2020-07-19T09:25:00.000Z","_content":"\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（3）.md","raw":"title: 基于JavaAgent的全链路监控（3）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 17:25:00\n\n---\n\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","slug":"基于JavaAgent的全链路监控（3）","published":1,"updated":"2020-07-19T14:50:37.560Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplj007kl0t9fza3bo2d","content":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2 id=\"（1）MyAgent-java\">（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        &#123;\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        &#125;, 0L, 1000L, TimeUnit.MILLISECONDS);\n     &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2 id=\"（1）MyAgent-java\">（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent &#123;\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) &#123;\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        &#123;\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        &#125;, 0L, 1000L, TimeUnit.MILLISECONDS);\n     &#125;\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) &#123;\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"基于JavaAgent的全链路监控（4）","author":"郑天祺","date":"2020-07-19T14:55:00.000Z","_content":"","source":"_posts/基于JavaAgent的全链路监控（4）.md","raw":"title: 基于JavaAgent的全链路监控（4）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 22:55:00\n\n---\n","slug":"基于JavaAgent的全链路监控（4）","published":1,"updated":"2020-07-20T23:17:55.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplj007nl0t9cboddl4c","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"对象存储与指针压缩","author":"郑天祺","date":"2019-11-20T11:50:00.000Z","_content":"\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","source":"_posts/对象存储与指针压缩.md","raw":"title: 对象存储与指针压缩\nauthor: 郑天祺\ntags:\n  - 内存模型\ncategories:\n  - java基础\ndate: 2019-11-20 19:50:00\n\n---\n\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","slug":"对象存储与指针压缩","published":1,"updated":"2019-11-20T12:03:43.218Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplk007ql0t93hjj593k","content":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：<a href=\"http://www.ideabuffer.cn/2017/05/06/Java%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/\">http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：<a href=\"http://www.ideabuffer.cn/2017/05/06/Java%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/\">http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</a></p>\n"},{"title":"并发编程总结","author":"郑天祺","date":"2020-11-17T08:57:00.000Z","_content":"\n1、Synchronized\n\n​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。\n\n​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。\n\n​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。\n\n​\t\tSynchronize是非公平锁。\n\n2、Synchronized锁对象\n\n​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。\n（2）如果没有明确指定:\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。\n\n3、Synchronized可重入锁\n\n​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。\n\n4、JVM对java的原生锁的优化\n\n​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。\n\n​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁\n\n​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。\n\n​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。\n\n​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。\n\n​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。\n\n5、Synchronize 和 ReentrantLock实现原理的不同\n\nSynchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。\n\n6、AQS框架\n\n​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。\n\n（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。\n\n（2）\n\n","source":"_posts/并发编程总结.md","raw":"title: 并发编程总结\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - 面试\ndate: 2020-11-17 16:57:00\n\n---\n\n1、Synchronized\n\n​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。\n\n​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。\n\n​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。\n\n​\t\tSynchronize是非公平锁。\n\n2、Synchronized锁对象\n\n​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。\n（2）如果没有明确指定:\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。\n\n3、Synchronized可重入锁\n\n​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。\n\n4、JVM对java的原生锁的优化\n\n​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。\n\n​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁\n\n​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。\n\n​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。\n\n​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。\n\n​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。\n\n5、Synchronize 和 ReentrantLock实现原理的不同\n\nSynchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。\n\n6、AQS框架\n\n​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。\n\n（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。\n\n（2）\n\n","slug":"并发编程总结","published":1,"updated":"2020-11-17T09:55:28.668Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpll007tl0t95sit8d1d","content":"<p>1、Synchronized</p>\n<p>​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。</p>\n<p>​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。</p>\n<p>​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。</p>\n<p>​\t\tSynchronize是非公平锁。</p>\n<p>2、Synchronized锁对象</p>\n<p>​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。<br>\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。<br>\n（2）如果没有明确指定:<br>\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;<br>\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。<br>\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。</p>\n<p>3、Synchronized可重入锁</p>\n<p>​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。<br>\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。</p>\n<p>4、JVM对java的原生锁的优化</p>\n<p>​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。</p>\n<p>​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁</p>\n<p>​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。</p>\n<p>​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。</p>\n<p>​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。</p>\n<p>​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。</p>\n<p>5、Synchronize 和 ReentrantLock实现原理的不同</p>\n<p>Synchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。</p>\n<p>6、AQS框架</p>\n<p>​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。</p>\n<p>（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。</p>\n<p>（2）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、Synchronized</p>\n<p>​\t\tSynchronized是由JVM实现的一种实现互斥同步的一种方式，如果你查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了monitor enter和monitor exit两个字节码指令。</p>\n<p>​\t\t这两个指令是什么意思呢?在虚拟机执行到monitor enter指令时，首先要尝试获取对象的锁︰如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1;当执行monitorexit指令时将锁计数器-1﹔当计数器为O时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。</p>\n<p>​\t\tJava中Synchronize通过在对象头设置标记，达到了获取锁和释放锁的目的。</p>\n<p>​\t\tSynchronize是非公平锁。</p>\n<p>2、Synchronized锁对象</p>\n<p>​\t\t“锁”的本质其实是monitorenter和monitorexit字节码指令的一个Reference类型的参数，即要锁定和解锁的对象。我们知道，使用Synchronized可以修饰不同的对象，因此，对应的对象锁可以这么确定。<br>\n（1）如果 Synchronized 明确指定了锁对象，比如 Synchronized(变量名)、Synchronized(this)等，说明加解锁对象为该对象。<br>\n（2）如果没有明确指定:<br>\n若Synchronized修饰的方法为非静态方法，表示此方法对应的对象为锁对象;<br>\n若Synchronized修饰的方法为静态方法，则表示此方法对应的类对象为锁对象。<br>\n注意，当一个对象被锁住时，对象里面所有用Synchronized修饰的方法都将产生堵塞，而对象里非 Synchronized修饰的方法可正常被调用，不受锁影响。</p>\n<p>3、Synchronized可重入锁</p>\n<p>​\t\t可重入性是锁的一个基本要求，是为了解决自己锁死自己的情况。比如下面的伪代码，一个类中的同步方法调用另一个同步方法，假如Synchronized 不支持重入，进入method2方法时当前线程获得锁，method2方法里面执行method1时当前线程又要去尝试获取锁，这时如果不支持重入，它就要等释放，把自己阻塞，导致自己锁死自己。<br>\n​\t\t对Synchronized来说，可重入性是显而易见的，刚才提到，在执行monitor enter指令时，如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁(而不是已拥有了锁则不能继续获取)，就把锁的计数器+1，其实本质上就通过这种方式实现了可重入性。</p>\n<p>4、JVM对java的原生锁的优化</p>\n<p>​\t\t在Java 6之前，Monitor的实现完全依赖底层操作系统的互斥锁来实现,也就是我们刚才在问题二中所阐述的获取/释放锁的逻辑。由于Java层面的线程与操作系统的原生线程有映射关系，如果要将一个线程进行阻塞或唤起都需要操作系统的协助，这就需要从用户态切换到内核态来执行，这种切换代价十分昂贵，很耗处理器时间，现代JDK中做了大量的优化。</p>\n<p>​\t\t一种优化是使用自旋锁，即在把线程进行阻塞操作之前先让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无需再让线程执行阻塞操作，避免了用户态到内核态的切换。现代JDK中还提供了三种不同的Monitor实现，也就是三种不同的锁: 偏向锁、轻量级锁、重量级锁</p>\n<p>​\t\t这三种锁使得JDK得以优化Synchronized的运行，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这就是锁的升级、降级。</p>\n<p>​\t\t当没有竞争出现时，默认会使用偏向锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁，因为在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。</p>\n<p>​\t\t如果有另一线程试图锁定某个被偏斜过的对象，JVM就撤销偏斜锁，切换到轻量级锁实现。</p>\n<p>​\t\t轻量级锁依赖CAS操作 Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁;否则,进—步升级为重量级锁。</p>\n<p>5、Synchronize 和 ReentrantLock实现原理的不同</p>\n<p>Synchronized通过在对象头中设置标记实现了这一目的，是一种JVM原生的锁实现方式，而 ReentrantLock 以及所有的基于Lock接口的实现类，都是通过用一个volitile 修饰的int型变量，并保证每个线程都能拥有对该int的可见性和原子修改，其本质是基于所谓的AQS框架。</p>\n<p>6、AQS框架</p>\n<p>​\t\tAQS(AbstractQueuedSynchronizer类)是一个用来构建锁和同步器的框架,各种Lock包中的锁(常用的有ReentrantLock、ReadWriteLock)，以及其他如Semaphore、CountDownLatch，甚至是早期的FutureTask 等，都是基于AQS来构建。</p>\n<p>（1）AQS内部定义了一个volatile int state变量，表示同步状态：当线程调用lock方法时，如果state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将state=1;如果state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。</p>\n<p>（2）</p>\n"},{"title":"悲观锁、乐观锁","author":"郑天祺","date":"2019-08-31T05:16:00.000Z","_content":"\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","source":"_posts/悲观锁、乐观锁.md","raw":"title: 悲观锁、乐观锁\nauthor: 郑天祺\ntags:\n  - 锁\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 13:16:00\n\n---\n\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","slug":"悲观锁、乐观锁","published":1,"updated":"2019-10-15T12:17:44.237Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplm007wl0t995kz61bd","content":"<h2 id=\"1、悲观锁\">1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2 id=\"2、缺点\">2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2 id=\"3、实现方法\">3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=“123” for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2 id=\"4、使用场景举例\">4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3 id=\"0、乐观锁：\">0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3 id=\"1、乐观锁：\">1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3 id=\"2、优缺点：\">2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3 id=\"3、步骤\">3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#&#123;id&#125;\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#&#123;id&#125; and versio&#123;139&#125;&#125;;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、悲观锁\">1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2 id=\"2、缺点\">2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2 id=\"3、实现方法\">3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=“123” for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2 id=\"4、使用场景举例\">4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3 id=\"0、乐观锁：\">0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3 id=\"1、乐观锁：\">1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3 id=\"2、优缺点：\">2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3 id=\"3、步骤\">3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#&#123;id&#125;\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#&#123;id&#125; and versio&#123;139&#125;&#125;;\n</code></pre>\n"},{"title":"手写一个简单Autowired","author":"郑天祺","date":"2020-09-11T02:49:00.000Z","_content":"\n# 1、按照惯例\n\n首先写一个controller和service\n\n```java\npackage cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController {\n    @Autowired\n    private UserService userService;\n    \n}\n\n```\n\n```java\npackage cn.edu.bjut.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n\n```\n\n# 2、Autowired注解的定义\n\n```java\npackage cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired {\n\n}\n\n```\n\n# 3、注入的方法\n\n利用反射\n\n```java\npackage cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired {\n    public static void main(String[] args) {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -> {\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) {\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class<?> type = field.getType();\n                try {\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                } catch (InstantiationException e) {\n                    e.printStackTrace();\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                }\n            }\n        });\n    }\n}\n```\n\n","source":"_posts/手写一个简单Autowired.md","raw":"title: 手写一个简单Autowired\nauthor: 郑天祺\ntags:\n  - spring\ncategories:\n  - spring\ndate: 2020-09-11 10:49:00\n\n---\n\n# 1、按照惯例\n\n首先写一个controller和service\n\n```java\npackage cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController {\n    @Autowired\n    private UserService userService;\n    \n}\n\n```\n\n```java\npackage cn.edu.bjut.spring.service;\n\npublic class UserService {\n    public String findUserById(String id) {\n        return null;\n    }\n}\n\n```\n\n# 2、Autowired注解的定义\n\n```java\npackage cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired {\n\n}\n\n```\n\n# 3、注入的方法\n\n利用反射\n\n```java\npackage cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired {\n    public static void main(String[] args) {\n        UserController userController = new UserController();\n        Class<? extends UserController> clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -> {\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) {\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class<?> type = field.getType();\n                try {\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                } catch (InstantiationException e) {\n                    e.printStackTrace();\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                }\n            }\n        });\n    }\n}\n```\n\n","slug":"手写一个简单Autowired","published":1,"updated":"2020-09-11T03:07:29.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpln0080l0t980cva536","content":"<h1>1、按照惯例</h1>\n<p>首先写一个controller和service</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController &#123;\n    @Autowired\n    private UserService userService;\n    \n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>2、Autowired注解的定义</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired &#123;\n\n&#125;\n\n</code></pre>\n<h1>3、注入的方法</h1>\n<p>利用反射</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired &#123;\n    public static void main(String[] args) &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -&gt; &#123;\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) &#123;\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class&lt;?&gt; type = field.getType();\n                try &#123;\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                &#125; catch (InstantiationException e) &#123;\n                    e.printStackTrace();\n                &#125; catch (IllegalAccessException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、按照惯例</h1>\n<p>首先写一个controller和service</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.controller;\n\nimport cn.edu.bjut.spring.Autowired;\nimport cn.edu.bjut.spring.service.UserService;\n\npublic class UserController &#123;\n    @Autowired\n    private UserService userService;\n    \n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring.service;\n\npublic class UserService &#123;\n    public String findUserById(String id) &#123;\n        return null;\n    &#125;\n&#125;\n\n</code></pre>\n<h1>2、Autowired注解的定义</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Autowired &#123;\n\n&#125;\n\n</code></pre>\n<h1>3、注入的方法</h1>\n<p>利用反射</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.spring;\n\nimport cn.edu.bjut.spring.controller.UserController;\nimport java.util.stream.Stream;\n\npublic class TestAutowired &#123;\n    public static void main(String[] args) &#123;\n        UserController userController = new UserController();\n        Class&lt;? extends UserController&gt; clazz = userController.getClass();\n        // 获取所有的属性值\n        Stream.of(clazz.getDeclaredFields()).forEach(field -&gt; &#123;\n            // 只有通过方法才能够设置具体的属性值\n            String name = field.getName();\n\n            Autowired annotation = field.getAnnotation(Autowired.class);\n            if (annotation != null) &#123;\n                field.setAccessible(true);\n                // 获取属性的类型\n                Class&lt;?&gt; type = field.getType();\n                try &#123;\n                    // new一个新实例\n                    Object o = type.newInstance();\n                    field.set(userController, o);\n                &#125; catch (InstantiationException e) &#123;\n                    e.printStackTrace();\n                &#125; catch (IllegalAccessException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n</code></pre>\n"},{"title":"排序之比较器Comparable<T>","author":"郑天祺","date":"2020-01-02T02:27:00.000Z","_content":"\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","source":"_posts/排序之比较器.md","raw":"title: 排序之比较器Comparable<T>\nauthor: 郑天祺\ntags:\n  - java\n  - 数据结构\n  - ''\ncategories:\n  - java基础\ndate: 2020-01-02 10:27:00\n---\n\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","slug":"排序之比较器","published":1,"updated":"2020-01-02T03:45:16.885Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpln0083l0t9eylug4ra","content":"<h1>一、Comparable<T>比较器的使用</h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：</p>\n<p>​\t\t\t1、实体继承Comparable<T></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; &#123;\n    private String name;\n    private int age;\n\n    public Student(String name, int age) &#123;\n        this.name = name;\n        this.age = age;\n    &#125;\n\n    @Override\n    public int compareTo(Student o) &#123;\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    &#125;\n\n    public void print() &#123;\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = &#123;stu1, stu2, stu3, stu4&#125;;\n\n        Arrays.sort(students);\n        for (Student stu : students) &#123;\n            stu.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>二、Comparable<T>比较器的原理</h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable &#123;\n\n    public static class BinaryTree&lt;T&gt; &#123;\n        class Node &#123;\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) &#123;\n                this.data = data;\n            &#125;\n\n            void addNode(Node newNode) &#123;\n                if (newNode.data.compareTo((T) this.data) &lt; 0) &#123;\n                    if (this.left == null) &#123;\n                        this.left = newNode;\n                    &#125; else &#123;\n                        this.left.addNode(newNode);\n                    &#125;\n                &#125;\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) &#123;\n                    if (this.right == null) &#123;\n                        this.right = newNode;\n                    &#125; else &#123;\n                        this.right.addNode(newNode);\n                    &#125;\n                &#125;\n            &#125;\n\n            void print() &#123;\n                if (this.left != null) &#123;\n                    left.print();\n                &#125;\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) &#123;\n                    this.right.print();\n                &#125;\n            &#125;\n        &#125;\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) &#123;\n            Node newNode = new Node(data);\n            if (root == null) &#123;\n                root = newNode;\n            &#125; else &#123;\n                root.addNode(newNode);\n            &#125;\n        &#125;\n\n        public void print() &#123;\n            this.root.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t&#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparable<T>比较器的使用</h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：</p>\n<p>​\t\t\t1、实体继承Comparable<T></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; &#123;\n    private String name;\n    private int age;\n\n    public Student(String name, int age) &#123;\n        this.name = name;\n        this.age = age;\n    &#125;\n\n    @Override\n    public int compareTo(Student o) &#123;\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    &#125;\n\n    public void print() &#123;\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    &#125;\n&#125;\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = &#123;stu1, stu2, stu3, stu4&#125;;\n\n        Arrays.sort(students);\n        for (Student stu : students) &#123;\n            stu.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h1>二、Comparable<T>比较器的原理</h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable &#123;\n\n    public static class BinaryTree&lt;T&gt; &#123;\n        class Node &#123;\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) &#123;\n                this.data = data;\n            &#125;\n\n            void addNode(Node newNode) &#123;\n                if (newNode.data.compareTo((T) this.data) &lt; 0) &#123;\n                    if (this.left == null) &#123;\n                        this.left = newNode;\n                    &#125; else &#123;\n                        this.left.addNode(newNode);\n                    &#125;\n                &#125;\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) &#123;\n                    if (this.right == null) &#123;\n                        this.right = newNode;\n                    &#125; else &#123;\n                        this.right.addNode(newNode);\n                    &#125;\n                &#125;\n            &#125;\n\n            void print() &#123;\n                if (this.left != null) &#123;\n                    left.print();\n                &#125;\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) &#123;\n                    this.right.print();\n                &#125;\n            &#125;\n        &#125;\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) &#123;\n            Node newNode = new Node(data);\n            if (root == null) &#123;\n                root = newNode;\n            &#125; else &#123;\n                root.addNode(newNode);\n            &#125;\n        &#125;\n\n        public void print() &#123;\n            this.root.print();\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<pre><code class=\"language-java\">public class Test &#123;\n    public static void main(String[] args) &#123;\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t&#125;\n&#125;\n</code></pre>\n"},{"title":"排序之比较器Comparator<T>","author":"郑天祺","date":"2020-01-02T03:34:00.000Z","_content":"\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","source":"_posts/排序之比较器Comparator-T.md","raw":"title: 排序之比较器Comparator<T>\nauthor: 郑天祺\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2020-01-02 11:34:00\n\n---\n\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","slug":"排序之比较器Comparator-T","published":1,"updated":"2020-01-02T04:00:12.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplo0087l0t950igc1a7","content":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) &#123;\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() &#123;\n            @Override\n            public int compare(Student o1, Student o2) &#123;\n                return o1.getAge() - o2.getAge();\n            &#125;\n        &#125;);\n    &#125;\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) &#123;\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() &#123;\n            @Override\n            public int compare(Student o1, Student o2) &#123;\n                return o1.getAge() - o2.getAge();\n            &#125;\n        &#125;);\n    &#125;\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n"},{"title":"排序方法","author":"郑天祺","date":"2020-10-23T07:49:00.000Z","_content":"","source":"_posts/排序方法.md","raw":"title: 排序方法\nauthor: 郑天祺\ntags:\n  - 排序\ncategories:\n  - 面试\ndate: 2020-10-23 15:49:00\n---\n","slug":"排序方法","published":1,"updated":"2020-10-23T07:49:55.206Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplp008al0t9gg3k75lz","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"数字签名","author":"郑天祺","date":"2019-09-01T12:23:00.000Z","_content":"# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","source":"_posts/数字签名.md","raw":"title: 数字签名\nauthor: 郑天祺\ntags:\n  - 可信\n  - 加密算法\ncategories:\n  - 可信\ndate: 2019-09-01 20:23:00\n---\n# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","slug":"数字签名","published":1,"updated":"2019-10-15T12:28:53.710Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplq008el0t976uq2sdy","content":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2 id=\"（1）椭圆曲线数字签名算法\">（1）椭圆曲线数字签名算法:</h2>\n<h3 id=\"生成数字签名\">生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3 id=\"验证数字签名\">验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3 id=\"验证方法解释\">验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3 id=\"原理解释：\">原理解释：</h3>\n<p><a href=\"https://www.cnblogs.com/wsonepiece/p/3977021.html\">https://www.cnblogs.com/wsonepiece/p/3977021.html</a></p>\n<h2 id=\"（2）Schnorr数字签名算法\">（2）Schnorr数字签名算法</h2>\n<h3 id=\"生成数字签名-2\">生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3 id=\"验证数字签名-2\">验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3 id=\"验证方法解释-2\">验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2 id=\"（1）椭圆曲线数字签名算法\">（1）椭圆曲线数字签名算法:</h2>\n<h3 id=\"生成数字签名\">生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3 id=\"验证数字签名\">验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3 id=\"验证方法解释\">验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3 id=\"原理解释：\">原理解释：</h3>\n<p><a href=\"https://www.cnblogs.com/wsonepiece/p/3977021.html\">https://www.cnblogs.com/wsonepiece/p/3977021.html</a></p>\n<h2 id=\"（2）Schnorr数字签名算法\">（2）Schnorr数字签名算法</h2>\n<h3 id=\"生成数字签名-2\">生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3 id=\"验证数字签名-2\">验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3 id=\"验证方法解释-2\">验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n"},{"title":"池化之线程池","author":"郑天祺","date":"2019-09-01T02:14:00.000Z","_content":"\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n## 3、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n\n","source":"_posts/池化之线程池.md","raw":"title: 池化之线程池\nauthor: 郑天祺\ntags:\n  - java\n  - 多线程\ncategories:\n  - java基础\ndate: 2019-09-01 10:14:00\n\n---\n\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n## 3、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n\n","slug":"池化之线程池","published":1,"updated":"2019-10-15T10:04:50.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplr008hl0t9hx0keem9","content":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2 id=\"1、池化技术\">1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2 id=\"2、线程池的创建\">2、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h2 id=\"3、线程池的关闭\">3、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2 id=\"1、池化技术\">1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2 id=\"2、线程池的创建\">2、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h2 id=\"3、线程池的关闭\">3、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n"},{"title":"文件上传之Content-Type","author":"郑天祺","date":"2019-08-31T08:15:00.000Z","_content":"\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","source":"_posts/文件上传.md","raw":"title: 文件上传之Content-Type\nauthor: 郑天祺\ntags:\n  - 文件上传\ncategories:\n  - java基础\ndate: 2019-08-31 16:15:00\n\n---\n\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","slug":"文件上传","published":1,"updated":"2019-10-15T12:09:54.589Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpls008kl0t9g8u1f1wa","content":"<h2 id=\"1、Content-Type介绍\">1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3 id=\"1-1、常见的页面资源类型\">1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3 id=\"1-2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\">1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file)&#123;\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n&#125;\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) &#123;\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n&#125;\n</code></pre>\n<p>​</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、Content-Type介绍\">1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3 id=\"1-1、常见的页面资源类型\">1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3 id=\"1-2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\">1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file)&#123;\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n&#125;\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) &#123;\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n&#125;\n</code></pre>\n<p>​</p>\n"},{"title":"用户行为特征提取","author":"郑天祺","date":"2019-12-23T03:10:00.000Z","_content":"\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","source":"_posts/特征提取-简单流程.md","raw":"title: 用户行为特征提取\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-23 11:10:00\n---\n\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","slug":"特征提取-简单流程","published":1,"updated":"2019-12-23T09:58:46.583Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpls008nl0t9d8z8g6yt","content":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2 id=\"（1）、时延\">（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2 id=\"（2）、吞吐量\">（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2 id=\"（3）、应用：\">（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2 id=\"（1）统计结果\">（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2 id=\"（2）分析项目的目的\">（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2 id=\"（1）数据源-与-统计指标（KPI）分析\">（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2 id=\"（2）数据源-与-统计指标（KPI）的关系\">（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2 id=\"（1）流程设计\">（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2 id=\"（2）统计指标设计\">（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况：<a href=\"https://blog.csdn.net/weixin_35353187/article/details/84303518\">https://blog.csdn.net/weixin_35353187/article/details/84303518</a></p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2 id=\"（1）、时延\">（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2 id=\"（2）、吞吐量\">（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2 id=\"（3）、应用：\">（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2 id=\"（1）统计结果\">（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2 id=\"（2）分析项目的目的\">（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2 id=\"（1）数据源-与-统计指标（KPI）分析\">（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2 id=\"（2）数据源-与-统计指标（KPI）的关系\">（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2 id=\"（1）流程设计\">（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2 id=\"（2）统计指标设计\">（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况：<a href=\"https://blog.csdn.net/weixin_35353187/article/details/84303518\">https://blog.csdn.net/weixin_35353187/article/details/84303518</a></p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n"},{"title":"理解IO阻塞与非阻塞","author":"郑天祺","date":"2019-08-30T09:34:00.000Z","_content":"\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll、poll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n### （1）select 时间复杂度O(n)\n\n#### 过程 \n\n（1）从用户空间拷贝fd_set到内核空间\n\n（2）注册回调函数\n\n（3）遍历所有fd，调用其对应的poll方法\n\n（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n\n（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列\n\n（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值\n\n（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n\n（8）把fd_set从内核空间拷贝到用户空间。\n\n#### 总结\n\n​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。\n\n​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。\n\n​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （2）poll 时间复杂度O(n)\n\n#### 过程\n\n```java\nwhile true  \n{  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    {  \n        if i has data  \n        read until unavailable  \n    }  \n} \n```\n\n#### 总结\n\n​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （3）epoll 时间复杂度O(1)\n\n#### 过程\n\n```java\n{  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    {  \n        read or write till  \n    }  \n}  \n```\n\n\n\n#### 总结\n\n​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。\n\n​\t\t内核和用户空间共享一块内存来实现的\n\n​\t优点：\n\n​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）\n\n​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。\n\n​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。\n\n#### 总结：\n\n​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。\n\n## 8、多路复用的好处\n\n​\t\tselect，poll，epoll都是IO多路复用的机制。\n\n​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。\n\n​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销\n\n​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。","source":"_posts/理解IO阻塞与非阻塞.md","raw":"title: 理解IO阻塞与非阻塞\nauthor: 郑天祺\ntags:\n  - IO\n  - 阻塞与非阻塞\ncategories:\n  - 网络\ndate: 2019-08-30 17:34:00\n\n---\n\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll、poll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n### （1）select 时间复杂度O(n)\n\n#### 过程 \n\n（1）从用户空间拷贝fd_set到内核空间\n\n（2）注册回调函数\n\n（3）遍历所有fd，调用其对应的poll方法\n\n（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n\n（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列\n\n（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值\n\n（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n\n（8）把fd_set从内核空间拷贝到用户空间。\n\n#### 总结\n\n​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。\n\n​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。\n\n​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （2）poll 时间复杂度O(n)\n\n#### 过程\n\n```java\nwhile true  \n{  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    {  \n        if i has data  \n        read until unavailable  \n    }  \n} \n```\n\n#### 总结\n\n​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。\n\n​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作\n\n### （3）epoll 时间复杂度O(1)\n\n#### 过程\n\n```java\n{  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    {  \n        read or write till  \n    }  \n}  \n```\n\n\n\n#### 总结\n\n​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。\n\n​\t\t内核和用户空间共享一块内存来实现的\n\n​\t优点：\n\n​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）\n\n​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。\n\n​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。\n\n#### 总结：\n\n​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。\n\n## 8、多路复用的好处\n\n​\t\tselect，poll，epoll都是IO多路复用的机制。\n\n​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。\n\n​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销\n\n​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。","slug":"理解IO阻塞与非阻塞","published":1,"updated":"2020-07-21T07:24:00.310Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplt008ql0t933lv7ppx","content":"<h2 id=\"1、饭店吃饭的例子\">1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 – 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 – 【非阻塞I/O 】</p>\n<h2 id=\"2、阻塞与非阻塞调用对比\">2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h2 id=\"3、阻塞IO\">3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"4、非阻塞IO\">4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"5、I-O复用模型\">5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2 id=\"6、I-O复用select模型\">6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"7、select、epoll、poll模型对比\">7、select、epoll、poll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h3 id=\"（1）select-时间复杂度O-n\">（1）select 时间复杂度O(n)</h3>\n<h4 id=\"过程\">过程</h4>\n<p>（1）从用户空间拷贝fd_set到内核空间</p>\n<p>（2）注册回调函数</p>\n<p>（3）遍历所有fd，调用其对应的poll方法</p>\n<p>（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。</p>\n<p>（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列</p>\n<p>（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值</p>\n<p>（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。</p>\n<p>（8）把fd_set从内核空间拷贝到用户空间。</p>\n<h4 id=\"总结\">总结</h4>\n<p>​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。</p>\n<p>​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。</p>\n<p>​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（2）poll-时间复杂度O-n\">（2）poll 时间复杂度O(n)</h3>\n<h4 id=\"过程-2\">过程</h4>\n<pre><code class=\"language-java\">while true  \n&#123;  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    &#123;  \n        if i has data  \n        read until unavailable  \n    &#125;  \n&#125; \n</code></pre>\n<h4 id=\"总结-2\">总结</h4>\n<p>​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（3）epoll-时间复杂度O-1\">（3）epoll 时间复杂度O(1)</h3>\n<h4 id=\"过程-3\">过程</h4>\n<pre><code class=\"language-java\">&#123;  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    &#123;  \n        read or write till  \n    &#125;  \n&#125;  \n</code></pre>\n<h4 id=\"总结-3\">总结</h4>\n<p>​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。</p>\n<p>​\t\t内核和用户空间共享一块内存来实现的</p>\n<p>​\t优点：</p>\n<p>​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）</p>\n<p>​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。</p>\n<p>​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。</p>\n<h4 id=\"总结：\">总结：</h4>\n<p>​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。</p>\n<h2 id=\"8、多路复用的好处\">8、多路复用的好处</h2>\n<p>​\t\tselect，poll，epoll都是IO多路复用的机制。</p>\n<p>​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。</p>\n<p>​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n<p>​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、饭店吃饭的例子\">1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 – 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 – 【非阻塞I/O 】</p>\n<h2 id=\"2、阻塞与非阻塞调用对比\">2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h2 id=\"3、阻塞IO\">3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"4、非阻塞IO\">4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt=\"\"></p>\n<h2 id=\"5、I-O复用模型\">5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2 id=\"6、I-O复用select模型\">6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt=\"\"></p>\n<h2 id=\"7、select、epoll、poll模型对比\">7、select、epoll、poll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt=\"\"></p>\n<h3 id=\"（1）select-时间复杂度O-n\">（1）select 时间复杂度O(n)</h3>\n<h4 id=\"过程\">过程</h4>\n<p>（1）从用户空间拷贝fd_set到内核空间</p>\n<p>（2）注册回调函数</p>\n<p>（3）遍历所有fd，调用其对应的poll方法</p>\n<p>（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。</p>\n<p>（5）把（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列</p>\n<p>（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值</p>\n<p>（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。</p>\n<p>（8）把fd_set从内核空间拷贝到用户空间。</p>\n<h4 id=\"总结\">总结</h4>\n<p>​\t\t内核仅仅知道，有I/O事件发生了，却并不知道是哪几个I/O流。</p>\n<p>​\t\t我们只能无差别轮询所有的流，找出能读出数据（或写入数据的流），对他们进行操作。</p>\n<p>​\t\t处理的流越多，无差别遍历的事件就越长（O(n)）</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（2）poll-时间复杂度O-n\">（2）poll 时间复杂度O(n)</h3>\n<h4 id=\"过程-2\">过程</h4>\n<pre><code class=\"language-java\">while true  \n&#123;  \n    // 知道有一个流有I/O事件时，才往下执行  \n    select(streams[]) \n    for i in streams[]  \n    &#123;  \n        if i has data  \n        read until unavailable  \n    &#125;  \n&#125; \n</code></pre>\n<h4 id=\"总结-2\">总结</h4>\n<p>​\t\tpoll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的。</p>\n<p>​\t\t内核需要将消息传递到用户空间，都需要内核拷贝动作</p>\n<h3 id=\"（3）epoll-时间复杂度O-1\">（3）epoll 时间复杂度O(1)</h3>\n<h4 id=\"过程-3\">过程</h4>\n<pre><code class=\"language-java\">&#123;  \n    active_stream[] = epoll_wait(epollfd)  \n    for i in active_stream[]  \n    &#123;  \n        read or write till  \n    &#125;  \n&#125;  \n</code></pre>\n<h4 id=\"总结-3\">总结</h4>\n<p>​\t\tepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知用户线程，epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。</p>\n<p>​\t\t内核和用户空间共享一块内存来实现的</p>\n<p>​\t优点：</p>\n<p>​\t\t[1]、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）</p>\n<p>​\t\t[2]、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。</p>\n<p>​\t\t[3]、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。</p>\n<h4 id=\"总结：\">总结：</h4>\n<p>​\t\t表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。</p>\n<h2 id=\"8、多路复用的好处\">8、多路复用的好处</h2>\n<p>​\t\tselect，poll，epoll都是IO多路复用的机制。</p>\n<p>​\t\tI/O多路复用可以通过把多个 I/O 的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。</p>\n<p>​\t\t它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n<p>​\t\t但是select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。</p>\n"},{"title":"直方图","author":"ztq","date":"2021-04-17T10:27:00.000Z","_content":"\n## 一、边缘直方图\n\n如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。\n\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n\n\n### 1、使用seaborn实现边缘直方图\n\n（1）格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.jointplot)\n```\n\n    Help on function jointplot in module seaborn.axisgrid:\n    \n    jointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n        Draw a plot of two variables with bivariate and univariate graphs.\n        \n        This function provides a convenient interface to the :class:`JointGrid`\n        class, with several canned plot kinds. This is intended to be a fairly\n        lightweight wrapper; if you need more flexibility, you should use\n        :class:`JointGrid` directly.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        kind : { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\" }\n            Kind of plot to draw. See the examples for references to the underlying functions.\n        color : :mod:`matplotlib color <matplotlib.colors>`\n            Single color specification for when hue mapping is not used. Otherwise, the\n            plot will try to hook into the matplotlib property cycle.\n        height : numeric\n            Size of the figure (it will be square).\n        ratio : numeric\n            Ratio of joint axes height to marginal axes height.\n        space : numeric\n            Space between the joint and marginal axes\n        dropna : bool\n            If True, remove observations that are missing from ``x`` and ``y``.\n        {x, y}lim : pairs of numbers\n            Axis limits to set before plotting.\n        marginal_ticks : bool\n            If False, suppress ticks on the count/density axis of the marginal plots.\n        {joint, marginal}_kws : dicts\n            Additional keyword arguments for the plot components.\n        hue : vector or key in ``data``\n            Semantic variable that is mapped to determine the color of plot elements.\n            Semantic variable that is mapped to determine the color of plot elements.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        kwargs\n            Additional keyword arguments are passed to the function used to\n            draw the plot on the joint Axes, superseding items in the\n            ``joint_kws`` dictionary.\n        \n        Returns\n        -------\n        :class:`JointGrid`\n            An object managing multiple subplots that correspond to joint and marginal axes\n            for plotting a bivariate relationship or distribution.\n        \n        See Also\n        --------\n        JointGrid : Set up a figure with joint and marginal views on bivariate data.\n        PairGrid : Set up a figure with joint and marginal views on multiple variables.\n        jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/jointplot.rst\n\n\n​    \n\n（2）举例\n\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n\n\n\n```python\nimport seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(\"tips\")\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\")\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c25dcf10>\n\n\n\n\n![png](/img/output_8_111.png)\n    \n\n\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n\n\n示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线\n\n\n```python\nimport seaborn as sns\n  \ntips = sns.load_dataset(\"tips\")\n  \n# here \"*\" is used as a marker for scatterplot\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"reg\")\n\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295ada536a0>\n\n\n\n\n![png](/img/output_11_111.png)\n    \n\n\nkind的其他取值\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='scatter')\n#默认kind=\"scatter\"\n#sns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex')\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='kde')\n\nplt.show()\n```\n\n\n![png](/img/output_13_011.png)\n    \n\n\n\n\n![png](/img/output_13_111.png)\n    \n\n\n使用marginal_kws修改直方图的样式\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n```\n\n\n![png](/img/output_15_011.png)\n    \n\n\n### 2、使用plotly express实现边缘直方图\n\n(1)plotly express简单介绍及安装\n\nPlotly Express 是一个新的高级 Python 可视化库：它是 Plotly.py 的高级封装，它为复杂的图表提供了一个简单的语法。 \n\n受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 Plotly.py 一样。\n\nPlotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。 \n\n最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！\n\n用 pip install plotly_express 命令可以安装 Plotly Express。\n\n\n(2)格式\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=\"\",marginal_y=\"\"）\n（2）示例\n\n<strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong>\n\n## 二、边缘箱线图\n\n边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数\n\n### 1、使用matplotlib实现边缘箱线图\n\n\n```python\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"Data\\mpg_ggplot2.csv\")\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"Set1\", edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=\"v\")\nsns.boxplot(df.displ, ax=ax_bottom, orient=\"h\")\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n      warnings.warn(single_var_warning.format(\"Vertical\", \"x\"))\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n\n\n\n\n![png](/img/output_26_111.png)\n    \n\n\n### 2、使用seaborn实现边缘箱线图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=\"Data\\palmer_penguin_species.tsv.txt\"\n# penguins_df = pd.read_csv(penguins_data, sep=\"\\t\")\npenguins=sns.load_dataset(\"penguins\")\npenguins.head()\n\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\n# # set plot context to set plot sizes \n# sns.set_context(\"talk\", font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=\"culmen_length_mm\",\n#                   y=\"culmen_depth_mm\")\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(\"Scatterplot_with_marginal_boxplot_Seaborn.png\",\n# #                    format='png',dpi=150)\n# plt.show()\n```\n\n\n```python\nimport seaborn\nseaborn.__version__\n```\n\n\n\n\n    '0.11.0'\n\n\n需要更的到0.11.0版本才可以正常使用\npip install seaborn==0.11.0\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2af95e0>\n\n\n\n\n![png](/img/output_32_111.png)\n    \n\n\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2ec4160>\n\n\n\n\n​    \n![png](/img/output_33_111.png)\n​    \n\n\n\n```python\n\n```","source":"_posts/直方图.md","raw":"title: 直方图\nauthor: ztq\ntags:\n  - python\ncategories:\n  - python基础\n  - ''\ndate: 2021-04-17 18:27:00\n\n---\n\n## 一、边缘直方图\n\n如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。\n\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n\n\n### 1、使用seaborn实现边缘直方图\n\n（1）格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.jointplot)\n```\n\n    Help on function jointplot in module seaborn.axisgrid:\n    \n    jointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n        Draw a plot of two variables with bivariate and univariate graphs.\n        \n        This function provides a convenient interface to the :class:`JointGrid`\n        class, with several canned plot kinds. This is intended to be a fairly\n        lightweight wrapper; if you need more flexibility, you should use\n        :class:`JointGrid` directly.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        kind : { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\" }\n            Kind of plot to draw. See the examples for references to the underlying functions.\n        color : :mod:`matplotlib color <matplotlib.colors>`\n            Single color specification for when hue mapping is not used. Otherwise, the\n            plot will try to hook into the matplotlib property cycle.\n        height : numeric\n            Size of the figure (it will be square).\n        ratio : numeric\n            Ratio of joint axes height to marginal axes height.\n        space : numeric\n            Space between the joint and marginal axes\n        dropna : bool\n            If True, remove observations that are missing from ``x`` and ``y``.\n        {x, y}lim : pairs of numbers\n            Axis limits to set before plotting.\n        marginal_ticks : bool\n            If False, suppress ticks on the count/density axis of the marginal plots.\n        {joint, marginal}_kws : dicts\n            Additional keyword arguments for the plot components.\n        hue : vector or key in ``data``\n            Semantic variable that is mapped to determine the color of plot elements.\n            Semantic variable that is mapped to determine the color of plot elements.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        kwargs\n            Additional keyword arguments are passed to the function used to\n            draw the plot on the joint Axes, superseding items in the\n            ``joint_kws`` dictionary.\n        \n        Returns\n        -------\n        :class:`JointGrid`\n            An object managing multiple subplots that correspond to joint and marginal axes\n            for plotting a bivariate relationship or distribution.\n        \n        See Also\n        --------\n        JointGrid : Set up a figure with joint and marginal views on bivariate data.\n        PairGrid : Set up a figure with joint and marginal views on multiple variables.\n        jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/jointplot.rst\n\n\n​    \n\n（2）举例\n\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n\n\n\n```python\nimport seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(\"tips\")\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\")\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c25dcf10>\n\n\n\n\n![png](/img/output_8_111.png)\n    \n\n\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n\n\n示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线\n\n\n```python\nimport seaborn as sns\n  \ntips = sns.load_dataset(\"tips\")\n  \n# here \"*\" is used as a marker for scatterplot\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"reg\")\n\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295ada536a0>\n\n\n\n\n![png](/img/output_11_111.png)\n    \n\n\nkind的其他取值\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='scatter')\n#默认kind=\"scatter\"\n#sns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex')\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='kde')\n\nplt.show()\n```\n\n\n![png](/img/output_13_011.png)\n    \n\n\n\n\n![png](/img/output_13_111.png)\n    \n\n\n使用marginal_kws修改直方图的样式\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n```\n\n\n![png](/img/output_15_011.png)\n    \n\n\n### 2、使用plotly express实现边缘直方图\n\n(1)plotly express简单介绍及安装\n\nPlotly Express 是一个新的高级 Python 可视化库：它是 Plotly.py 的高级封装，它为复杂的图表提供了一个简单的语法。 \n\n受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 Plotly.py 一样。\n\nPlotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。 \n\n最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！\n\n用 pip install plotly_express 命令可以安装 Plotly Express。\n\n\n(2)格式\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=\"\",marginal_y=\"\"）\n（2）示例\n\n<strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong>\n\n## 二、边缘箱线图\n\n边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数\n\n### 1、使用matplotlib实现边缘箱线图\n\n\n```python\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"Data\\mpg_ggplot2.csv\")\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=\"Set1\", edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=\"v\")\nsns.boxplot(df.displ, ax=ax_bottom, orient=\"h\")\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n```\n\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n      warnings.warn(single_var_warning.format(\"Vertical\", \"x\"))\n    C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n      warnings.warn(\n\n\n\n\n![png](/img/output_26_111.png)\n    \n\n\n### 2、使用seaborn实现边缘箱线图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=\"Data\\palmer_penguin_species.tsv.txt\"\n# penguins_df = pd.read_csv(penguins_data, sep=\"\\t\")\npenguins=sns.load_dataset(\"penguins\")\npenguins.head()\n\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    \n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n\n```python\n# # set plot context to set plot sizes \n# sns.set_context(\"talk\", font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=\"culmen_length_mm\",\n#                   y=\"culmen_depth_mm\")\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(\"Scatterplot_with_marginal_boxplot_Seaborn.png\",\n# #                    format='png',dpi=150)\n# plt.show()\n```\n\n\n```python\nimport seaborn\nseaborn.__version__\n```\n\n\n\n\n    '0.11.0'\n\n\n需要更的到0.11.0版本才可以正常使用\npip install seaborn==0.11.0\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2af95e0>\n\n\n\n\n![png](/img/output_32_111.png)\n    \n\n\n\n```python\ng = sns.JointGrid(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n```\n\n\n\n\n    <seaborn.axisgrid.JointGrid at 0x295c2ec4160>\n\n\n\n\n​    \n![png](/img/output_33_111.png)\n​    \n\n\n\n```python\n\n```","slug":"直方图","published":1,"updated":"2021-04-17T10:31:39.262Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplu008tl0t9781gfxxm","content":"<h2 id=\"一、边缘直方图\">一、边缘直方图</h2>\n<p>如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。</p>\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n<h3 id=\"1、使用seaborn实现边缘直方图\">1、使用seaborn实现边缘直方图</h3>\n<p>（1）格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.jointplot)\n</code></pre>\n<pre><code>Help on function jointplot in module seaborn.axisgrid:\n\njointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n    Draw a plot of two variables with bivariate and univariate graphs.\n    \n    This function provides a convenient interface to the :class:`JointGrid`\n    class, with several canned plot kinds. This is intended to be a fairly\n    lightweight wrapper; if you need more flexibility, you should use\n    :class:`JointGrid` directly.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    kind : &#123; &quot;scatter&quot; | &quot;kde&quot; | &quot;hist&quot; | &quot;hex&quot; | &quot;reg&quot; | &quot;resid&quot; &#125;\n        Kind of plot to draw. See the examples for references to the underlying functions.\n    color : :mod:`matplotlib color &lt;matplotlib.colors&gt;`\n        Single color specification for when hue mapping is not used. Otherwise, the\n        plot will try to hook into the matplotlib property cycle.\n    height : numeric\n        Size of the figure (it will be square).\n    ratio : numeric\n        Ratio of joint axes height to marginal axes height.\n    space : numeric\n        Space between the joint and marginal axes\n    dropna : bool\n        If True, remove observations that are missing from ``x`` and ``y``.\n    &#123;x, y&#125;lim : pairs of numbers\n        Axis limits to set before plotting.\n    marginal_ticks : bool\n        If False, suppress ticks on the count/density axis of the marginal plots.\n    &#123;joint, marginal&#125;_kws : dicts\n        Additional keyword arguments for the plot components.\n    hue : vector or key in ``data``\n        Semantic variable that is mapped to determine the color of plot elements.\n        Semantic variable that is mapped to determine the color of plot elements.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    kwargs\n        Additional keyword arguments are passed to the function used to\n        draw the plot on the joint Axes, superseding items in the\n        ``joint_kws`` dictionary.\n    \n    Returns\n    -------\n    :class:`JointGrid`\n        An object managing multiple subplots that correspond to joint and marginal axes\n        for plotting a bivariate relationship or distribution.\n    \n    See Also\n    --------\n    JointGrid : Set up a figure with joint and marginal views on bivariate data.\n    PairGrid : Set up a figure with joint and marginal views on multiple variables.\n    jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/jointplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）举例</p>\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n<pre><code class=\"language-python\">import seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c25dcf10&gt;\n</code></pre>\n<p><img src=\"/img/output_8_111.png\" alt=\"png\"></p>\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n<p>示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线</p>\n<pre><code class=\"language-python\">import seaborn as sns\n  \ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# here &quot;*&quot; is used as a marker for scatterplot\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;, kind=&quot;reg&quot;)\n\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295ada536a0&gt;\n</code></pre>\n<p><img src=\"/img/output_11_111.png\" alt=\"png\"></p>\n<p>kind的其他取值</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='scatter')\n#默认kind=&quot;scatter&quot;\n#sns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex')\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='kde')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_011.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_13_111.png\" alt=\"png\"></p>\n<p>使用marginal_kws修改直方图的样式</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_15_011.png\" alt=\"png\"></p>\n<h3 id=\"2、使用plotly-express实现边缘直方图\">2、使用plotly express实现边缘直方图</h3>\n<p>(1)plotly express简单介绍及安装</p>\n<p>Plotly Express 是一个新的高级 Python 可视化库：它是 <a href=\"http://Plotly.py\">Plotly.py</a> 的高级封装，它为复杂的图表提供了一个简单的语法。</p>\n<p>受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 <a href=\"http://Plotly.py\">Plotly.py</a> 一样。</p>\n<p>Plotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。</p>\n<p>最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！</p>\n<p>用 pip install plotly_express 命令可以安装 Plotly Express。</p>\n<p>(2)格式<br>\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）<br>\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=“”,marginal_y=“”）<br>\n（2）示例</p>\n<p><strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong></p>\n<h2 id=\"二、边缘箱线图\">二、边缘箱线图</h2>\n<p>边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数</p>\n<h3 id=\"1、使用matplotlib实现边缘箱线图\">1、使用matplotlib实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(&quot;Data\\mpg_ggplot2.csv&quot;)\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=&quot;Set1&quot;, edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=&quot;v&quot;)\nsns.boxplot(df.displ, ax=ax_bottom, orient=&quot;h&quot;)\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n  warnings.warn(single_var_warning.format(&quot;Vertical&quot;, &quot;x&quot;))\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n</code></pre>\n<p><img src=\"/img/output_26_111.png\" alt=\"png\"></p>\n<h3 id=\"2、使用seaborn实现边缘箱线图\">2、使用seaborn实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=&quot;Data\\palmer_penguin_species.tsv.txt&quot;\n# penguins_df = pd.read_csv(penguins_data, sep=&quot;\\t&quot;)\npenguins=sns.load_dataset(&quot;penguins&quot;)\npenguins.head()\n\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\"># # set plot context to set plot sizes \n# sns.set_context(&quot;talk&quot;, font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=&quot;culmen_length_mm&quot;,\n#                   y=&quot;culmen_depth_mm&quot;)\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(&quot;Scatterplot_with_marginal_boxplot_Seaborn.png&quot;,\n# #                    format='png',dpi=150)\n# plt.show()\n</code></pre>\n<pre><code class=\"language-python\">import seaborn\nseaborn.__version__\n</code></pre>\n<pre><code>'0.11.0'\n</code></pre>\n<p>需要更的到0.11.0版本才可以正常使用<br>\npip install seaborn==0.11.0</p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2af95e0&gt;\n</code></pre>\n<p><img src=\"/img/output_32_111.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2ec4160&gt;\n</code></pre>\n<p>​<br>\n<img src=\"/img/output_33_111.png\" alt=\"png\"><br>\n​</p>\n<pre><code class=\"language-python\">\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、边缘直方图\">一、边缘直方图</h2>\n<p>如果你想要在同一幅图中既展示数据之间的关系，又展示数据的分布，便可以使用marginal histogram(边缘直方图)，它可以在散点图的边缘画出X和Y变量的分布直方图。</p>\n<p>seaborn的jointplot可以实现边缘直方图\n<p>plotly的easyplot可以实现边缘直方图\n<h3 id=\"1、使用seaborn实现边缘直方图\">1、使用seaborn实现边缘直方图</h3>\n<p>（1）格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.jointplot)\n</code></pre>\n<pre><code>Help on function jointplot in module seaborn.axisgrid:\n\njointplot(*, x=None, y=None, data=None, kind='scatter', color=None, height=6, ratio=5, space=0.2, dropna=False, xlim=None, ylim=None, marginal_ticks=False, joint_kws=None, marginal_kws=None, hue=None, palette=None, hue_order=None, hue_norm=None, **kwargs)\n    Draw a plot of two variables with bivariate and univariate graphs.\n    \n    This function provides a convenient interface to the :class:`JointGrid`\n    class, with several canned plot kinds. This is intended to be a fairly\n    lightweight wrapper; if you need more flexibility, you should use\n    :class:`JointGrid` directly.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    kind : &#123; &quot;scatter&quot; | &quot;kde&quot; | &quot;hist&quot; | &quot;hex&quot; | &quot;reg&quot; | &quot;resid&quot; &#125;\n        Kind of plot to draw. See the examples for references to the underlying functions.\n    color : :mod:`matplotlib color &lt;matplotlib.colors&gt;`\n        Single color specification for when hue mapping is not used. Otherwise, the\n        plot will try to hook into the matplotlib property cycle.\n    height : numeric\n        Size of the figure (it will be square).\n    ratio : numeric\n        Ratio of joint axes height to marginal axes height.\n    space : numeric\n        Space between the joint and marginal axes\n    dropna : bool\n        If True, remove observations that are missing from ``x`` and ``y``.\n    &#123;x, y&#125;lim : pairs of numbers\n        Axis limits to set before plotting.\n    marginal_ticks : bool\n        If False, suppress ticks on the count/density axis of the marginal plots.\n    &#123;joint, marginal&#125;_kws : dicts\n        Additional keyword arguments for the plot components.\n    hue : vector or key in ``data``\n        Semantic variable that is mapped to determine the color of plot elements.\n        Semantic variable that is mapped to determine the color of plot elements.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    kwargs\n        Additional keyword arguments are passed to the function used to\n        draw the plot on the joint Axes, superseding items in the\n        ``joint_kws`` dictionary.\n    \n    Returns\n    -------\n    :class:`JointGrid`\n        An object managing multiple subplots that correspond to joint and marginal axes\n        for plotting a bivariate relationship or distribution.\n    \n    See Also\n    --------\n    JointGrid : Set up a figure with joint and marginal views on bivariate data.\n    PairGrid : Set up a figure with joint and marginal views on multiple variables.\n    jointplot : Draw multiple bivariate plots with univariate marginal distributions.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/jointplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）举例</p>\n<P><STRONG>Example 1:&nbsp;</STRONG>\n    最简单的使用\n<pre><code class=\"language-python\">import seaborn as sns\n  \n# loading tips dataset\ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# plotting scatterplot with histograms for features total bill and tip.\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c25dcf10&gt;\n</code></pre>\n<p><img src=\"/img/output_8_111.png\" alt=\"png\"></p>\n<P><STRONG>Example 2:</STRONG> Using kind=”reg” attribute you can add a linear \nregression fit and univariate KDE curves.\n<p>示例2：使用kind=“reg”属性可以添加线性回归拟合和单变量KDE曲线</p>\n<pre><code class=\"language-python\">import seaborn as sns\n  \ntips = sns.load_dataset(&quot;tips&quot;)\n  \n# here &quot;*&quot; is used as a marker for scatterplot\nsns.jointplot(data=tips, x=&quot;total_bill&quot;, y=&quot;tip&quot;, kind=&quot;reg&quot;)\n\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295ada536a0&gt;\n</code></pre>\n<p><img src=\"/img/output_11_111.png\" alt=\"png\"></p>\n<p>kind的其他取值</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='scatter')\n#默认kind=&quot;scatter&quot;\n#sns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex')\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='kde')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_011.png\" alt=\"png\"></p>\n<p><img src=\"/img/output_13_111.png\" alt=\"png\"></p>\n<p>使用marginal_kws修改直方图的样式</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Custom the histogram:\nsns.jointplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;], kind='hex', marginal_kws=dict(bins=400))\n\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_15_011.png\" alt=\"png\"></p>\n<h3 id=\"2、使用plotly-express实现边缘直方图\">2、使用plotly express实现边缘直方图</h3>\n<p>(1)plotly express简单介绍及安装</p>\n<p>Plotly Express 是一个新的高级 Python 可视化库：它是 <a href=\"http://Plotly.py\">Plotly.py</a> 的高级封装，它为复杂的图表提供了一个简单的语法。</p>\n<p>受 Seaborn 和 ggplot2 的启发，它专门设计为具有简洁，一致且易于学习的 API ：只需一次导入，您就可以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面板和主题，就像 <a href=\"http://Plotly.py\">Plotly.py</a> 一样。</p>\n<p>Plotly Express 完全免费：凭借其宽松的开源 MIT 许可证，您可以随意使用它（是的，甚至在商业产品中！）。</p>\n<p>最重要的是，Plotly Express 与 Plotly 生态系统的其他部分完全兼容：在您的 Dash 应用程序中使用它，使用 Orca 将您的数据导出为几乎任何文件格式，或使用JupyterLab 图表编辑器在 GUI 中编辑它们！</p>\n<p>用 pip install plotly_express 命令可以安装 Plotly Express。</p>\n<p>(2)格式<br>\n基本散点图： px.scatter（data，x =“column_name”，y =“column_name”）<br>\n边缘直方图px.scatter（data，x =“column_name”，y =“column_name”,marginal_x=“”,marginal_y=“”）<br>\n（2）示例</p>\n<p><strong>exmaple 自行查找资料，实现Plotly Express的边缘直方图</strong></p>\n<h2 id=\"二、边缘箱线图\">二、边缘箱线图</h2>\n<p>边缘箱图与边缘直方图具有相似的用途。 然而，箱线图有助于精确定位X和25的中位数，第25和第75百分位数</p>\n<h3 id=\"1、使用matplotlib实现边缘箱线图\">1、使用matplotlib实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(&quot;Data\\mpg_ggplot2.csv&quot;)\n\n# Create Fig and gridspec\nfig = plt.figure(figsize=(16, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category').cat.codes, alpha=.9, data=df, cmap=&quot;Set1&quot;, edgecolors='black', linewidths=.5)\n\n# Add a graph in each part\nsns.boxplot(df.hwy, ax=ax_right, orient=&quot;v&quot;)\nsns.boxplot(df.displ, ax=ax_bottom, orient=&quot;h&quot;)\n\n# Decorations ------------------\n# Remove x axis name for the boxplot\nax_bottom.set(xlabel='')\nax_right.set(ylabel='')\n\n# Main Title, Xlabel and YLabel\nax_main.set(title='Scatterplot with Histograms \\n displ vs hwy', xlabel='displ', ylabel='hwy')\n\n# Set font size of different components\nax_main.title.set_fontsize(20)\nfor item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.get_xticklabels() + ax_main.get_yticklabels()):\n    item.set_fontsize(14)\n\nplt.show()\n\n</code></pre>\n<pre><code>C:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.\n  warnings.warn(single_var_warning.format(&quot;Vertical&quot;, &quot;x&quot;))\nC:\\Users\\shili\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n</code></pre>\n<p><img src=\"/img/output_26_111.png\" alt=\"png\"></p>\n<h3 id=\"2、使用seaborn实现边缘箱线图\">2、使用seaborn实现边缘箱线图</h3>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# https://seaborn.pydata.org/tutorial/distributions.html\n# penguins_data=&quot;Data\\palmer_penguin_species.tsv.txt&quot;\n# penguins_df = pd.read_csv(penguins_data, sep=&quot;\\t&quot;)\npenguins=sns.load_dataset(&quot;penguins&quot;)\npenguins.head()\n\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<pre><code class=\"language-python\"># # set plot context to set plot sizes \n# sns.set_context(&quot;talk&quot;, font_scale=1.2)\n# plt.figure(figsize=(12,10))\n# g = sns.JointGrid(data=penguins_df, \n#                   x=&quot;culmen_length_mm&quot;,\n#                   y=&quot;culmen_depth_mm&quot;)\n# #现在我们可以在JointGrid（）绘图上添加绘图层。\n# #这里我们使用Seaborn的plot_joint（）函数来绘制散点图。为此，我们调用Seaborn的scatterplot（）作为plot_joint（）的参数。\n# g.plot_joint(sns.scatterplot)\n\n# g.plot_marginals(sns.boxplot)\n# #plt.savefig(&quot;Scatterplot_with_marginal_boxplot_Seaborn.png&quot;,\n# #                    format='png',dpi=150)\n# plt.show()\n</code></pre>\n<pre><code class=\"language-python\">import seaborn\nseaborn.__version__\n</code></pre>\n<pre><code>'0.11.0'\n</code></pre>\n<p>需要更的到0.11.0版本才可以正常使用<br>\npip install seaborn==0.11.0</p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2af95e0&gt;\n</code></pre>\n<p><img src=\"/img/output_32_111.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">g = sns.JointGrid(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;)\ng.plot_joint(sns.histplot)\ng.plot_marginals(sns.boxplot)\n</code></pre>\n<pre><code>&lt;seaborn.axisgrid.JointGrid at 0x295c2ec4160&gt;\n</code></pre>\n<p>​<br>\n<img src=\"/img/output_33_111.png\" alt=\"png\"><br>\n​</p>\n<pre><code class=\"language-python\">\n</code></pre>\n"},{"title":"线程相关的知识","author":"郑天祺","date":"2019-11-20T11:46:00.000Z","_content":"\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","source":"_posts/线程相关的知识.md","raw":"title: 线程相关的知识\nauthor: 郑天祺\ntags:\n  - 线程\ncategories:\n  - java基础\ndate: 2019-11-20 19:46:00\n\n---\n\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","slug":"线程相关的知识","published":1,"updated":"2019-11-20T11:50:24.878Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplv008xl0t91tqeg6jj","content":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n"},{"title":"自旋锁","author":"郑天祺","date":"2019-08-31T04:54:00.000Z","_content":"\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","source":"_posts/自旋锁.md","raw":"title: 自旋锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 12:54:00\n\n---\n\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","slug":"自旋锁","published":1,"updated":"2019-08-31T04:59:21.273Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplv0091l0t964312api","content":"<h1>自旋锁</h1>\n<h2 id=\"1、自旋锁概念（spinlock）\">1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2 id=\"2、自旋锁的优点\">2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2 id=\"3、自旋锁应用\">3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2 id=\"4、简单自旋锁的实现-：\">4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock &#123;\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() &#123;\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) &#123;\n         &#125;\n     &#125;\n\n   public void unLock() &#123;\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) &#123;\n         &#125;\n         ref.set(null);\n     &#125;\n &#125;\n\ntest：\n\npublic class SimpleSpinLockTest &#123;\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"5、可重入的自旋锁和不可重入的自旋锁-：\">5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2 id=\"6、-另有三种常见的形式\">6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：<a href=\"https://www.cnblogs.com/stevenczp/p/7136416.html\">https://www.cnblogs.com/stevenczp/p/7136416.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>自旋锁</h1>\n<h2 id=\"1、自旋锁概念（spinlock）\">1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2 id=\"2、自旋锁的优点\">2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2 id=\"3、自旋锁应用\">3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2 id=\"4、简单自旋锁的实现-：\">4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock &#123;\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() &#123;\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) &#123;\n         &#125;\n     &#125;\n\n   public void unLock() &#123;\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) &#123;\n         &#125;\n         ref.set(null);\n     &#125;\n &#125;\n\ntest：\n\npublic class SimpleSpinLockTest &#123;\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"5、可重入的自旋锁和不可重入的自旋锁-：\">5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2 id=\"6、-另有三种常见的形式\">6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：<a href=\"https://www.cnblogs.com/stevenczp/p/7136416.html\">https://www.cnblogs.com/stevenczp/p/7136416.html</a></p>\n"},{"title":"读写锁","author":"郑天祺","date":"2019-08-31T05:08:00.000Z","_content":"\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","source":"_posts/读写锁.md","raw":"title: 读写锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:08:00\n\n---\n\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","slug":"读写锁","published":1,"updated":"2019-10-15T10:05:19.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplw0094l0t960id36qq","content":"<h1>4、读写锁</h1>\n<h2 id=\"1、读写锁介绍：\">1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，<br>\n​         读-写不能共存，<br>\n​         写-写不能共存。</p>\n<p>连接：<a href=\"https://blog.csdn.net/j080624/article/details/82790372%E3%80%81https://ifeve.com/read-write-locks/\">https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</a></p>\n<h2 id=\"2、总结：\">2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2 id=\"3、写锁的获取：\">3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) &#123;\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) &#123;\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     &#125;\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n &#125;\n\n \n\n static int exclusiveCount(int c) &#123; \n\n        return c &amp; EXCLUSIVE_MASK;\n\n &#125;\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    &#123; \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n&#125;\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2 id=\"4、写锁的释放：\">4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) &#123;\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n &#125;\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2 id=\"5、读锁的获取\">5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) &#123;\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) &#123;\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         &#125; else if (firstReader == current) &#123;\n             firstReaderHoldCount++;\n         &#125; else &#123;\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         &#125;\n         return 1;\n     &#125;\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n &#125;\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2 id=\"6、读锁的释放\">6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) &#123;\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     &#125; else &#123;\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) &#123;\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         &#125;\n         --rh.count;\n     &#125;     for (;;) &#123;\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"7、锁降级\">7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() &#123;\n         rwl.readLock().lock();\n         if (!cacheValid) &#123;\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try &#123;\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) &#123;\n                     data = ...\n             cacheValid = true;\n           &#125;\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         &#125; finally &#123;\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         &#125;\n       &#125;\n       try &#123;\n         use(data);\n       &#125; finally &#123;\n         rwl.readLock().unlock();\n       &#125;\n     &#125;\n &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>4、读写锁</h1>\n<h2 id=\"1、读写锁介绍：\">1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，<br>\n​         读-写不能共存，<br>\n​         写-写不能共存。</p>\n<p>连接：<a href=\"https://blog.csdn.net/j080624/article/details/82790372%E3%80%81https://ifeve.com/read-write-locks/\">https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</a></p>\n<h2 id=\"2、总结：\">2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2 id=\"3、写锁的获取：\">3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) &#123;\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) &#123;\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     &#125;\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n &#125;\n\n \n\n static int exclusiveCount(int c) &#123; \n\n        return c &amp; EXCLUSIVE_MASK;\n\n &#125;\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    &#123; \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n&#125;\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2 id=\"4、写锁的释放：\">4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) &#123;\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n &#125;\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2 id=\"5、读锁的获取\">5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) &#123;\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) &#123;\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         &#125; else if (firstReader == current) &#123;\n             firstReaderHoldCount++;\n         &#125; else &#123;\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         &#125;\n         return 1;\n     &#125;\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n &#125;\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2 id=\"6、读锁的释放\">6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) &#123;\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) &#123;\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     &#125; else &#123;\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) &#123;\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         &#125;\n         --rh.count;\n     &#125;     for (;;) &#123;\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     &#125;\n &#125;\n</code></pre>\n<h2 id=\"7、锁降级\">7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() &#123;\n         rwl.readLock().lock();\n         if (!cacheValid) &#123;\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try &#123;\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) &#123;\n                     data = ...\n             cacheValid = true;\n           &#125;\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         &#125; finally &#123;\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         &#125;\n       &#125;\n       try &#123;\n         use(data);\n       &#125; finally &#123;\n         rwl.readLock().unlock();\n       &#125;\n     &#125;\n &#125;\n</code></pre>\n"},{"title":"责任链模式","author":"郑天祺","date":"2020-01-03T07:21:00.000Z","_content":"\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","source":"_posts/责任链模式.md","raw":"title: 责任链模式\nauthor: 郑天祺\ntags:\n\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 15:21:00\n\n---\n\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","slug":"责任链模式","published":1,"updated":"2020-01-03T07:58:18.113Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplx0097l0t932fs6blj","content":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90487984\">https://blog.csdn.net/qq_23034755/article/details/90487984</a></p>\n<p>单例模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215\">https://blog.csdn.net/qq_23034755/article/details/90547215</a></p>\n<p>观察者模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90705205\">https://blog.csdn.net/qq_23034755/article/details/90705205</a></p>\n<p>发布订阅模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/91340383\">https://blog.csdn.net/qq_23034755/article/details/91340383</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90487984\">https://blog.csdn.net/qq_23034755/article/details/90487984</a></p>\n<p>单例模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90547215\">https://blog.csdn.net/qq_23034755/article/details/90547215</a></p>\n<p>观察者模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/90705205\">https://blog.csdn.net/qq_23034755/article/details/90705205</a></p>\n<p>发布订阅模式：<a href=\"https://blog.csdn.net/qq_23034755/article/details/91340383\">https://blog.csdn.net/qq_23034755/article/details/91340383</a></p>\n"},{"title":"轻量级锁","author":"郑天祺","date":"2019-08-31T07:08:00.000Z","_content":"\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","source":"_posts/轻量级锁.md","raw":"title: 轻量级锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 15:08:00\n\n---\n\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","slug":"轻量级锁","published":1,"updated":"2019-08-31T07:10:30.223Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvplz009al0t91swbfggr","content":"<h2 id=\"1、轻量级锁\">1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。<br>\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。<br>\n​     c) 将锁记录中的Owner指针指向锁对象。<br>\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2 id=\"2、锁消除\">2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2 id=\"3、锁膨胀\">3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1、轻量级锁\">1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。<br>\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。<br>\n​     c) 将锁记录中的Owner指针指向锁对象。<br>\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2 id=\"2、锁消除\">2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2 id=\"3、锁膨胀\">3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n"},{"title":"运算符","author":"郑天祺","date":"2020-09-01T01:24:00.000Z","_content":"\n## ^ 位异或运算\n\n运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1\n\n比如：8^11\n\n8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(\"0011\",2)=3","source":"_posts/运算符.md","raw":"title: 运算符\nauthor: 郑天祺\ntags:\n  - 运算符\ncategories:\n  - java基础\ndate: 2020-09-01 09:24:00\n\n---\n\n## ^ 位异或运算\n\n运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1\n\n比如：8^11\n\n8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(\"0011\",2)=3","slug":"运算符","published":1,"updated":"2020-09-01T02:07:31.136Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpmp00d6l0t9hqjyckqb","content":"<h2 id=\"位异或运算\">^ 位异或运算</h2>\n<p>运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1</p>\n<p>比如：8^11</p>\n<p>8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(“0011”,2)=3</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"位异或运算\">^ 位异或运算</h2>\n<p>运算规则：两个数转为二进制，然后从高位开始比较，如果  相同  则为0，不相同  则为1</p>\n<p>比如：8^11</p>\n<p>8转为二进制是1000，11转为二进制是1011.从高位开始比较得到的是：0011.然后二进制转为十进制，就是Integer.parseInt(“0011”,2)=3</p>\n"},{"title":"重放攻击","author":"郑天祺","date":"2019-09-04T11:54:00.000Z","_content":"\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","source":"_posts/重放攻击.md","raw":"title: 重放攻击\nauthor: 郑天祺\ntags:\n  - 网络安全\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-04 19:54:00\n\n---\n\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","slug":"重放攻击","published":1,"updated":"2019-09-04T13:24:43.315Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpmq00d7l0t95q8q8hsq","content":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n"},{"title":"锁粗化","author":"郑天祺","date":"2019-08-31T05:32:00.000Z","_content":"\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","source":"_posts/锁粗化.md","raw":"title: 锁粗化\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:32:00\n---\n\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","slug":"锁粗化","published":1,"updated":"2019-10-15T12:35:31.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpmr00d9l0t911d520yl","content":"<p>转自：<a href=\"https://blog.csdn.net/qq_26222859/article/details/80546917\">https://blog.csdn.net/qq_26222859/article/details/80546917</a></p>\n<p>参考：<a href=\"https://www.jianshu.com/p/f05423a21e78\">https://www.jianshu.com/p/f05423a21e78</a></p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     synchronized(lock)&#123;\n         //do some thing\n     &#125;\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock)&#123;\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock)&#123;\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++)&#123;\n     synchronized(lock)&#123;\n     &#125;\n &#125;\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock)&#123;\n     for(int i=0;i&lt;size;i++)&#123;\n     &#125;\n &#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>转自：<a href=\"https://blog.csdn.net/qq_26222859/article/details/80546917\">https://blog.csdn.net/qq_26222859/article/details/80546917</a></p>\n<p>参考：<a href=\"https://www.jianshu.com/p/f05423a21e78\">https://www.jianshu.com/p/f05423a21e78</a></p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     synchronized(lock)&#123;\n         //do some thing\n     &#125;\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock)&#123;\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod()&#123;\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock)&#123;\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     &#125;\n &#125;\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++)&#123;\n     synchronized(lock)&#123;\n     &#125;\n &#125;\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock)&#123;\n     for(int i=0;i&lt;size;i++)&#123;\n     &#125;\n &#125;\n</code></pre>\n"},{"title":"阻塞锁","author":"郑天祺","date":"2019-08-31T05:00:00.000Z","_content":"\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","source":"_posts/阻塞锁.md","raw":"title: 阻塞锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:00:00\n\n---\n\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","slug":"阻塞锁","published":1,"updated":"2019-08-31T05:04:33.756Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpmr00dal0t985if20x6","content":"<h1>阻塞锁</h1>\n<h2 id=\"1、阻塞锁优势\">1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。<br>\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2 id=\"2、阻塞锁的优势：\">2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2 id=\"3、阻塞锁应用：\">3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2 id=\"4、阻塞锁的简单实现：\">4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock &#123;\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode &#123;\n         private volatile Thread isLocked;\n     &#125;\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() &#123;\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) &#123;\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         &#125;\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     &#125;\n\npublic void unLock() &#123;\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) &#123;\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         &#125;\n         node = null;\n     &#125;\n &#125;\n</code></pre>\n<h3 id=\"5、demo：\">5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest &#123;\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     &#125;\n &#125;\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>阻塞锁</h1>\n<h2 id=\"1、阻塞锁优势\">1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。<br>\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2 id=\"2、阻塞锁的优势：\">2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2 id=\"3、阻塞锁应用：\">3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2 id=\"4、阻塞锁的简单实现：\">4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock &#123;\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode &#123;\n         private volatile Thread isLocked;\n     &#125;\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() &#123;\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) &#123;\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         &#125;\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     &#125;\n\npublic void unLock() &#123;\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) &#123;\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         &#125;\n         node = null;\n     &#125;\n &#125;\n</code></pre>\n<h3 id=\"5、demo：\">5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest &#123;\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException &#123;\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) &#123;\n             pool.submit(() -&gt; &#123;\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             &#125;);\n         &#125;\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     &#125;\n &#125;\n\n \n</code></pre>\n"},{"title":"散点图","author":"ztq","date":"2021-04-17T10:18:00.000Z","_content":"\n# <center> 散点图</center>\n\n# 一、什么是散点图\n\n散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。\n\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n\n\n\n# 二、散点图的好处\n\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n\n\n\n## 三、matplotlib的散点图\n\n<b>1、plot画散点图</b>\n\n在折线图的基础上设置linestyle='none'\n\n\n```python\n # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame({'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) })\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n```\n\n\n![png](/img/output_8_0.png)\n    \n\n\n<b>2,使用scatter画散点图</b>\n\n简单的一个点的散点图\n\n\n```python\nimport matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n```\n\n\n![png](/img/output_11_0.png)\n    \n\n\n添加标题，给轴加上标签,并确保所有文本都大到能够看清： \n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.title(\"Square Numbers\", fontsize=24)\n\nplt.scatter(2, 4, s=200,color=\"r\") \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(\"Square of Value\", fontsize=14) \n\nplt.xlabel(\"x value\")\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n```\n\n\n![png](/img/output_13_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_14_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_15_0.png)\n    \n\n\n绘制一系列点,x_values,y_values为list\n\n\n```python\nimport matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_17_0.png)\n    \n\n\n自动生成x,y值\n\n\n```python\nx_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n```\n\n\n![png](/img/output_19_0.png)\n    \n\n\n\n```python\nplt.scatter(x_values, y_values, edgecolor='none', s=40) \n```\n\n\n\n\n    <matplotlib.collections.PathCollection at 0x2b4aa653280>\n\n\n\n\n![png](/img/output_20_1.png)\n    \n\n\n<strong>保存到文件</strong>\n\n\n```python\nimport matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(\"Square Numbers\", fontsize=24) \nplt.xlabel(\"Value\", fontsize=14) \nplt.ylabel(\"Square of Value\", fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n```\n\n\n![png](/img/output_22_0.png)\n    \n\n\n\n```python\nimport matplotlib as mpl\nhelp(mpl.markers)\n```\n\n    Help on module matplotlib.markers in matplotlib:\n    \n    NAME\n        matplotlib.markers\n    \n    DESCRIPTION\n        This module contains functions to handle markers.  Used by both the\n        marker functionality of `~matplotlib.axes.Axes.plot` and\n        `~matplotlib.axes.Axes.scatter`.\n        \n        All possible markers are defined here:\n        \n        ============================== ====== =========================================\n        marker                         symbol description\n        ============================== ====== =========================================\n        ``\".\"``                        |m00|  point\n        ``\",\"``                        |m01|  pixel\n        ``\"o\"``                        |m02|  circle\n        ``\"v\"``                        |m03|  triangle_down\n        ``\"^\"``                        |m04|  triangle_up\n        ``\"<\"``                        |m05|  triangle_left\n        ``\">\"``                        |m06|  triangle_right\n        ``\"1\"``                        |m07|  tri_down\n        ``\"2\"``                        |m08|  tri_up\n        ``\"3\"``                        |m09|  tri_left\n        ``\"4\"``                        |m10|  tri_right\n        ``\"8\"``                        |m11|  octagon\n        ``\"s\"``                        |m12|  square\n        ``\"p\"``                        |m13|  pentagon\n        ``\"P\"``                        |m23|  plus (filled)\n        ``\"*\"``                        |m14|  star\n        ``\"h\"``                        |m15|  hexagon1\n        ``\"H\"``                        |m16|  hexagon2\n        ``\"+\"``                        |m17|  plus\n        ``\"x\"``                        |m18|  x\n        ``\"X\"``                        |m24|  x (filled)\n        ``\"D\"``                        |m19|  diamond\n        ``\"d\"``                        |m20|  thin_diamond\n        ``\"|\"``                        |m21|  vline\n        ``\"_\"``                        |m22|  hline\n        ``0`` (``TICKLEFT``)           |m25|  tickleft\n        ``1`` (``TICKRIGHT``)          |m26|  tickright\n        ``2`` (``TICKUP``)             |m27|  tickup\n        ``3`` (``TICKDOWN``)           |m28|  tickdown\n        ``4`` (``CARETLEFT``)          |m29|  caretleft\n        ``5`` (``CARETRIGHT``)         |m30|  caretright\n        ``6`` (``CARETUP``)            |m31|  caretup\n        ``7`` (``CARETDOWN``)          |m32|  caretdown\n        ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n        ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n        ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n        ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n        ``\"None\"``, ``\" \"`` or  ``\"\"``        nothing\n        ``'$...$'``                    |m37|  Render the string using mathtext.\n                                              E.g ``\"$f$\"`` for marker showing the\n                                              letter ``f``.\n        ``verts``                             A list of (x, y) pairs used for Path\n                                              vertices. The center of the marker is\n                                              located at (0, 0) and the size is\n                                              normalized, such that the created path\n                                              is encapsulated inside the unit cell.\n        path                                  A `~matplotlib.path.Path` instance.\n        ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                              rotated by ``angle``.\n        ============================== ====== =========================================\n        \n        ``None`` is the default which means 'nothing', however this table is\n        referred to from other docs for the valid inputs from marker inputs and in\n        those cases ``None`` still means 'default'.\n        \n        Note that special symbols can be defined via the\n        :doc:`STIX math font </tutorials/text/mathtext>`,\n        e.g. ``\"$\\u266B$\"``. For an overview over the STIX font symbols refer to the\n        `STIX font table <http://www.stixfonts.org/allGlyphs.html>`_.\n        Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n        \n        Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n        equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n        Hence the following are equivalent::\n        \n            plt.plot([1, 2, 3], marker=11)\n            plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n        \n        Examples showing the use of markers:\n        \n        * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n        * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n        * :doc:`/gallery/shapes_and_collections/marker_path`\n\n\n​        \n\n        .. |m00| image:: /_static/markers/m00.png\n        .. |m01| image:: /_static/markers/m01.png\n        .. |m02| image:: /_static/markers/m02.png\n        .. |m03| image:: /_static/markers/m03.png\n        .. |m04| image:: /_static/markers/m04.png\n        .. |m05| image:: /_static/markers/m05.png\n        .. |m06| image:: /_static/markers/m06.png\n        .. |m07| image:: /_static/markers/m07.png\n        .. |m08| image:: /_static/markers/m08.png\n        .. |m09| image:: /_static/markers/m09.png\n        .. |m10| image:: /_static/markers/m10.png\n        .. |m11| image:: /_static/markers/m11.png\n        .. |m12| image:: /_static/markers/m12.png\n        .. |m13| image:: /_static/markers/m13.png\n        .. |m14| image:: /_static/markers/m14.png\n        .. |m15| image:: /_static/markers/m15.png\n        .. |m16| image:: /_static/markers/m16.png\n        .. |m17| image:: /_static/markers/m17.png\n        .. |m18| image:: /_static/markers/m18.png\n        .. |m19| image:: /_static/markers/m19.png\n        .. |m20| image:: /_static/markers/m20.png\n        .. |m21| image:: /_static/markers/m21.png\n        .. |m22| image:: /_static/markers/m22.png\n        .. |m23| image:: /_static/markers/m23.png\n        .. |m24| image:: /_static/markers/m24.png\n        .. |m25| image:: /_static/markers/m25.png\n        .. |m26| image:: /_static/markers/m26.png\n        .. |m27| image:: /_static/markers/m27.png\n        .. |m28| image:: /_static/markers/m28.png\n        .. |m29| image:: /_static/markers/m29.png\n        .. |m30| image:: /_static/markers/m30.png\n        .. |m31| image:: /_static/markers/m31.png\n        .. |m32| image:: /_static/markers/m32.png\n        .. |m33| image:: /_static/markers/m33.png\n        .. |m34| image:: /_static/markers/m34.png\n        .. |m35| image:: /_static/markers/m35.png\n        .. |m36| image:: /_static/markers/m36.png\n        .. |m37| image:: /_static/markers/m37.png\n    \n    CLASSES\n        builtins.object\n            MarkerStyle\n        \n        class MarkerStyle(builtins.object)\n         |  MarkerStyle(marker=None, fillstyle=None)\n         |  \n         |  Methods defined here:\n         |  \n         |  __bool__(self)\n         |  \n         |  __init__(self, marker=None, fillstyle=None)\n         |      Attributes\n         |      ----------\n         |      markers : list of known marks\n         |      \n         |      fillstyles : list of known fillstyles\n         |      \n         |      filled_markers : list of known filled markers.\n         |      \n         |      Parameters\n         |      ----------\n         |      marker : str or array-like, optional, default: None\n         |          See the descriptions of possible markers in the module docstring.\n         |      \n         |      fillstyle : str, optional, default: 'full'\n         |          'full', 'left\", 'right', 'bottom', 'top', 'none'\n         |  \n         |  get_alt_path(self)\n         |  \n         |  get_alt_transform(self)\n         |  \n         |  get_capstyle(self)\n         |  \n         |  get_fillstyle(self)\n         |  \n         |  get_joinstyle(self)\n         |  \n         |  get_marker(self)\n         |  \n         |  get_path(self)\n         |  \n         |  get_snap_threshold(self)\n         |  \n         |  get_transform(self)\n         |  \n         |  is_filled(self)\n         |  \n         |  set_fillstyle(self, fillstyle)\n         |      Sets fillstyle\n         |      \n         |      Parameters\n         |      ----------\n         |      fillstyle : string amongst known fillstyles\n         |  \n         |  set_marker(self, marker)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data descriptors defined here:\n         |  \n         |  __dict__\n         |      dictionary for instance variables (if defined)\n         |  \n         |  __weakref__\n         |      list of weak references to the object (if defined)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data and other attributes defined here:\n         |  \n         |  filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H...\n         |  \n         |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n         |  \n         |  markers = {'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n    \n    DATA\n        CARETDOWN = 7\n        CARETDOWNBASE = 11\n        CARETLEFT = 4\n        CARETLEFTBASE = 8\n        CARETRIGHT = 5\n        CARETRIGHTBASE = 9\n        CARETUP = 6\n        CARETUPBASE = 10\n        TICKDOWN = 3\n        TICKLEFT = 0\n        TICKRIGHT = 1\n        TICKUP = 2\n        rcParams = RcParams({'_internal.classic_mode': False,\n             ...nor.widt...\n    \n    FILE\n        c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n\n\n​    \n​    \n\n## 四、seaborn画散点图\n\n<b>1、scatterplot函数</b>\n\n(1)格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.scatterplot)\n\n```\n\n    Help on function scatterplot in module seaborn.relational:\n    \n    scatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n        Draw a scatter plot with possibility of several semantic groupings.\n        \n        The relationship between ``x`` and ``y`` can be shown for different subsets\n        of the data using the ``hue``, ``size``, and ``style`` parameters. These\n        parameters control what visual semantics are used to identify the different\n        subsets. It is possible to show up to three dimensions independently by\n        using all three semantic types, but this style of plot can be hard to\n        interpret and is often ineffective. Using redundant semantics (i.e. both\n        ``hue`` and ``style`` for the same variable) can be helpful for making\n        graphics more accessible.\n        \n        See the :ref:`tutorial <relational_tutorial>` for more information.\n        \n        The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n        semantic, if present, depends on whether the variable is inferred to\n        represent \"numeric\" or \"categorical\" data. In particular, numeric variables\n        are represented with a sequential colormap by default, and the legend\n        entries show regular \"ticks\" with values that may or may not exist in the\n        data. This behavior can be controlled through various parameters, as\n        described and illustrated below.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        hue : vector or key in ``data``\n            Grouping variable that will produce points with different colors.\n            Can be either categorical or numeric, although color mapping will\n            behave differently in latter case.\n        size : vector or key in ``data``\n            Grouping variable that will produce points with different sizes.\n            Can be either categorical or numeric, although size mapping will\n            behave differently in latter case.\n        style : vector or key in ``data``\n            Grouping variable that will produce points with different markers.\n            Can have a numeric dtype but will always be treated as categorical.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        sizes : list, dict, or tuple\n            An object that determines how sizes are chosen when ``size`` is used.\n            It can always be a list of size values or a dict mapping levels of the\n            ``size`` variable to sizes. When ``size``  is numeric, it can also be\n            a tuple specifying the minimum and maximum size to use such that other\n            values are normalized within this range.\n        size_order : list\n            Specified order for appearance of the ``size`` variable levels,\n            otherwise they are determined from the data. Not relevant when the\n            ``size`` variable is numeric.\n        size_norm : tuple or Normalize object\n            Normalization in data units for scaling plot objects when the\n            ``size`` variable is numeric.\n        markers : boolean, list, or dictionary\n            Object determining how to draw the markers for different levels of the\n            ``style`` variable. Setting to ``True`` will use default markers, or\n            you can pass a list of markers or a dictionary mapping levels of the\n            ``style`` variable to markers. Setting to ``False`` will draw\n            marker-less lines.  Markers are specified as in matplotlib.\n        style_order : list\n            Specified order for appearance of the ``style`` variable levels\n            otherwise they are determined from the data. Not relevant when the\n            ``style`` variable is numeric.\n        {x,y}_bins : lists or arrays or functions\n            *Currently non-functional.*\n        units : vector or key in ``data``\n            Grouping variable identifying sampling units. When used, a separate\n            line will be drawn for each unit with appropriate semantics, but no\n            legend entry will be added. Useful for showing distribution of\n            experimental replicates when exact identities are not needed.\n            *Currently non-functional.*\n        estimator : name of pandas method or callable or None\n            Method for aggregating across multiple observations of the ``y``\n            variable at the same ``x`` level. If ``None``, all observations will\n            be drawn.\n            *Currently non-functional.*\n        ci : int or \"sd\" or None\n            Size of the confidence interval to draw when aggregating with an\n            estimator. \"sd\" means to draw the standard deviation of the data.\n            Setting to ``None`` will skip bootstrapping.\n            *Currently non-functional.*\n        n_boot : int\n            Number of bootstraps to use for computing the confidence interval.\n            *Currently non-functional.*\n        alpha : float\n            Proportional opacity of the points.\n        {x,y}_jitter : booleans or floats\n            *Currently non-functional.*\n        legend : \"auto\", \"brief\", \"full\", or False\n            How to draw the legend. If \"brief\", numeric ``hue`` and ``size``\n            variables will be represented with a sample of evenly spaced values.\n            If \"full\", every group will get an entry in the legend. If \"auto\",\n            choose between brief or full representation based on number of levels.\n            If ``False``, no legend data is added and no legend is drawn.\n        ax : :class:`matplotlib.axes.Axes`\n            Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n            internally.\n        kwargs : key, value mappings\n            Other keyword arguments are passed down to\n            :meth:`matplotlib.axes.Axes.scatter`.\n        \n        Returns\n        -------\n        :class:`matplotlib.axes.Axes`\n            The matplotlib axes containing the plot.\n        \n        See Also\n        --------\n        lineplot : Plot data using lines.\n        stripplot : Plot a categorical scatter with jitter.\n        swarmplot : Plot a categorical scatter with non-overlapping points.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/scatterplot.rst\n\n\n​    \n\n（2）简单例子\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(\"tips\")\n\"\"\"\n案例1：散点图\n\"\"\"\nsns.scatterplot( x=\"total_bill\", y=\"tip\",data=tips)\nplt.show()\n```\n\n\n![png](/img/output_29_0.png)\n    \n\n\n<strong>分组</strong>\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(\"iris\")\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=\"brief\")\n#legend的取值为brief,full和False\n```\n\n\n![png](/img/output_31_0.png)\n    \n\n\n<b>2、regplot函数</b>\n\nregplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n#help(sns.regplot)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490>\n\n\n\n\n![png](/img/output_34_1.png)\n    \n\n如不显示拟合线，可将fit_reg设置为false\n\nregplot完整用法请参阅\nhttps://seaborn.pydata.org/generated/seaborn.regplot.html\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520>\n\n\n\n\n![png](/img/output_36_1.png)\n    \n\n\n<b>3、lmplot函数画散点图</b>\n\nlmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=\"species\"。\n\n(1)implot 简单散点图\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_40_0.png)\n    \n\n\n（2）每组使用不同的标记（markers属性）\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, markers=[\"o\", \"x\", \"2\"])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_42_0.png)\n    \n\n\n（3）使用不同的调色板(palette)\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=\"Set2\")\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_44_0.png)\n    \n\n\n（4）控制每组的颜色\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=\"#9b59b6\", virginica=\"#3498db\", versicolor=\"#95a5a6\"))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_46_0.png)\n    \n\n\n# <center> 气泡图</center>\n\n散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。\n\n假设某个农产品的产量与温度和降雨量的关系如下表所示。<br>\n\n\n```python\nimport pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf_product\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    \n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n作出产量与温度的散点图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_52_0.png)\n    \n\n\n若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\nrainfall=df_product[u\"降雨量\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_54_0.png)\n    \n\n\n### 课堂练习：使用seaborn画气泡图\n\n自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```","source":"_posts/散点图.md","raw":"title: 散点图\nauthor: ztq\ntags:\n  - python\ncategories:\n  - python基础\ndate: 2021-04-17 18:18:00\n---\n\n# <center> 散点图</center>\n\n# 一、什么是散点图\n\n散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。\n\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n\n\n\n# 二、散点图的好处\n\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n\n\n\n## 三、matplotlib的散点图\n\n<b>1、plot画散点图</b>\n\n在折线图的基础上设置linestyle='none'\n\n\n```python\n # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame({'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) })\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n```\n\n\n![png](/img/output_8_0.png)\n    \n\n\n<b>2,使用scatter画散点图</b>\n\n简单的一个点的散点图\n\n\n```python\nimport matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n```\n\n\n![png](/img/output_11_0.png)\n    \n\n\n添加标题，给轴加上标签,并确保所有文本都大到能够看清： \n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.title(\"Square Numbers\", fontsize=24)\n\nplt.scatter(2, 4, s=200,color=\"r\") \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(\"Square of Value\", fontsize=14) \n\nplt.xlabel(\"x value\")\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n```\n\n\n![png](/img/output_13_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_14_0.png)\n    \n\n\n\n```python\nimport matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_15_0.png)\n    \n\n\n绘制一系列点,x_values,y_values为list\n\n\n```python\nimport matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n```\n\n\n![png](/img/output_17_0.png)\n    \n\n\n自动生成x,y值\n\n\n```python\nx_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(\"Square Numbers\", fontsize=24) \n\nplt.xlabel(\"Value\", fontsize=14) \n\nplt.ylabel(\"Square of Value\", fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n```\n\n\n![png](/img/output_19_0.png)\n    \n\n\n\n```python\nplt.scatter(x_values, y_values, edgecolor='none', s=40) \n```\n\n\n\n\n    <matplotlib.collections.PathCollection at 0x2b4aa653280>\n\n\n\n\n![png](/img/output_20_1.png)\n    \n\n\n<strong>保存到文件</strong>\n\n\n```python\nimport matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(\"Square Numbers\", fontsize=24) \nplt.xlabel(\"Value\", fontsize=14) \nplt.ylabel(\"Square of Value\", fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n```\n\n\n![png](/img/output_22_0.png)\n    \n\n\n\n```python\nimport matplotlib as mpl\nhelp(mpl.markers)\n```\n\n    Help on module matplotlib.markers in matplotlib:\n    \n    NAME\n        matplotlib.markers\n    \n    DESCRIPTION\n        This module contains functions to handle markers.  Used by both the\n        marker functionality of `~matplotlib.axes.Axes.plot` and\n        `~matplotlib.axes.Axes.scatter`.\n        \n        All possible markers are defined here:\n        \n        ============================== ====== =========================================\n        marker                         symbol description\n        ============================== ====== =========================================\n        ``\".\"``                        |m00|  point\n        ``\",\"``                        |m01|  pixel\n        ``\"o\"``                        |m02|  circle\n        ``\"v\"``                        |m03|  triangle_down\n        ``\"^\"``                        |m04|  triangle_up\n        ``\"<\"``                        |m05|  triangle_left\n        ``\">\"``                        |m06|  triangle_right\n        ``\"1\"``                        |m07|  tri_down\n        ``\"2\"``                        |m08|  tri_up\n        ``\"3\"``                        |m09|  tri_left\n        ``\"4\"``                        |m10|  tri_right\n        ``\"8\"``                        |m11|  octagon\n        ``\"s\"``                        |m12|  square\n        ``\"p\"``                        |m13|  pentagon\n        ``\"P\"``                        |m23|  plus (filled)\n        ``\"*\"``                        |m14|  star\n        ``\"h\"``                        |m15|  hexagon1\n        ``\"H\"``                        |m16|  hexagon2\n        ``\"+\"``                        |m17|  plus\n        ``\"x\"``                        |m18|  x\n        ``\"X\"``                        |m24|  x (filled)\n        ``\"D\"``                        |m19|  diamond\n        ``\"d\"``                        |m20|  thin_diamond\n        ``\"|\"``                        |m21|  vline\n        ``\"_\"``                        |m22|  hline\n        ``0`` (``TICKLEFT``)           |m25|  tickleft\n        ``1`` (``TICKRIGHT``)          |m26|  tickright\n        ``2`` (``TICKUP``)             |m27|  tickup\n        ``3`` (``TICKDOWN``)           |m28|  tickdown\n        ``4`` (``CARETLEFT``)          |m29|  caretleft\n        ``5`` (``CARETRIGHT``)         |m30|  caretright\n        ``6`` (``CARETUP``)            |m31|  caretup\n        ``7`` (``CARETDOWN``)          |m32|  caretdown\n        ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n        ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n        ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n        ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n        ``\"None\"``, ``\" \"`` or  ``\"\"``        nothing\n        ``'$...$'``                    |m37|  Render the string using mathtext.\n                                              E.g ``\"$f$\"`` for marker showing the\n                                              letter ``f``.\n        ``verts``                             A list of (x, y) pairs used for Path\n                                              vertices. The center of the marker is\n                                              located at (0, 0) and the size is\n                                              normalized, such that the created path\n                                              is encapsulated inside the unit cell.\n        path                                  A `~matplotlib.path.Path` instance.\n        ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                              sides, rotated by ``angle``.\n        ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                              rotated by ``angle``.\n        ============================== ====== =========================================\n        \n        ``None`` is the default which means 'nothing', however this table is\n        referred to from other docs for the valid inputs from marker inputs and in\n        those cases ``None`` still means 'default'.\n        \n        Note that special symbols can be defined via the\n        :doc:`STIX math font </tutorials/text/mathtext>`,\n        e.g. ``\"$\\u266B$\"``. For an overview over the STIX font symbols refer to the\n        `STIX font table <http://www.stixfonts.org/allGlyphs.html>`_.\n        Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n        \n        Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n        equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n        Hence the following are equivalent::\n        \n            plt.plot([1, 2, 3], marker=11)\n            plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n        \n        Examples showing the use of markers:\n        \n        * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n        * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n        * :doc:`/gallery/shapes_and_collections/marker_path`\n\n\n​        \n\n        .. |m00| image:: /_static/markers/m00.png\n        .. |m01| image:: /_static/markers/m01.png\n        .. |m02| image:: /_static/markers/m02.png\n        .. |m03| image:: /_static/markers/m03.png\n        .. |m04| image:: /_static/markers/m04.png\n        .. |m05| image:: /_static/markers/m05.png\n        .. |m06| image:: /_static/markers/m06.png\n        .. |m07| image:: /_static/markers/m07.png\n        .. |m08| image:: /_static/markers/m08.png\n        .. |m09| image:: /_static/markers/m09.png\n        .. |m10| image:: /_static/markers/m10.png\n        .. |m11| image:: /_static/markers/m11.png\n        .. |m12| image:: /_static/markers/m12.png\n        .. |m13| image:: /_static/markers/m13.png\n        .. |m14| image:: /_static/markers/m14.png\n        .. |m15| image:: /_static/markers/m15.png\n        .. |m16| image:: /_static/markers/m16.png\n        .. |m17| image:: /_static/markers/m17.png\n        .. |m18| image:: /_static/markers/m18.png\n        .. |m19| image:: /_static/markers/m19.png\n        .. |m20| image:: /_static/markers/m20.png\n        .. |m21| image:: /_static/markers/m21.png\n        .. |m22| image:: /_static/markers/m22.png\n        .. |m23| image:: /_static/markers/m23.png\n        .. |m24| image:: /_static/markers/m24.png\n        .. |m25| image:: /_static/markers/m25.png\n        .. |m26| image:: /_static/markers/m26.png\n        .. |m27| image:: /_static/markers/m27.png\n        .. |m28| image:: /_static/markers/m28.png\n        .. |m29| image:: /_static/markers/m29.png\n        .. |m30| image:: /_static/markers/m30.png\n        .. |m31| image:: /_static/markers/m31.png\n        .. |m32| image:: /_static/markers/m32.png\n        .. |m33| image:: /_static/markers/m33.png\n        .. |m34| image:: /_static/markers/m34.png\n        .. |m35| image:: /_static/markers/m35.png\n        .. |m36| image:: /_static/markers/m36.png\n        .. |m37| image:: /_static/markers/m37.png\n    \n    CLASSES\n        builtins.object\n            MarkerStyle\n        \n        class MarkerStyle(builtins.object)\n         |  MarkerStyle(marker=None, fillstyle=None)\n         |  \n         |  Methods defined here:\n         |  \n         |  __bool__(self)\n         |  \n         |  __init__(self, marker=None, fillstyle=None)\n         |      Attributes\n         |      ----------\n         |      markers : list of known marks\n         |      \n         |      fillstyles : list of known fillstyles\n         |      \n         |      filled_markers : list of known filled markers.\n         |      \n         |      Parameters\n         |      ----------\n         |      marker : str or array-like, optional, default: None\n         |          See the descriptions of possible markers in the module docstring.\n         |      \n         |      fillstyle : str, optional, default: 'full'\n         |          'full', 'left\", 'right', 'bottom', 'top', 'none'\n         |  \n         |  get_alt_path(self)\n         |  \n         |  get_alt_transform(self)\n         |  \n         |  get_capstyle(self)\n         |  \n         |  get_fillstyle(self)\n         |  \n         |  get_joinstyle(self)\n         |  \n         |  get_marker(self)\n         |  \n         |  get_path(self)\n         |  \n         |  get_snap_threshold(self)\n         |  \n         |  get_transform(self)\n         |  \n         |  is_filled(self)\n         |  \n         |  set_fillstyle(self, fillstyle)\n         |      Sets fillstyle\n         |      \n         |      Parameters\n         |      ----------\n         |      fillstyle : string amongst known fillstyles\n         |  \n         |  set_marker(self, marker)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data descriptors defined here:\n         |  \n         |  __dict__\n         |      dictionary for instance variables (if defined)\n         |  \n         |  __weakref__\n         |      list of weak references to the object (if defined)\n         |  \n         |  ----------------------------------------------------------------------\n         |  Data and other attributes defined here:\n         |  \n         |  filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H...\n         |  \n         |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n         |  \n         |  markers = {'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n    \n    DATA\n        CARETDOWN = 7\n        CARETDOWNBASE = 11\n        CARETLEFT = 4\n        CARETLEFTBASE = 8\n        CARETRIGHT = 5\n        CARETRIGHTBASE = 9\n        CARETUP = 6\n        CARETUPBASE = 10\n        TICKDOWN = 3\n        TICKLEFT = 0\n        TICKRIGHT = 1\n        TICKUP = 2\n        rcParams = RcParams({'_internal.classic_mode': False,\n             ...nor.widt...\n    \n    FILE\n        c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n\n\n​    \n​    \n\n## 四、seaborn画散点图\n\n<b>1、scatterplot函数</b>\n\n(1)格式\n\n\n```python\nimport seaborn as sns\nhelp(sns.scatterplot)\n\n```\n\n    Help on function scatterplot in module seaborn.relational:\n    \n    scatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n        Draw a scatter plot with possibility of several semantic groupings.\n        \n        The relationship between ``x`` and ``y`` can be shown for different subsets\n        of the data using the ``hue``, ``size``, and ``style`` parameters. These\n        parameters control what visual semantics are used to identify the different\n        subsets. It is possible to show up to three dimensions independently by\n        using all three semantic types, but this style of plot can be hard to\n        interpret and is often ineffective. Using redundant semantics (i.e. both\n        ``hue`` and ``style`` for the same variable) can be helpful for making\n        graphics more accessible.\n        \n        See the :ref:`tutorial <relational_tutorial>` for more information.\n        \n        The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n        semantic, if present, depends on whether the variable is inferred to\n        represent \"numeric\" or \"categorical\" data. In particular, numeric variables\n        are represented with a sequential colormap by default, and the legend\n        entries show regular \"ticks\" with values that may or may not exist in the\n        data. This behavior can be controlled through various parameters, as\n        described and illustrated below.\n        \n        Parameters\n        ----------\n        x, y : vectors or keys in ``data``\n            Variables that specify positions on the x and y axes.\n        hue : vector or key in ``data``\n            Grouping variable that will produce points with different colors.\n            Can be either categorical or numeric, although color mapping will\n            behave differently in latter case.\n        size : vector or key in ``data``\n            Grouping variable that will produce points with different sizes.\n            Can be either categorical or numeric, although size mapping will\n            behave differently in latter case.\n        style : vector or key in ``data``\n            Grouping variable that will produce points with different markers.\n            Can have a numeric dtype but will always be treated as categorical.\n        data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n            Input data structure. Either a long-form collection of vectors that can be\n            assigned to named variables or a wide-form dataset that will be internally\n            reshaped.\n        palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n            Method for choosing the colors to use when mapping the ``hue`` semantic.\n            String values are passed to :func:`color_palette`. List or dict values\n            imply categorical mapping, while a colormap object implies numeric mapping.\n        hue_order : vector of strings\n            Specify the order of processing and plotting for categorical levels of the\n            ``hue`` semantic.\n        hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n            Either a pair of values that set the normalization range in data units\n            or an object that will map from data units into a [0, 1] interval. Usage\n            implies numeric mapping.\n        sizes : list, dict, or tuple\n            An object that determines how sizes are chosen when ``size`` is used.\n            It can always be a list of size values or a dict mapping levels of the\n            ``size`` variable to sizes. When ``size``  is numeric, it can also be\n            a tuple specifying the minimum and maximum size to use such that other\n            values are normalized within this range.\n        size_order : list\n            Specified order for appearance of the ``size`` variable levels,\n            otherwise they are determined from the data. Not relevant when the\n            ``size`` variable is numeric.\n        size_norm : tuple or Normalize object\n            Normalization in data units for scaling plot objects when the\n            ``size`` variable is numeric.\n        markers : boolean, list, or dictionary\n            Object determining how to draw the markers for different levels of the\n            ``style`` variable. Setting to ``True`` will use default markers, or\n            you can pass a list of markers or a dictionary mapping levels of the\n            ``style`` variable to markers. Setting to ``False`` will draw\n            marker-less lines.  Markers are specified as in matplotlib.\n        style_order : list\n            Specified order for appearance of the ``style`` variable levels\n            otherwise they are determined from the data. Not relevant when the\n            ``style`` variable is numeric.\n        {x,y}_bins : lists or arrays or functions\n            *Currently non-functional.*\n        units : vector or key in ``data``\n            Grouping variable identifying sampling units. When used, a separate\n            line will be drawn for each unit with appropriate semantics, but no\n            legend entry will be added. Useful for showing distribution of\n            experimental replicates when exact identities are not needed.\n            *Currently non-functional.*\n        estimator : name of pandas method or callable or None\n            Method for aggregating across multiple observations of the ``y``\n            variable at the same ``x`` level. If ``None``, all observations will\n            be drawn.\n            *Currently non-functional.*\n        ci : int or \"sd\" or None\n            Size of the confidence interval to draw when aggregating with an\n            estimator. \"sd\" means to draw the standard deviation of the data.\n            Setting to ``None`` will skip bootstrapping.\n            *Currently non-functional.*\n        n_boot : int\n            Number of bootstraps to use for computing the confidence interval.\n            *Currently non-functional.*\n        alpha : float\n            Proportional opacity of the points.\n        {x,y}_jitter : booleans or floats\n            *Currently non-functional.*\n        legend : \"auto\", \"brief\", \"full\", or False\n            How to draw the legend. If \"brief\", numeric ``hue`` and ``size``\n            variables will be represented with a sample of evenly spaced values.\n            If \"full\", every group will get an entry in the legend. If \"auto\",\n            choose between brief or full representation based on number of levels.\n            If ``False``, no legend data is added and no legend is drawn.\n        ax : :class:`matplotlib.axes.Axes`\n            Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n            internally.\n        kwargs : key, value mappings\n            Other keyword arguments are passed down to\n            :meth:`matplotlib.axes.Axes.scatter`.\n        \n        Returns\n        -------\n        :class:`matplotlib.axes.Axes`\n            The matplotlib axes containing the plot.\n        \n        See Also\n        --------\n        lineplot : Plot data using lines.\n        stripplot : Plot a categorical scatter with jitter.\n        swarmplot : Plot a categorical scatter with non-overlapping points.\n        \n        Examples\n        --------\n        \n        .. include:: ../docstrings/scatterplot.rst\n\n\n​    \n\n（2）简单例子\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(\"tips\")\n\"\"\"\n案例1：散点图\n\"\"\"\nsns.scatterplot( x=\"total_bill\", y=\"tip\",data=tips)\nplt.show()\n```\n\n\n![png](/img/output_29_0.png)\n    \n\n\n<strong>分组</strong>\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(\"iris\")\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=\"brief\")\n#legend的取值为brief,full和False\n```\n\n\n![png](/img/output_31_0.png)\n    \n\n\n<b>2、regplot函数</b>\n\nregplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n#help(sns.regplot)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490>\n\n\n\n\n![png](/img/output_34_1.png)\n    \n\n如不显示拟合线，可将fit_reg设置为false\n\nregplot完整用法请参阅\nhttps://seaborn.pydata.org/generated/seaborn.regplot.html\n\n\n```python\n# library & dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"])\nsns.regplot(x=df[\"sepal_length\"], y=df[\"sepal_width\"],fit_reg=False)\n# library & dataset\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520>\n\n\n\n\n![png](/img/output_36_1.png)\n    \n\n\n<b>3、lmplot函数画散点图</b>\n\nlmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=\"species\"。\n\n(1)implot 简单散点图\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_40_0.png)\n    \n\n\n（2）每组使用不同的标记（markers属性）\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, markers=[\"o\", \"x\", \"2\"])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n```\n\n\n![png](/img/output_42_0.png)\n    \n\n\n（3）使用不同的调色板(palette)\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=\"Set2\")\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_44_0.png)\n    \n\n\n（4）控制每组的颜色\n\n\n```python\n# library & dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=\"sepal_length\", y=\"sepal_width\", data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=\"#9b59b6\", virginica=\"#3498db\", versicolor=\"#95a5a6\"))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n```\n\n\n![png](/img/output_46_0.png)\n    \n\n\n# <center> 气泡图</center>\n\n散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。\n\n假设某个农产品的产量与温度和降雨量的关系如下表所示。<br>\n\n\n```python\nimport pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf_product\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n    \n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n\n作出产量与温度的散点图\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_52_0.png)\n    \n\n\n若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u\"产量\"]\ntem = df_product[u\"温度\"]\nrainfall=df_product[u\"降雨量\"]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```\n\n\n![png](/img/output_54_0.png)\n    \n\n\n### 课堂练习：使用seaborn画气泡图\n\n自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(\"Data\\product.csv\",encoding=\"gbk\")\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n```","slug":"散点图","published":1,"updated":"2021-04-17T10:26:25.042Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknllvpms00ddl0t936u7b07v","content":"<h1><center> 散点图</center></h1>\n<h1>一、什么是散点图</h1>\n<p>散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。</p>\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n<h1>二、散点图的好处</h1>\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n<h2 id=\"三、matplotlib的散点图\">三、matplotlib的散点图</h2>\n<p><b>1、plot画散点图</b></p>\n<p>在折线图的基础上设置linestyle=‘none’</p>\n<pre><code class=\"language-python\"> # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame(&#123;'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) &#125;)\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_8_0.png\" alt=\"png\"></p>\n<p><b>2,使用scatter画散点图</b></p>\n<p>简单的一个点的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n</code></pre>\n<p><img src=\"/img/output_11_0.png\" alt=\"png\"></p>\n<p>添加标题，给轴加上标签,并确保所有文本都大到能够看清：</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24)\n\nplt.scatter(2, 4, s=200,color=&quot;r&quot;) \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\nplt.xlabel(&quot;x value&quot;)\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_14_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_15_0.png\" alt=\"png\"></p>\n<p>绘制一系列点,x_values,y_values为list</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_17_0.png\" alt=\"png\"></p>\n<p>自动生成x,y值</p>\n<pre><code class=\"language-python\">x_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_19_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">plt.scatter(x_values, y_values, edgecolor='none', s=40) \n</code></pre>\n<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b4aa653280&gt;\n</code></pre>\n<p><img src=\"/img/output_20_1.png\" alt=\"png\"></p>\n<p><strong>保存到文件</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(&quot;Square Numbers&quot;, fontsize=24) \nplt.xlabel(&quot;Value&quot;, fontsize=14) \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n</code></pre>\n<p><img src=\"/img/output_22_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nhelp(mpl.markers)\n</code></pre>\n<pre><code>Help on module matplotlib.markers in matplotlib:\n\nNAME\n    matplotlib.markers\n\nDESCRIPTION\n    This module contains functions to handle markers.  Used by both the\n    marker functionality of `~matplotlib.axes.Axes.plot` and\n    `~matplotlib.axes.Axes.scatter`.\n    \n    All possible markers are defined here:\n    \n    ============================== ====== =========================================\n    marker                         symbol description\n    ============================== ====== =========================================\n    ``&quot;.&quot;``                        |m00|  point\n    ``&quot;,&quot;``                        |m01|  pixel\n    ``&quot;o&quot;``                        |m02|  circle\n    ``&quot;v&quot;``                        |m03|  triangle_down\n    ``&quot;^&quot;``                        |m04|  triangle_up\n    ``&quot;&lt;&quot;``                        |m05|  triangle_left\n    ``&quot;&gt;&quot;``                        |m06|  triangle_right\n    ``&quot;1&quot;``                        |m07|  tri_down\n    ``&quot;2&quot;``                        |m08|  tri_up\n    ``&quot;3&quot;``                        |m09|  tri_left\n    ``&quot;4&quot;``                        |m10|  tri_right\n    ``&quot;8&quot;``                        |m11|  octagon\n    ``&quot;s&quot;``                        |m12|  square\n    ``&quot;p&quot;``                        |m13|  pentagon\n    ``&quot;P&quot;``                        |m23|  plus (filled)\n    ``&quot;*&quot;``                        |m14|  star\n    ``&quot;h&quot;``                        |m15|  hexagon1\n    ``&quot;H&quot;``                        |m16|  hexagon2\n    ``&quot;+&quot;``                        |m17|  plus\n    ``&quot;x&quot;``                        |m18|  x\n    ``&quot;X&quot;``                        |m24|  x (filled)\n    ``&quot;D&quot;``                        |m19|  diamond\n    ``&quot;d&quot;``                        |m20|  thin_diamond\n    ``&quot;|&quot;``                        |m21|  vline\n    ``&quot;_&quot;``                        |m22|  hline\n    ``0`` (``TICKLEFT``)           |m25|  tickleft\n    ``1`` (``TICKRIGHT``)          |m26|  tickright\n    ``2`` (``TICKUP``)             |m27|  tickup\n    ``3`` (``TICKDOWN``)           |m28|  tickdown\n    ``4`` (``CARETLEFT``)          |m29|  caretleft\n    ``5`` (``CARETRIGHT``)         |m30|  caretright\n    ``6`` (``CARETUP``)            |m31|  caretup\n    ``7`` (``CARETDOWN``)          |m32|  caretdown\n    ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n    ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n    ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n    ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n    ``&quot;None&quot;``, ``&quot; &quot;`` or  ``&quot;&quot;``        nothing\n    ``'$...$'``                    |m37|  Render the string using mathtext.\n                                          E.g ``&quot;$f$&quot;`` for marker showing the\n                                          letter ``f``.\n    ``verts``                             A list of (x, y) pairs used for Path\n                                          vertices. The center of the marker is\n                                          located at (0, 0) and the size is\n                                          normalized, such that the created path\n                                          is encapsulated inside the unit cell.\n    path                                  A `~matplotlib.path.Path` instance.\n    ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                          rotated by ``angle``.\n    ============================== ====== =========================================\n    \n    ``None`` is the default which means 'nothing', however this table is\n    referred to from other docs for the valid inputs from marker inputs and in\n    those cases ``None`` still means 'default'.\n    \n    Note that special symbols can be defined via the\n    :doc:`STIX math font &lt;/tutorials/text/mathtext&gt;`,\n    e.g. ``&quot;$\\u266B$&quot;``. For an overview over the STIX font symbols refer to the\n    `STIX font table &lt;http://www.stixfonts.org/allGlyphs.html&gt;`_.\n    Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n    \n    Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n    equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n    Hence the following are equivalent::\n    \n        plt.plot([1, 2, 3], marker=11)\n        plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n    \n    Examples showing the use of markers:\n    \n    * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n    * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n    * :doc:`/gallery/shapes_and_collections/marker_path`\n</code></pre>\n<p>​</p>\n<pre><code>    .. |m00| image:: /_static/markers/m00.png\n    .. |m01| image:: /_static/markers/m01.png\n    .. |m02| image:: /_static/markers/m02.png\n    .. |m03| image:: /_static/markers/m03.png\n    .. |m04| image:: /_static/markers/m04.png\n    .. |m05| image:: /_static/markers/m05.png\n    .. |m06| image:: /_static/markers/m06.png\n    .. |m07| image:: /_static/markers/m07.png\n    .. |m08| image:: /_static/markers/m08.png\n    .. |m09| image:: /_static/markers/m09.png\n    .. |m10| image:: /_static/markers/m10.png\n    .. |m11| image:: /_static/markers/m11.png\n    .. |m12| image:: /_static/markers/m12.png\n    .. |m13| image:: /_static/markers/m13.png\n    .. |m14| image:: /_static/markers/m14.png\n    .. |m15| image:: /_static/markers/m15.png\n    .. |m16| image:: /_static/markers/m16.png\n    .. |m17| image:: /_static/markers/m17.png\n    .. |m18| image:: /_static/markers/m18.png\n    .. |m19| image:: /_static/markers/m19.png\n    .. |m20| image:: /_static/markers/m20.png\n    .. |m21| image:: /_static/markers/m21.png\n    .. |m22| image:: /_static/markers/m22.png\n    .. |m23| image:: /_static/markers/m23.png\n    .. |m24| image:: /_static/markers/m24.png\n    .. |m25| image:: /_static/markers/m25.png\n    .. |m26| image:: /_static/markers/m26.png\n    .. |m27| image:: /_static/markers/m27.png\n    .. |m28| image:: /_static/markers/m28.png\n    .. |m29| image:: /_static/markers/m29.png\n    .. |m30| image:: /_static/markers/m30.png\n    .. |m31| image:: /_static/markers/m31.png\n    .. |m32| image:: /_static/markers/m32.png\n    .. |m33| image:: /_static/markers/m33.png\n    .. |m34| image:: /_static/markers/m34.png\n    .. |m35| image:: /_static/markers/m35.png\n    .. |m36| image:: /_static/markers/m36.png\n    .. |m37| image:: /_static/markers/m37.png\n\nCLASSES\n    builtins.object\n        MarkerStyle\n    \n    class MarkerStyle(builtins.object)\n     |  MarkerStyle(marker=None, fillstyle=None)\n     |  \n     |  Methods defined here:\n     |  \n     |  __bool__(self)\n     |  \n     |  __init__(self, marker=None, fillstyle=None)\n     |      Attributes\n     |      ----------\n     |      markers : list of known marks\n     |      \n     |      fillstyles : list of known fillstyles\n     |      \n     |      filled_markers : list of known filled markers.\n     |      \n     |      Parameters\n     |      ----------\n     |      marker : str or array-like, optional, default: None\n     |          See the descriptions of possible markers in the module docstring.\n     |      \n     |      fillstyle : str, optional, default: 'full'\n     |          'full', 'left&quot;, 'right', 'bottom', 'top', 'none'\n     |  \n     |  get_alt_path(self)\n     |  \n     |  get_alt_transform(self)\n     |  \n     |  get_capstyle(self)\n     |  \n     |  get_fillstyle(self)\n     |  \n     |  get_joinstyle(self)\n     |  \n     |  get_marker(self)\n     |  \n     |  get_path(self)\n     |  \n     |  get_snap_threshold(self)\n     |  \n     |  get_transform(self)\n     |  \n     |  is_filled(self)\n     |  \n     |  set_fillstyle(self, fillstyle)\n     |      Sets fillstyle\n     |      \n     |      Parameters\n     |      ----------\n     |      fillstyle : string amongst known fillstyles\n     |  \n     |  set_marker(self, marker)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  filled_markers = ('o', 'v', '^', '&lt;', '&gt;', '8', 's', 'p', '*', 'h', 'H...\n     |  \n     |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n     |  \n     |  markers = &#123;'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n\nDATA\n    CARETDOWN = 7\n    CARETDOWNBASE = 11\n    CARETLEFT = 4\n    CARETLEFTBASE = 8\n    CARETRIGHT = 5\n    CARETRIGHTBASE = 9\n    CARETUP = 6\n    CARETUPBASE = 10\n    TICKDOWN = 3\n    TICKLEFT = 0\n    TICKRIGHT = 1\n    TICKUP = 2\n    rcParams = RcParams(&#123;'_internal.classic_mode': False,\n         ...nor.widt...\n\nFILE\n    c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n</code></pre>\n<p>​<br>\n​</p>\n<h2 id=\"四、seaborn画散点图\">四、seaborn画散点图</h2>\n<p><b>1、scatterplot函数</b></p>\n<p>(1)格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.scatterplot)\n\n</code></pre>\n<pre><code>Help on function scatterplot in module seaborn.relational:\n\nscatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n    Draw a scatter plot with possibility of several semantic groupings.\n    \n    The relationship between ``x`` and ``y`` can be shown for different subsets\n    of the data using the ``hue``, ``size``, and ``style`` parameters. These\n    parameters control what visual semantics are used to identify the different\n    subsets. It is possible to show up to three dimensions independently by\n    using all three semantic types, but this style of plot can be hard to\n    interpret and is often ineffective. Using redundant semantics (i.e. both\n    ``hue`` and ``style`` for the same variable) can be helpful for making\n    graphics more accessible.\n    \n    See the :ref:`tutorial &lt;relational_tutorial&gt;` for more information.\n    \n    The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n    semantic, if present, depends on whether the variable is inferred to\n    represent &quot;numeric&quot; or &quot;categorical&quot; data. In particular, numeric variables\n    are represented with a sequential colormap by default, and the legend\n    entries show regular &quot;ticks&quot; with values that may or may not exist in the\n    data. This behavior can be controlled through various parameters, as\n    described and illustrated below.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    hue : vector or key in ``data``\n        Grouping variable that will produce points with different colors.\n        Can be either categorical or numeric, although color mapping will\n        behave differently in latter case.\n    size : vector or key in ``data``\n        Grouping variable that will produce points with different sizes.\n        Can be either categorical or numeric, although size mapping will\n        behave differently in latter case.\n    style : vector or key in ``data``\n        Grouping variable that will produce points with different markers.\n        Can have a numeric dtype but will always be treated as categorical.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    sizes : list, dict, or tuple\n        An object that determines how sizes are chosen when ``size`` is used.\n        It can always be a list of size values or a dict mapping levels of the\n        ``size`` variable to sizes. When ``size``  is numeric, it can also be\n        a tuple specifying the minimum and maximum size to use such that other\n        values are normalized within this range.\n    size_order : list\n        Specified order for appearance of the ``size`` variable levels,\n        otherwise they are determined from the data. Not relevant when the\n        ``size`` variable is numeric.\n    size_norm : tuple or Normalize object\n        Normalization in data units for scaling plot objects when the\n        ``size`` variable is numeric.\n    markers : boolean, list, or dictionary\n        Object determining how to draw the markers for different levels of the\n        ``style`` variable. Setting to ``True`` will use default markers, or\n        you can pass a list of markers or a dictionary mapping levels of the\n        ``style`` variable to markers. Setting to ``False`` will draw\n        marker-less lines.  Markers are specified as in matplotlib.\n    style_order : list\n        Specified order for appearance of the ``style`` variable levels\n        otherwise they are determined from the data. Not relevant when the\n        ``style`` variable is numeric.\n    &#123;x,y&#125;_bins : lists or arrays or functions\n        *Currently non-functional.*\n    units : vector or key in ``data``\n        Grouping variable identifying sampling units. When used, a separate\n        line will be drawn for each unit with appropriate semantics, but no\n        legend entry will be added. Useful for showing distribution of\n        experimental replicates when exact identities are not needed.\n        *Currently non-functional.*\n    estimator : name of pandas method or callable or None\n        Method for aggregating across multiple observations of the ``y``\n        variable at the same ``x`` level. If ``None``, all observations will\n        be drawn.\n        *Currently non-functional.*\n    ci : int or &quot;sd&quot; or None\n        Size of the confidence interval to draw when aggregating with an\n        estimator. &quot;sd&quot; means to draw the standard deviation of the data.\n        Setting to ``None`` will skip bootstrapping.\n        *Currently non-functional.*\n    n_boot : int\n        Number of bootstraps to use for computing the confidence interval.\n        *Currently non-functional.*\n    alpha : float\n        Proportional opacity of the points.\n    &#123;x,y&#125;_jitter : booleans or floats\n        *Currently non-functional.*\n    legend : &quot;auto&quot;, &quot;brief&quot;, &quot;full&quot;, or False\n        How to draw the legend. If &quot;brief&quot;, numeric ``hue`` and ``size``\n        variables will be represented with a sample of evenly spaced values.\n        If &quot;full&quot;, every group will get an entry in the legend. If &quot;auto&quot;,\n        choose between brief or full representation based on number of levels.\n        If ``False``, no legend data is added and no legend is drawn.\n    ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n        internally.\n    kwargs : key, value mappings\n        Other keyword arguments are passed down to\n        :meth:`matplotlib.axes.Axes.scatter`.\n    \n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n        The matplotlib axes containing the plot.\n    \n    See Also\n    --------\n    lineplot : Plot data using lines.\n    stripplot : Plot a categorical scatter with jitter.\n    swarmplot : Plot a categorical scatter with non-overlapping points.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/scatterplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）简单例子</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(&quot;tips&quot;)\n&quot;&quot;&quot;\n案例1：散点图\n&quot;&quot;&quot;\nsns.scatterplot( x=&quot;total_bill&quot;, y=&quot;tip&quot;,data=tips)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_29_0.png\" alt=\"png\"></p>\n<p><strong>分组</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(&quot;iris&quot;)\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=&quot;brief&quot;)\n#legend的取值为brief,full和False\n</code></pre>\n<p><img src=\"/img/output_31_0.png\" alt=\"png\"></p>\n<p><b>2、regplot函数</b></p>\n<p>regplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n#help(sns.regplot)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490&gt;\n</code></pre>\n<p><img src=\"/img/output_34_1.png\" alt=\"png\"></p>\n<p>如不显示拟合线，可将fit_reg设置为false</p>\n<p>regplot完整用法请参阅<br>\n<a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\">https://seaborn.pydata.org/generated/seaborn.regplot.html</a></p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520&gt;\n</code></pre>\n<p><img src=\"/img/output_36_1.png\" alt=\"png\"></p>\n<p><b>3、lmplot函数画散点图</b></p>\n<p>lmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=“species”。</p>\n<p>(1)implot 简单散点图</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_0.png\" alt=\"png\"></p>\n<p>（2）每组使用不同的标记（markers属性）</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, markers=[&quot;o&quot;, &quot;x&quot;, &quot;2&quot;])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0.png\" alt=\"png\"></p>\n<p>（3）使用不同的调色板(palette)</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=&quot;Set2&quot;)\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0.png\" alt=\"png\"></p>\n<p>（4）控制每组的颜色</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=&quot;#9b59b6&quot;, virginica=&quot;#3498db&quot;, versicolor=&quot;#95a5a6&quot;))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_46_0.png\" alt=\"png\"></p>\n<h1><center> 气泡图</center></h1>\n<p>散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。</p>\n<p>假设某个农产品的产量与温度和降雨量的关系如下表所示。<br></p>\n<pre><code class=\"language-python\">import pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf_product\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>作出产量与温度的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_52_0.png\" alt=\"png\"></p>\n<p>若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/><br>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\nrainfall=df_product[u&quot;降雨量&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_54_0.png\" alt=\"png\"></p>\n<h3 id=\"课堂练习：使用seaborn画气泡图\">课堂练习：使用seaborn画气泡图</h3>\n<p>自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1><center> 散点图</center></h1>\n<h1>一、什么是散点图</h1>\n<p>散点图，顾名思义就是由一些散乱的点组成的图表，这些点在哪个位置，是由其X值和Y值确定的。所以也叫做XY散点图。</p>\n<p>散点图经常用来显示分布或者比较几个变量的相关性或者分组。\n<p>一般用正相关、负相关和不相关描述。点分布在某一条直线附近，若是从左下角区域分布到右上角区域,则是正相关；\n<p>若是从左上角分布到右下角区域,则是负相关；\n<p>点的分布无规律则不相关。\n<p>相关性还可以分强弱，点分布越靠近一直线，相关性也强，否则越弱。\n<h1>二、散点图的好处</h1>\n<p>（1）数据用图表来展示，显然比较直观，在工作汇报等场合能起到事半功倍的效果，让听者更容易接受，理解你所处理的数据。\n<p>（2）散点图更偏向于研究型图表，能让我们发现变量之间隐藏的关系为我们决策作出重要的引导作用。\n<p>（3）散点图核心的价值在于发现变量之间的关系，千万不要简单地将这个关系理解为线性回归关系。变量间的关系有很多，如线性关系、指数关系、对数关系等等，当然，没有关系也是一种重要的关系。\n<p>（4）散点图经过回归分析之后，可以对相关对象进行预测分析，进而做出科学的决策，而不是模棱两可。比如说：医学里的白细胞散点图可以在医学检测方面为我们健康提供精确的分析，为医生后续的判断做出重要的技术支持。\n<h2 id=\"三、matplotlib的散点图\">三、matplotlib的散点图</h2>\n<p><b>1、plot画散点图</b></p>\n<p>在折线图的基础上设置linestyle=‘none’</p>\n<pre><code class=\"language-python\"> # libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \n# Create a dataset:\ndf=pd.DataFrame(&#123;'x_values': range(1,101), 'y_values': np.random.randn(100)*15+range(1,101) &#125;)\n \n# plot\nplt.plot( 'x_values', 'y_values', data=df, linestyle='-', marker='o')\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_8_0.png\" alt=\"png\"></p>\n<p><b>2,使用scatter画散点图</b></p>\n<p>简单的一个点的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nplt.scatter(2, 4) \nplt.show() \n</code></pre>\n<p><img src=\"/img/output_11_0.png\" alt=\"png\"></p>\n<p>添加标题，给轴加上标签,并确保所有文本都大到能够看清：</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24)\n\nplt.scatter(2, 4, s=200,color=&quot;r&quot;) \n\n# 设置图表标题并给坐标轴加上标签 \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\nplt.xlabel(&quot;x value&quot;)\n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_13_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='minor', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_14_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nplt.scatter(2, 4, s=200) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='both', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_15_0.png\" alt=\"png\"></p>\n<p>绘制一系列点,x_values,y_values为list</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \n\nx_values = [1, 2, 3, 4, 5] \n\ny_values = [1, 4, 9, 16, 25] \n\nplt.scatter(x_values, y_values,s=100) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_17_0.png\" alt=\"png\"></p>\n<p>自动生成x,y值</p>\n<pre><code class=\"language-python\">x_values = list(range(1, 1001)) \n\ny_values = [x**2 for x in x_values] \n\nplt.scatter(x_values, y_values, s=40) \n\n# 设置图表标题并给坐标轴加上标签 \n\nplt.title(&quot;Square Numbers&quot;, fontsize=24) \n\nplt.xlabel(&quot;Value&quot;, fontsize=14) \n\nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n\n# 设置刻度标记的大小 \n\nplt.tick_params(axis='both', which='major', labelsize=14) \n\n# 设置每个坐标轴的取值范围 \n\nplt.axis([0, 1100, 0, 1100000]) \n\nplt.show() \n</code></pre>\n<p><img src=\"/img/output_19_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">plt.scatter(x_values, y_values, edgecolor='none', s=40) \n</code></pre>\n<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b4aa653280&gt;\n</code></pre>\n<p><img src=\"/img/output_20_1.png\" alt=\"png\"></p>\n<p><strong>保存到文件</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt \nx_values = list(range(1001)) \ny_values = [x**2 for x in x_values] \nplt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolor='none', s=40) \n# 设置图表标题并给坐标轴加上标签 \nplt.title(&quot;Square Numbers&quot;, fontsize=24) \nplt.xlabel(&quot;Value&quot;, fontsize=14) \nplt.ylabel(&quot;Square of Value&quot;, fontsize=14) \n# 设置刻度标记的大小 \nplt.tick_params(axis='both', which='major', labelsize=14) \n# 设置每个坐标轴的取值范围 \nplt.axis([0, 1100, 0, 1100000]) \n#plt.show() \n\nplt.savefig('images/squares_plot.png', bbox_inches='tight')\n\n</code></pre>\n<p><img src=\"/img/output_22_0.png\" alt=\"png\"></p>\n<pre><code class=\"language-python\">import matplotlib as mpl\nhelp(mpl.markers)\n</code></pre>\n<pre><code>Help on module matplotlib.markers in matplotlib:\n\nNAME\n    matplotlib.markers\n\nDESCRIPTION\n    This module contains functions to handle markers.  Used by both the\n    marker functionality of `~matplotlib.axes.Axes.plot` and\n    `~matplotlib.axes.Axes.scatter`.\n    \n    All possible markers are defined here:\n    \n    ============================== ====== =========================================\n    marker                         symbol description\n    ============================== ====== =========================================\n    ``&quot;.&quot;``                        |m00|  point\n    ``&quot;,&quot;``                        |m01|  pixel\n    ``&quot;o&quot;``                        |m02|  circle\n    ``&quot;v&quot;``                        |m03|  triangle_down\n    ``&quot;^&quot;``                        |m04|  triangle_up\n    ``&quot;&lt;&quot;``                        |m05|  triangle_left\n    ``&quot;&gt;&quot;``                        |m06|  triangle_right\n    ``&quot;1&quot;``                        |m07|  tri_down\n    ``&quot;2&quot;``                        |m08|  tri_up\n    ``&quot;3&quot;``                        |m09|  tri_left\n    ``&quot;4&quot;``                        |m10|  tri_right\n    ``&quot;8&quot;``                        |m11|  octagon\n    ``&quot;s&quot;``                        |m12|  square\n    ``&quot;p&quot;``                        |m13|  pentagon\n    ``&quot;P&quot;``                        |m23|  plus (filled)\n    ``&quot;*&quot;``                        |m14|  star\n    ``&quot;h&quot;``                        |m15|  hexagon1\n    ``&quot;H&quot;``                        |m16|  hexagon2\n    ``&quot;+&quot;``                        |m17|  plus\n    ``&quot;x&quot;``                        |m18|  x\n    ``&quot;X&quot;``                        |m24|  x (filled)\n    ``&quot;D&quot;``                        |m19|  diamond\n    ``&quot;d&quot;``                        |m20|  thin_diamond\n    ``&quot;|&quot;``                        |m21|  vline\n    ``&quot;_&quot;``                        |m22|  hline\n    ``0`` (``TICKLEFT``)           |m25|  tickleft\n    ``1`` (``TICKRIGHT``)          |m26|  tickright\n    ``2`` (``TICKUP``)             |m27|  tickup\n    ``3`` (``TICKDOWN``)           |m28|  tickdown\n    ``4`` (``CARETLEFT``)          |m29|  caretleft\n    ``5`` (``CARETRIGHT``)         |m30|  caretright\n    ``6`` (``CARETUP``)            |m31|  caretup\n    ``7`` (``CARETDOWN``)          |m32|  caretdown\n    ``8`` (``CARETLEFTBASE``)      |m33|  caretleft (centered at base)\n    ``9`` (``CARETRIGHTBASE``)     |m34|  caretright (centered at base)\n    ``10`` (``CARETUPBASE``)       |m35|  caretup (centered at base)\n    ``11`` (``CARETDOWNBASE``)     |m36|  caretdown (centered at base)\n    ``&quot;None&quot;``, ``&quot; &quot;`` or  ``&quot;&quot;``        nothing\n    ``'$...$'``                    |m37|  Render the string using mathtext.\n                                          E.g ``&quot;$f$&quot;`` for marker showing the\n                                          letter ``f``.\n    ``verts``                             A list of (x, y) pairs used for Path\n                                          vertices. The center of the marker is\n                                          located at (0, 0) and the size is\n                                          normalized, such that the created path\n                                          is encapsulated inside the unit cell.\n    path                                  A `~matplotlib.path.Path` instance.\n    ``(numsides, 0, angle)``              A regular polygon with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 1, angle)``              A star-like symbol with ``numsides``\n                                          sides, rotated by ``angle``.\n    ``(numsides, 2, angle)``              An asterisk with ``numsides`` sides,\n                                          rotated by ``angle``.\n    ============================== ====== =========================================\n    \n    ``None`` is the default which means 'nothing', however this table is\n    referred to from other docs for the valid inputs from marker inputs and in\n    those cases ``None`` still means 'default'.\n    \n    Note that special symbols can be defined via the\n    :doc:`STIX math font &lt;/tutorials/text/mathtext&gt;`,\n    e.g. ``&quot;$\\u266B$&quot;``. For an overview over the STIX font symbols refer to the\n    `STIX font table &lt;http://www.stixfonts.org/allGlyphs.html&gt;`_.\n    Also see the :doc:`/gallery/text_labels_and_annotations/stix_fonts_demo`.\n    \n    Integer numbers from ``0`` to ``11`` create lines and triangles. Those are\n    equally accessible via capitalized variables, like ``CARETDOWNBASE``.\n    Hence the following are equivalent::\n    \n        plt.plot([1, 2, 3], marker=11)\n        plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n    \n    Examples showing the use of markers:\n    \n    * :doc:`/gallery/lines_bars_and_markers/marker_reference`\n    * :doc:`/gallery/lines_bars_and_markers/marker_fillstyle_reference`\n    * :doc:`/gallery/shapes_and_collections/marker_path`\n</code></pre>\n<p>​</p>\n<pre><code>    .. |m00| image:: /_static/markers/m00.png\n    .. |m01| image:: /_static/markers/m01.png\n    .. |m02| image:: /_static/markers/m02.png\n    .. |m03| image:: /_static/markers/m03.png\n    .. |m04| image:: /_static/markers/m04.png\n    .. |m05| image:: /_static/markers/m05.png\n    .. |m06| image:: /_static/markers/m06.png\n    .. |m07| image:: /_static/markers/m07.png\n    .. |m08| image:: /_static/markers/m08.png\n    .. |m09| image:: /_static/markers/m09.png\n    .. |m10| image:: /_static/markers/m10.png\n    .. |m11| image:: /_static/markers/m11.png\n    .. |m12| image:: /_static/markers/m12.png\n    .. |m13| image:: /_static/markers/m13.png\n    .. |m14| image:: /_static/markers/m14.png\n    .. |m15| image:: /_static/markers/m15.png\n    .. |m16| image:: /_static/markers/m16.png\n    .. |m17| image:: /_static/markers/m17.png\n    .. |m18| image:: /_static/markers/m18.png\n    .. |m19| image:: /_static/markers/m19.png\n    .. |m20| image:: /_static/markers/m20.png\n    .. |m21| image:: /_static/markers/m21.png\n    .. |m22| image:: /_static/markers/m22.png\n    .. |m23| image:: /_static/markers/m23.png\n    .. |m24| image:: /_static/markers/m24.png\n    .. |m25| image:: /_static/markers/m25.png\n    .. |m26| image:: /_static/markers/m26.png\n    .. |m27| image:: /_static/markers/m27.png\n    .. |m28| image:: /_static/markers/m28.png\n    .. |m29| image:: /_static/markers/m29.png\n    .. |m30| image:: /_static/markers/m30.png\n    .. |m31| image:: /_static/markers/m31.png\n    .. |m32| image:: /_static/markers/m32.png\n    .. |m33| image:: /_static/markers/m33.png\n    .. |m34| image:: /_static/markers/m34.png\n    .. |m35| image:: /_static/markers/m35.png\n    .. |m36| image:: /_static/markers/m36.png\n    .. |m37| image:: /_static/markers/m37.png\n\nCLASSES\n    builtins.object\n        MarkerStyle\n    \n    class MarkerStyle(builtins.object)\n     |  MarkerStyle(marker=None, fillstyle=None)\n     |  \n     |  Methods defined here:\n     |  \n     |  __bool__(self)\n     |  \n     |  __init__(self, marker=None, fillstyle=None)\n     |      Attributes\n     |      ----------\n     |      markers : list of known marks\n     |      \n     |      fillstyles : list of known fillstyles\n     |      \n     |      filled_markers : list of known filled markers.\n     |      \n     |      Parameters\n     |      ----------\n     |      marker : str or array-like, optional, default: None\n     |          See the descriptions of possible markers in the module docstring.\n     |      \n     |      fillstyle : str, optional, default: 'full'\n     |          'full', 'left&quot;, 'right', 'bottom', 'top', 'none'\n     |  \n     |  get_alt_path(self)\n     |  \n     |  get_alt_transform(self)\n     |  \n     |  get_capstyle(self)\n     |  \n     |  get_fillstyle(self)\n     |  \n     |  get_joinstyle(self)\n     |  \n     |  get_marker(self)\n     |  \n     |  get_path(self)\n     |  \n     |  get_snap_threshold(self)\n     |  \n     |  get_transform(self)\n     |  \n     |  is_filled(self)\n     |  \n     |  set_fillstyle(self, fillstyle)\n     |      Sets fillstyle\n     |      \n     |      Parameters\n     |      ----------\n     |      fillstyle : string amongst known fillstyles\n     |  \n     |  set_marker(self, marker)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  filled_markers = ('o', 'v', '^', '&lt;', '&gt;', '8', 's', 'p', '*', 'h', 'H...\n     |  \n     |  fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')\n     |  \n     |  markers = &#123;'.': 'point', ',': 'pixel', 'o': 'circle', 'v': 'triangle_d...\n\nDATA\n    CARETDOWN = 7\n    CARETDOWNBASE = 11\n    CARETLEFT = 4\n    CARETLEFTBASE = 8\n    CARETRIGHT = 5\n    CARETRIGHTBASE = 9\n    CARETUP = 6\n    CARETUPBASE = 10\n    TICKDOWN = 3\n    TICKLEFT = 0\n    TICKRIGHT = 1\n    TICKUP = 2\n    rcParams = RcParams(&#123;'_internal.classic_mode': False,\n         ...nor.widt...\n\nFILE\n    c:\\users\\shili\\anaconda3\\lib\\site-packages\\matplotlib\\markers.py\n</code></pre>\n<p>​<br>\n​</p>\n<h2 id=\"四、seaborn画散点图\">四、seaborn画散点图</h2>\n<p><b>1、scatterplot函数</b></p>\n<p>(1)格式</p>\n<pre><code class=\"language-python\">import seaborn as sns\nhelp(sns.scatterplot)\n\n</code></pre>\n<pre><code>Help on function scatterplot in module seaborn.relational:\n\nscatterplot(*, x=None, y=None, hue=None, style=None, size=None, data=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=True, style_order=None, x_bins=None, y_bins=None, units=None, estimator=None, ci=95, n_boot=1000, alpha=None, x_jitter=None, y_jitter=None, legend='auto', ax=None, **kwargs)\n    Draw a scatter plot with possibility of several semantic groupings.\n    \n    The relationship between ``x`` and ``y`` can be shown for different subsets\n    of the data using the ``hue``, ``size``, and ``style`` parameters. These\n    parameters control what visual semantics are used to identify the different\n    subsets. It is possible to show up to three dimensions independently by\n    using all three semantic types, but this style of plot can be hard to\n    interpret and is often ineffective. Using redundant semantics (i.e. both\n    ``hue`` and ``style`` for the same variable) can be helpful for making\n    graphics more accessible.\n    \n    See the :ref:`tutorial &lt;relational_tutorial&gt;` for more information.\n    \n    The default treatment of the ``hue`` (and to a lesser extent, ``size``)\n    semantic, if present, depends on whether the variable is inferred to\n    represent &quot;numeric&quot; or &quot;categorical&quot; data. In particular, numeric variables\n    are represented with a sequential colormap by default, and the legend\n    entries show regular &quot;ticks&quot; with values that may or may not exist in the\n    data. This behavior can be controlled through various parameters, as\n    described and illustrated below.\n    \n    Parameters\n    ----------\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    hue : vector or key in ``data``\n        Grouping variable that will produce points with different colors.\n        Can be either categorical or numeric, although color mapping will\n        behave differently in latter case.\n    size : vector or key in ``data``\n        Grouping variable that will produce points with different sizes.\n        Can be either categorical or numeric, although size mapping will\n        behave differently in latter case.\n    style : vector or key in ``data``\n        Grouping variable that will produce points with different markers.\n        Can have a numeric dtype but will always be treated as categorical.\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    sizes : list, dict, or tuple\n        An object that determines how sizes are chosen when ``size`` is used.\n        It can always be a list of size values or a dict mapping levels of the\n        ``size`` variable to sizes. When ``size``  is numeric, it can also be\n        a tuple specifying the minimum and maximum size to use such that other\n        values are normalized within this range.\n    size_order : list\n        Specified order for appearance of the ``size`` variable levels,\n        otherwise they are determined from the data. Not relevant when the\n        ``size`` variable is numeric.\n    size_norm : tuple or Normalize object\n        Normalization in data units for scaling plot objects when the\n        ``size`` variable is numeric.\n    markers : boolean, list, or dictionary\n        Object determining how to draw the markers for different levels of the\n        ``style`` variable. Setting to ``True`` will use default markers, or\n        you can pass a list of markers or a dictionary mapping levels of the\n        ``style`` variable to markers. Setting to ``False`` will draw\n        marker-less lines.  Markers are specified as in matplotlib.\n    style_order : list\n        Specified order for appearance of the ``style`` variable levels\n        otherwise they are determined from the data. Not relevant when the\n        ``style`` variable is numeric.\n    &#123;x,y&#125;_bins : lists or arrays or functions\n        *Currently non-functional.*\n    units : vector or key in ``data``\n        Grouping variable identifying sampling units. When used, a separate\n        line will be drawn for each unit with appropriate semantics, but no\n        legend entry will be added. Useful for showing distribution of\n        experimental replicates when exact identities are not needed.\n        *Currently non-functional.*\n    estimator : name of pandas method or callable or None\n        Method for aggregating across multiple observations of the ``y``\n        variable at the same ``x`` level. If ``None``, all observations will\n        be drawn.\n        *Currently non-functional.*\n    ci : int or &quot;sd&quot; or None\n        Size of the confidence interval to draw when aggregating with an\n        estimator. &quot;sd&quot; means to draw the standard deviation of the data.\n        Setting to ``None`` will skip bootstrapping.\n        *Currently non-functional.*\n    n_boot : int\n        Number of bootstraps to use for computing the confidence interval.\n        *Currently non-functional.*\n    alpha : float\n        Proportional opacity of the points.\n    &#123;x,y&#125;_jitter : booleans or floats\n        *Currently non-functional.*\n    legend : &quot;auto&quot;, &quot;brief&quot;, &quot;full&quot;, or False\n        How to draw the legend. If &quot;brief&quot;, numeric ``hue`` and ``size``\n        variables will be represented with a sample of evenly spaced values.\n        If &quot;full&quot;, every group will get an entry in the legend. If &quot;auto&quot;,\n        choose between brief or full representation based on number of levels.\n        If ``False``, no legend data is added and no legend is drawn.\n    ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n        internally.\n    kwargs : key, value mappings\n        Other keyword arguments are passed down to\n        :meth:`matplotlib.axes.Axes.scatter`.\n    \n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n        The matplotlib axes containing the plot.\n    \n    See Also\n    --------\n    lineplot : Plot data using lines.\n    stripplot : Plot a categorical scatter with jitter.\n    swarmplot : Plot a categorical scatter with non-overlapping points.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/scatterplot.rst\n</code></pre>\n<p>​</p>\n<p>（2）简单例子</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \nsns.set()\ntips = sns.load_dataset(&quot;tips&quot;)\n&quot;&quot;&quot;\n案例1：散点图\n&quot;&quot;&quot;\nsns.scatterplot( x=&quot;total_bill&quot;, y=&quot;tip&quot;,data=tips)\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_29_0.png\" alt=\"png\"></p>\n<p><strong>分组</strong></p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport seaborn as sns; \niris = sns.load_dataset(&quot;iris&quot;)\nax = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width,hue=iris.species, style=iris.species,legend=&quot;brief&quot;)\n#legend的取值为brief,full和False\n</code></pre>\n<p><img src=\"/img/output_31_0.png\" alt=\"png\"></p>\n<p><b>2、regplot函数</b></p>\n<p>regplot绘制散点图时，只需要指定自变量和因变量即可，regplot会自动完成散点图及线性回归拟合。</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n#help(sns.regplot)\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4acb12490&gt;\n</code></pre>\n<p><img src=\"/img/output_34_1.png\" alt=\"png\"></p>\n<p>如不显示拟合线，可将fit_reg设置为false</p>\n<p>regplot完整用法请参阅<br>\n<a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\">https://seaborn.pydata.org/generated/seaborn.regplot.html</a></p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n\n# use the function regplot to make a scatterplot\n#.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;])\nsns.regplot(x=df[&quot;sepal_length&quot;], y=df[&quot;sepal_width&quot;],fit_reg=False)\n# library &amp; dataset\n</code></pre>\n<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2b4aa4c9520&gt;\n</code></pre>\n<p><img src=\"/img/output_36_1.png\" alt=\"png\"></p>\n<p><b>3、lmplot函数画散点图</b></p>\n<p>lmplot同样是用于绘制散点图加拟合趋势线图，但lmplot支持引入第三维度进行对比，例如我们设置hue=“species”。</p>\n<p>(1)implot 简单散点图</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'hue' argument to provide a factor variable\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, hue='species',fit_reg=False,legend=False)\n \n    \n# Move the legend to an empty part of the plot\n#plt.legend(loc='best')\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_40_0.png\" alt=\"png\"></p>\n<p>（2）每组使用不同的标记（markers属性）</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# give a list to the marker argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, markers=[&quot;o&quot;, &quot;x&quot;, &quot;2&quot;])\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n\nplt.show()\n</code></pre>\n<p><img src=\"/img/output_42_0.png\" alt=\"png\"></p>\n<p>（3）使用不同的调色板(palette)</p>\n<pre><code class=\"language-python\">import seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Use the 'palette' argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=&quot;Set2&quot;)\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_44_0.png\" alt=\"png\"></p>\n<p>（4）控制每组的颜色</p>\n<pre><code class=\"language-python\"># library &amp; dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = sns.load_dataset('iris')\n \n# Provide a dictionary to the palette argument\nsns.lmplot( x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=df, fit_reg=False, hue='species', legend=False, palette=dict(setosa=&quot;#9b59b6&quot;, virginica=&quot;#3498db&quot;, versicolor=&quot;#95a5a6&quot;))\n \n# Move the legend to an empty part of the plot\nplt.legend(loc='lower right')\n \nplt.show()\n</code></pre>\n<p><img src=\"/img/output_46_0.png\" alt=\"png\"></p>\n<h1><center> 气泡图</center></h1>\n<p>散点图一般研究的是两个变量之间的关系，往往满足不了我们日常的需求。因此，气泡图的诞生就是为散点图增加变量，提供更加丰富的信息，点的大小或者颜色可以定义为第三个变量，因为，做出来的散点图类似气泡，也由此得名为气泡图。</p>\n<p>假设某个农产品的产量与温度和降雨量的关系如下表所示。<br></p>\n<pre><code class=\"language-python\">import pandas as pd\n#help(pd.read_csv)\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf_product\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>产量</th>\n      <th>温度</th>\n      <th>降雨量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1125</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1725</td>\n      <td>8</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2250</td>\n      <td>10</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2875</td>\n      <td>13</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2900</td>\n      <td>14</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3750</td>\n      <td>16</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4125</td>\n      <td>21</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>作出产量与温度的散点图</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem, production, s=200, c=colors)  # 画散点图，大小为 200\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('产量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_52_0.png\" alt=\"png\"></p>\n<p>若将散点大小的数据换为第三个变量的数值，则可以作出反映三个变量关系的气泡图。下面的代码和图形做出了一个气泡图。<br/><br>\n下图反映了产量与温度、降雨量的关系：温度数值在横坐标轴，降雨量数值在纵坐标轴，产量的大小用气泡的大小表示。</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\n\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 输入产量与温度数据\nproduction = df_product[u&quot;产量&quot;]\ntem = df_product[u&quot;温度&quot;]\nrainfall=df_product[u&quot;降雨量&quot;]\n\ncolors = np.random.rand(len(tem))  # 颜色数组\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n<p><img src=\"/img/output_54_0.png\" alt=\"png\"></p>\n<h3 id=\"课堂练习：使用seaborn画气泡图\">课堂练习：使用seaborn画气泡图</h3>\n<p>自行设计实验，使用seaborn包下的函数画气泡图，并解释图形含义</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ndf_product=pd.read_csv(&quot;Data\\product.csv&quot;,encoding=&quot;gbk&quot;)\ndf=sns.load_data_set('')\nplt.scatter(tem,rainfall, s=production, c=colors,alpha=0.6)  # 画散点图，大小为 降雨量\nplt.xlabel('温度')  # 横坐标轴标题\nplt.ylabel('降雨量')  # 纵坐标轴标题\nplt.show()\n\n</code></pre>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cknllvpj60001l0t977hteo77","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpjd000dl0t9aqve1zmv"},{"post_id":"cknllvpj70002l0t93yf25snr","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpjg000jl0t9bnei99oh"},{"post_id":"cknllvpje000gl0t9995uhumq","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpjj000pl0t97dv6fi5q"},{"post_id":"cknllvpj90005l0t9eg2t7xu1","category_id":"cknllvpjd000el0t94vmzcqng","_id":"cknllvpjk000tl0t9e1q3e2qf"},{"post_id":"cknllvpja0006l0t9byt49oqd","category_id":"cknllvpjd000el0t94vmzcqng","_id":"cknllvpjl000wl0t9blvthwqv"},{"post_id":"cknllvpjb0007l0t9fwvmckbb","category_id":"cknllvpjj000ql0t9a0ej2kxk","_id":"cknllvpjn0012l0t99lx1ghnh"},{"post_id":"cknllvpjm000zl0t980zz1su7","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpjp0018l0t94a6779ja"},{"post_id":"cknllvpjc000bl0t95osoe99j","category_id":"cknllvpjl000xl0t950z1g48h","_id":"cknllvpjp001bl0t9diw7441d"},{"post_id":"cknllvpjd000cl0t94jg72fm9","category_id":"cknllvpjj000ql0t9a0ej2kxk","_id":"cknllvpjt001hl0t9brrb8yyu"},{"post_id":"cknllvpjf000il0t99s7xhs04","category_id":"cknllvpjl000xl0t950z1g48h","_id":"cknllvpjv001ml0t97oag7muw"},{"post_id":"cknllvpju001jl0t95xbrfz31","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpjw001sl0t9hvqpffea"},{"post_id":"cknllvpjh000ml0t94s9pfq48","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpjx001wl0t9epkybqje"},{"post_id":"cknllvpju001ll0t938gxdoht","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpjy001zl0t90rh0ghtj"},{"post_id":"cknllvpjv001pl0t90sbac8dl","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpjz0023l0t9ftxe2dg1"},{"post_id":"cknllvpji000ol0t97zxj815u","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpk00026l0t9hdzaheiq"},{"post_id":"cknllvpjw001rl0t9gqvkh45l","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpk10029l0t92k7ya2w4"},{"post_id":"cknllvpjx001vl0t9c10e4syg","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpk2002cl0t90r82ex2o"},{"post_id":"cknllvpjk000sl0t9bohzgex9","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpk3002fl0t9228chh4u"},{"post_id":"cknllvpjl000vl0t97wlg0qyk","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpk4002il0t9ceqpfo3r"},{"post_id":"cknllvpk00025l0t99nvucxc9","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpk5002ml0t967lshqlx"},{"post_id":"cknllvpk10028l0t96ke502ap","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpk6002ql0t98khkh9du"},{"post_id":"cknllvpjm0011l0t9ac1z25pv","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpk7002ul0t99cfd44gn"},{"post_id":"cknllvpk2002bl0t9gt4cem9h","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpk8002yl0t9fm9p63hs"},{"post_id":"cknllvpjn0013l0t9dcfb4n3s","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpk90032l0t99m2lfwje"},{"post_id":"cknllvpk4002hl0t9cn3g00z4","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpka0036l0t91ptwhoon"},{"post_id":"cknllvpk5002ll0t97vx9e1u7","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkb003al0t98f6j21ke"},{"post_id":"cknllvpjo0017l0t98xzy0u6q","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkc003dl0t9d4qx4rn3"},{"post_id":"cknllvpk6002pl0t91v7fbiw1","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkd003hl0t9c2g24h6h"},{"post_id":"cknllvpjp0019l0t94qz425mz","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpke003kl0t98ufz4ygm"},{"post_id":"cknllvpk90031l0t9hfdofkvs","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkf003ol0t96pbh88n6"},{"post_id":"cknllvpjq001el0t93w5vfp5f","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkg003rl0t93avm3dp6"},{"post_id":"cknllvpjr001fl0t91lisds2l","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpkh003vl0t9b8s6hvcp"},{"post_id":"cknllvpkd003gl0t94h26hgdk","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpki003yl0t9ajpm7u9a"},{"post_id":"cknllvpjy001yl0t9crgqbdrx","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkj0042l0t91a41dgzc"},{"post_id":"cknllvpkd003jl0t94djea7cn","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkk0045l0t96gwyfjrt"},{"post_id":"cknllvpkf003nl0t96cwocymp","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkl0049l0t99x8reqwk"},{"post_id":"cknllvpjz0022l0t92xw7frxo","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpkm004cl0t97yuqdxk9"},{"post_id":"cknllvpkg003ql0t9h31x6c1u","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkn004gl0t9brysc919"},{"post_id":"cknllvpkh003ul0t9g2kw0bb5","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpko004jl0t9f23g28qi"},{"post_id":"cknllvpk3002el0t99x5pbyxp","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkp004nl0t9f6hc15dr"},{"post_id":"cknllvpki003xl0t9asf35drm","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkq004ql0t9fazxdeav"},{"post_id":"cknllvpkj0041l0t92h1i2n2z","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkr004ul0t9f93w0t7x"},{"post_id":"cknllvpk7002tl0t97bpf7gwd","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpks004xl0t9846eak08"},{"post_id":"cknllvpkk0044l0t9famt8rri","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkt0050l0t9h00p32u3"},{"post_id":"cknllvpkl0048l0t9erry0mu1","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpku0052l0t95fbw05un"},{"post_id":"cknllvpk8002xl0t91xe96n4x","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpkw0056l0t906509vkq"},{"post_id":"cknllvpkm004bl0t9bofs3nx7","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkx0058l0t9ajc5aiv1"},{"post_id":"cknllvpka0035l0t95slr89if","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpky005cl0t99o22fl1x"},{"post_id":"cknllvpko004il0t9ekzh3rx3","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpkz005fl0t9560bbb67"},{"post_id":"cknllvpkp004ml0t9anjk3n5e","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpl0005jl0t99j7i4xkn"},{"post_id":"cknllvpka0039l0t91tb3aa5o","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpl0005ml0t91p9lh8g9"},{"post_id":"cknllvpkq004pl0t9fk4u9mij","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpl1005pl0t9a4smamy3"},{"post_id":"cknllvpkr004tl0t9d254ek1e","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpl2005sl0t9d2ud4v1w"},{"post_id":"cknllvpkb003cl0t98jojf437","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvpl2005ul0t910y5b6ub"},{"post_id":"cknllvpks004wl0t98srfdt7j","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpl3005zl0t92iz9av3g"},{"post_id":"cknllvpkt004zl0t9fzd82gwh","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpl40061l0t9gvwk62n9"},{"post_id":"cknllvpkt0051l0t9cyafetl6","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpl50066l0t9gqa10p6z"},{"post_id":"cknllvpku0055l0t94m2i6ioc","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpl60068l0t9c4b1ckx3"},{"post_id":"cknllvpkw0057l0t9dft21pht","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpl7006dl0t9g77n6p30"},{"post_id":"cknllvpkx005bl0t90ezb92ve","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpl8006fl0t9bbhrgw54"},{"post_id":"cknllvpky005el0t97nxka69g","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpl9006jl0t960k55ipt"},{"post_id":"cknllvpkz005il0t9ed8ia0gm","category_id":"cknllvpjj000ql0t9a0ej2kxk","_id":"cknllvpl9006ml0t9fdc2911t"},{"post_id":"cknllvpl0005ll0t9feli2ora","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpla006pl0t9hg86fr9z"},{"post_id":"cknllvpl1005ol0t9batc4ph2","category_id":"cknllvpjl000xl0t950z1g48h","_id":"cknllvplc006rl0t97e8oa315"},{"post_id":"cknllvpl30060l0t9g171f34p","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpld006vl0t93sfd5qgg"},{"post_id":"cknllvpl1005rl0t96d5r2evn","category_id":"cknllvpl3005wl0t95wx0djm3","_id":"cknllvpld006yl0t9fd4afknx"},{"post_id":"cknllvpl60067l0t95fn4937q","category_id":"cknllvpjl000xl0t950z1g48h","_id":"cknllvple0073l0t94is49x6z"},{"post_id":"cknllvpl2005tl0t9327v6h8e","category_id":"cknllvpl3005wl0t95wx0djm3","_id":"cknllvplf0075l0t932c36v7w"},{"post_id":"cknllvpl6006bl0t92b7w27aq","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplg0077l0t9cm815kjn"},{"post_id":"cknllvpl7006el0t9dlelh4bo","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplh007cl0t9hqy23xvc"},{"post_id":"cknllvpl3005yl0t9edor7yus","category_id":"cknllvpl3005wl0t95wx0djm3","_id":"cknllvplh007fl0t92ct75zfe"},{"post_id":"cknllvpl8006hl0t99ulub7zb","category_id":"cknllvpjd000el0t94vmzcqng","_id":"cknllvpli007il0t92u2b4ln9"},{"post_id":"cknllvpl9006ll0t94bg5b9lo","category_id":"cknllvpjd000el0t94vmzcqng","_id":"cknllvplj007ll0t9hqbpbr1o"},{"post_id":"cknllvpl40064l0t904cj3r4v","category_id":"cknllvpl9006il0t94j24ayu2","_id":"cknllvplk007ol0t9526wctqt"},{"post_id":"cknllvpld006xl0t96kvg9wlx","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplk007rl0t97xeya5ek"},{"post_id":"cknllvpla006ol0t9ahzo64rq","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvpll007ul0t9btlfbonp"},{"post_id":"cknllvple0071l0t94am09o5a","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvplm007xl0t9b06hdz6u"},{"post_id":"cknllvple0074l0t93njvhcbd","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpln0081l0t9c4wp7rdn"},{"post_id":"cknllvplb006ql0t913xmd2i3","category_id":"cknllvple0072l0t9dkmqe3wb","_id":"cknllvplo0084l0t9095z5vkl"},{"post_id":"cknllvplf0076l0t92cxpf9qk","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvplp0088l0t90k90g4us"},{"post_id":"cknllvplg007bl0t9amlfdmj9","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvplp008bl0t97qd20i4r"},{"post_id":"cknllvplc006ul0t96mii96qi","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvplq008fl0t9ajjpfmzj"},{"post_id":"cknllvplh007el0t9bt990bz3","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplr008il0t95jio3fsg"},{"post_id":"cknllvpli007hl0t9cr0w7yby","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpls008ll0t9gdcwb559"},{"post_id":"cknllvplj007kl0t9fza3bo2d","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplt008ol0t93wyoeld0"},{"post_id":"cknllvplj007nl0t9cboddl4c","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplt008rl0t9avag9mjo"},{"post_id":"cknllvplk007ql0t93hjj593k","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplu008ul0t94w3cfcd3"},{"post_id":"cknllvpll007tl0t95sit8d1d","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvplv008yl0t9e8gg479d"},{"post_id":"cknllvplm007wl0t995kz61bd","category_id":"cknllvpl3005wl0t95wx0djm3","_id":"cknllvplw0092l0t93lf68euy"},{"post_id":"cknllvpln0080l0t980cva536","category_id":"cknllvpkc003el0t92huv2fz4","_id":"cknllvplx0095l0t9g9qfc6h1"},{"post_id":"cknllvpln0083l0t9eylug4ra","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvplx0098l0t915m8gwuo"},{"post_id":"cknllvplo0087l0t950igc1a7","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm0009bl0t94n66a964"},{"post_id":"cknllvplp008al0t9gg3k75lz","category_id":"cknllvpj80003l0t95tgc0d99","_id":"cknllvpm1009dl0t986dd87tw"},{"post_id":"cknllvplq008el0t976uq2sdy","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvpm2009fl0t983w7b82e"},{"post_id":"cknllvplr008hl0t9hx0keem9","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm2009hl0t96kc4d3q0"},{"post_id":"cknllvpls008kl0t9g8u1f1wa","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm3009jl0t9221se8iu"},{"post_id":"cknllvpls008nl0t9d8z8g6yt","category_id":"cknllvpjt001gl0t9hfg3hjvj","_id":"cknllvpm3009ll0t95wxmgqgr"},{"post_id":"cknllvplt008ql0t933lv7ppx","category_id":"cknllvpjb0008l0t9c49yd6hi","_id":"cknllvpm3009nl0t92x4sfhks"},{"post_id":"cknllvplv008xl0t91tqeg6jj","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm3009pl0t9fvpceftp"},{"post_id":"cknllvplv0091l0t964312api","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm4009rl0t9dj62cajk"},{"post_id":"cknllvplw0094l0t960id36qq","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm5009tl0t9h7389xus"},{"post_id":"cknllvplu008tl0t9781gfxxm","category_id":"cknllvplv0090l0t95wfc5nb9","_id":"cknllvpm5009vl0t92uxb1uk5"},{"post_id":"cknllvplx0097l0t932fs6blj","category_id":"cknllvple0072l0t9dkmqe3wb","_id":"cknllvpm5009xl0t9cjpc7y55"},{"post_id":"cknllvplz009al0t91swbfggr","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpm5009zl0t942poci73"},{"post_id":"cknllvpmp00d6l0t9hqjyckqb","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpms00dbl0t9hyfiejj7"},{"post_id":"cknllvpmq00d7l0t95q8q8hsq","category_id":"cknllvplc006tl0t93lwm7jrh","_id":"cknllvpmt00del0t90f8qayjt"},{"post_id":"cknllvpmr00d9l0t911d520yl","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpmu00dhl0t9dwgofrkt"},{"post_id":"cknllvpmr00dal0t985if20x6","category_id":"cknllvpke003ll0t96nxhg4bf","_id":"cknllvpmu00djl0t9dvgj29y9"},{"post_id":"cknllvpms00ddl0t936u7b07v","category_id":"cknllvplv0090l0t95wfc5nb9","_id":"cknllvpmu00dll0t9521c0xmv"}],"PostTag":[{"post_id":"cknllvpj60001l0t977hteo77","tag_id":"cknllvpj90004l0t9axd61nw4","_id":"cknllvpjc000al0t9gdb81ptd"},{"post_id":"cknllvpj70002l0t93yf25snr","tag_id":"cknllvpjc0009l0t9gc1lhj9v","_id":"cknllvpje000hl0t90wgqdklp"},{"post_id":"cknllvpj90005l0t9eg2t7xu1","tag_id":"cknllvpje000fl0t9f7xhe2ov","_id":"cknllvpji000nl0t97me809d4"},{"post_id":"cknllvpja0006l0t9byt49oqd","tag_id":"cknllvpje000fl0t9f7xhe2ov","_id":"cknllvpjk000ul0t9e81o35kf"},{"post_id":"cknllvpjb0007l0t9fwvmckbb","tag_id":"cknllvpjj000rl0t95jmv9qid","_id":"cknllvpjm0010l0t9h0jr9qoz"},{"post_id":"cknllvpjc000bl0t95osoe99j","tag_id":"cknllvpjl000yl0t96fe3awhd","_id":"cknllvpjo0016l0t9032i62dn"},{"post_id":"cknllvpjd000cl0t94jg72fm9","tag_id":"cknllvpjj000rl0t95jmv9qid","_id":"cknllvpjq001dl0t94lbsauax"},{"post_id":"cknllvpje000gl0t9995uhumq","tag_id":"cknllvpjp001cl0t9hf4cbsjl","_id":"cknllvpju001kl0t990b06y34"},{"post_id":"cknllvpjf000il0t99s7xhs04","tag_id":"cknllvpjt001il0t93ise27hz","_id":"cknllvpjw001ql0t92vuzfk8i"},{"post_id":"cknllvpjh000ml0t94s9pfq48","tag_id":"cknllvpjv001ol0t9fryxbd2l","_id":"cknllvpjy001xl0t900p4d1y7"},{"post_id":"cknllvpji000ol0t97zxj815u","tag_id":"cknllvpjv001ol0t9fryxbd2l","_id":"cknllvpk00024l0t9cq9s099i"},{"post_id":"cknllvpjk000sl0t9bohzgex9","tag_id":"cknllvpjz0021l0t96b9n9p4x","_id":"cknllvpk4002jl0t97bq1cnwp"},{"post_id":"cknllvpjk000sl0t9bohzgex9","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvpk5002nl0t995ejhkr2"},{"post_id":"cknllvpk5002ll0t97vx9e1u7","tag_id":"cknllvpjv001ol0t9fryxbd2l","_id":"cknllvpk7002sl0t9b8rl1s17"},{"post_id":"cknllvpk6002pl0t91v7fbiw1","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvpk7002wl0t9dasu0u0q"},{"post_id":"cknllvpjl000vl0t97wlg0qyk","tag_id":"cknllvpjz0021l0t96b9n9p4x","_id":"cknllvpk90030l0t99w3p8zid"},{"post_id":"cknllvpjl000vl0t97wlg0qyk","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvpk90034l0t9fzvl11ra"},{"post_id":"cknllvpjm000zl0t980zz1su7","tag_id":"cknllvpk7002vl0t9fgqo1394","_id":"cknllvpka0038l0t9cqlde2vq"},{"post_id":"cknllvpjm0011l0t9ac1z25pv","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkd003fl0t91s865ary"},{"post_id":"cknllvpjn0013l0t9dcfb4n3s","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpke003ml0t9d6t6gluq"},{"post_id":"cknllvpjo0017l0t98xzy0u6q","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkg003sl0t9628xe0if"},{"post_id":"cknllvpjp0019l0t94qz425mz","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkj003zl0t94l4z7mko"},{"post_id":"cknllvpjq001el0t93w5vfp5f","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkl0046l0t9hetj4p8y"},{"post_id":"cknllvpjr001fl0t91lisds2l","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkn004dl0t9fooqbuv5"},{"post_id":"cknllvpju001jl0t95xbrfz31","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkp004kl0t9bpg84vly"},{"post_id":"cknllvpju001ll0t938gxdoht","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkr004rl0t902n5ehpz"},{"post_id":"cknllvpjv001pl0t90sbac8dl","tag_id":"cknllvpk90033l0t9hx6p1m30","_id":"cknllvpkt004yl0t9ebv60sv1"},{"post_id":"cknllvpjw001rl0t9gqvkh45l","tag_id":"cknllvpks004vl0t9fbaw3er3","_id":"cknllvpku0054l0t92tlvb5z8"},{"post_id":"cknllvpku0055l0t94m2i6ioc","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvpkx005al0t9h6pjgioq"},{"post_id":"cknllvpjx001vl0t9c10e4syg","tag_id":"cknllvpku0053l0t91fp0fkis","_id":"cknllvpky005dl0t9hyqv9nz1"},{"post_id":"cknllvpkx005bl0t90ezb92ve","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvpkz005hl0t95hst5fo0"},{"post_id":"cknllvpjy001yl0t9crgqbdrx","tag_id":"cknllvpkx0059l0t9d9ek7y4s","_id":"cknllvpl0005kl0t99frv7r4d"},{"post_id":"cknllvpjz0022l0t92xw7frxo","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvpl1005ql0t9b60wg865"},{"post_id":"cknllvpk00025l0t99nvucxc9","tag_id":"cknllvpl0005nl0t9h5h59bv5","_id":"cknllvpl3005xl0t9g8gxbzhx"},{"post_id":"cknllvpk10028l0t96ke502ap","tag_id":"cknllvpl0005nl0t9h5h59bv5","_id":"cknllvpl40063l0t9f111ajt8"},{"post_id":"cknllvpk2002bl0t9gt4cem9h","tag_id":"cknllvpl0005nl0t9h5h59bv5","_id":"cknllvpl6006al0t9home0kvs"},{"post_id":"cknllvpk3002el0t99x5pbyxp","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpl9006kl0t9evrv83g5"},{"post_id":"cknllvpk7002tl0t97bpf7gwd","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpld006wl0t9h91kh6q1"},{"post_id":"cknllvpk7002tl0t97bpf7gwd","tag_id":"cknllvpl9006nl0t9eo39evbm","_id":"cknllvpld006zl0t9cn60cske"},{"post_id":"cknllvpk8002xl0t91xe96n4x","tag_id":"cknllvplc006sl0t97euoa3rl","_id":"cknllvplg007al0t92v24bflb"},{"post_id":"cknllvpk8002xl0t91xe96n4x","tag_id":"cknllvpld0070l0t91ofx3jts","_id":"cknllvplh007dl0t932e0b944"},{"post_id":"cknllvpk90031l0t9hfdofkvs","tag_id":"cknllvplg0078l0t933u9akmv","_id":"cknllvpli007jl0t91a1dewuu"},{"post_id":"cknllvpka0035l0t95slr89if","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplk007pl0t9d35ecu63"},{"post_id":"cknllvpka0039l0t91tb3aa5o","tag_id":"cknllvpkx0059l0t9d9ek7y4s","_id":"cknllvpll007vl0t9a2t68xgu"},{"post_id":"cknllvpll007tl0t95sit8d1d","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvplm007zl0t9hh4sal2r"},{"post_id":"cknllvpkb003cl0t98jojf437","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpln0082l0t90zs06vcc"},{"post_id":"cknllvpln0080l0t980cva536","tag_id":"cknllvpkx0059l0t9d9ek7y4s","_id":"cknllvplo0086l0t953eh42qk"},{"post_id":"cknllvpkd003gl0t94h26hgdk","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplp0089l0t999122wql"},{"post_id":"cknllvplo0087l0t950igc1a7","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvplq008dl0t9gbnk4j72"},{"post_id":"cknllvpkd003jl0t94djea7cn","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplr008gl0t92dcr1k9c"},{"post_id":"cknllvpkf003nl0t96cwocymp","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpls008ml0t9brapfm6m"},{"post_id":"cknllvpkg003ql0t9h31x6c1u","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplt008sl0t9a3fx9fr5"},{"post_id":"cknllvpls008nl0t9d8z8g6yt","tag_id":"cknllvpk1002al0t98m5iew0i","_id":"cknllvplu008vl0t983fh3psu"},{"post_id":"cknllvpkh003ul0t9g2kw0bb5","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplv008zl0t90pc3fbpy"},{"post_id":"cknllvpki003xl0t9asf35drm","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvplx0096l0t972da2e8s"},{"post_id":"cknllvpkj0041l0t92h1i2n2z","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm0009cl0t9aa8eddgo"},{"post_id":"cknllvpkk0044l0t9famt8rri","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm2009gl0t9b0bg2q58"},{"post_id":"cknllvpkl0048l0t9erry0mu1","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm3009kl0t90q3c79gb"},{"post_id":"cknllvpkm004bl0t9bofs3nx7","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm3009ol0t9bwzs7hn5"},{"post_id":"cknllvpkn004fl0t9h9t3bljr","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm4009sl0t98g7o5htj"},{"post_id":"cknllvpko004il0t9ekzh3rx3","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm5009wl0t9fnnx5mn8"},{"post_id":"cknllvpkp004ml0t9anjk3n5e","tag_id":"cknllvpl60069l0t96hjm8ju2","_id":"cknllvpm500a0l0t99ao2cjl1"},{"post_id":"cknllvpkq004pl0t9fk4u9mij","tag_id":"cknllvpm5009yl0t9dj4f2iod","_id":"cknllvpm600a3l0t9alo00xo7"},{"post_id":"cknllvpkq004pl0t9fk4u9mij","tag_id":"cknllvpm600a1l0t9hoyn08du","_id":"cknllvpm600a4l0t977ci9n7j"},{"post_id":"cknllvpkr004tl0t9d254ek1e","tag_id":"cknllvpm5009yl0t9dj4f2iod","_id":"cknllvpm700a6l0t99v7ggmn8"},{"post_id":"cknllvpks004wl0t98srfdt7j","tag_id":"cknllvpm5009yl0t9dj4f2iod","_id":"cknllvpm700a8l0t94p2475xx"},{"post_id":"cknllvpkt004zl0t9fzd82gwh","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvpm700aal0t9dprsbl0z"},{"post_id":"cknllvpkt0051l0t9cyafetl6","tag_id":"cknllvpm700a9l0t9bx53buqg","_id":"cknllvpm800acl0t99l4s1ggw"},{"post_id":"cknllvpkw0057l0t9dft21pht","tag_id":"cknllvpm700abl0t9c0qhfpzt","_id":"cknllvpm800ael0t9djqsajt5"},{"post_id":"cknllvpky005el0t97nxka69g","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvpm800agl0t9c1de6598"},{"post_id":"cknllvpkz005il0t9ed8ia0gm","tag_id":"cknllvpm800afl0t94espeh89","_id":"cknllvpm900ajl0t96anx10gc"},{"post_id":"cknllvpkz005il0t9ed8ia0gm","tag_id":"cknllvpm800ahl0t97mkce6gz","_id":"cknllvpm900akl0t94na67eh4"},{"post_id":"cknllvpl0005ll0t9feli2ora","tag_id":"cknllvpm800ail0t950svfzbz","_id":"cknllvpm900aol0t9cyg3cgtb"},{"post_id":"cknllvpl0005ll0t9feli2ora","tag_id":"cknllvpm900all0t91mkde6ra","_id":"cknllvpm900apl0t99wvo6qk3"},{"post_id":"cknllvpl0005ll0t9feli2ora","tag_id":"cknllvpm900aml0t9acrl5r42","_id":"cknllvpm900arl0t9acypgtms"},{"post_id":"cknllvpl1005ol0t9batc4ph2","tag_id":"cknllvpm900anl0t9aeyw0l49","_id":"cknllvpma00asl0t909qweayk"},{"post_id":"cknllvpl1005rl0t96d5r2evn","tag_id":"cknllvpm900aql0t9cft83y1f","_id":"cknllvpma00aul0t931m2hkyq"},{"post_id":"cknllvpl2005tl0t9327v6h8e","tag_id":"cknllvpm900aql0t9cft83y1f","_id":"cknllvpma00awl0t98nie4vhi"},{"post_id":"cknllvpl3005yl0t9edor7yus","tag_id":"cknllvpm900aql0t9cft83y1f","_id":"cknllvpmb00ayl0t90iqi9fjt"},{"post_id":"cknllvpl30060l0t9g171f34p","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmb00b0l0t9f4sza2cv"},{"post_id":"cknllvpl40064l0t904cj3r4v","tag_id":"cknllvpmb00azl0t9fdvaeoc7","_id":"cknllvpmb00b2l0t9493ta6tg"},{"post_id":"cknllvpl60067l0t95fn4937q","tag_id":"cknllvpmb00b1l0t9fdnogfv3","_id":"cknllvpmb00b4l0t9csrjhya9"},{"post_id":"cknllvpl6006bl0t92b7w27aq","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmc00b6l0t9415gf589"},{"post_id":"cknllvpl7006el0t9dlelh4bo","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmc00b8l0t9h61d5c9o"},{"post_id":"cknllvpl8006hl0t99ulub7zb","tag_id":"cknllvpmc00b7l0t9aeua40ym","_id":"cknllvpmc00bal0t96da35fm7"},{"post_id":"cknllvpl9006ll0t94bg5b9lo","tag_id":"cknllvpmc00b9l0t976gdevh0","_id":"cknllvpmd00bcl0t9g5sjelb7"},{"post_id":"cknllvpla006ol0t9ahzo64rq","tag_id":"cknllvpmc00bbl0t98ptwanda","_id":"cknllvpmd00bfl0t956b8ejqp"},{"post_id":"cknllvpla006ol0t9ahzo64rq","tag_id":"cknllvpmd00bdl0t9d7px5ny6","_id":"cknllvpmd00bgl0t92t2d9eqe"},{"post_id":"cknllvplb006ql0t913xmd2i3","tag_id":"cknllvpmd00bel0t9hjz40r5w","_id":"cknllvpmd00bil0t94c3qemco"},{"post_id":"cknllvplc006ul0t96mii96qi","tag_id":"cknllvpmd00bhl0t9484i5o1r","_id":"cknllvpmd00bkl0t9g4br4axp"},{"post_id":"cknllvpld006xl0t96kvg9wlx","tag_id":"cknllvpmd00bjl0t98hw227wd","_id":"cknllvpme00bml0t9g8w670va"},{"post_id":"cknllvple0071l0t94am09o5a","tag_id":"cknllvpmc00bbl0t98ptwanda","_id":"cknllvpme00bol0t93w0b94j6"},{"post_id":"cknllvple0074l0t93njvhcbd","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmf00bql0t97s3t3k3s"},{"post_id":"cknllvplf0076l0t92cxpf9qk","tag_id":"cknllvpme00bpl0t93mrbf6ch","_id":"cknllvpmf00btl0t9gpj45oni"},{"post_id":"cknllvplf0076l0t92cxpf9qk","tag_id":"cknllvpmf00brl0t97161a43q","_id":"cknllvpmf00bul0t97a0w69lz"},{"post_id":"cknllvplg007bl0t9amlfdmj9","tag_id":"cknllvpmc00bbl0t98ptwanda","_id":"cknllvpmg00bxl0t99yicde1f"},{"post_id":"cknllvplg007bl0t9amlfdmj9","tag_id":"cknllvpmd00bdl0t9d7px5ny6","_id":"cknllvpmg00byl0t9gwmy3dgc"},{"post_id":"cknllvplh007el0t9bt990bz3","tag_id":"cknllvpmg00bwl0t93flzhvyi","_id":"cknllvpmg00c0l0t97dssg6kg"},{"post_id":"cknllvpli007hl0t9cr0w7yby","tag_id":"cknllvpmg00bwl0t93flzhvyi","_id":"cknllvpmh00c2l0t9dxd99wwb"},{"post_id":"cknllvplj007kl0t9fza3bo2d","tag_id":"cknllvpmg00bwl0t93flzhvyi","_id":"cknllvpmh00c4l0t9hz8rg4dm"},{"post_id":"cknllvplj007nl0t9cboddl4c","tag_id":"cknllvpmg00bwl0t93flzhvyi","_id":"cknllvpmh00c6l0t9hwq7b3nr"},{"post_id":"cknllvplk007ql0t93hjj593k","tag_id":"cknllvpmh00c5l0t91mz37b94","_id":"cknllvpmi00c8l0t9507p1b2s"},{"post_id":"cknllvplm007wl0t995kz61bd","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmi00cbl0t9clmk7bi1"},{"post_id":"cknllvplm007wl0t995kz61bd","tag_id":"cknllvpm900aql0t9cft83y1f","_id":"cknllvpmi00ccl0t9aqb7h50x"},{"post_id":"cknllvpln0083l0t9eylug4ra","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvpmj00cel0t92xm51kwd"},{"post_id":"cknllvpln0083l0t9eylug4ra","tag_id":"cknllvpmi00cal0t94rxy9e19","_id":"cknllvpmj00cfl0t9g9fog85t"},{"post_id":"cknllvplp008al0t9gg3k75lz","tag_id":"cknllvpmi00cdl0t991tg1sif","_id":"cknllvpmj00chl0t9gybs3k3j"},{"post_id":"cknllvplq008el0t976uq2sdy","tag_id":"cknllvpmc00bbl0t98ptwanda","_id":"cknllvpmk00ckl0t9eun48041"},{"post_id":"cknllvplq008el0t976uq2sdy","tag_id":"cknllvpmj00cil0t9ggg1h3l7","_id":"cknllvpmk00cll0t9azrigk9u"},{"post_id":"cknllvplr008hl0t9hx0keem9","tag_id":"cknllvpkz005gl0t94bi2c2kx","_id":"cknllvpmk00cnl0t9e4237f88"},{"post_id":"cknllvplr008hl0t9hx0keem9","tag_id":"cknllvpmd00bjl0t98hw227wd","_id":"cknllvpmk00col0t96ayafiaa"},{"post_id":"cknllvpls008kl0t9g8u1f1wa","tag_id":"cknllvpmk00cml0t9ginx6spw","_id":"cknllvpml00cql0t92z06d8ji"},{"post_id":"cknllvplt008ql0t933lv7ppx","tag_id":"cknllvpmk00cpl0t91m29363d","_id":"cknllvpml00ctl0t9aqq96885"},{"post_id":"cknllvplt008ql0t933lv7ppx","tag_id":"cknllvpml00crl0t92l5pdnrt","_id":"cknllvpml00cul0t94cblfyoy"},{"post_id":"cknllvplu008tl0t9781gfxxm","tag_id":"cknllvpml00csl0t9b36o8hbk","_id":"cknllvpml00cwl0t93q5h4erw"},{"post_id":"cknllvplv008xl0t91tqeg6jj","tag_id":"cknllvpml00cvl0t91w9e5x97","_id":"cknllvpmm00cyl0t9anuq5rqg"},{"post_id":"cknllvplv0091l0t964312api","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmm00d0l0t99jwkbwis"},{"post_id":"cknllvplw0094l0t960id36qq","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmn00d2l0t94dvo3bqi"},{"post_id":"cknllvplx0097l0t932fs6blj","tag_id":"cknllvpmd00bel0t9hjz40r5w","_id":"cknllvpmn00d4l0t91nrc1dxi"},{"post_id":"cknllvplz009al0t91swbfggr","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmn00d5l0t982qs6fsz"},{"post_id":"cknllvpmr00d9l0t911d520yl","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpms00dcl0t954p0fajf"},{"post_id":"cknllvpmr00dal0t985if20x6","tag_id":"cknllvpma00axl0t9fyx53v7j","_id":"cknllvpmt00dgl0t952sk0p8u"},{"post_id":"cknllvpmp00d6l0t9hqjyckqb","tag_id":"cknllvpmq00d8l0t9b8q25efn","_id":"cknllvpmu00dil0t92xjbc0oi"},{"post_id":"cknllvpms00ddl0t936u7b07v","tag_id":"cknllvpml00csl0t9b36o8hbk","_id":"cknllvpmu00dkl0t998s86wyz"},{"post_id":"cknllvpmq00d7l0t95q8q8hsq","tag_id":"cknllvpmt00dfl0t9dj3bbvyp","_id":"cknllvpmu00dml0t9982be31c"},{"post_id":"cknllvpmq00d7l0t95q8q8hsq","tag_id":"cknllvpmc00bbl0t98ptwanda","_id":"cknllvpmu00dnl0t9bx84hrqp"}],"Tag":[{"name":"java基础","_id":"cknllvpj90004l0t9axd61nw4"},{"name":"DNS","_id":"cknllvpjc0009l0t9gc1lhj9v"},{"name":"Disruptor","_id":"cknllvpje000fl0t9f7xhe2ov"},{"name":"es","_id":"cknllvpjj000rl0t95jmv9qid"},{"name":"docker","_id":"cknllvpjl000yl0t96fe3awhd"},{"name":"GET/POST","_id":"cknllvpjp001cl0t9hf4cbsjl"},{"name":"git","_id":"cknllvpjt001il0t93ise27hz"},{"name":"hadoop","_id":"cknllvpjv001ol0t9fryxbd2l"},{"name":"HDFS","_id":"cknllvpjz0021l0t96b9n9p4x"},{"name":"HADOOP","_id":"cknllvpk1002al0t98m5iew0i"},{"name":"哈希表","_id":"cknllvpk7002vl0t9fgqo1394"},{"name":"hive","_id":"cknllvpk90033l0t9hx6p1m30"},{"name":"httpclient","_id":"cknllvpks004vl0t9fbaw3er3"},{"name":"https","_id":"cknllvpku0053l0t91fp0fkis"},{"name":"spring","_id":"cknllvpkx0059l0t9d9ek7y4s"},{"name":"java","_id":"cknllvpkz005gl0t94bi2c2kx"},{"name":"JVM","_id":"cknllvpl0005nl0t9h5h59bv5"},{"name":"SpringCloud","_id":"cknllvpl60069l0t96hjm8ju2"},{"name":"nacos-config","_id":"cknllvpl9006nl0t9eo39evbm"},{"name":"并发","_id":"cknllvplc006sl0t97euoa3rl"},{"name":"线程安全","_id":"cknllvpld0070l0t91ofx3jts"},{"name":"Spark","_id":"cknllvplg0078l0t933u9akmv"},{"name":"TCP/IP","_id":"cknllvpm5009yl0t9dj4f2iod"},{"name":"UDP","_id":"cknllvpm600a1l0t9hoyn08du"},{"name":"tomcat","_id":"cknllvpm700a9l0t9bx53buqg"},{"name":"JDK1.8新特性","_id":"cknllvpm700abl0t9c0qhfpzt"},{"name":"k8s","_id":"cknllvpm800afl0t94espeh89"},{"name":"elk","_id":"cknllvpm800ahl0t97mkce6gz"},{"name":"String","_id":"cknllvpm800ail0t950svfzbz"},{"name":"StringBuilder","_id":"cknllvpm900all0t91mkde6ra"},{"name":"StringBuffer","_id":"cknllvpm900aml0t9acrl5r42"},{"name":"maven","_id":"cknllvpm900anl0t9aeyw0l49"},{"name":"mysql","_id":"cknllvpm900aql0t9cft83y1f"},{"name":"锁","_id":"cknllvpma00axl0t9fyx53v7j"},{"name":"伪共享","_id":"cknllvpmb00azl0t9fdvaeoc7"},{"name":"信息系统项目管理师","_id":"cknllvpmb00b1l0t9fdnogfv3"},{"name":"CAP","_id":"cknllvpmc00b7l0t9aeua40ym"},{"name":"分布式","_id":"cknllvpmc00b9l0t976gdevh0"},{"name":"可信","_id":"cknllvpmc00bbl0t98ptwanda"},{"name":"密码学","_id":"cknllvpmd00bdl0t9d7px5ny6"},{"name":"设计模式","_id":"cknllvpmd00bel0t9hjz40r5w"},{"name":"可信计算","_id":"cknllvpmd00bhl0t9484i5o1r"},{"name":"多线程","_id":"cknllvpmd00bjl0t98hw227wd"},{"name":"可靠","_id":"cknllvpme00bpl0t93mrbf6ch"},{"name":"容错","_id":"cknllvpmf00brl0t97161a43q"},{"name":"javaagent","_id":"cknllvpmg00bwl0t93flzhvyi"},{"name":"内存模型","_id":"cknllvpmh00c5l0t91mz37b94"},{"name":"数据结构","_id":"cknllvpmi00cal0t94rxy9e19"},{"name":"排序","_id":"cknllvpmi00cdl0t991tg1sif"},{"name":"加密算法","_id":"cknllvpmj00cil0t9ggg1h3l7"},{"name":"文件上传","_id":"cknllvpmk00cml0t9ginx6spw"},{"name":"IO","_id":"cknllvpmk00cpl0t91m29363d"},{"name":"阻塞与非阻塞","_id":"cknllvpml00crl0t92l5pdnrt"},{"name":"python","_id":"cknllvpml00csl0t9b36o8hbk"},{"name":"线程","_id":"cknllvpml00cvl0t91w9e5x97"},{"name":"运算符","_id":"cknllvpmq00d8l0t9b8q25efn"},{"name":"网络安全","_id":"cknllvpmt00dfl0t9dj3bbvyp"}]}}
{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/Git工作流程.png","path":"img/Git工作流程.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP服务器接收请求.png","path":"img/TCPIP服务器接收请求.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP用户发送请求.png","path":"img/TCPIP用户发送请求.png","modified":1,"renderable":0},{"_id":"source/img/UDPTCPcompare.png","path":"img/UDPTCPcompare.png","modified":1,"renderable":0},{"_id":"source/img/UDPHeader.png","path":"img/UDPHeader.png","modified":1,"renderable":0},{"_id":"source/img/clip_image002.png","path":"img/clip_image002.png","modified":1,"renderable":0},{"_id":"source/img/es2.png","path":"img/es2.png","modified":1,"renderable":0},{"_id":"source/img/dataSource-behaviour-relative.png","path":"img/dataSource-behaviour-relative.png","modified":1,"renderable":0},{"_id":"source/img/es1.png","path":"img/es1.png","modified":1,"renderable":0},{"_id":"source/img/floattype.png","path":"img/floattype.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合2.png","path":"img/hive-聚合2.png","modified":1,"renderable":0},{"_id":"source/img/javaagent1.png","path":"img/javaagent1.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储2.png","path":"img/java对象存储2.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储.png","path":"img/java对象存储.png","modified":1,"renderable":0},{"_id":"source/img/java对象存储3.png","path":"img/java对象存储3.png","modified":1,"renderable":0},{"_id":"source/img/maven配置.png","path":"img/maven配置.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序1.png","path":"img/mysql排序1.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序2.png","path":"img/mysql排序2.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序5.png","path":"img/mysql排序5.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序4.png","path":"img/mysql排序4.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序6.png","path":"img/mysql排序6.png","modified":1,"renderable":0},{"_id":"source/img/nacos-producer.png","path":"img/nacos-producer.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud1.png","path":"img/nacos-springCloud1.png","modified":1,"renderable":0},{"_id":"source/img/nacos-springCloud2.png","path":"img/nacos-springCloud2.png","modified":1,"renderable":0},{"_id":"source/img/nacos1.png","path":"img/nacos1.png","modified":1,"renderable":0},{"_id":"source/img/pasted-0.png","path":"img/pasted-0.png","modified":1,"renderable":0},{"_id":"source/img/secondaryNameNode.jpg","path":"img/secondaryNameNode.jpg","modified":1,"renderable":0},{"_id":"source/img/user-log.png","path":"img/user-log.png","modified":1,"renderable":0},{"_id":"source/img/wordcount.png","path":"img/wordcount.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议1.png","path":"img/三次握手协议1.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议2.png","path":"img/三次握手协议2.png","modified":1,"renderable":0},{"_id":"source/img/使用协议进行通讯.png","path":"img/使用协议进行通讯.png","modified":1,"renderable":0},{"_id":"source/img/信任链.png","path":"img/信任链.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥6.png","path":"img/公钥私钥6.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥8.png","path":"img/公钥私钥8.png","modified":1,"renderable":0},{"_id":"source/img/加密算法.png","path":"img/加密算法.png","modified":1,"renderable":0},{"_id":"source/img/可信根.png","path":"img/可信根.png","modified":1,"renderable":0},{"_id":"source/img/对称加密算法.png","path":"img/对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/数据库分布式ID生成.png","path":"img/数据库分布式ID生成.png","modified":1,"renderable":0},{"_id":"source/img/线程相关1.jpg","path":"img/线程相关1.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关3.jpg","path":"img/线程相关3.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关2.jpg","path":"img/线程相关2.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关4.jpg","path":"img/线程相关4.jpg","modified":1,"renderable":0},{"_id":"source/img/线程相关5.jpg","path":"img/线程相关5.jpg","modified":1,"renderable":0},{"_id":"source/img/锁的创建.png","path":"img/锁的创建.png","modified":1,"renderable":0},{"_id":"source/img/锁的创建2.png","path":"img/锁的创建2.png","modified":1,"renderable":0},{"_id":"source/img/阻塞IO.png","path":"img/阻塞IO.png","modified":1,"renderable":0},{"_id":"source/img/雪花算法.png","path":"img/雪花算法.png","modified":1,"renderable":0},{"_id":"source/img/GETPOST.png","path":"img/GETPOST.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS2.png","path":"img/HTTPS2.png","modified":1,"renderable":0},{"_id":"source/img/HDFS-liucheng.png","path":"img/HDFS-liucheng.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS3.png","path":"img/HTTPS3.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS4.png","path":"img/HTTPS4.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS5.png","path":"img/HTTPS5.png","modified":1,"renderable":0},{"_id":"source/img/IO复用select模型.png","path":"img/IO复用select模型.png","modified":1,"renderable":0},{"_id":"source/img/Spark.png","path":"img/Spark.png","modified":1,"renderable":0},{"_id":"source/img/TCPIP模型.png","path":"img/TCPIP模型.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程.png","path":"img/TCP协议通讯过程.png","modified":1,"renderable":0},{"_id":"source/img/ThreadLocal内部存储.png","path":"img/ThreadLocal内部存储.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime.png","path":"img/agent-costtime.png","modified":1,"renderable":0},{"_id":"source/img/agent-costtime2.png","path":"img/agent-costtime2.png","modified":1,"renderable":0},{"_id":"source/img/chartype.png","path":"img/chartype.png","modified":1,"renderable":0},{"_id":"source/img/data-collect-analysis.png","path":"img/data-collect-analysis.png","modified":1,"renderable":0},{"_id":"source/img/clip_image004.png","path":"img/clip_image004.png","modified":1,"renderable":0},{"_id":"source/img/data-collect.png","path":"img/data-collect.png","modified":1,"renderable":0},{"_id":"source/img/hdfs-read-file.png","path":"img/hdfs-read-file.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构1.png","path":"img/hive数据结构1.png","modified":1,"renderable":0},{"_id":"source/img/hive数据结构.png","path":"img/hive数据结构.png","modified":1,"renderable":0},{"_id":"source/img/hive文本文件数据编码.png","path":"img/hive文本文件数据编码.png","modified":1,"renderable":0},{"_id":"source/img/image-20200117113506023.png","path":"img/image-20200117113506023.png","modified":1,"renderable":0},{"_id":"source/img/inttype.png","path":"img/inttype.png","modified":1,"renderable":0},{"_id":"source/img/mysql时间存储.png","path":"img/mysql时间存储.png","modified":1,"renderable":0},{"_id":"source/img/mysql的ip存储.png","path":"img/mysql的ip存储.png","modified":1,"renderable":0},{"_id":"source/img/typetrans.png","path":"img/typetrans.png","modified":1,"renderable":0},{"_id":"source/img/user-behaviour.png","path":"img/user-behaviour.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-split.png","path":"img/wordcount-split.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-map.png","path":"img/wordcount-map.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥10.png","path":"img/公钥私钥10.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥11.png","path":"img/公钥私钥11.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥1.png","path":"img/公钥私钥1.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥13.png","path":"img/公钥私钥13.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥2.png","path":"img/公钥私钥2.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥3.png","path":"img/公钥私钥3.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥4.png","path":"img/公钥私钥4.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥5.png","path":"img/公钥私钥5.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥7.png","path":"img/公钥私钥7.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥9.png","path":"img/公钥私钥9.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩2.png","path":"img/指针压缩2.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩3.png","path":"img/指针压缩3.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩4.png","path":"img/指针压缩4.png","modified":1,"renderable":0},{"_id":"source/img/非对称加密算法.png","path":"img/非对称加密算法.png","modified":1,"renderable":0},{"_id":"source/img/HTTPS1.png","path":"img/HTTPS1.png","modified":1,"renderable":0},{"_id":"source/img/Hive-运算2.png","path":"img/Hive-运算2.png","modified":1,"renderable":0},{"_id":"source/img/SpringBean3.png","path":"img/SpringBean3.png","modified":1,"renderable":0},{"_id":"source/img/hive-算数运算符.png","path":"img/hive-算数运算符.png","modified":1,"renderable":0},{"_id":"source/img/hive-聚合3.png","path":"img/hive-聚合3.png","modified":1,"renderable":0},{"_id":"source/img/mysql排序3.png","path":"img/mysql排序3.png","modified":1,"renderable":0},{"_id":"source/img/simpleDateFormat-alibaba.png","path":"img/simpleDateFormat-alibaba.png","modified":1,"renderable":0},{"_id":"source/img/spark+hdfs.png","path":"img/spark+hdfs.png","modified":1,"renderable":0},{"_id":"source/img/可信在云平台的基础架构.png","path":"img/可信在云平台的基础架构.png","modified":1,"renderable":0},{"_id":"source/img/非阻塞IO.png","path":"img/非阻塞IO.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/css/tomorrow-night.scss","path":"css/tomorrow-night.scss","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/mobile.styl","path":"css/mobile.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/school-book.png","path":"img/school-book.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/brown-papersq.png","path":"img/brown-papersq.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/iconfont.js","path":"js/iconfont.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/jquery.autocomplete.min.js","path":"js/jquery.autocomplete.min.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"source/img/HTTPS6.png","path":"img/HTTPS6.png","modified":1,"renderable":0},{"_id":"source/img/TCP协议通讯过程2.png","path":"img/TCP协议通讯过程2.png","modified":1,"renderable":0},{"_id":"source/img/Yarn.png","path":"img/Yarn.png","modified":1,"renderable":0},{"_id":"source/img/公钥私钥12.png","path":"img/公钥私钥12.png","modified":1,"renderable":0},{"_id":"source/img/指针压缩1.png","path":"img/指针压缩1.png","modified":1,"renderable":0},{"_id":"source/img/椭圆曲线算法的基本原理.png","path":"img/椭圆曲线算法的基本原理.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/css/gitalk.css","path":"css/gitalk.css","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/avatar.jpg","path":"img/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","path":"img/article-list-background.jpeg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/alipay.jpg","path":"img/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","path":"js/jquery.pjax.js","modified":1,"renderable":1},{"_id":"source/img/hdfs-write-file.png","path":"img/hdfs-write-file.png","modified":1,"renderable":0},{"_id":"source/img/hive-partition.png","path":"img/hive-partition.png","modified":1,"renderable":0},{"_id":"source/img/hdfs.png","path":"img/hdfs.png","modified":1,"renderable":0},{"_id":"source/img/hive-数学函数.png","path":"img/hive-数学函数.png","modified":1,"renderable":0},{"_id":"source/img/hive-运算3.png","path":"img/hive-运算3.png","modified":1,"renderable":0},{"_id":"source/img/hive集合数据类型.png","path":"img/hive集合数据类型.png","modified":1,"renderable":0},{"_id":"source/img/spark-all.png","path":"img/spark-all.png","modified":1,"renderable":0},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","path":"img/主动免疫可信架构信任链传递示意图.png","modified":1,"renderable":0},{"_id":"source/img/混合加密的方式.png","path":"img/混合加密的方式.png","modified":1,"renderable":0},{"_id":"source/img/读写锁.png","path":"img/读写锁.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","path":"css/hl_theme/atom-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","path":"css/hl_theme/brown-paper.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","path":"css/hl_theme/atom-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","path":"css/hl_theme/darcula.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","path":"css/hl_theme/github.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","path":"css/hl_theme/github-gist.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","path":"css/hl_theme/gruvbox-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","path":"css/hl_theme/gruvbox-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","path":"css/hl_theme/kimbie-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","path":"css/hl_theme/kimbie-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","path":"css/hl_theme/school-book.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","path":"css/hl_theme/railscasts.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","path":"css/hl_theme/sublime.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","path":"css/hl_theme/sunburst.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","path":"css/hl_theme/rainbow.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","path":"css/hl_theme/zenbum.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","path":"css/fonts/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","path":"css/fonts/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","path":"css/fonts/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","path":"css/fonts/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","path":"css/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","path":"css/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","path":"css/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","path":"css/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"source/img/TCP协议通讯过程1.png","path":"img/TCP协议通讯过程1.png","modified":1,"renderable":0},{"_id":"source/img/hive运算1.png","path":"img/hive运算1.png","modified":1,"renderable":0},{"_id":"source/img/wordcount-reduce.png","path":"img/wordcount-reduce.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议.png","path":"img/四次挥手协议.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/css/fonts/selection.json","path":"css/fonts/selection.json","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/weixin.jpg","path":"img/weixin.jpg","modified":1,"renderable":1},{"_id":"source/img/hive-表生成函数.png","path":"img/hive-表生成函数.png","modified":1,"renderable":0},{"_id":"source/img/对象存储1.png","path":"img/对象存储1.png","modified":1,"renderable":0},{"_id":"source/img/阻塞与非阻塞调用对比.png","path":"img/阻塞与非阻塞调用对比.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/js/gitment.js","path":"js/gitment.js","modified":1,"renderable":1},{"_id":"source/img/select、epoll模型对比.png","path":"img/select、epoll模型对比.png","modified":1,"renderable":0},{"_id":"source/img/Socket通讯模型.png","path":"img/Socket通讯模型.png","modified":1,"renderable":0},{"_id":"themes/3-hexo/source/js/gitalk.js","path":"js/gitalk.js","modified":1,"renderable":1},{"_id":"source/img/hive-聚合1.png","path":"img/hive-聚合1.png","modified":1,"renderable":0},{"_id":"source/img/网络连接模型.png","path":"img/网络连接模型.png","modified":1,"renderable":0},{"_id":"source/img/四次挥手协议2.png","path":"img/四次挥手协议2.png","modified":1,"renderable":0},{"_id":"source/img/三次握手协议3.png","path":"img/三次握手协议3.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/3-hexo/.DS_Store","hash":"0770f9d42bfdd8d420de48fed463015e001cf579","modified":1566357905093},{"_id":"themes/3-hexo/_config.yml","hash":"2bd8fb1dcf031d97ec25251dcbd9f7f928b61334","modified":1571142777564},{"_id":"themes/3-hexo/.gitignore","hash":"560a43fddfe4559ed1a17e7362874454519f189b","modified":1566357905094},{"_id":"themes/3-hexo/README.md","hash":"754d6f873d73f4a7faf5530fa4062e9a696a8fd4","modified":1571110367855},{"_id":"source/_discarded/hello-world.md","hash":"3de445ce428eb9d987b765b94bd6e0428eaebdbc","modified":1567305094926},{"_id":"source/_posts/ElasticSearch客户端.md","hash":"7ccbf29697de8f6f6b0b5f93511411877cf79f6d","modified":1594783976606},{"_id":"source/_posts/ElasticSearch近实时性介绍.md","hash":"d793c6568c0d639875205c587d23f4319fb377bd","modified":1594782660975},{"_id":"source/_posts/GET与POST区别.md","hash":"99282f35ceadcd64eab6a9b404316087c228c435","modified":1595295043727},{"_id":"source/_posts/Git梳理.md","hash":"d0a674882ec53bb07989a7f0dfaba94e0799342a","modified":1571142092785},{"_id":"source/_posts/HDFS文件操作.md","hash":"02424d27fdab1eab02babaf246e4c4f09edf904a","modified":1576487061726},{"_id":"source/_posts/HDFS概述.md","hash":"f93a0a44dd6c86d661add6adb695ac900dad5e16","modified":1576482443395},{"_id":"source/_posts/HiveQL视图.md","hash":"d5530bb5709c2e1afc05e1416194f4455f1a33a3","modified":1579575513416},{"_id":"source/_posts/Hive数据操作.md","hash":"d0d1b62844f29132a921c4034cbda39d27ee0fed","modified":1579420181742},{"_id":"source/_posts/Hive数据定义.md","hash":"9e687194aeec3eeaf1b2f88d8f3576cb464e2cd8","modified":1579247585936},{"_id":"source/_posts/Hive数据操作（2）.md","hash":"13a8dd7c6c5d339fe38f11345d387756211b082f","modified":1579502773866},{"_id":"source/_posts/Hive数据操作（3）.md","hash":"dee288d667445de9a892708dfffbc6c8192e47ae","modified":1585358257098},{"_id":"source/_posts/Hive数据类型和文件格式.md","hash":"5bb268f69c76b18c83e8b3045b08ebc348bf09e7","modified":1579241571383},{"_id":"source/_posts/Hive模式设计.md","hash":"caf50befcb1e9dfb86c6b7f868b2d76594b834b4","modified":1579596483705},{"_id":"source/_posts/Hive索引.md","hash":"08cc7d276a1037eed15156d7497a61816d956cf8","modified":1579585803179},{"_id":"source/_posts/Hive调优.md","hash":"14ff0ea2973448cd9121e83728042952c7333859","modified":1579600013242},{"_id":"source/_posts/HttpClient.md","hash":"5e8869ed7e12977ecd3e754f196405ec5ce3d1b5","modified":1594823957273},{"_id":"source/_posts/JAVA数据类型.md","hash":"26d4e513161224daad2a53674d5ffade057f621c","modified":1578304400441},{"_id":"source/_posts/Nacos配置中心使用.md","hash":"36f687e96443079769f0417538326b29b8c77510","modified":1574740644175},{"_id":"source/_posts/SimpleDateFormat引发的线程安全问题.md","hash":"d8c4c82a697598498aa98a78adab7cb0b0c383b6","modified":1570882111315},{"_id":"source/_posts/Spark相关概述.md","hash":"51e1ebe27a5fddc703347127dd8c172e6515fc64","modified":1576660372500},{"_id":"source/_posts/MapReduce概述.md","hash":"e24632c1742518951c8b5bb2b378a54a7cc8dbac","modified":1576490842822},{"_id":"source/_posts/Spring-Bean生命周期.md","hash":"cce576cd1e7c9260398e5ce7dbbf93f78ad65fa3","modified":1573723161652},{"_id":"source/_posts/TCP-IP四层网络模型.md","hash":"ad3086d71dd922d2a5fbf1e53d45ed848ebd16dd","modified":1571141979272},{"_id":"source/_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","hash":"9421a39ea2426ef3f0d785148671b14f946bd332","modified":1575360178242},{"_id":"source/_posts/TCP与UDP的区别.md","hash":"2ea423fb37ccdcabb7eba166a5ea84b4bb9e0c8d","modified":1595290967635},{"_id":"source/_posts/TCP握手、挥手协议.md","hash":"707e37bd71fa2bf44d653374fff403f8eaa0396e","modified":1595287084754},{"_id":"source/_posts/ThreadLocal.md","hash":"d4b27d71fd595e522dedf09d3e51688caf8fcb66","modified":1585660357975},{"_id":"source/_posts/WordCount简析.md","hash":"ed975037b0341d6c16022e5914f000ac38f00a4d","modified":1576641550021},{"_id":"source/_posts/Yarn概述.md","hash":"b126a9351edb7982bb71c5a55d509408c6b51c84","modified":1576635779255},{"_id":"source/_posts/java8新特性.md","hash":"f4389eb1abf000d7ff97b9a8cedc1dac2030f4b7","modified":1573720237235},{"_id":"source/_posts/maven梳理.md","hash":"a21cc6519d6d4afe06493568a44b07f9a536ac74","modified":1571141975470},{"_id":"source/_posts/mysql事务.md","hash":"73ea46a2e681177f5e8b19604a5c279b2c9a119a","modified":1595295771809},{"_id":"source/_posts/mysql排序.md","hash":"b4a56da0c4b2024b2bbe87c8450cd3076106282a","modified":1574753513949},{"_id":"source/_posts/mysql表设计及优化.md","hash":"1e592b36e153b129de094310283598e4694a75fa","modified":1571134218948},{"_id":"source/_posts/互斥锁.md","hash":"a2a6a0b26fd73482c98c84867c93c02e851c9fb0","modified":1571134239998},{"_id":"source/_posts/偏向锁.md","hash":"47b6a6a3fec8455311a590cbb47a66f788fd7cb4","modified":1585357234402},{"_id":"source/_posts/分布式全局唯一ID生成策略.md","hash":"a060a38e0f2c4a5ef4633cc2be911bcd9a1bb64e","modified":1571133944357},{"_id":"source/_posts/公平锁、非公平锁.md","hash":"7d520324daf56a0c36885fef17f2366800438a0f","modified":1571133968660},{"_id":"source/_posts/加密解密.md","hash":"edf660583a53e0d44ba9a0a74785a3c5a98b6626","modified":1571143218778},{"_id":"source/_posts/单例模式.md","hash":"2ead30e35f5bf0dda43ab6908563876337154980","modified":1578043442502},{"_id":"source/_posts/可信与可信计算.md","hash":"622f1a97783b9830838ec63f7eafd319107daf78","modified":1571134041402},{"_id":"source/_posts/可信基本概念.md","hash":"8e83f8be54cd21559f858755e6abb9ea577c487f","modified":1571134252269},{"_id":"source/_posts/可重入锁.md","hash":"a62dbbe3f15a6527490ea786371b62b31f782e79","modified":1571134060790},{"_id":"source/_posts/可靠性和容错技术.md","hash":"a397449f53f9042cb8de56580572210a9d18ba2e","modified":1569998915455},{"_id":"source/_posts/图解公钥与私钥.md","hash":"0a89b8fa785c291ff753fdfaa6bcc7668d80e733","modified":1569333117731},{"_id":"source/_posts/基于JavaAgent的全链路监控（1）.md","hash":"e6447921b7a38289c85539bade9d521edf640d33","modified":1595170251434},{"_id":"source/_posts/基于JavaAgent的全链路监控（2）.md","hash":"ebe71baa24d4986588097f66652b76adf4b638e8","modified":1595170244890},{"_id":"source/_posts/基于JavaAgent的全链路监控（3）.md","hash":"4174d8f8dce9bbefd4c42f981a6a45882257cd34","modified":1595170237560},{"_id":"source/_posts/基于JavaAgent的全链路监控（4）.md","hash":"aa66dca05ebd0f57a2c0cbc8cd7c553bd2a67b78","modified":1595287075292},{"_id":"source/_posts/对象存储与指针压缩.md","hash":"690c2453d97b741ffe8dcdfeb93d81d787c0bcfd","modified":1574251423218},{"_id":"source/_posts/悲观锁、乐观锁.md","hash":"d18befc289385445e68c92be35441c0aaaf692c3","modified":1571141864237},{"_id":"source/_posts/排序之比较器.md","hash":"225e3478c1e2bc7ab8c6dd1e0245dadc7b0d8d39","modified":1577936716885},{"_id":"source/_posts/数字签名.md","hash":"bf07570f5afa7175d432492564c548654191a38e","modified":1571142533710},{"_id":"source/_posts/排序之比较器Comparator-T.md","hash":"fc1ed5224ef4318537061477d1f5a4b40e784b89","modified":1577937612166},{"_id":"source/_posts/池化之线程池.md","hash":"c77f5acddf612ad5f2743864bf4edcc4a6dd6786","modified":1571133890697},{"_id":"source/_posts/文件上传.md","hash":"b77879b87814fef9745a3a203e53c79db3c5cebe","modified":1571141394589},{"_id":"source/_posts/特征提取-简单流程.md","hash":"72c62832ec836dd303b214806c7ce2d0fc63cd85","modified":1577095126583},{"_id":"source/_posts/线程相关的知识.md","hash":"dae1ea9b088c4125eb2a0d41bae42c9d5bbdc94b","modified":1574250624878},{"_id":"source/_posts/理解IO阻塞与非阻塞.md","hash":"8aa3817b497cd4acc5559f9f811da0514abeaf13","modified":1567159309767},{"_id":"source/_posts/自旋锁.md","hash":"d7fe8a130eba821be23ddb2c9e6aea2b0b2d2b67","modified":1567227561273},{"_id":"source/_posts/读写锁.md","hash":"3e17aded1f87f787910f807cf8696e0b2d428c3d","modified":1571133919181},{"_id":"source/_posts/责任链模式.md","hash":"2cbfcbb8a09a899bc3d20d9597ab8c4e1d8055cb","modified":1578038298113},{"_id":"source/_posts/重放攻击.md","hash":"b00712cae3ee556704f0f0163aba3a45da842264","modified":1567603483315},{"_id":"source/_posts/轻量级锁.md","hash":"fbf5342b86be3b20dca77f472c390340256d49ec","modified":1567235430223},{"_id":"source/_posts/锁粗化.md","hash":"d96860384ca8bdd805950ce24cfb782282c6b178","modified":1571142931866},{"_id":"themes/3-hexo/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1566357905055},{"_id":"source/_posts/阻塞锁.md","hash":"bc39dd0b2789e9b4efaa084473dbbd7f0545ff26","modified":1567227873756},{"_id":"themes/3-hexo/.git/FETCH_HEAD","hash":"d1870580b3e08daea5698a4cf75a6daaba7a25a1","modified":1578274123502},{"_id":"themes/3-hexo/.git/ORIG_HEAD","hash":"36e1610a86bddf4a3dd3b00a29a138d2168f7624","modified":1578274123579},{"_id":"themes/3-hexo/.git/config","hash":"e0bfd9b35852beab6bb3aa547e10d0805cbdf751","modified":1566357905068},{"_id":"themes/3-hexo/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1566357867204},{"_id":"themes/3-hexo/.git/index","hash":"14e835aae657d259ea77737b75fa87a9eabe1bb6","modified":1594783198425},{"_id":"themes/3-hexo/.git/packed-refs","hash":"12c6a6c5d8c824d9777819513a9105d201be16c1","modified":1566357905044},{"_id":"source/img/Git工作流程.png","hash":"13f5329292c5f0b42cb969b2981141981bc62c12","modified":1567048479116},{"_id":"source/img/TCPIP服务器接收请求.png","hash":"dd6986ec9df5f3ffe5ff743e97408b53147743fc","modified":1567149345292},{"_id":"source/img/TCPIP用户发送请求.png","hash":"f4428ca4bf21e4d067da5b944bb742892939d131","modified":1567149324326},{"_id":"source/img/UDPTCPcompare.png","hash":"fc2c1380a717ad5e1ac5a85250d26d21e56a9f92","modified":1595290931067},{"_id":"source/img/UDPHeader.png","hash":"03991c09cb8a800bc68acc974430c1d6df3073d2","modified":1595290558000},{"_id":"source/img/clip_image002.png","hash":"773f0b16657a294e5b51f44c9adc7cf0d4813728","modified":1573721616774},{"_id":"source/img/es2.png","hash":"07f92f31d703b5659cf537a5b31b2ff9c55cbdc1","modified":1594782445385},{"_id":"source/img/dataSource-behaviour-relative.png","hash":"702859e0f00906daf67b67515b0bd683148f8109","modified":1577073211083},{"_id":"source/img/es1.png","hash":"89281b1bef0d51742b32b70b774525c6ae08c95f","modified":1594782221769},{"_id":"source/img/floattype.png","hash":"a7c29f3c863dfe4b4f3ea2ccea6b35002b7eb7ba","modified":1578294604312},{"_id":"source/img/hive-聚合2.png","hash":"69ab607095185fb63a23851f371ac09c3261728a","modified":1579415860533},{"_id":"source/img/javaagent1.png","hash":"93c2b34bbbbb8ce1fccb0d0eb0daff97a7a2bdac","modified":1594963356047},{"_id":"source/img/java对象存储2.png","hash":"a5e540b6699183dcad89a9912c25c6a0c5eed9f3","modified":1574250903564},{"_id":"source/img/java对象存储.png","hash":"78231e4a1c56b2078cdd4454d20593ba3e2b48e3","modified":1567229269140},{"_id":"source/img/java对象存储3.png","hash":"4b83f6c1accd8ea2753e0dc16f52820d52abb700","modified":1574250991794},{"_id":"source/img/maven配置.png","hash":"c966dcfb8e36138b51cbaa08a8e15bc73a615778","modified":1567045274281},{"_id":"source/img/mysql排序1.png","hash":"09e8d6d507cd2b06ed4e6e9a76c832e226981ed0","modified":1574253764887},{"_id":"source/img/mysql排序2.png","hash":"0f0e1c1417f85b8e5f7bf1bb929ad63223569faf","modified":1574253839369},{"_id":"source/img/mysql排序5.png","hash":"044b6bc8c3b4d33f3a25598b9b99771efe75bccf","modified":1574254222538},{"_id":"source/img/mysql排序4.png","hash":"9ef66bb2173d553fb58106a4959b3c0a5751d482","modified":1574254046965},{"_id":"source/img/mysql排序6.png","hash":"254e269dc8da6b5358e883e82a16c52243a4286e","modified":1574254481724},{"_id":"source/img/nacos-producer.png","hash":"36eefd319d40a5a11e9343d3d817d66098d9d043","modified":1575358053823},{"_id":"source/img/nacos-springCloud1.png","hash":"afd0806eb1d72ed4988811e08d1c2e7cb621682b","modified":1574848818956},{"_id":"source/img/nacos-springCloud2.png","hash":"6ce7c90123f1a1eb0e3ebbe93ae50eded7392603","modified":1574848848065},{"_id":"source/img/nacos1.png","hash":"07c9cfee060b4090c04d5a9d422b97e2dd1cbbd2","modified":1574671501527},{"_id":"source/img/pasted-0.png","hash":"d4f20b5880ebe1cd4114af86e182d3a8042f29b6","modified":1571142530740},{"_id":"source/img/secondaryNameNode.jpg","hash":"6186b1ee9e43435bafb7abefde1824d5f66e8f8b","modified":1576479793155},{"_id":"source/img/user-log.png","hash":"f7bd7155b8a465ad32afb87f8ae386768833c64f","modified":1577072956150},{"_id":"source/img/wordcount.png","hash":"78dbf6fe038d27891bb860321f1087fa9f642e3c","modified":1576488969747},{"_id":"source/img/三次握手协议1.png","hash":"b7cf27e8ba8b7e299c4e02519394600d0269ae84","modified":1567152263777},{"_id":"source/img/三次握手协议2.png","hash":"149fa2d315b1471325a89b68ee659409c709c10c","modified":1567152281871},{"_id":"source/img/使用协议进行通讯.png","hash":"fe93735a4d871a544feb31b0b713f98ab4c715fb","modified":1567157267759},{"_id":"source/img/信任链.png","hash":"23517850fc98c07ad66d80bac4556b77e0688214","modified":1569663179090},{"_id":"source/img/公钥私钥6.png","hash":"c7104a322604a0b51de2d988bd9522b5deab2045","modified":1569332262735},{"_id":"source/img/公钥私钥8.png","hash":"6b13644b98068f7f44ddb51532f17030906031c0","modified":1569332306598},{"_id":"source/img/加密算法.png","hash":"5a0a4d8fbdc43f929dd5aa86aded45086ba38cbb","modified":1571142476298},{"_id":"source/img/可信根.png","hash":"875a1cdae0d77993f07a1df1dd968f907a2c1d1e","modified":1569664599137},{"_id":"source/img/对称加密算法.png","hash":"6a7ed179fb8b48e9054caba56308b1691b6ebe92","modified":1567425941515},{"_id":"source/img/数据库分布式ID生成.png","hash":"7376ea76ee62cc47d2e0389f90ca9fa1a3173f74","modified":1570600820250},{"_id":"source/img/线程相关1.jpg","hash":"cd2dfeb2b0a367d208e46d1fec4ed9241164256e","modified":1574250522288},{"_id":"source/img/线程相关3.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1574250560177},{"_id":"source/img/线程相关2.jpg","hash":"2146aff9f267536a9a49fac7c34267380cb8dea5","modified":1574250550341},{"_id":"source/img/线程相关4.jpg","hash":"3268689c6020136cda73d95d1553dfa6816aa60b","modified":1574250574158},{"_id":"source/img/线程相关5.jpg","hash":"54f4bd928b107b4da2faaa87fa74484b76900c0b","modified":1574250583745},{"_id":"source/img/锁的创建.png","hash":"ac72c945c263be025d83970d2cfdb6c240c38bbc","modified":1571144077063},{"_id":"source/img/锁的创建2.png","hash":"ebf05f8b81c02d9c880b9081e9ccc0b2b6c5e4ae","modified":1571144110083},{"_id":"source/img/阻塞IO.png","hash":"f5cfd961b871078b9b202d1e8d4810f3f126aaef","modified":1567157770850},{"_id":"source/img/雪花算法.png","hash":"522b23e1a5bc4bdf46b1d285233d815004847271","modified":1570600786906},{"_id":"themes/3-hexo/source/.DS_Store","hash":"fdcc907c46e093a14b153c5dc8c038461997ed3c","modified":1566357905108},{"_id":"themes/3-hexo/layout/index.ejs","hash":"a5c464897e7dc9d45d03d8b61e742b1ac4173a95","modified":1566357905106},{"_id":"themes/3-hexo/layout/indexs.md","hash":"74f41909ea63fb6e9599a5d9bb3d164d9b2ea8d3","modified":1567325571619},{"_id":"themes/3-hexo/layout/post.ejs","hash":"aeda285031ba8d4e94225e82b364bcf5f79fce1d","modified":1566357905107},{"_id":"source/img/GETPOST.png","hash":"2a4f00d2d7cf57f885f347c1f6cdd5d0f477408c","modified":1595293877166},{"_id":"source/img/HTTPS2.png","hash":"b792dee35f602f38fce149164dad4593c64a9650","modified":1569332481676},{"_id":"source/img/HDFS-liucheng.png","hash":"b37eef3d1e61aeaf650ea9860b40acff7c5fc1e4","modified":1577071488198},{"_id":"source/img/HTTPS3.png","hash":"e3e58b8e4961a4533ac2a63154aee3c49322e379","modified":1569332500332},{"_id":"source/img/HTTPS4.png","hash":"98c925d3de7c588950cb1ce2be36b3346a0ee848","modified":1569332518053},{"_id":"source/img/HTTPS5.png","hash":"7f96023634200693631288ec5047f024ee53f91c","modified":1569332538597},{"_id":"source/img/IO复用select模型.png","hash":"9f0c021b9b258025552f261e3ab8cbb2f96093ad","modified":1567157813191},{"_id":"source/img/Spark.png","hash":"ab45bdfcdd54f893ab046360fea5ccd51b887b3f","modified":1576647742604},{"_id":"source/img/TCPIP模型.png","hash":"69c031664c0698c88a8b5d40b7b8e7e7afc53748","modified":1595290168314},{"_id":"source/img/TCP协议通讯过程.png","hash":"46efae085ca09e1445b9eac506c02c3462e2a16b","modified":1567157460145},{"_id":"source/img/ThreadLocal内部存储.png","hash":"1bcf081078acb1a514dd19ef48b740663837e899","modified":1585659506550},{"_id":"source/img/agent-costtime.png","hash":"1659bf6791dac773e8eb6d5ee3357b79d8702c12","modified":1595149419543},{"_id":"source/img/agent-costtime2.png","hash":"657b28cdec63d882c44e6010a7a78fdffbd6c186","modified":1595149535210},{"_id":"source/img/chartype.png","hash":"965a1655bf9364c0ec0b7a8773c74d8ce9d91805","modified":1578295245919},{"_id":"source/img/data-collect-analysis.png","hash":"19e4706ad65dfa32bf285624af839d4437db17df","modified":1577073489053},{"_id":"source/img/clip_image004.png","hash":"762418fea78372f2c8b67fc6f4dffb7045e9d449","modified":1573721637689},{"_id":"source/img/data-collect.png","hash":"f46c55acbc3e188b8c9cce341fb61d2db1027f31","modified":1577071668167},{"_id":"source/img/hdfs-read-file.png","hash":"861f90f65c5c1eac3ffea16430c75833aff5e089","modified":1576482855424},{"_id":"source/img/hive数据结构1.png","hash":"1251b48e87a11c926b56969b3de0db1077627a3a","modified":1579229765391},{"_id":"source/img/hive数据结构.png","hash":"6d0a423af256a0aecb19cd1ef7d34f4eb327efde","modified":1579229663822},{"_id":"source/img/hive文本文件数据编码.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1579232129199},{"_id":"source/img/image-20200117113506023.png","hash":"8cf2aa54ca23e32190701629cf42b334f4653623","modified":1579232453859},{"_id":"source/img/inttype.png","hash":"e01f1199570a67cd2ae26c3012227718e2575056","modified":1578292566587},{"_id":"source/img/mysql时间存储.png","hash":"1db17e59e7b6dad480009153803c159d5a97cbfe","modified":1567298126454},{"_id":"source/img/mysql的ip存储.png","hash":"d06d3619438f57be43aa1236ec9ace4e1bf3d126","modified":1567298167198},{"_id":"source/img/typetrans.png","hash":"485490a772062455bc238936c0607d48d4aa93b7","modified":1578297875081},{"_id":"source/img/user-behaviour.png","hash":"b0ce3ff640e61461a545a39c56d01a3b6447a68a","modified":1577072979634},{"_id":"source/img/wordcount-split.png","hash":"27610016538c3dca06409dcfbd57f8a805f86624","modified":1576489083300},{"_id":"source/img/wordcount-map.png","hash":"a1c10dc70e0fdef09e3db7f305e16cd2fdf631a1","modified":1576489400587},{"_id":"source/img/公钥私钥10.png","hash":"767005c089b3be3b91213d12fab4e462718a7787","modified":1569332359302},{"_id":"source/img/公钥私钥11.png","hash":"10755e3bcfc39d9bf3c8b9b5060ec5682a18cad3","modified":1569332383529},{"_id":"source/img/公钥私钥1.png","hash":"498ea1f36ce52b0ff2964c9423bf5eb5ab0ae804","modified":1569332132500},{"_id":"source/img/公钥私钥13.png","hash":"afd0297e43e61b9193b6dcbf9a71da610938dd2f","modified":1569332430586},{"_id":"source/img/公钥私钥2.png","hash":"c60ac4f1a39665d9a2b923f3a78541fc663b0e6b","modified":1569332150578},{"_id":"source/img/公钥私钥3.png","hash":"63b5f3e415343c2a65d3274fe8d6e4ce4c26f490","modified":1569332201849},{"_id":"source/img/公钥私钥4.png","hash":"683543865f578ea89be62b1af374df4de0f42069","modified":1569332222890},{"_id":"source/img/公钥私钥5.png","hash":"b2721631bbbac0306b12e70466fa49531f8bae30","modified":1569332242107},{"_id":"source/img/公钥私钥7.png","hash":"e7f712acb8d5e47d717f18494d4c5e22ea9b5468","modified":1569332285257},{"_id":"source/img/公钥私钥9.png","hash":"4c4d25e78aea05b34ccf9cd0c5025cfcb7bf94e3","modified":1569332337273},{"_id":"source/img/指针压缩2.png","hash":"2db8b3b15563eedb0a35b192c37760fec8f9c8fa","modified":1574251229188},{"_id":"source/img/指针压缩3.png","hash":"f142f7d0e15d58e03d02d99c0eba541e51e30a7f","modified":1574251277019},{"_id":"source/img/指针压缩4.png","hash":"945b359ce34488105eab6422d8f397d8c76b4ea0","modified":1574251320221},{"_id":"source/img/非对称加密算法.png","hash":"260798ec83168388a70e55c60b1a7e5a47e78246","modified":1567425887927},{"_id":"themes/3-hexo/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1566357867218},{"_id":"themes/3-hexo/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1566357867216},{"_id":"themes/3-hexo/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1566357867210},{"_id":"themes/3-hexo/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1566357867213},{"_id":"themes/3-hexo/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1566357867222},{"_id":"themes/3-hexo/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1566357867225},{"_id":"themes/3-hexo/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1566357867227},{"_id":"themes/3-hexo/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1566357867235},{"_id":"themes/3-hexo/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1566357867232},{"_id":"themes/3-hexo/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1566357867237},{"_id":"themes/3-hexo/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1566357867229},{"_id":"themes/3-hexo/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1566357867243},{"_id":"themes/3-hexo/.git/logs/HEAD","hash":"8b723d9f01348f811c9a75afbb7486873dc1d656","modified":1571110367864},{"_id":"source/img/HTTPS1.png","hash":"2cc9b05768fb05bd25da12dd3428e6f02dfe50ec","modified":1569332459290},{"_id":"source/img/Hive-运算2.png","hash":"1422e43a9e0aba7dac5ea4b6eecddbf6adc6bfe7","modified":1579415672114},{"_id":"source/img/SpringBean3.png","hash":"2b96172eecc1cbd1e41ab9756c0a12836ebad948","modified":1573721725755},{"_id":"source/img/hive-算数运算符.png","hash":"54c7db33bbd7b5b8484727d7a988869609539d4c","modified":1579415475356},{"_id":"source/img/hive-聚合3.png","hash":"e3efe8d25521a72b1180deab85b38de9ecda0f5f","modified":1579415874687},{"_id":"source/img/mysql排序3.png","hash":"a91f727d4c99fb6289678a86f64ddd1c246568c7","modified":1574253993339},{"_id":"source/img/simpleDateFormat-alibaba.png","hash":"f45845a64160deda103dcca59d212712bfe544e6","modified":1585640320743},{"_id":"source/img/spark+hdfs.png","hash":"5ea801d62e8a34b8f08c627826d7f71369c2b749","modified":1576649860216},{"_id":"source/img/可信在云平台的基础架构.png","hash":"b507acb0ecbe5f39f783b7a82340a30bb6fd42d4","modified":1567327147941},{"_id":"source/img/非阻塞IO.png","hash":"58573268ff130afca209099ecda84d1c4eeff17f","modified":1567157791409},{"_id":"themes/3-hexo/source/css/tomorrow-night.scss","hash":"06db3e5b8a8320b325d5b48566108569c79115fb","modified":1571118552633},{"_id":"themes/3-hexo/source/css/mobile.styl","hash":"d10bdd736aa343f38fe15cba4c81d45d3d259de4","modified":1566357905129},{"_id":"themes/3-hexo/source/css/style.styl","hash":"322abe325d5fe9ff37347bdd19e772315a8ebfed","modified":1566357905130},{"_id":"themes/3-hexo/source/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1566357905133},{"_id":"themes/3-hexo/source/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1566357905133},{"_id":"themes/3-hexo/source/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1566357905140},{"_id":"themes/3-hexo/source/js/jquery.autocomplete.min.js","hash":"2462169ad7f4a8ae9f9f4063995cbe7fed45cd77","modified":1566357905140},{"_id":"themes/3-hexo/source/js/script.js","hash":"eddf1fe782a3383a051aeccaf298cbe278a9a928","modified":1569334090626},{"_id":"themes/3-hexo/source/js/search.js","hash":"788c610149a5f9361295f9f0207c8523f37ddb8b","modified":1566357905141},{"_id":"themes/3-hexo/layout/_partial/article.ejs","hash":"d3c928954057bcebdf3fc294b2123885b08396d3","modified":1571110367856},{"_id":"themes/3-hexo/layout/_partial/article_copyright.ejs","hash":"05d3cddf5f3a53577452db4efa811bd361f7c0c4","modified":1566357905096},{"_id":"themes/3-hexo/layout/_partial/copyright.ejs","hash":"faffe25aec33936fa2ec9d8f0e34e16ef3d90c25","modified":1566357905100},{"_id":"themes/3-hexo/layout/_partial/comment.ejs","hash":"d18f94e04ef0cf7abb432a8e707ccb3abc7fe435","modified":1566357905097},{"_id":"themes/3-hexo/layout/_partial/friends.ejs","hash":"558a3d4bad578819fb07729fe1b50d9b81da8b93","modified":1566357905102},{"_id":"themes/3-hexo/layout/_partial/dashang.ejs","hash":"6eab1e5fae6bd60928325d026a1bed61c43d11a9","modified":1566357905100},{"_id":"themes/3-hexo/layout/_partial/full-toc.ejs","hash":"0e976208d79b0396eb51ca1af5016c963c6e4618","modified":1566357905102},{"_id":"themes/3-hexo/layout/_partial/footer.ejs","hash":"311489fe2050ada8a0eaad3d0c57e5d5012f470e","modified":1571110367857},{"_id":"themes/3-hexo/layout/_partial/header.ejs","hash":"915d5f10dd8f3dcd19cb75010e23689e8f385caf","modified":1571109509544},{"_id":"themes/3-hexo/layout/_partial/mathjax.ejs","hash":"e2be0e37f3d48e63e65a47d819bfb800b9aa3784","modified":1566357905103},{"_id":"themes/3-hexo/layout/_partial/nav-right.ejs","hash":"98326675546fc6828a45e8b95250899b4ee2d821","modified":1566357905105},{"_id":"themes/3-hexo/layout/_partial/meta.ejs","hash":"ef387e80043b62e1925a068267f2377cac64adc7","modified":1566357905104},{"_id":"themes/3-hexo/layout/_partial/nav-left.ejs","hash":"c14bc1393f779dff7854089621804dceea236d82","modified":1566357905104},{"_id":"themes/3-hexo/layout/_partial/tag.ejs","hash":"87e932476754f27424f9ec397ed66a4ab8a59ad8","modified":1566357905105},{"_id":"themes/3-hexo/layout/_partial/toc-ref.ejs","hash":"33f7a4bfca1bb9835ec8f0d1e73188d1f56cc8b9","modified":1566357905106},{"_id":"source/img/HTTPS6.png","hash":"be9aa6a1578dd12e1970db4731f7c9727f9bc17e","modified":1569332587298},{"_id":"source/img/TCP协议通讯过程2.png","hash":"9ae8696d16fd4ffdc46b6c005b598999dec3f9c1","modified":1567157484636},{"_id":"source/img/Yarn.png","hash":"8b15f23ee56b0f7e2e5aa00157c1de50a0979da7","modified":1576631940342},{"_id":"source/img/公钥私钥12.png","hash":"fdadf38dfcfd50d4afb6939f7403946ebcf1217b","modified":1569332408375},{"_id":"source/img/指针压缩1.png","hash":"d26943d292b23b4cf544cee8b1a73aa4172a8920","modified":1574251137573},{"_id":"source/img/椭圆曲线算法的基本原理.png","hash":"5f848a66630c6ba515286fa96dc36ed3a9a11106","modified":1567343921566},{"_id":"themes/3-hexo/source/css/gitalk.css","hash":"3dc58e9a3fd63a3144d5fe850eb55e3dc885c9fb","modified":1566357905121},{"_id":"themes/3-hexo/source/img/avatar.jpg","hash":"525dc2b6ef38fee9d4c66e554f928934eadd9117","modified":1567299464243},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1566357905132},{"_id":"themes/3-hexo/source/img/alipay.jpg","hash":"749a93e2c1925763846c18294cf0a27171f3a30f","modified":1567299308141},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","hash":"ca5d71e4a70ca0792ef26315b1b7263a01ab0d8d","modified":1571118258831},{"_id":"themes/3-hexo/.git/objects/08/f3f45718fbe8ad37fe1f939546529c1384b033","hash":"20e12e8a049dbe1c987dd181e4bfc359a5178aff","modified":1571110367636},{"_id":"themes/3-hexo/.git/objects/0a/6065aab85f97325dc2ffd41557ab3e2fa64f4d","hash":"b5d02c27f8136e2fd01773ff930a23c3a8edfaf9","modified":1571110367660},{"_id":"themes/3-hexo/.git/objects/12/4eb97ff9228afdf8e9094ba072b786a227763e","hash":"45e2cf84512bc83e73b5a39be6b783588cbca35f","modified":1574051405615},{"_id":"themes/3-hexo/.git/objects/14/e027e912621a5083d6a8d200885d3dcc09d5e4","hash":"496982d736ba7b36a675a9b0c9986b75cc48eaa9","modified":1571110367646},{"_id":"themes/3-hexo/.git/objects/19/6fe084360854de56b55c88f0e7e0c40760fdd6","hash":"4052161d8fee0878d9e806bb1624ef4f52e5f19f","modified":1571110367641},{"_id":"themes/3-hexo/.git/objects/41/63bdeb30eb856080c33d91801fee699f8b098b","hash":"2024a45d3546f3dfe6336722dee5c7066b4b6358","modified":1571110367628},{"_id":"themes/3-hexo/.git/objects/42/6d029f7fe10004bd4a88ae2c89a1de4c7f1711","hash":"193a15faf669a332bd06ce253e0bf40b05debd3d","modified":1574051405622},{"_id":"themes/3-hexo/.git/objects/25/7c60dffc50a203945c7fb81e03ac78aa540b2f","hash":"c19d68cc5d0903ef79a448e5eeab8605e1f916ef","modified":1571110367573},{"_id":"themes/3-hexo/.git/objects/4c/ed9203eb27af55cf001c24111aaf6da9a2bc82","hash":"3905158769e19c42b844e1e6bd1d1d1cd5759084","modified":1574051405630},{"_id":"themes/3-hexo/.git/objects/4f/ed4680e7c8ef567fb0a96baf60e8416e492dc8","hash":"c4607a33444576c00d00b4c2cd4f9438926b0649","modified":1574051405654},{"_id":"themes/3-hexo/.git/objects/53/0c7a5957ca41720c1486a0e798c1acbb1c40b7","hash":"4bfd824b0f9c32c015bea7a6960d2352496f819d","modified":1571110367658},{"_id":"themes/3-hexo/.git/objects/55/b7f9f95e210787ca22dc5cf4bbcde6462706c5","hash":"973bd6d0b9efeb2df7ef9b7222327d807b763827","modified":1574051405648},{"_id":"themes/3-hexo/.git/objects/58/e1459f9ccf452bea04761d66ffd1d071cc0e46","hash":"1193491fff3769511ebbb4464d727e6280d7ef43","modified":1571110367663},{"_id":"themes/3-hexo/.git/objects/5e/669a1bcd5871f73c52baf4ce2fc4d728be009c","hash":"762dd70e5461baa2394d2e9debd0a11b6920fd97","modified":1571110367668},{"_id":"themes/3-hexo/.git/objects/5e/f2bf5cbb66ecf4d5757424128ab11237b1a8a5","hash":"e1cc02d212004518fc667b843a00d5ec472a8242","modified":1574051405626},{"_id":"themes/3-hexo/.git/objects/6d/b387e3375911f397b909bb52702f3515909311","hash":"3eb06f827bde44da887cfc6b436adeea56cf3861","modified":1574051405643},{"_id":"themes/3-hexo/.git/objects/71/79b850585450db8da132f02cee17233b1ef6e3","hash":"5f2b5f8de4aacdec25cca4086de9cabfa1fbe288","modified":1574051405637},{"_id":"themes/3-hexo/.git/objects/78/ce963b18628b6955ae36d0def0d89982aea752","hash":"e468b130e71f328dcb045fb05a567f3c7612f6f2","modified":1574051405610},{"_id":"themes/3-hexo/.git/objects/7d/8c7069fd6ba7171186d92d6fb02b7c214e5c88","hash":"43d7db05128009dfd4ed38c4a002d09f65b39fd9","modified":1571110367644},{"_id":"themes/3-hexo/.git/objects/7e/5afc2fa6d443053ad70da6ecd2fd19eaad2d48","hash":"3ad14d3445b8a8e07fd49238218bcb5ab850392d","modified":1574051405618},{"_id":"themes/3-hexo/.git/objects/76/119dc8b1a6e6227fc913495880c5a542cfd0be","hash":"7ed01208564e56f5f47039853c1dddc79ea0a695","modified":1571110367648},{"_id":"themes/3-hexo/.git/objects/83/f4796d5d05505b0913b4ec70309ad713803128","hash":"18faff62337f3885e90022b779f84b52f73bf31b","modified":1571110367656},{"_id":"themes/3-hexo/.git/objects/89/d2ae00dc60925bafe6df4786c793fc2b412d44","hash":"71964999e9873760272d5111d56a0299c34744ed","modified":1571110367622},{"_id":"themes/3-hexo/.git/objects/96/8933ba48ce0bd43e65ee4124afeae4c209f691","hash":"36ca0028910ef3f98e52090149e7dd438bb3d43c","modified":1574051405628},{"_id":"themes/3-hexo/.git/objects/9d/a0b356deb7df7f74b371aa521bcd8dc07908e5","hash":"bc903f24a59c382559a28f52df2a2160de94cd78","modified":1574051405601},{"_id":"themes/3-hexo/.git/objects/8b/7cba2780f2055549ea8d71bbc639184b194d4d","hash":"0609d0ba9ca4c5092a78f3b275f0005ffc435afa","modified":1574051405639},{"_id":"themes/3-hexo/.git/objects/b0/884ae310196e8639d8f710906c76b1964b6414","hash":"added0f47c2f9298524950da24663cb5ff84a9a9","modified":1574051405641},{"_id":"themes/3-hexo/.git/objects/c2/0e84895fac8aeef603ed8a07f705cba8f87b12","hash":"3c2fd85f436096666fddd05932938090b06b6f72","modified":1574051405632},{"_id":"themes/3-hexo/.git/objects/9f/5b3752c384eebf3dad31c10781fc388d67a9a1","hash":"36fa2d53da9c62cc0867ed64b2670881ed29bca9","modified":1574051405650},{"_id":"themes/3-hexo/.git/objects/c9/e216642e2cfd5b8c75c818c3cf71946f056b66","hash":"f299dba79de7403509512de345098a1452bde3d9","modified":1571110367650},{"_id":"themes/3-hexo/.git/objects/ae/42d951b20115e82d1df1a3a405e87881b55c1e","hash":"9affa9b6eea5470be405da63dd1cf5b09e1b16ff","modified":1571110367625},{"_id":"themes/3-hexo/.git/objects/b2/e70f80fa64697689044677d0edf72a3a85e66e","hash":"3a7e236d5f56b9bec0412afdb8a91ecfa68ade05","modified":1571110367653},{"_id":"themes/3-hexo/.git/objects/a0/467959c08057b36c468b5c05c633db2aa74885","hash":"85ce56017287a4f3c55ac2f5475fb45dd590de9d","modified":1571110367666},{"_id":"themes/3-hexo/.git/objects/de/d806239784cc24d162b5f17529deedd319abbf","hash":"e15db1216a30ee1b25f5ffbfa2a9c78e46a3d45e","modified":1574051405613},{"_id":"themes/3-hexo/.git/objects/dd/e4b0782a77d697afe7852ea59ea0ffdb3efbd6","hash":"ba583e42aafbc8587b9db7efe7da89666c87c9ca","modified":1571110367639},{"_id":"themes/3-hexo/.git/objects/e4/45c0be361ce735d5c7c36d4b341b9230126323","hash":"e98f245e7a9dbf741a7f7cac91d7edbe0240143e","modified":1574051405624},{"_id":"themes/3-hexo/.git/objects/cd/0e2e6dc99e22a7d01b16617e516e324e4bee52","hash":"28d15689a97be5a033914c3ba4ee86afc80400b7","modified":1571110367631},{"_id":"themes/3-hexo/.git/objects/e9/acab7802fd162cb889d62b47a7ca9b17a1ea1f","hash":"4bcfbfafa929d1ca035a315fce1e057f9cfe62ba","modified":1574051405652},{"_id":"themes/3-hexo/.git/objects/e6/f6e1573bf42a6268f889fe2ffd25d36f7ecabf","hash":"3db0915b8b2ecaee95f834e85efa0793220435bb","modified":1574051405635},{"_id":"themes/3-hexo/.git/objects/e6/1c4e686f371037296bad61a0a0d6d6836e3a4e","hash":"6eb5fac0e5fa6b3cfc8a154ba6368ca2ee6e731c","modified":1574051405645},{"_id":"themes/3-hexo/.git/objects/fe/29eed9e715cda66839f260b03d8929289aad68","hash":"59645d6ee8981fc6141d250984bab86f76917597","modified":1574051405620},{"_id":"themes/3-hexo/.git/objects/f3/374e0c06cbc5e69e50dfea606e96244e267f20","hash":"c4fbc21681e3825ae20277c198b112ed8d558bde","modified":1571110367633},{"_id":"themes/3-hexo/.git/objects/pack/pack-7043c5b8bc471239c47a8bb3fd601c77cd08e244.idx","hash":"4ebcc75525e02c7111f0ea164ec926f194a02838","modified":1566357904945},{"_id":"themes/3-hexo/.git/refs/heads/master","hash":"36e1610a86bddf4a3dd3b00a29a138d2168f7624","modified":1571110367863},{"_id":"source/img/hdfs-write-file.png","hash":"3482163375f44f53c6ff7791e005d5ebc42bf0f8","modified":1576485576556},{"_id":"source/img/hive-partition.png","hash":"c587e39d656360af68418b82172cf91d1b0e7cd2","modified":1579406393393},{"_id":"source/img/hdfs.png","hash":"c3be7dde1cdd24c4765a87ddf07c75c70148e970","modified":1576476667162},{"_id":"source/img/hive-数学函数.png","hash":"e15b0006e6290c17b2348d0840272b52be732e0e","modified":1579415575739},{"_id":"source/img/hive-运算3.png","hash":"5bc45de4ce61992130c1c3636d2031969dd81cb1","modified":1579415710576},{"_id":"source/img/hive集合数据类型.png","hash":"3803153d332cc20f22be920a84a9e371617099f3","modified":1579230723575},{"_id":"source/img/spark-all.png","hash":"d52bb136b90c8f63acaba9398811dbf1642de23f","modified":1576649188404},{"_id":"source/img/主动免疫可信架构信任链传递示意图.png","hash":"ec45ff26518a046bd0f6274a8f7317222f05ea81","modified":1567337278597},{"_id":"source/img/混合加密的方式.png","hash":"45b8a92df2e3c57f6af2d483c444a96e59929843","modified":1567426174461},{"_id":"source/img/读写锁.png","hash":"f5c76702b3a276fc48835d2ad1a02859a4767e08","modified":1567228164910},{"_id":"themes/3-hexo/source/css/_partial/comment.styl","hash":"fe00fb1269b4fe1f3d5ab917891926222ce47275","modified":1566357905110},{"_id":"themes/3-hexo/source/css/_partial/autocomplete.styl","hash":"1ffe51e3b77afefcd94d386a718506d5b055ad94","modified":1566357905110},{"_id":"themes/3-hexo/source/css/_partial/fade.styl","hash":"02c7510a26f306e240f23ddbf772a69be2c890dd","modified":1566357905111},{"_id":"themes/3-hexo/source/css/_partial/font.styl","hash":"c200f3fabcfe83f3e45746e186b4bb111e73ad47","modified":1566357905111},{"_id":"themes/3-hexo/source/css/_partial/dashang.styl","hash":"f0eac1dc1f5dbed1769d032bb5fd5f002faaee26","modified":1566357905111},{"_id":"themes/3-hexo/source/css/_partial/nav-left.styl","hash":"bf29eab9ea75fa191d678b6eefec440505ddf6e3","modified":1566357905112},{"_id":"themes/3-hexo/source/css/_partial/nav-right.styl","hash":"1d01247f974b059d9ef6a2178a724b4f72acd659","modified":1566357905113},{"_id":"themes/3-hexo/source/css/_partial/full-toc.styl","hash":"4102753dad0cc1ee9ed673f7253ba097a960c3b7","modified":1566357905112},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","hash":"f3eb4e5feda9cbd6242ccf44ca064e2979b5d719","modified":1566357905122},{"_id":"themes/3-hexo/source/css/_partial/num-load.styl","hash":"f7ef35459ece22e1da950b86126be1c2bfe97fcf","modified":1566357905114},{"_id":"themes/3-hexo/source/css/_partial/post.styl","hash":"8a462cf9b0b026e71eda9e704c0fbb952b4615c5","modified":1571110367858},{"_id":"themes/3-hexo/source/css/_partial/nprogress.styl","hash":"2620a02169a6aeb75137fd368eac2c36423d8498","modified":1566357905113},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","hash":"03af387edcc1cf8c18d12e9c440fd51b6cf425b6","modified":1566357905123},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","hash":"69d184a682bcaeba2b180b437dc4431bc3be38aa","modified":1566357905122},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","hash":"2bfc14f27ccca108b4b3755782de8366e8bd001e","modified":1566357905123},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","hash":"53276ff1f224f691dfe811e82c0af7f4476abf5d","modified":1566357905124},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","hash":"5e05b19832c1099bd9d284bc3ed00dc8a3d7ee23","modified":1566357905124},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","hash":"315ad610d303caba9eac80a7d51002193a15478a","modified":1566357905125},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","hash":"1bece084b1dbbbd4af064f05feffd8c332b96a48","modified":1566357905125},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","hash":"e9c190f9ffc37a13cac430512e4e0c760205be4a","modified":1566357905126},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","hash":"0c3ccd0d64e7504c7061d246dc32737f502f64e4","modified":1566357905126},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","hash":"51659351b391a2be5c68728bb51b7ad467c5e0db","modified":1566357905127},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","hash":"a6e8cfd2202afd7893f5268f3437421e35066e7b","modified":1566357905127},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","hash":"501d75ef0f4385bea24d9b9b4cc434ba68d4be27","modified":1566357905128},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","hash":"2aa9817e68fb2ed216781ea04b733039ebe18214","modified":1566357905128},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","hash":"e5c37646a9d9c1094f9aab7a7c65a4b242e8db00","modified":1566357905127},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","hash":"92941a6ae73b74f44ad7c559c5548c44073c644a","modified":1566357905129},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1566357905115},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","hash":"b5e7562c8494b0ddb3a70ecc5545ef7340d8e971","modified":1566357905116},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1566357905116},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1566357905117},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","hash":"3dfe8e557d9dfaf39bca088a02b76deb82dbaa3d","modified":1566357905117},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","hash":"7e54ae44c02faa319c4fe128e1e6bda38eae5c9d","modified":1566357905118},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","hash":"aa087561480fb9c2cfd541e33d1e99d5ac1a56bb","modified":1566357905119},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","hash":"f8ed131ccf13f4bdd3ec11fc3e997339dd7b66ba","modified":1566357905119},{"_id":"themes/3-hexo/layout/_partial/comments/click2show.ejs","hash":"8a3a175c2da956366ce91bfc4f4012a487f4bdfc","modified":1566357905098},{"_id":"themes/3-hexo/layout/_partial/comments/gentie.ejs","hash":"908d9046502612d24780ca354bd9392a009b4d7b","modified":1566357905099},{"_id":"themes/3-hexo/layout/_partial/comments/gitalk.ejs","hash":"01567e010cf4f2dd141fe2019490d3f0d5aa2529","modified":1566357905099},{"_id":"themes/3-hexo/layout/_partial/comments/disqus.ejs","hash":"32ce7b48d366b9c888ff2ceb911a3cd82f864537","modified":1566357905098},{"_id":"themes/3-hexo/layout/_partial/comments/gitment.ejs","hash":"eaf2b6f297282606b630ad55fb9e38af7e2829dc","modified":1566357905099},{"_id":"source/img/TCP协议通讯过程1.png","hash":"100f6ea49b8b585a10e0ac88e2486a902e221a5c","modified":1567157472656},{"_id":"source/img/hive运算1.png","hash":"472120109358e0ec58e980b9d0fc9ab533764d4c","modified":1579415684134},{"_id":"source/img/wordcount-reduce.png","hash":"99f7334d973e7a4ffaf5d34e7870b55d0016858b","modified":1576489761247},{"_id":"source/img/四次挥手协议.png","hash":"112d408966bbd5f77373bda95aaa07fd952e23d4","modified":1567152300617},{"_id":"themes/3-hexo/source/css/fonts/selection.json","hash":"b6456a4eabcffd95e822d1d7adce96da524d481a","modified":1566357905120},{"_id":"themes/3-hexo/source/img/weixin.jpg","hash":"59cf33d0f6ce9324dabe183f3d4551620959e987","modified":1567299346911},{"_id":"themes/3-hexo/.git/logs/refs/heads/master","hash":"8b723d9f01348f811c9a75afbb7486873dc1d656","modified":1571110367864},{"_id":"themes/3-hexo/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1566357905053},{"_id":"themes/3-hexo/.git/refs/remotes/origin/master","hash":"26d22e3aa2bcc2d6eb3c0e83ca4d2915ee607d25","modified":1574051405730},{"_id":"source/img/hive-表生成函数.png","hash":"cf8bb24a3e4d1263993cc3df54d7777a3931eb64","modified":1579416817156},{"_id":"source/img/对象存储1.png","hash":"281b0b0c7d4c55df4000800b3f00888663e5ee25","modified":1574250820011},{"_id":"source/img/阻塞与非阻塞调用对比.png","hash":"cdee42373e7940d8f09575bf844ce08f49b4d1af","modified":1567157755596},{"_id":"themes/3-hexo/source/js/gitment.js","hash":"67984b83cd46ff4300d4fd959bf6c17dd66b4136","modified":1566357905139},{"_id":"source/img/select、epoll模型对比.png","hash":"cdc93cacbedcf65fb68878bd36da3929e6333e2b","modified":1567157832363},{"_id":"themes/3-hexo/.git/logs/refs/remotes/origin/master","hash":"c878c52013da84fae14ae548f6a41199653276ec","modified":1574051405732},{"_id":"themes/3-hexo/.git/logs/refs/remotes/origin/HEAD","hash":"36b1e76054f246c3ef309b294d31e9cf5a76d47c","modified":1566357905053},{"_id":"source/img/Socket通讯模型.png","hash":"2e260e5f30b7e17b7dba072e65ac7860e8d7e6ce","modified":1567156905642},{"_id":"themes/3-hexo/source/js/gitalk.js","hash":"536f28c4354a13582af826d9d9b2cb27cec07dc6","modified":1566357905137},{"_id":"source/img/hive-聚合1.png","hash":"070d7feb1ef4626223019a5b205d43b24116dc4a","modified":1579415841935},{"_id":"source/img/网络连接模型.png","hash":"f2f48effb8cb134f597011fda639932ea0d2fac2","modified":1567149380436},{"_id":"source/img/四次挥手协议2.png","hash":"1a4890549075feacfa4010df20911537fc726e4b","modified":1567152739865},{"_id":"source/img/三次握手协议3.png","hash":"e6fe59d8473f94be2b361f4bda5be7d791022eec","modified":1567152339366},{"_id":"themes/3-hexo/.git/objects/pack/pack-7043c5b8bc471239c47a8bb3fd601c77cd08e244.pack","hash":"8deb96fdf8557c2b51e4043fa4dca5e2ad8444bc","modified":1566357904942}],"Category":[{"name":"elasticsearch","_id":"ckcvapozr0002x8vhb18cl064"},{"name":"网络","_id":"ckcvapp04000bx8vh02ersvf2"},{"name":"大数据","_id":"ckcvapp0b000jx8vhctg9qmne"},{"name":"软件管理","_id":"ckcvapp0h000px8vh0rvb7jtt"},{"name":"SpringCloud","_id":"ckcvapp16001nx8vhwrpqqeg1"},{"name":"java基础","_id":"ckcvapp1f001xx8vhyrl1w8d2"},{"name":"数据库","_id":"ckcvapp24002ux8vhfsgxhseu"},{"name":"分布式","_id":"ckcvapp2k003gx8vhcz4yu7nc"},{"name":"可信","_id":"ckcvapp2q003ox8vhr94pprwz"},{"name":"设计模式","_id":"ckcvapp2w003vx8vh9patoq0c"}],"Data":[],"Page":[],"Post":[{"title":"ElasticSearch客户端","author":"郑天祺","date":"2020-07-15T03:10:00.000Z","_content":"\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","source":"_posts/ElasticSearch客户端.md","raw":"title: ElasticSearch客户端\nauthor: 郑天祺\ntags:\n\n  - EsClient\n  - es\ncategories:\n  - elasticsearch\ndate: 2020-07-15 11:10:00\n\n---\n\n以下为springboot整合elasticsearch\n\nes版本为7.2.1\n\n# 1、先引入es的依赖\n\n```java\n  <!-- ES  -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-client-sniffer</artifactId>\n            <version>7.2.1</version>\n        </dependency>\n```\n\n# 2、编写工具类\n\nEsRestHighLevelClient.java\n\n```java\npackage com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i < ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, \"http\");\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -> {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -> {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(\"初始化RestHighLevelClient时出错!\");\n            }\n            if (null == client) {\n                log.severe(\"创建ES连接失败!\");\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n```\n\n# 3、查询（简单举例）\n\n```java\n    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(\"person\");\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name && !\"\".equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(\"name\", name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(\"查询信息时异常，查询es失败\");\n            return null;\n        }\n    }\n```\n\n# 4、插入或更新（简单举例）\n\n```java\n try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map<String, Object> jsonMap = new HashMap<>(1);\n            jsonMap.put(\"id\", person.getId());\n            jsonMap.put(\"name\", person.getName());\n            jsonMap.put(\"age\", person.getAge());\n            jsonMap.put(\"isNeighbourhood\", person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(\"person\")\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(\"数据库写入/更新 ES成功\");\n        } catch (IOException e) {\n            log.warning(\"数据写入/更新 ES发生IO异常!\");\n        } catch (Throwable e) {\n            log.warning(\"数据写入/更新 ES发生异常!\");\n        }\n```\n\n# 5、删除（简单举例）\n\n```java\n        DeleteRequest deleteRequest = new DeleteRequest(\"person\", String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(\"删除\" + deleteResponse.getId() + \", 状态为:\" + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(\"ES删除数据发生IO异常!\");\n        }\n```\n\n# 6、遍历篮子（简单举例）\n\n```java\n    @Override\n    public List<Person> listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List<Person> personList = new ArrayList<>(64);\n        for (SearchHit searchHit : hs) {\n            Map<String, Object> hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(\"id\"));\n            person.setName((String) hitMap.get(\"name\"));\n            person.setAge((Integer) hitMap.get(\"age\"));\n            person.setIsNeighbourhood((String) hitMap.get(\"isNeighbourhood\"));\n            personList.add(person);\n        }\n        return personList;\n    }\n```\n\n","slug":"ElasticSearch客户端","published":1,"updated":"2020-07-15T03:32:56.606Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapozl0000x8vhqvpzvxfo","content":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -&gt; {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            }\n            if (null == client) {\n                log.severe(&quot;创建ES连接失败!&quot;);\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        }\n    }\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        } catch (IOException e) {\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        } catch (Throwable e) {\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        }\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        }\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) {\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        }\n        return personList;\n    }\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>以下为springboot整合elasticsearch</p>\n<p>es版本为7.2.1</p>\n<h1>1、先引入es的依赖</h1>\n<pre><code class=\"language-java\">  &lt;!-- ES  --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;\n            &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt;\n            &lt;version&gt;7.2.1&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h1>2、编写工具类</h1>\n<p>EsRestHighLevelClient.java</p>\n<pre><code class=\"language-java\">package com.example.utils;\n\nimport com.example.constant.Constants;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.sniff.SniffOnFailureListener;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\nimport java.io.IOException;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\n/**\n * es连接客户端\n *\n * @author zhengtianqi\n */\npublic class EsRestHighLevelClient {\n\n    public static Logger log = Logger.getLogger(EsRestHighLevelClient.class.toString());\n\n    private EsRestHighLevelClient() {\n    }\n\n    /**\n     * 返回单例的Client(ES)\n     */\n    public static RestHighLevelClient getEsClient() {\n        return InternalClass.client;\n    }\n\n    private static class InternalClass {\n        private static RestHighLevelClient client;\n\n        static {\n            try {\n                String ip = Constants.ES_HTTP_PORT;\n                String[] ips = ip.split(Constants.COMMA_SPLIT);\n\n                HttpHost[] httpHosts = new HttpHost[ips.length];\n                for (int i = 0; i &lt; ips.length; i++) {\n                    httpHosts[i] = new HttpHost(ips[i], 9200, &quot;http&quot;);\n                }\n\n                SniffOnFailureListener sniffOnFailureListener = new SniffOnFailureListener();\n                RestClientBuilder restClientBuilder = RestClient.builder(httpHosts).setFailureListener(sniffOnFailureListener).setHttpClientConfigCallback(httpClientBuilder -&gt; {\n                    //最大连接数\n                    httpClientBuilder.setMaxConnTotal(100);\n                    httpClientBuilder.setMaxConnPerRoute(50);\n                    return httpClientBuilder;\n                }).setRequestConfigCallback(requestConfigBuilder -&gt; {\n                    // 超时设置\n                    requestConfigBuilder.setConnectTimeout(2000).setConnectionRequestTimeout(2000);\n                    return requestConfigBuilder;\n                });\n\n                client = Optional.of(restClientBuilder).map(RestHighLevelClient::new).orElse(null);\n\n            } catch (Exception e) {\n                log.severe(&quot;初始化RestHighLevelClient时出错!&quot;);\n            }\n            if (null == client) {\n                log.severe(&quot;创建ES连接失败!&quot;);\n            }\n        }\n    }\n\n\n    /**\n     * 测试类\n     *\n     * @param args main方法参数\n     * @throws IOException 抛出异常 无需处理\n     */\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient esClient = EsRestHighLevelClient.getEsClient();\n\n        BoolQueryBuilder query = QueryBuilders.boolQuery();\n        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(query).size(10);\n        SearchRequest searchRequest = new SearchRequest(Constants.INDEX_PERSON).source(searchSourceBuilder);\n        SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n        esClient = getEsClient();\n        search = esClient.search(searchRequest, RequestOptions.DEFAULT);\n        System.out.println(search);\n    }\n}\n\n</code></pre>\n<h1>3、查询（简单举例）</h1>\n<pre><code class=\"language-java\">    /**\n     * 查询\n     *\n     * @return 返回SearchHit 篮子对象\n     */\n    public SearchHit[] listPerson(String name) {\n        try {\n            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n            if (null != name &amp;&amp; !&quot;&quot;.equals(name)) {\n                boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, name));\n            }\n            sourceBuilder.query(boolQueryBuilder);\n            searchRequest.source(sourceBuilder);\n            SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);\n            SearchHit[] hs = searchResponse.getHits().getHits();\n            return hs;\n        } catch (Exception e) {\n            log.warning(&quot;查询信息时异常，查询es失败&quot;);\n            return null;\n        }\n    }\n</code></pre>\n<h1>4、插入或更新（简单举例）</h1>\n<pre><code class=\"language-java\"> try {\n            BulkRequest bulkRequest = new BulkRequest();\n            Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(1);\n            jsonMap.put(&quot;id&quot;, person.getId());\n            jsonMap.put(&quot;name&quot;, person.getName());\n            jsonMap.put(&quot;age&quot;, person.getAge());\n            jsonMap.put(&quot;isNeighbourhood&quot;, person.getIsNeighbourhood());\n\n            IndexRequest indexRequest = new IndexRequest(&quot;person&quot;)\n                    .id(String.valueOf(person.getId())).source(jsonMap);\n            bulkRequest.add(indexRequest);\n            BulkResponse bulk = client.bulk(bulkRequest, RequestOptions.DEFAULT);\n            log.warning(&quot;数据库写入/更新 ES成功&quot;);\n        } catch (IOException e) {\n            log.warning(&quot;数据写入/更新 ES发生IO异常!&quot;);\n        } catch (Throwable e) {\n            log.warning(&quot;数据写入/更新 ES发生异常!&quot;);\n        }\n</code></pre>\n<h1>5、删除（简单举例）</h1>\n<pre><code class=\"language-java\">        DeleteRequest deleteRequest = new DeleteRequest(&quot;person&quot;, String.valueOf(person.getId()));\n        try {\n            DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n            log.info(&quot;删除&quot; + deleteResponse.getId() + &quot;, 状态为:&quot; + deleteResponse.status());\n        } catch (IOException e) {\n            log.warning(&quot;ES删除数据发生IO异常!&quot;);\n        }\n</code></pre>\n<h1>6、遍历篮子（简单举例）</h1>\n<pre><code class=\"language-java\">    @Override\n    public List&lt;Person&gt; listPerson(String name) {\n        SearchHit[] hs = personDao.listPerson(name);\n        List&lt;Person&gt; personList = new ArrayList&lt;&gt;(64);\n        for (SearchHit searchHit : hs) {\n            Map&lt;String, Object&gt; hitMap = searchHit.getSourceAsMap();\n            Person person = new Person();\n            person.setId((Integer) hitMap.get(&quot;id&quot;));\n            person.setName((String) hitMap.get(&quot;name&quot;));\n            person.setAge((Integer) hitMap.get(&quot;age&quot;));\n            person.setIsNeighbourhood((String) hitMap.get(&quot;isNeighbourhood&quot;));\n            personList.add(person);\n        }\n        return personList;\n    }\n</code></pre>\n"},{"title":"ElasticSearch近实时性介绍","author":"郑天祺","date":"2020-07-15T02:51:00.000Z","_content":"\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","source":"_posts/ElasticSearch近实时性介绍.md","raw":"title: ElasticSearch近实时性介绍\nauthor: 郑天祺\ntags:\n  - es\ncategories:\n  - elasticsearch\ndate: 2020-07-15 10:51:00\n---\n\n​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。\n\n​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。\n\n​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\n\n\n图1、在内存缓冲区中包含了新文档的 Lucene 索引\n\n![image-20200715110330925](/img/es1.png)\n\nLucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n\n\n图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n![image-20200715110718600](/img/es2.png)\n\n​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 *refresh* 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 *近* 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。","slug":"ElasticSearch近实时性介绍","published":1,"updated":"2020-07-15T03:11:00.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapozp0001x8vhaqinletp","content":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tElasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。</p>\n<p>​\t\tElasticsearch 是一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据。</p>\n<p>​\t\tElasticsearch和磁盘之间是文件系统缓存，在内存索引缓冲区中的文档  会被写入到一个新的段中 ，但是这里新段会被先写入到文件系统缓存，这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n<p>图1、在内存缓冲区中包含了新文档的 Lucene 索引</p>\n<p><img src=\"/img/es1.png\" alt=\"image-20200715110330925\"></p>\n<p>Lucene 允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n<p>图2、缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</p>\n<p><img src=\"/img/es2.png\" alt=\"image-20200715110718600\"></p>\n<p>​\t\t在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n<p>​\t\t尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n"},{"title":"GET与POST区别","author":"郑天祺","date":"2020-07-21T00:29:00.000Z","_content":"\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","source":"_posts/GET与POST区别.md","raw":"title: GET与POST区别\nauthor: 郑天祺\ntags:\n  - GET/POST\ncategories:\n  - 网络\ndate: 2020-07-21 08:29:00\n\n---\n\n# 1、介绍\n\n​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。\n\n​\t先看看GET和POST的样貌：\n\n## GET请求\n\n```java\nGET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n```\n\nAccept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间\n\n## GET响应\n\n```java\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n```\n\n响应行 (协议/版本 状态码 状态码解析)\n\n响应头 （key/value格式）\n\n空行\n\n响应正文\n\n## POST请求\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&password=password\n```\n\nContent-Type 使用application/x-www-form-urlencoded\n\n转化为字节 -- 加上128 -- 转化为16进制 -- 添加%\n\n## POST响应\n\n```java\nPOST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"username\"\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=\"password\"\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n```\n\n# 2、W3C表格对比\n\n![image-20200721091103941](/img/GETPOST.png)\n\n## （1）表格对比：\n\n​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存\n\n​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。\n\n## （2）交互对比\n\n​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：\n\n​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）\n\n​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、\n\n# 3、面试问题\n\n## （1）GET方法的参数写法是固定的吗？\n\n​\t\t一般约定中我们都是把参数写在?后边，用&分割\n\n​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。\n\n​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。\n\n​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。\n\n## （2）GET 方法的长度限制是怎么回事？\n\n​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。\n\n​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。\n\n​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。\n\n## （3）POST 方法比 GET 方法安全？\n\n​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。\n\n​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）\n\n​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。\n\n参考：http://www.javanx.cn/20190227/get-post/\n\n## （4）POST 方法会产生两个 TCP 数据包？\n\n​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\n\n​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n\n​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。","slug":"GET与POST区别","published":1,"updated":"2020-07-21T01:30:43.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapozu0004x8vh6gaav85j","content":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2>GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间</p>\n<h2>GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2>POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 -- 加上128 -- 转化为16进制 -- 添加%</p>\n<h2>POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2>（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2>（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2>（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2>（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2>（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：http://www.javanx.cn/20190227/get-post/</p>\n<h2>（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、介绍</h1>\n<p>​\t\t最常用的利用GET和POST请求后端数据。GET和POST是HTTP与服务器交互的方式，交互方式还有DELETE、PUT、HEAD、OPTIONS、CONNECT等。</p>\n<p>​\t先看看GET和POST的样貌：</p>\n<h2>GET请求</h2>\n<pre><code class=\"language-java\">GET /empty_project/inde.jsp HTTP/1.1\n  Host: localhost:8088\n  Connection: keep-alive\n  Upgrade-Insecure-Requests: 1\n  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)       Chrome/55.0.2883.87 Safari/537.36\n  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n  Accept-Encoding: gzip, deflate, sdch, br\n  Accept-Language: zh-CN,zh;q=0.8\n  Cookie: pgv_pvi=4403687424\n</code></pre>\n<p>Accept 浏览器支持的类型\nAccept-Language 浏览器支持的语言\nAccept-Encoding 浏览器支持的压缩格式\nHost 请求的主机\nConnection keep-alive 这个是链接一小段时间</p>\n<h2>GET响应</h2>\n<pre><code class=\"language-java\">HTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: JSESSIONID=F463F5132A34573215C941893534BF26; Path=/empty_project; HttpOnly\nContent-Type: text/html;charset=utf-8\nContent-Length: 196\nDate: Mon, 02 Jan 2017 08:52:48 GMT\n</code></pre>\n<p>响应行 (协议/版本 状态码 状态码解析)</p>\n<p>响应头 （key/value格式）</p>\n<p>空行</p>\n<p>响应正文</p>\n<h2>POST请求</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 35\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424\n\nusername=username&amp;password=password\n</code></pre>\n<p>Content-Type 使用application/x-www-form-urlencoded</p>\n<p>转化为字节 -- 加上128 -- 转化为16进制 -- 添加%</p>\n<h2>POST响应</h2>\n<pre><code class=\"language-java\">POST /index.jsp HTTP/1.1\nHost: localhost:8088\nConnection: keep-alive\nContent-Length: 252\nCache-Control: max-age=0\nOrigin: http://localhost:8088\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundarySN8ehdkx6tF3Ngiq\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nReferer: http://localhost:8088/login.html\nAccept-Encoding: gzip, deflate, br\nAccept-Language: zh-CN,zh;q=0.8\nCookie: pgv_pvi=4403687424; JSESSIONID=061657A0C03921CB478ACB889502C93A\n\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;username&quot;\n\nsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq\nContent-Disposition: form-data; name=&quot;password&quot;\n\nsdfdsfsdfdsf\n------WebKitFormBoundarySN8ehdkx6tF3Ngiq--\n</code></pre>\n<h1>2、W3C表格对比</h1>\n<p><img src=\"/img/GETPOST.png\" alt=\"image-20200721091103941\"></p>\n<h2>（1）表格对比：</h2>\n<p>​\t\tGET 用于获取信息，是无副作用的，是幂等的，且可缓存\n​\t\tPOST 用于修改服务器上的数据，有副作用，非幂等，不可缓存</p>\n<p>​\t\t幂等性：是指无论调用多少次都不会有不同结果的一种特性，一般是指HTTP GET的查询方法。</p>\n<h2>（2）交互对比</h2>\n<p>​\t\tGET产生一个TCP数据包，POST产生两个TCP数据包：</p>\n<p>​\t\t对于GET方式的请求，游览器会把http header和data一并发送出去，服务器响应200（返回数据）</p>\n<p>​\t\t对于POST请求。游览器先发送header，服务器响应100 continue，游览器再发送data，服务器响应200 ok（返回数据）3、</p>\n<h1>3、面试问题</h1>\n<h2>（1）GET方法的参数写法是固定的吗？</h2>\n<p>​\t\t一般约定中我们都是把参数写在?后边，用&amp;分割</p>\n<p>​\t\t但是我们知道，解析报文的过程是通过获取 TCP 数据，用正则等工具从数据中获取 Header 和 Body，从而提取参数。</p>\n<p>​\t\t比如header请求头中添加token，来验证用户是否登录等权限问题。</p>\n<p>​\t\t也就是说，我们可以自己约定参数的写法，只要服务端能够解释出来就行，万变不离其宗。</p>\n<h2>（2）GET 方法的长度限制是怎么回事？</h2>\n<p>​\t\t网络上都会提到浏览器地址栏输入的参数是有限的。</p>\n<p>​\t\t首先说明一点，HTTP 协议没有 Body 和 URL 的长度限制，对 URL 限制的大多是浏览器和服务器的原因。</p>\n<p>​\t\t浏览器原因就不说了，服务器是因为处理长 URL 要消耗比较多的资源，为了性能和安全（防止恶意构造长 URL 来攻击）考虑，会给 URL 长度加限制。</p>\n<h2>（3）POST 方法比 GET 方法安全？</h2>\n<p>​\t\t有人说POST 比 GET 安全，因为数据在地址栏上不可见。</p>\n<p>​\t\t然而，从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文。（个人发现某60和某讯电脑管家，会将GET和POST请求数据包完整的上传到他们的服务器，解析后你提交的信息就会被破解。类似于中间人攻击也会导致泄露，不安全）</p>\n<p>​\t\t要想安全传输，就只有利用非对称加密，也就是 HTTPS。</p>\n<p>参考：http://www.javanx.cn/20190227/get-post/</p>\n<h2>（4）POST 方法会产生两个 TCP 数据包？</h2>\n<p>​\t\t上述文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。</p>\n<p>​\t\tHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。</p>\n<p>​\t\t所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</p>\n"},{"title":"HDFS概述","author":"郑天祺","date":"2019-12-16T02:10:00.000Z","_content":"\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","source":"_posts/HDFS概述.md","raw":"title: HDFS概述\nauthor: 郑天祺\ntags:\n\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 10:10:00\n\n---\n\n本文权威指南读书笔记\n\n# 一、HDFS的设计前提和目标\n\n​\t（1）存储大文件：HDFS支持GB级别大小的文件；\n\n​\t（2）流式数据访问：保证高吞吐量\n\n​\t（3）容错性：完善的冗余备份机制；\n\n​\t（4）简单的一致性模型：一次写入多次读取；\n\n​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；\n\n​\t（6）兼容各种硬件和软件平台。\n\n​\tHDFS不适合的场景：\n\n​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；\n\n​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；\n\n​\t（3）多用户写入：导致一致性维护困难。\n\n# 二、主要组件与架构\n\n​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode\n\n​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）\n\n​\tNameNode负责信息维护者，DateNode负责存取数据。\n\n​\t![image-20191216141048512](/img/hdfs.png)\n\n## (1) NameNode\n\n​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录\n\n​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。\n\n​\t\ta. 文件名目录名及它们之间的层级关系\n\n​\t\tb. 文件目录的所有者及其权限\n\n​\t\tc.每个文件块的名及文件有哪些块组成\n\n​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。\n\n​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要\n\n## (2) DataNode\n\n​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。\n\n​\t负责实际文件数据的保存于操作，与客户端直接交互。\n\n​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。\n\n​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。\n\n## （3）元信息的持久化\n\n​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。\n\n## （4）SecondaryNameNode\n\n​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。\n\n​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：\n\n![](/img/secondaryNameNode.jpg)\n\n​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：\n\n​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。\n\n​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。\n\n​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。\n\n​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并\n\n​\t使用的检查点：\n\n​\t\tfsimage：保存的是上个检查点的HDFS的元信息\n\n​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息\n\n​\t\tfstime：保存了最后一个检查点的时间戳\n\n# 三、数据备份\n\n​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。\n\n​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。\n\n​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。\n\n# 四、通信协议\n\n​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的\n\n​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。\n\n​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。\n\n​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。\n\n# 五、可靠性保证\n\n​\tHDFS可以允许DataNode失败。\n\n​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。\n\n​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。","slug":"HDFS概述","published":1,"updated":"2019-12-16T07:47:23.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapozv0005x8vhjhio2h8l","content":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2>(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2>(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2>（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2>（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文权威指南读书笔记</p>\n<h1>一、HDFS的设计前提和目标</h1>\n<p>​\t（1）存储大文件：HDFS支持GB级别大小的文件；</p>\n<p>​\t（2）流式数据访问：保证高吞吐量</p>\n<p>​\t（3）容错性：完善的冗余备份机制；</p>\n<p>​\t（4）简单的一致性模型：一次写入多次读取；</p>\n<p>​\t（5）移动计算优于移动数据：HDFS使应用计算移动到离他最近数据位置的接口；</p>\n<p>​\t（6）兼容各种硬件和软件平台。</p>\n<p>​\tHDFS不适合的场景：</p>\n<p>​\t（1）大量小文件：文件的元数据存储在NameNode内容中，大量小文件意味着元数据增加，会占用大量内存；</p>\n<p>​\t（2）低延迟数据访问：HDFS是专门针对吞吐量而不是用户低延迟；</p>\n<p>​\t（3）多用户写入：导致一致性维护困难。</p>\n<h1>二、主要组件与架构</h1>\n<p>​\t主要三个组件：NameNode、SecondaryNameNode 和 DataNode</p>\n<p>​\t（HDFS以主从模式运行，其中NameNode、SecondaryNameNode运行再Master节点，DataNode运行再Slave节点上）</p>\n<p>​\tNameNode负责信息维护者，DateNode负责存取数据。</p>\n<p>​\t<img src=\"/img/hdfs.png\" alt=\"image-20191216141048512\"></p>\n<h2>(1) NameNode</h2>\n<p>​\tNameNode管理着文件系统的命名空间 , 它维护文件系统树及树中的所有文件和目录</p>\n<p>​\tNameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。</p>\n<p>​\t\ta. 文件名目录名及它们之间的层级关系</p>\n<p>​\t\tb. 文件目录的所有者及其权限</p>\n<p>​\t\tc.每个文件块的名及文件有哪些块组成</p>\n<p>​\tNameNode启动时加载到内存中，元信息会保存各个块的名称及文件由哪些块组成。</p>\n<p>​\tNameNode占用大量内存和I/O资源，对Name容错机制也十分重要</p>\n<h2>(2) DataNode</h2>\n<p>​\tDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。</p>\n<p>​\t负责实际文件数据的保存于操作，与客户端直接交互。</p>\n<p>​\t例子：一条元信息记录会占用200B内存空间。 假设块大小为64MB，备份数量是3，那么一个1GB大小的文件将占用16<em>3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000</em>3=3000个文件块（多个文件不能放到一个块中）。</p>\n<p>​\t可以得出，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>\n<h2>（3）元信息的持久化</h2>\n<p>​\t在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个edits中，并且edits文件和fsimage文件会SecondaryNameNode周期性地合并。</p>\n<h2>（4）SecondaryNameNode</h2>\n<p>​\t在NameNode启动时，首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在内存中，同时为了防止数据丢失，这些操作又会不断的持久化到本地edits文件中。</p>\n<p>​\tedits文件的目的是为了提高系统的操作效率，NameNode在更新内存的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题：</p>\n<p><img src=\"/img/secondaryNameNode.jpg\" alt></p>\n<p>​\tSecondaryNameNode的角色就是定期合并edits和fsimage文件：</p>\n<p>​\ta、合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</p>\n<p>​\tb、SecondaryNameNode从NameNode请求fsimage和edits文件。</p>\n<p>​\tc、SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件。</p>\n<p>​\td、NameNode从SecondaryName获取合并好的新的fsimage并将旧的替换掉，并</p>\n<p>​\t使用的检查点：</p>\n<p>​\t\tfsimage：保存的是上个检查点的HDFS的元信息</p>\n<p>​\t\tedits：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</p>\n<p>​\t\tfstime：保存了最后一个检查点的时间戳</p>\n<h1>三、数据备份</h1>\n<p>​\tHDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其他所有数据块大小都是一样的，数据块的大小和备份银子都是可以配置的。</p>\n<p>​\tNmaeNode负责各个数据块的备份，DataNode会通过心跳的方式定期向NameNode发送自己节点上的Block报告，这个报告包含了DataNode节点上的所有数据块的列表。</p>\n<p>​\t写数据时候通过负载均衡，进行同步，但是会影响效率。当Hadoop的NameNode节点启动时，会进入安全模式。当副本数满足最小副本数，系统会退出安全模式。</p>\n<h1>四、通信协议</h1>\n<p>​\t所有的HDFS中的沟通协议都是基于TCP/IP协议的</p>\n<p>​\t（1）一个客户端通过指定的TCP端口与NameNode机器建立连接，并通过Client Protocol协议与NameNode交互。 NameNode只被动接受请求。</p>\n<p>​\t（2）DataNode则通过DataNode Protocol协议与NameNode进行沟通。</p>\n<p>​\t（3）HDFS的RPC对Client Protocol 和 DataNode Protocol做了封装。</p>\n<h1>五、可靠性保证</h1>\n<p>​\tHDFS可以允许DataNode失败。</p>\n<p>​\tDataNode会定期（默认3s）向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时NameNode把失败节点的数据备份到另一个健康的节点，这就保证了集群始终维持指定的副本数。</p>\n<p>​\tHDFS可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果不匹配，NameNode会重新备份损坏的数据块。</p>\n"},{"title":"Git梳理","author":"郑天祺","date":"2019-08-29T02:28:00.000Z","_content":"## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n        $ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： $ git clone http:/zzzzzzzz.git\n\n查看远程仓库：$ git remote -v\n\n添加远程仓库：$ git remote add [name] [url]\n\n删除远程仓库：$ git remote rm [name]\n\n修改远程仓库：$ git remote set-url --push       [name] [newUrl]\n\n拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支\n\n查看远程分支：$ git branch -r\n\n创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n\n切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：$ git checkout -b       [name]\n\n删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n\n合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并\n\n创建远程分支(本地分支push到远程)：$ git push origin [name]\n\n删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n    $git symbolic-ref HEAD refs/heads/[name]\n    $rm .git/index\n    $git clean -fdx\n\n####         c）版本(tag)操作相关命令\n\n查看版本：$ git tag\n\n创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：$ git tag -d [name]\n\n查看远程版本：$ git tag -r\n\n创建远程版本(本地版本push到远程)：$ git push origin [name]\n\n删除远程版本：$ git push origin       :refs/tags/[name]\n\n合并远程仓库的tag到本地：$ git pull origin --tags\n\n上传本地tag到远程仓库：$ git push origin --tags\n\n创建带注释的tag：$       git tag -a [name] -m 'yourMessage' \n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：$ git submodule add [url]       [path]\n\n​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n\n初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行\n\n更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下\n\n删除子模块：（分4步走） \n\n```java\n\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n```\n\n\n\n####         e）补充\n\n更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n\n删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'\n\n查看历史日志：$ git log\n\n还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：$ git commit    //       当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\npush -u [url]         \n\n####         f）忽略一些文件、文件夹不提交\n\n​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n​        IDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected","source":"_posts/Git梳理.md","raw":"title: Git梳理\ntags:\n  - git\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-29 10:28:00\n---\n## 1、Git介绍：\n\n​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点\n\n​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项\n\n​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到\n\n开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。\n\n## 2、为什么用GitLab？\n\n​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。\n\n## 3、Git 工作区、暂存区和版本库\n\n工作区：就是你在电脑里能看到的目录。\n\n暂存区：英文叫stage, 或index。一般存放在 \".git目录下\" 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\n\n## 4、Git工作流程\n\n###     1、一个分支\n\n克隆 Git 资源作为工作目录。\n\n在克隆的资源上添加或修改文件。\n\n如果其他人修改了，你可以更新资源。\n\n在提交前查看修改。\n\n提交修改。\n\n###     2、多个分支\n\nfork项目，建立自己的分支[name]（直接git网页操作）\n\n将master分支clone 下来（git clone）\n\n修改当前分支为fork 的分支（git      checkoout [name]）\n\n代码的修改(commit and push)\n\n如果需要合并到主分支：pull request merge\n\n![](\\img\\Git工作流程.png)\n\n## 5、Git配置及使用：\n\n###     1）配置用户信息\n\n​    配置个人的用户名称和电子邮件地址：\n\n​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）\n\n​        a）设置Git端上的用户名和用户邮箱（公司邮箱）\n\n```java\n$ git config --global user.name \"yourname\"\n$ git config --global user.email       \"yourname@xxxx.com\"\n```\n\n​        b）生成ssh公钥和私钥\n\n```java\n$ ssh -keygen -t rsa -C       \"yourname@xxxx.com\"\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -> Profile Settings -> SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n```\n\n\n\n###     2）查看配置\n\n```java\n        $ git config --list \n```\n\n###     3）创建仓库\n\n​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。 \n\n 例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下 \n\n​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。 \n\n​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制， \n\n​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。 \n\n```java\n例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m \"初始化项目版本\"   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n```\n\n###     4）基本操作\n\n####         a）远程仓库相关命令 \n\n检出仓库： $ git clone http:/zzzzzzzz.git\n\n查看远程仓库：$ git remote -v\n\n添加远程仓库：$ git remote add [name] [url]\n\n删除远程仓库：$ git remote rm [name]\n\n修改远程仓库：$ git remote set-url --push       [name] [newUrl]\n\n拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝\n\n推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中\n\n如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：\n\n```java\n$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n```\n\n####         b）分支(branch)操作相关命令 \n\n查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支\n\n查看远程分支：$ git branch -r\n\n创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支\n\n切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。\n\n创建新分支并立即切换到新分支：$ git checkout -b       [name]\n\n删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项\n\n合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并\n\n创建远程分支(本地分支push到远程)：$ git push origin [name]\n\n删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name] \n\n创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n    $git symbolic-ref HEAD refs/heads/[name]\n    $rm .git/index\n    $git clean -fdx\n\n####         c）版本(tag)操作相关命令\n\n查看版本：$ git tag\n\n创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了\n\n删除版本：$ git tag -d [name]\n\n查看远程版本：$ git tag -r\n\n创建远程版本(本地版本push到远程)：$ git push origin [name]\n\n删除远程版本：$ git push origin       :refs/tags/[name]\n\n合并远程仓库的tag到本地：$ git pull origin --tags\n\n上传本地tag到远程仓库：$ git push origin --tags\n\n创建带注释的tag：$       git tag -a [name] -m 'yourMessage' \n\n####         d）子模块(submodule)相关操作命令\n\n添加子模块：$ git submodule add [url]       [path]\n\n​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs\n\n初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行\n\n更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下\n\n删除子模块：（分4步走） \n\n```java\n\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n```\n\n\n\n####         e）补充\n\n更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中\n\n删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'\n\n查看历史日志：$ git log\n\n还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值\n\n提交：$ git commit    //       当前工作空间的修改内容\n\n强制pull \n\n```java\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\n强制push       \n\npush -u [url]         \n\n####         f）忽略一些文件、文件夹不提交\n\n​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如\n\n```java\ntarget\nbin\n*.db\n```\n\n###     5)解决冲突\n\n​        IDEA  ->   VCS  -> git  ->  Branches  ->   选中需要合并的远程分支  - >  Rebase current onto selected","slug":"Git梳理","published":1,"updated":"2019-10-15T12:21:32.785Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapozx0006x8vh4p6nezja","content":"<h2>1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2>2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2>3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 &quot;.git目录下&quot; 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2>4、Git工作流程</h2>\n<h3>1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3>2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt></p>\n<h2>5、Git配置及使用：</h2>\n<h3>1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3>2）查看配置</h3>\n<pre><code class=\"language-java\">        $ git config --list \n</code></pre>\n<h3>3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3>4）基本操作</h3>\n<h4>a）远程仓库相关命令</h4>\n<p>检出仓库： $ git clone http:/zzzzzzzz.git</p>\n<p>查看远程仓库：$ git remote -v</p>\n<p>添加远程仓库：$ git remote add [name] [url]</p>\n<p>删除远程仓库：$ git remote rm [name]</p>\n<p>修改远程仓库：$ git remote set-url --push       [name] [newUrl]</p>\n<p>拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4>b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支</p>\n<p>查看远程分支：$ git branch -r</p>\n<p>创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支</p>\n<p>切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：$ git checkout -b       [name]</p>\n<p>删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</p>\n<p>合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并</p>\n<p>创建远程分支(本地分支push到远程)：$ git push origin [name]</p>\n<p>删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name]</p>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx</p>\n<h4>c）版本(tag)操作相关命令</h4>\n<p>查看版本：$ git tag</p>\n<p>创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：$ git tag -d [name]</p>\n<p>查看远程版本：$ git tag -r</p>\n<p>创建远程版本(本地版本push到远程)：$ git push origin [name]</p>\n<p>删除远程版本：$ git push origin       :refs/tags/[name]</p>\n<p>合并远程仓库的tag到本地：$ git pull origin --tags</p>\n<p>上传本地tag到远程仓库：$ git push origin --tags</p>\n<p>创建带注释的tag：$       git tag -a [name] -m 'yourMessage'</p>\n<h4>d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：$ git submodule add [url]       [path]</p>\n<p>​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>\n<p>初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行</p>\n<p>更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下</p>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n</code></pre>\n<h4>e）补充</h4>\n<p>更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中</p>\n<p>删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'</p>\n<p>查看历史日志：$ git log</p>\n<p>还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值</p>\n<p>提交：$ git commit    //       当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<p>push -u [url]</p>\n<h4>f）忽略一些文件、文件夹不提交</h4>\n<p>​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3>5)解决冲突</h3>\n<p>​        IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、Git介绍：</h2>\n<p>​\tGit是目前世界上最先进的分布式版本控制系统。gitlab是公司搭建的代码版本控制平台，使用方法与github类似，项目负责人在gitlab上新建一个项目，并分享URL给开发人员。开发人员在负责人的gitlab项目页面上点</p>\n<p>​\t击“fork”按钮，将此项目fork到自己的gitlab上，这相当于是从负责人那拷贝了一份项目副本，无论开发人员如何修改代码都不会影响负责人那master分支上的代码。然后开发人员可以根据自己的项目分工，像对待普通项</p>\n<p>​\t目一样做clone、add、commit、push等操作。如果开发人员人为一个小模块做好了，可以点击“pull request”按钮，向负责人发送代码合并请求，要合并的代码文件也会以列表的形式同时发送给负责人，此时负责人会看到</p>\n<p>开发人员的请求，经审核如果代码没问题则会合并模块，并向开发人员发送确认合并的通知。</p>\n<h2>2、为什么用GitLab？</h2>\n<p>​\t清晰的项目管理和责任明确\n​\t清晰的看到产品迭代，为产品研发提供参考\n​\t能够形成项目管理课程，为我们的后续产品做准备，同时课程设计过程完全公开，降低产品和运营不匹配的问题。</p>\n<h2>3、Git 工作区、暂存区和版本库</h2>\n<p>工作区：就是你在电脑里能看到的目录。</p>\n<p>暂存区：英文叫stage, 或index。一般存放在 &quot;.git目录下&quot; 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</p>\n<p>版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。</p>\n<h2>4、Git工作流程</h2>\n<h3>1、一个分支</h3>\n<p>克隆 Git 资源作为工作目录。</p>\n<p>在克隆的资源上添加或修改文件。</p>\n<p>如果其他人修改了，你可以更新资源。</p>\n<p>在提交前查看修改。</p>\n<p>提交修改。</p>\n<h3>2、多个分支</h3>\n<p>fork项目，建立自己的分支[name]（直接git网页操作）</p>\n<p>将master分支clone 下来（git clone）</p>\n<p>修改当前分支为fork 的分支（git      checkoout [name]）</p>\n<p>代码的修改(commit and push)</p>\n<p>如果需要合并到主分支：pull request merge</p>\n<p><img src=\"%5Cimg%5CGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png\" alt></p>\n<h2>5、Git配置及使用：</h2>\n<h3>1）配置用户信息</h3>\n<p>​    配置个人的用户名称和电子邮件地址：</p>\n<p>​    右键打开git bash命令行（如果设置了git的系统环境变量，就可以直接使用cmd命令行进行git操作）</p>\n<p>​        a）设置Git端上的用户名和用户邮箱（公司邮箱）</p>\n<pre><code class=\"language-java\">$ git config --global user.name &quot;yourname&quot;\n$ git config --global user.email       &quot;yourname@xxxx.com&quot;\n</code></pre>\n<p>​        b）生成ssh公钥和私钥</p>\n<pre><code class=\"language-java\">$ ssh -keygen -t rsa -C       &quot;yourname@xxxx.com&quot;\n一路回车\nC:/Users/admin/.ssh会生成一个id_rsa.pub公钥文件\nword打开id_rsa.pub将公钥添加进GitLab -&gt; Profile Settings -&gt; SSH Keys\n添加成功钉邮中会收到SSH key was added to your account邮件\n</code></pre>\n<h3>2）查看配置</h3>\n<pre><code class=\"language-java\">        $ git config --list \n</code></pre>\n<h3>3）创建仓库</h3>\n<p>​        a）$ git clone：这是一种较为简单的初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份。</p>\n<p>例：$ git  clone  http://zzzzz.git   // 'http://zzzzzz.git'  这个URL地址的远程版本库，完全克隆到本地demo目录下</p>\n<p>​\t注：git clone 时，可以所用不同的协议，包括 ssh, git, https 等，其中最常用的是 ssh，因为速度较快，还可以配置公钥免输入密码。</p>\n<p>​        b）$ git init 和 $  git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化；Git以后就会对该目录下的文件进行版本控制，</p>\n<p>​\t这时候如果你需要将它放到远程服务器上，可以在远程服务器gitlab上创建一个目录，并把可访问的URL记录下来，此时你就可以利用'git remote add'命令来增加一个远程服务器端。</p>\n<pre><code class=\"language-java\">例：\n$ git init   // 该命令执行完后会在当前目录生成一个 .git 目录。 \n$ git add .   // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步 \n$ touch README.md   // 初始化一个README.md文件 \n$ git commit -m &quot;初始化项目版本&quot;   // 提交当前工作空间的修改内容 \n$ git remote add origin  git@git.gag.cn:yourname/demo.git   // 关联远程仓库 \n$ git push -u origin master   // 将操作提交到gitlab \n$ git log   // 查看历史日志 \n</code></pre>\n<h3>4）基本操作</h3>\n<h4>a）远程仓库相关命令</h4>\n<p>检出仓库： $ git clone http:/zzzzzzzz.git</p>\n<p>查看远程仓库：$ git remote -v</p>\n<p>添加远程仓库：$ git remote add [name] [url]</p>\n<p>删除远程仓库：$ git remote rm [name]</p>\n<p>修改远程仓库：$ git remote set-url --push       [name] [newUrl]</p>\n<p>拉取远程仓库：$ git pull [remoteName]       [localBranchName]   // 从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：'git pull origin master'就是将origin这个版本库的代码更新到本地的master主枝</p>\n<p>推送远程仓库：$ git push [remoteName]       [localBranchName]    // 将本地commit的代码更新到远程版本库中，例如'git push origin'就会将本地的代码更新到名为orgin的远程版本库中</p>\n<p>如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：</p>\n<pre><code class=\"language-java\">$git push origin test:master    // 提交本地test分支作为远程的master分支 \n\n$git push origin test:test    // 提交本地test分支作为远程的test分支 \n</code></pre>\n<h4>b）分支(branch)操作相关命令</h4>\n<p>查看本地分支：$ git branch          // 列出本地所有的分支 对分支的增、删、查等操作，例如'git branch       new_branch'会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch'就会强制删除叫做new_branch的分支</p>\n<p>查看远程分支：$ git branch -r</p>\n<p>创建本地分支：$ git branch [name] ----注意新分支创建后不会自动切换为当前分支</p>\n<p>切换分支：$ git checkout [name]    // Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如'git checkout       new_branch'就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如'git checkout app/model/user.rb'就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚。</p>\n<p>创建新分支并立即切换到新分支：$ git checkout -b       [name]</p>\n<p>删除分支：$ git branch -d [name] ---- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</p>\n<p>合并分支：$ git merge [name] ----将名称为[name]的分支与当前分支合并</p>\n<p>创建远程分支(本地分支push到远程)：$ git push origin [name]</p>\n<p>删除远程分支：$ git push origin       :heads/[name] 或 $ gitpush origin :[name]</p>\n<p>创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净)\n$git symbolic-ref HEAD refs/heads/[name]\n$rm .git/index\n$git clean -fdx</p>\n<h4>c）版本(tag)操作相关命令</h4>\n<p>查看版本：$ git tag</p>\n<p>创建版本：$ git tag [name]          // 可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用'git tag revert_version       bbaf6fb5060b4875b18ff9ff637ce118256d6f20'来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p>\n<p>删除版本：$ git tag -d [name]</p>\n<p>查看远程版本：$ git tag -r</p>\n<p>创建远程版本(本地版本push到远程)：$ git push origin [name]</p>\n<p>删除远程版本：$ git push origin       :refs/tags/[name]</p>\n<p>合并远程仓库的tag到本地：$ git pull origin --tags</p>\n<p>上传本地tag到远程仓库：$ git push origin --tags</p>\n<p>创建带注释的tag：$       git tag -a [name] -m 'yourMessage'</p>\n<h4>d）子模块(submodule)相关操作命令</h4>\n<p>添加子模块：$ git submodule add [url]       [path]</p>\n<p>​                如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>\n<p>初始化子模块：$ git submodule init        ----只在首次检出仓库时运行一次就行</p>\n<p>更新子模块：$ git submodule update ----每次更新或切换分支后都需要运行一下</p>\n<p>删除子模块：（分4步走）</p>\n<pre><code class=\"language-java\">\t1)$ git rm --cached [path]\n\n\t2)编辑“.gitmodules”文件，将子模块的相关配置节点删除掉\n\n\t3)编辑“ .git/config”文件，将子模块的相关配置节点删除掉\n\n\t4)手动删除子模块残留的目录\n</code></pre>\n<h4>e）补充</h4>\n<p>更改或者新增的文件：$ git add          // 是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如'git       add app/model/user.rb'就会增加app/model/user.rb文件到Git的索引中</p>\n<p>删除文件：$ git rm    // 从当前的工作空间中和索引中删除文件，例如'git rm app/model/user.rb'</p>\n<p>查看历史日志：$ git log</p>\n<p>还原：$ git revert          // 还原一个版本的修改，必须提供一个具体的Git版本号，例如'git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20'，Git的版本号都是生成的一个哈希值</p>\n<p>提交：$ git commit    //       当前工作空间的修改内容</p>\n<p>强制pull</p>\n<pre><code class=\"language-java\">git fetch --all \ngit reset --hard origin/master\ngit pull\n</code></pre>\n<p>强制push</p>\n<p>push -u [url]</p>\n<h4>f）忽略一些文件、文件夹不提交</h4>\n<p>​              在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如</p>\n<pre><code class=\"language-java\">target\nbin\n*.db\n</code></pre>\n<h3>5)解决冲突</h3>\n<p>​        IDEA  -&gt;   VCS  -&gt; git  -&gt;  Branches  -&gt;   选中需要合并的远程分支  - &gt;  Rebase current onto selected</p>\n"},{"title":"HiveQL视图","author":"郑天祺","date":"2020-01-20T08:27:00.000Z","_content":"\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","source":"_posts/HiveQL视图.md","raw":"title: HiveQL视图\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 16:27:00\n\n---\n\n​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。\n\n# 一、使用视图来降低查询复杂度\n\n​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。\n\n例如：\n\n改进前：Hive 查询语句中含有多层嵌套\n\n```java\nFROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n```\n\n改进后：利用视图进行查询\n\n```java\nCREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n```\n\n# 二、使用视图来限制基于条件过滤的数据\n\n​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：\n\n```java\nhive> CREATE TABLE userinfo(\n\t> firstname string, lastname string, ssn string, password string);\n\nhive> CREATE VIEW safer_user_info AS\n    > SELECT firstname, lastname FROM userinfo;\n```\n\n​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。\n\n# 三、 动态分区中的视图和 map 类型\n\n​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。\n\n​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。\n\n​\t\t视图如下：三个字段作为key, 视图名为 orders\n\n```java\nCREATE VIEW orders(state, city, part) AS SELECT cols[\"state\"], cols[\"city\"], cols[\"part\"]\nFROM dynamicatable WHERE cols[\"type\"] = \"request\";\n```\n\n","slug":"HiveQL视图","published":1,"updated":"2020-01-21T02:58:33.416Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp010009x8vh9yfoaxvh","content":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\t视图可以允许保存一个查询（并）像对待表一样对这个查询进行操作。（这是一个逻辑结构，因为它不像一个表会存储数据。</p>\n<h1>一、使用视图来降低查询复杂度</h1>\n<p>​\t当查询长且复杂，通过使用视图将这个查询语句分割成多个小的、更可控的片段可以降低这种复杂度。</p>\n<p>例如：</p>\n<p>改进前：Hive 查询语句中含有多层嵌套</p>\n<pre><code class=\"language-java\">FROM(\n\tSELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n) a SELECT a.lastname WHERE a.id=3;\n</code></pre>\n<p>改进后：利用视图进行查询</p>\n<pre><code class=\"language-java\">CREATE VIEW shorter_join AS SELECT * FROM people JOIN cart ON (cart.people_id=people.id) WHERE firstname='john'\n// 这样就可以像操作表一样来操作这个视图了，简化了查询语句\nSELECT lastname FROM shorter_join WHERE id=3;\n</code></pre>\n<h1>二、使用视图来限制基于条件过滤的数据</h1>\n<p>​\t\t基于一个 或者 多个列的值 来限制 输出结果。可以通过创建视图来限制数据访问，可以用来保护信息不被随意查询：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE userinfo(\n\t&gt; firstname string, lastname string, ssn string, password string);\n\nhive&gt; CREATE VIEW safer_user_info AS\n    &gt; SELECT firstname, lastname FROM userinfo;\n</code></pre>\n<p>​\t\tHive目前不支持的功能：有的数据库，允许将视图作为一个安全机制，也就是不给用户直接访问具有敏感数据的原始表，而是提供给用户一个通过WHERE子句限制了的视图，以供访问。Hive中用户必须能够访问整个底层的原始表的权限，视图才能工作。</p>\n<h1>三、 动态分区中的视图和 map 类型</h1>\n<p>​\t\tHive 支持 array、map 和 struct 数据类型，这些数据类型在传统的数据库中并不常见，因为他们破坏了第一范式。</p>\n<p>​\t\tHive 可将一整行文本作为一个 map，加上视图功能，就允许用户可以基于同一个物理表构建多个逻辑表。</p>\n<p>​\t\t视图如下：三个字段作为key, 视图名为 orders</p>\n<pre><code class=\"language-java\">CREATE VIEW orders(state, city, part) AS SELECT cols[&quot;state&quot;], cols[&quot;city&quot;], cols[&quot;part&quot;]\nFROM dynamicatable WHERE cols[&quot;type&quot;] = &quot;request&quot;;\n</code></pre>\n"},{"title":"Hive数据操作（1）","author":"郑天祺","date":"2020-01-17T08:11:00.000Z","_content":"\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","source":"_posts/Hive数据操作.md","raw":"title: Hive数据操作（1）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 16:11:00\n\n---\n\n# 一、加载数据\t\t\n\nHive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。\n\n```java\n// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n```\n\n​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。\n\n# 二、设置分区\n\n![image-20200119115936949](/img/hive-partition.png)\n\n上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。\n\n设置分区的方式\n\n```java\nhive> set hive.exec.dynamic.partition=true;\nhive> set hive.exec.dynamic.partition.mode=nonstrict;\nhive> set hive.exec.max.dynamic.partitions.pernode=1000;\nhive> INSERT OVERWRITE TABLE employees\n\t> PARTITION (country, state)\n\t> PARTITION ..., se.cty, se.st\n\t> FROM staged_employees se;\n```\n\n# 三、单个查询语句中创建表并加载数据\n\n```java\nCREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n```\n\n# 四、导出数据\n\n```java\n//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n```\n\n# 五、HiveQL 查询\n\nSELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；\n\nFROM子句标识了从哪个表、试图或嵌套查询中选择记录。\n\n```java\n// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[\"State Taxes\"] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n```\n\n（1）Hive支持的算数运算符\n\n![image-20200119143105348](/img/hive-算数运算符.png)\n\n（2）Hive 内置数学函数\n\n![image-20200119143246489](/img/hive-数学函数.png)\n\n![image-20200119143332168](/img/hive运算1.png)\n\n![image-20200119143405105](/img/Hive-运算2.png)\n\n![image-20200119143501626](/img/hive-运算3.png)\n\n（3）Hive聚合函数\n\n最有名的是count avg\n\n![image-20200119143648157](/img/hive-聚合1.png)\n\n![image-20200119143701027](/img/hive-聚合2.png)\n\n![image-20200119143709165](/img/hive-聚合3.png)\n\n```java\n// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive> SET hive.map.aggr=true;   \nhive> SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive> SELECT count(DISTINCT symbol) FROM stocks;\n```\n\n# 六、表生成函数\n\n与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句\n\n例子：\n\n```java\nSELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n```\n\n![image-20200119145321380](/img/hive-表生成函数.png)\n\n# 七、其他内置函数\n\n有很多，关于时间的和关于字符串的。\n\n# 八、LIMIT 句式\n\nLIMIT子句勇于限制返回的行数。\n\n```java\n// 下面只返回两行\nhive> SELECT upper(name), salary, deductions[\"Federal Taxes\"],\n> round(salary * (1 - deductions[\"Federal Taxes\"])) FROM employees \n> LIMIT 2;\n```\n\n# 九、CASE ... WHEN ... THEN句式\n\n和if条件语句类似，用于处理单个列的查询结果。\n\n```java\nhive> SELECT name, salary,\n> CASE\n> WHEN salary  <  50000.0 THEN 'low'\n> WHEN salary  >=  50000.0  AND  salary  < 70000.0  THEN  'middle'\n> WHEN  salary  >=  70000.0 AND  salary  <  100000.0  THEN 'high'\n> ELSE  'very high'\n> END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n```\n\n# 十、LIKE和RLIKE\n\nRLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。\n\n```java\n// LIKE\nhive> SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive> SELECT name, address.street\n    > FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n```\n\n# 十一、GROUP BY\n\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n例如：\n\n```java\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive> SELECT year(ymd), avg(price_close) FROM stocks \n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    >GROUP BY year(ymd);\n\t>HAVING avg(price_close) > 50.0\n// 等价于下边嵌套查询\nhive> SELECT s2.year, s2.avg FROM\n    >(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    >WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    >GROUP BY year(yml)) s2\n    >WHERE s2.avg > 50.0\n\n1987    53.88923482352342\n...\n```\n\n","slug":"Hive数据操作","published":1,"updated":"2020-01-19T07:49:41.742Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp02000ax8vhht7vvui7","content":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE ... WHEN ... THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、加载数据</h1>\n<p>Hive 没有行级别的数据插入、数据更新和删除操作，那么网表中装载数据的唯一途径就是使用一种 “ 大量 ” 的数据装载操作。或者通过其他方式仅仅将文件写入到正确的目录下。</p>\n<pre><code class=\"language-java\">// OVERWRITE关键字换成INTO关键字的话，Hive将会以追加的方式写入数据而不会覆盖之前已经存在的内容\nLOAD DATA LOCAL INPATH '${env:HOME}/california-employees'  \nOVERWRITE INTO TABLE employees\n// 非分区表省略此行\nPARTITION (country = 'US', state = 'CA')  \n</code></pre>\n<p>​\t\t如果分区目录不存在的话，会先创建分区目录，然后再将数据拷贝到该目录下。</p>\n<h1>二、设置分区</h1>\n<p><img src=\"/img/hive-partition.png\" alt=\"image-20200119115936949\"></p>\n<p>上表为动态分区属性，如果不小心按照秒分区，每秒建立一个分区，则十分浪费资源，设置hive.exec.max.dynamic.partitions可以创建最大动态分区个数，如果超过这个值就会抛出一个致命错误。</p>\n<p>设置分区的方式</p>\n<pre><code class=\"language-java\">hive&gt; set hive.exec.dynamic.partition=true;\nhive&gt; set hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;\nhive&gt; INSERT OVERWRITE TABLE employees\n\t&gt; PARTITION (country, state)\n\t&gt; PARTITION ..., se.cty, se.st\n\t&gt; FROM staged_employees se;\n</code></pre>\n<h1>三、单个查询语句中创建表并加载数据</h1>\n<pre><code class=\"language-java\">CREATE TABLE ca_employees\nAS SELECT name, salary, address\nFROM employees WHERE se.state = 'CA';\n</code></pre>\n<h1>四、导出数据</h1>\n<pre><code class=\"language-java\">//（1）直接拷贝文件夹\nhadoop fs -cp source_path DIRECTORY '/tmp/ca_employees'\n    \n//（2）或者用INSERT ... DICTORY ...,\n// 也可以写成全路径 hdfs://master-server/tmp/ca_employees\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/ca_employees' \nSELECT name, salary, adress \nFROM employees \nWHERE se.state = 'CA';\n</code></pre>\n<h1>五、HiveQL 查询</h1>\n<p>SELECT是SQL中的映射算子，指定了要保存的列以输出函数需要调用的一个或多个列；</p>\n<p>FROM子句标识了从哪个表、试图或嵌套查询中选择记录。</p>\n<pre><code class=\"language-java\">// 查询ARRAY的第一个元素\nSELECT name, subordinates[0] FROM employees;\n// 查询键值\nSELECT name, deductions[&quot;State Taxes&quot;] FROM employees;\n// 查询一个元素，也可以用 ‘点’\nSELECT name, address.city FROM employees;\n</code></pre>\n<p>（1）Hive支持的算数运算符</p>\n<p><img src=\"/img/hive-%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.png\" alt=\"image-20200119143105348\"></p>\n<p>（2）Hive 内置数学函数</p>\n<p><img src=\"/img/hive-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119143246489\"></p>\n<p><img src=\"/img/hive%E8%BF%90%E7%AE%971.png\" alt=\"image-20200119143332168\"></p>\n<p><img src=\"/img/Hive-%E8%BF%90%E7%AE%972.png\" alt=\"image-20200119143405105\"></p>\n<p><img src=\"/img/hive-%E8%BF%90%E7%AE%973.png\" alt=\"image-20200119143501626\"></p>\n<p>（3）Hive聚合函数</p>\n<p>最有名的是count avg</p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%881.png\" alt=\"image-20200119143648157\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%882.png\" alt=\"image-20200119143701027\"></p>\n<p><img src=\"/img/hive-%E8%81%9A%E5%90%883.png\" alt=\"image-20200119143709165\"></p>\n<pre><code class=\"language-java\">// 下边设置可以调高聚合的性能,这个设置会触发map阶段进行“顶级”聚合过秤，非顶级将会在执行一个GROUP BY后进行，不过这个设置会需要更多的内存。\nhive&gt; SET hive.map.aggr=true;   \nhive&gt; SELECT count(*), avg(salary) FROM employees;\n\n// 多个函数排重后的孤僻交易码个数\nhive&gt; SELECT count(DISTINCT symbol) FROM stocks;\n</code></pre>\n<h1>六、表生成函数</h1>\n<p>与聚合函数“相反的”一类函数就是表生成函数，其可以将单列扩展成多列或者多行。例如 AS 语句</p>\n<p>例子：</p>\n<pre><code class=\"language-java\">SELECT parse_url_tuple(url, 'HOST', 'PATH', 'QUERY') AS (host, path, query) FROM url_table;\n</code></pre>\n<p><img src=\"/img/hive-%E8%A1%A8%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0.png\" alt=\"image-20200119145321380\"></p>\n<h1>七、其他内置函数</h1>\n<p>有很多，关于时间的和关于字符串的。</p>\n<h1>八、LIMIT 句式</h1>\n<p>LIMIT子句勇于限制返回的行数。</p>\n<pre><code class=\"language-java\">// 下面只返回两行\nhive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],\n&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees \n&gt; LIMIT 2;\n</code></pre>\n<h1>九、CASE ... WHEN ... THEN句式</h1>\n<p>和if条件语句类似，用于处理单个列的查询结果。</p>\n<pre><code class=\"language-java\">hive&gt; SELECT name, salary,\n&gt; CASE\n&gt; WHEN salary  &lt;  50000.0 THEN 'low'\n&gt; WHEN salary  &gt;=  50000.0  AND  salary  &lt; 70000.0  THEN  'middle'\n&gt; WHEN  salary  &gt;=  70000.0 AND  salary  &lt;  100000.0  THEN 'high'\n&gt; ELSE  'very high'\n&gt; END AS bracket FROM employees;\n\n//返回结果\nJohn Doe   100000.0  veryhigh\nMary Smith 80000.0 high\n...\n</code></pre>\n<h1>十、LIKE和RLIKE</h1>\n<p>RLIKE 是 Hive 功能的拓展，可以通过 Java 的正则表达式来指定匹配条件。</p>\n<pre><code class=\"language-java\">// LIKE\nhive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.'\nJohn Doe   1  Michigan Ave.\nTodd Jones 200 Chicago Ave.\n...\n    \n// RLIKE  后加正则表达式\n//（参照Tony Stubbleine《正则表达式参考手册》、JanGoyvaerts和Tony Stubbleine（O' Reilly）所著的《正则表达式参考手册》）\n// '.'表示和任意的字符匹配\n// '*'表示重复“左边的字符串”零次到无数次\n// '|'表示和x或者y匹配 \nhive&gt; SELECT name, address.street\n    &gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';\nMary Smith 100 Ontario St.\nTodd Jones 200 Chicago Ave.\n</code></pre>\n<h1>十一、GROUP BY</h1>\n<p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\n1984   25.123142341341\n1985   20.123145234131\n...\n\n// 有时候会用HAVING子句来补充条件查询\nhive&gt; SELECT year(ymd), avg(price_close) FROM stocks \n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' \n    &gt;GROUP BY year(ymd);\n\t&gt;HAVING avg(price_close) &gt; 50.0\n// 等价于下边嵌套查询\nhive&gt; SELECT s2.year, s2.avg FROM\n    &gt;(SELECT year(ymd) AS year, avg(price_close) AS avg FROM stocks)\n    &gt;WHERE exchange = 'NASDAQ' AND symbol = 'AAPL'\n    &gt;GROUP BY year(yml)) s2\n    &gt;WHERE s2.avg &gt; 50.0\n\n1987    53.88923482352342\n...\n</code></pre>\n"},{"title":"HDFS文件操作","author":"郑天祺","date":"2019-12-16T07:47:00.000Z","_content":"\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","source":"_posts/HDFS文件操作.md","raw":"title: HDFS文件操作\nauthor: 郑天祺\ntags:\n  - HDFS\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 15:47:00\n\n---\n\n# \t一、读文件\n\n​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。\n\n​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。\n\n​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。\n\n​\t具体过程：\n\n​\t![image-20191216155358635](/img/hdfs-read-file.png)\n\n​\t（1）客户端发起请求\n\n​\t（2）客户端与NameNode得到文件的块及位置信息列表\n\n​\t（3）客户端直接和DataNode交互读取数据\n\n​\t（4）读取完成关闭连接\n\n​\t这样设计的巧妙之处有：\n\n​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。\n\n​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。\n\n​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：\n\n​\ta、同一个节点中的进程\n\n​\tb、同一机架上的不同节点\n\n​\tc、同一数据中心不同机架\n\n​\td、不同数据中心的节点\n\n例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1\n\n​\ta、distance(d1/r1/n1,d1/r1/n1)=0;\n\n​\tb、distance(d1/r1/n1,d1/r1/n2)=2;\n\n​\tc、distance(d1/r1/n1,d1/r2/n3)=4;\n\n​\td、distance(d1/r1/n1,d2/r3/n4)=6; \n\n# 二、写文件\n\nHDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。\n\nDFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。\n\n在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。\n\n若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；\n\n若不通过会向客户端抛出IOException。\n\n![image-20191216163905988](/img/hdfs-write-file.png)\n\n（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。\n\n（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。\n\n（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。\n\n（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。\n\n（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类[控制字符](https://baike.baidu.com/item/控制字符/6913704)。表示发来的数据已确认接收无误。）\n\n（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。\n\n（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）\n\n（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。\n\n解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。\n\n# 三、删除文件\n\nHADOOP\t删除文件三部曲\n\n（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）\n\n​\t（在这个期间文件可以恢复）；\n\n（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；\n\n（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。\n\n# 四、修改文件\n\n想啥呢?\n\n","slug":"HDFS文件操作","published":1,"updated":"2019-12-16T09:04:21.726Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp06000ex8vhbs8efl6f","content":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\" target=\"_blank\" rel=\"noopener\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、读文件</h1>\n<p>​\tHDFS有一个文件系统实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。</p>\n<p>​\tHDFS通过RPC调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回该块副本DataNode的节点地址。</p>\n<p>​\t另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就储存在客户端所在的节点上。</p>\n<p>​\t具体过程：</p>\n<p>​\t<img src=\"/img/hdfs-read-file.png\" alt=\"image-20191216155358635\"></p>\n<p>​\t（1）客户端发起请求</p>\n<p>​\t（2）客户端与NameNode得到文件的块及位置信息列表</p>\n<p>​\t（3）客户端直接和DataNode交互读取数据</p>\n<p>​\t（4）读取完成关闭连接</p>\n<p>​\t这样设计的巧妙之处有：</p>\n<p>​\t（1）在运行MapReduce任务时，每个客户端就是一个DataNode节点。</p>\n<p>​\t（2）NameNode 仅需要相应块的位置信息请求，否则随着客户端的增加，NameNode会很快成为瓶颈。</p>\n<p>​\tHadoop的网络拓扑。在海量数据处理过程中，主要限制因素时节点之间的带宽。衡量两个节点之间的带宽往往很难实现，在这里Hadoop采取了一个简单的方法，它把网络拓扑看成一棵树，两个节点的距离等于他们到最近共同祖先距离的综合，而树的层次可以这么划分：</p>\n<p>​\ta、同一个节点中的进程</p>\n<p>​\tb、同一机架上的不同节点</p>\n<p>​\tc、同一数据中心不同机架</p>\n<p>​\td、不同数据中心的节点</p>\n<p>例如：数据中心d1中有一个机架r1中一个节点n1表示为d1/r1/n1</p>\n<p>​\ta、distance(d1/r1/n1,d1/r1/n1)=0;</p>\n<p>​\tb、distance(d1/r1/n1,d1/r1/n2)=2;</p>\n<p>​\tc、distance(d1/r1/n1,d1/r2/n3)=4;</p>\n<p>​\td、distance(d1/r1/n1,d2/r3/n4)=6;</p>\n<h1>二、写文件</h1>\n<p>HDFS有一个分布式系统，客户端通过调用这个实例的create()方法就可以创建文件。</p>\n<p>DFS会发给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件。</p>\n<p>在创建文件前NameNode会做一些检查，看看文件是否存在，客户端是否有创建权限等。</p>\n<p>若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog；</p>\n<p>若不通过会向客户端抛出IOException。</p>\n<p><img src=\"/img/hdfs-write-file.png\" alt=\"image-20191216163905988\"></p>\n<p>（1）首先，第一个DataNode是以数据包（4KB）的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</p>\n<p>（2）在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包。</p>\n<p>（3）第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点。</p>\n<p>（4）传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK，最终 第一个DataNode会向客户端发回一个ACK。</p>\n<p>（感觉这个ACK和TCP/IP协议中的差不多：ACK (Acknowledge character）即是确认字符，在数据通信中，接收站发给发送站的一种传输类<a href=\"https://baike.baidu.com/item/%E6%8E%A7%E5%88%B6%E5%AD%97%E7%AC%A6/6913704\" target=\"_blank\" rel=\"noopener\">控制字符</a>。表示发来的数据已确认接收无误。）</p>\n<p>（5）当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点，然后客户端会向NameNode发送一个确认。</p>\n<p>（这里是最后一次ACK吗？还有有一个seq？因为上边说每次发送的数据包是4KB比较小，每次都有ACK吧应该，还是最后检验程序完整性？感觉和文件上传很类似，期待研究源码！）</p>\n<p>（6）如果管道中的任何一个DataNode失败，管道会被关闭，数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点。</p>\n<p>解答上述疑问：数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在HDFS中，供读取时进行完整性校验。</p>\n<h1>三、删除文件</h1>\n<p>HADOOP\t删除文件三部曲</p>\n<p>（1）NameNode只是重命名被删除的文件到 /trash 目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在 /trash 中文件会被保留一定间隔的时间（默认6h）</p>\n<p>​\t（在这个期间文件可以恢复）；</p>\n<p>（2）当指定的时间到达，NameNode将会把文件从命名空间中删除；</p>\n<p>（3）标记删除的文件块释放空间，HDFS文件系统显示空间增加。</p>\n<h1>四、修改文件</h1>\n<p>想啥呢?</p>\n"},{"title":"Hive数据定义","author":"郑天祺","date":"2020-01-17T06:18:00.000Z","_content":"\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","source":"_posts/Hive数据定义.md","raw":"title: Hive数据定义\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 14:18:00\n\n---\n\n# 一、Hive 与 Mysql不同\n\n​\t\tHive不支持行级插入操作、更新操作和删除操作，\n\n​\t\tHive不支持事务。\t\n\n# 二、Hive中的数据库\n\nHive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。\n\n```java\n// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n```\n\n​\t\n\n```java\n// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n```\n\n\n\n# 三、分区表、管理表\n\n​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。\n\n​\t先按照 国家 ， 后按照 州 分区\n\n```java\nCREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY<STRING>,\n\tdeductions\tMAP<STRING, FLOAT>,\n\tadress\tSTRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING)\n```\n\n分区表改变了 Hive 对数据存储的组织方式。\n\n对比：\n\n​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：\n\n​\t\n\n```java\nhdfs://master_server/user/hive/warehouse/mydb.db/employees\n```\n\n​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：\n\n```java\n...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n```\n\n当我们查询美国伊利诺斯州所有雇员：\n\n```java\nSELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n```\n\n更快，所以分区显著的提高查询性能。\n\n但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。\n\n建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：\n\n```java\nset hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n```\n\n```java\n// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n```\n\n","slug":"Hive数据定义","published":1,"updated":"2020-01-17T07:53:05.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp08000gx8vhb0jv5w1n","content":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Hive 与 Mysql不同</h1>\n<p>​\t\tHive不支持行级插入操作、更新操作和删除操作，</p>\n<p>​\t\tHive不支持事务。</p>\n<h1>二、Hive中的数据库</h1>\n<p>Hive 中数据库的概念本质上仅仅是表的一个目录或者命名空间。</p>\n<pre><code class=\"language-java\">// 1、数据库目录为：\nhive.metastore.warehouse.dir\n\n// 2、创建数据库 ：\nCREATE DATABASE financials;    \n\n// 3、已经存在则： \nCREATE DATABASE IF NOT EXISTS financials;\n\n// 4、查看数据库：\nSHOW DATABASES;    SHOW DATABASES LIKE 'f.*';\n\n// 5、修改默数据库位置：\nCREATE DATABASE financials LOCATION '/my/preferred/directory';\n\n// 6、切换工作数据库：\nUSE financials;\n\n (Hive v0.8.0，可以修改当前工作数据库为默认数据库，set hive.cli.print.current.db=true;)\n\n// 7、删除数据库：\nDROP DATABASE IF EXISTS financials;\n</code></pre>\n<p>​</p>\n<pre><code class=\"language-java\">// 8、级联删除数据库（含表）：\nDROP DATABASE IF EXISTS financials CASCADE;\n\n// 9、可以使用  ALTER DATABASE 为数据库的 DBPROPERTIES 设置键-值对属性值，来描述数据库的属性信息，其他不可以更改：\nALTER DATABASES financials SET DBPROPERTIES('edited-by' = 'Joe Dba')\n    \n// 10、删除表\nDROP TABLE IF EXISTS employees;\n\n// 11、表重命名\nALTER TABLE log_messages RENAME TO logmsgs;\n\n// 12、 对某个字段重命名，并修改位置、类型或者注释\nALTER TABLE log_messages\nCHANGE COLUMN hms hours_minutes_seconds INT\nCOMMENT 'The hours, minutes, and seconds part of the timestamp'\nAFTER severity;\n// 13、增加列\nALTER TABLE log_messages ADD COLUMNS(\n\tapp_name STRING COMMENT 'Application name',\n    session_id LONG COMMENT 'The current session id'\n);\n// 14、删除或者替换列\nALTER TABLE log_messages REPLACE COLUMNS(\n\thours_mins_secs INT COMMENT 'hour, minute, seconds from timestamp',\n    severity STRING COMMENT 'The message severity'\n    message STRING COMMENT 'The rest of the message'\n);\n// 15、修改表属性\nALTER TABLE log_messages SET TBLPROPERTIES(\n\t'notes' = 'The process id is no longer captured; this column is always NULL'\n);\n// 16、修改存储属性\nALTER TABLE log_messages PARTITION(year = 2012, month = 1, day =1) SET FILEFORMAT SEQUENCEFILE;\n</code></pre>\n<h1>三、分区表、管理表</h1>\n<p>​\t数据分区：通常使用分区来水平分散压力，将数据从物理上转移到和使用最频繁的用户更近的地方，以及实现其他目的。</p>\n<p>​\t先按照 国家 ， 后按照 州 分区</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname\tSTRING,\n\tsalary\tFLOAT,\n\tsubordinates\tARRAY&lt;STRING&gt;,\n\tdeductions\tMAP&lt;STRING, FLOAT&gt;,\n\tadress\tSTRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING)\n</code></pre>\n<p>分区表改变了 Hive 对数据存储的组织方式。</p>\n<p>对比：</p>\n<p>​\t（1）如果我们是在mydb数据库中创建的这个表，那么对于这个表只会有一个employees目录与之对应：</p>\n<p>​</p>\n<pre><code class=\"language-java\">hdfs://master_server/user/hive/warehouse/mydb.db/employees\n</code></pre>\n<p>​\t（2）但是，Hive 现在将会创建好可以反映分区结构的子目录。如：</p>\n<pre><code class=\"language-java\">...\n.../employees/country=CA/state=AB\n.../employees/country=CA/state=BC\n...\n.../employees/country=US/state=AL\n.../employees/country=US/state=AK\n...\n</code></pre>\n<p>当我们查询美国伊利诺斯州所有雇员：</p>\n<pre><code class=\"language-java\">SELECT * FROM employees WHERE country  = 'US' AND state = 'IL';\n</code></pre>\n<p>更快，所以分区显著的提高查询性能。</p>\n<p>但是如果全查询数据非常大，会执行巨大的 MapReduce 任务。</p>\n<p>建议将Hive设置为 “strict(严格)” 模式，如果没有WHERE过滤的话，会禁止提交这个任务：</p>\n<pre><code class=\"language-java\">set hive.mapred.mode=strict\n    \n// SHOW PARTITIONS命令查看表中存在的所有分区：\nSHOW PARTITION employees;\n\n// 查看指定分区\nSHOW PARTITIONS employees PARTITION(country='US')\nSHOW PARTITIONS employees PARTITION(country='US', state='AK')\n</code></pre>\n<pre><code class=\"language-java\">// 日志文件\nALTER TABLE log_messages ADD PARTITION(year = 2012,month = 1,day = 2)\nLOCATION 'hdfs://master_server/data/log_message/2012/01/02';\n</code></pre>\n"},{"title":"Hive数据操作（2）","author":"郑天祺","date":"2020-01-19T07:50:00.000Z","_content":"\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","source":"_posts/Hive数据操作（2）.md","raw":"title: Hive数据操作（2）\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-19 15:50:00\n\n---\n\nHive 中 SQL  JOIN 语句，只支持等值连接\n\n# 一、INNER JOIN\n\n​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 >= 等不相等匹配、ON子句中谓词之间不能使用OR。\n\n```java\n// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive> SELECT a.ymd, a.price_close, b.price_close\n\t>FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t>WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n```\n\n大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。\n\n​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。\n\n​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。\n\n```java\n... 小表 JOIN 大表 ON ...\n```\n\n# 二、LEFT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL\n\n```java\n... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n```\n\n# 三、RIGHT OUTER JOIN\n\n​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。\n\n# 四、FULL OUTER JOIN\n\n​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。\n\n​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。\n\n```java\nhive>SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t>FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t>WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n```\n\n# 五、LEFT SEMI-JOIN\n\n​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。\n\n​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。\n\n```java\n// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive> SELECT s.yml, s.symbol, s.price_close\n    > FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n```\n\n​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。\n\n# 六、map-side JOIN\n\n​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。\n\n​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。\n\n```java\n// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive>set hive.auto.convert.join=true\n\nhive> SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    > FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  > WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n类似的：\n\n​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：\n\n​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。\n\n```java\n// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n# 七、ORDER BY 和 SORT BY\n\n​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）\n\n​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）\n\n​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT\n\n# 八、CLUSTER BY\n\n```java\n// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive> SELECT a.ymd, s.symbol, s.price_close\n    > FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n```\n\n","slug":"Hive数据操作（2）","published":1,"updated":"2020-01-20T06:46:13.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0d000lx8vhh8tkl29q","content":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Hive 中 SQL  JOIN 语句，只支持等值连接</p>\n<h1>一、INNER JOIN</h1>\n<p>​\t内连接（INNER JOIN）中，只有进行连接的两个表中都存在于连接标准相匹配的数据才会被保留下来。不支持 &gt;= 等不相等匹配、ON子句中谓词之间不能使用OR。</p>\n<pre><code class=\"language-java\">// 苹果公司股价 AAPL   IBM股价IBM\n// ON子句指定了两个表间数据进行连接的条件\n// WHERE子句限制了左边表是AAPL的记录，右边表是IBM的记录\n\nhive&gt; SELECT a.ymd, a.price_close, b.price_close\n\t&gt;FROM stocks a JOIN stocks b ON a.ymd = b.ymd \n\t&gt;WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';\n\n2010-01-04  214.01  132.45\n2010-01-05  214.38  130.85\n...\n</code></pre>\n<p>大多数情况下，Hive会对每对 JOIN 连接对象启动一个 MapReduce 任务。</p>\n<p>​\t\tHive同时假定查询中最后一个表是对打的那个表。在对每行记录进行连接操作时，它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。</p>\n<p>​\t\t所以优化JOIN的时候，将小表放在前边，大表放到后边。</p>\n<pre><code class=\"language-java\">... 小表 JOIN 大表 ON ...\n</code></pre>\n<h1>二、LEFT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，但是这种操作，会返回左侧表所有的记录，当右边表根据连接条件没有对应的记录时，那么右表响应的列的值是NULL</p>\n<pre><code class=\"language-java\">... 全部数据表 LEFT OUTER JOIN 对应条件的表 ON ...\n</code></pre>\n<h1>三、RIGHT OUTER JOIN</h1>\n<p>​\t\t用法和 INNER JOIN 一致，右外连接（RIGHT OUTER JOIN）会返回右边表所有符合WHERE语句的记录。左表中匹配不上的字段值用NULL代替。</p>\n<h1>四、FULL OUTER JOIN</h1>\n<p>​\t\t最后介绍的完全外连接（FULL OUTER JOIN）将会返回所有表中符合 WHERE 语句条件的所有记录。</p>\n<p>​\t\t如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值代替。</p>\n<pre><code class=\"language-java\">hive&gt;SELECT s.ymd, s.symbol, s.price_close, d.divided\n\t&gt;FROM dividends d FULL OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol\n\t&gt;WHERE s.symbol = 'AAPL';\n\n...\n1987-05-07 AAPL 80.25 NULL\n1987-05-08 AAPL 97.0 NULL\n1987-05-11 AAPL 77.0 0.015\n...\n</code></pre>\n<h1>五、LEFT SEMI-JOIN</h1>\n<p>​\t\t左开半连接（LEFT SEMI-JOIN）会返回左边表的记录，前提是其记录对于右表满足 ON 语句中的判定条件。</p>\n<p>​\t\t这个子句的出现是为了解决 IN ... EXISTS结构的。</p>\n<pre><code class=\"language-java\">// 因为 Hive 不支持以下查询：\nSELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN(SELECT d.yml, d.symbol FROM dividends d);\n\n// 所以利用 LEFT SEMI JOIN\n// SELECT 和 WHERE 语句中不能引用到右边表中的字段\nhive&gt; SELECT s.yml, s.symbol, s.price_close\n    &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;\n\n...\n1962-11-05  IBM   361.5\n1962-08-07\tIBM   373.25\n1962-05-08  IBM   459.5\n1962-02-06  IBM   551.5\n</code></pre>\n<p>​\t\t注：SEMI-JOIN 比通常的 INNER JOIN 要更加高效：对于左表的一条指定的记录，在右边表中一旦找到匹配的记录，Hive 就会立即停止扫描。从这点来看，左边表中选择的列是可以预测的。</p>\n<h1>六、map-side JOIN</h1>\n<p>​\t\t如果所有表中只有一张表是小表，那么可以在最大的表通过 mapper 的时候将小表完全放到内存中。</p>\n<p>​\t\tHive 可以在 map 段执行连接过程（称为 map-side JOIN），这是因为 Hive 可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的 reduce 过程。即使对于很小的数据集，这个优化也明显地要快于常规的连接操作：不仅减少了 reduce 过程，而且有时还可以同时减少 map 过程的执行步骤。</p>\n<pre><code class=\"language-java\">// 当设置了以下的属性，内连接也可以使用这个优化(hive v0.7+) \n// 但是右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）不支持此优化\nhive&gt;set hive.auto.convert.join=true\n\nhive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend\n    &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol \t  &gt; WHERE s.symbol = 'AAPL';\n\n// 属于小表的属性\nhive.mapjoin.smalltable.filesize=25000000\n</code></pre>\n<p>类似的：</p>\n<p>​\t\t表中的数据必须是按照 ON 语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍，当满足这些条件时：</p>\n<p>​\t\tHive 可以在 map 阶段按照分桶数据进行连接。因此这种情况下，不需要先获取到表中所有的内容，之后采取和另一张表中每个分桶进行匹配连接。</p>\n<pre><code class=\"language-java\">// 默认没有开启\nset hive.optimize.bucketmapJOIN=true\n// 涉及的分桶表具有相同的分桶数，而且数据是按照 连接键 或 桶的键进行排序的\n// 此时 Hive 可以执行一个更快的分类-合并连接（sort-merge JOIN）\n// 默认没有开启\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n</code></pre>\n<h1>七、ORDER BY 和 SORT BY</h1>\n<p>​\t\tHive 中 ORDER BY 语句和其他的 SQL 方言中的定义是一样的。会对查询结果集执行一个全局排序：所有数据都通过一个 reducer 进行处理的过程。对于大数据集，这个过程可能会消耗太漫长的时间来执行。（全局有序）</p>\n<p>​\t\tHive 增加了一个可供选择的方式，也就是 SORT BY，其只会在每个 reducer 中对数据进行排序，也就是执行一个局部排序过程。这可以保证每个 reducer 的输出数据都是有序的（但并非全局有序）。这样可以提高后面进行的全局排序的效率。（每个reducer有序）</p>\n<p>​\t\t注：当只有一个reducer时上述结果相同；默认升序ASC  降序DESC；若hive.maperd.mode=strict 时，语句必须加 LIMIT</p>\n<h1>八、CLUSTER BY</h1>\n<pre><code class=\"language-java\">// CLUSTER BY = DISTRIBUTE BY ... SORT BY 语句。\n// 此语句会剥夺 SORT BY 的并行性\nhive&gt; SELECT a.ymd, s.symbol, s.price_close\n    &gt; FROM stocks s CLUSTER BY s.symbol\n    \n2010-02-08 AAPL 194.12\n2010-02-05 AAPL 195.46\n2010-02-04 AAPL 192.05\n...\n2010-01-27 AAPL 207.88\n...\n</code></pre>\n"},{"title":"Hive数据操作（3）","author":"郑天祺","date":"2020-01-20T06:27:00.000Z","_content":"\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","source":"_posts/Hive数据操作（3）.md","raw":"title: Hive数据操作（3）\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-20 14:27:00\n---\n\n# 一、类型转换\n\n​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。\n\n例如：\n\n```java\n// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) < 100000.0;\n```\n\n注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast\n\n​\t\t（2）类型转换 BINARY 值（hive v0.8.0）\n\n```java\n// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n```\n\n# 二、抽样查询\n\n​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。\n\n例如：\n\n```java\n// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive> SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n```\n\n# 三、数据块抽样\n\n​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：\n\n```java\nhive> SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n```\n\n注：这种抽样方式不一定适用于所有的文件格式。\n\n# 四、UNION ALL\n\n​\t\tUNION ALL 可以将 2个或多个表进行合并。\n\n​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。","slug":"Hive数据操作（3）","published":1,"updated":"2020-03-28T01:17:37.098Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0f000nx8vh3278nb1q","content":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、类型转换</h1>\n<p>​\t\t（1）cast() 函数，可以使用这个函数对指定的值进行显式的类型转换。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 当salary字段的值是不合法的浮点数字符串的话，Hive会返回NULL\nSELECT name, salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;\n</code></pre>\n<p>注：将浮点数转换成整数的推荐方式是round()或者floor()函数，而不是使用类型转换操作符cast</p>\n<p>​\t\t（2）类型转换 BINARY 值（hive v0.8.0）</p>\n<pre><code class=\"language-java\">// 只支持将 BINARY 转换为 STRING 类型(也可以 STRING 转为 BINARY)\nSELECT (2.0 * cast(cast(b string) as double)) from src;\n</code></pre>\n<h1>二、抽样查询</h1>\n<p>​\t\t对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行分桶抽样来满足这个需求。</p>\n<p>例如：</p>\n<pre><code class=\"language-java\">// 假设 numbers 表只有 number 字段，其值是 1 到 10\n// 可以利用 rand() 函数进行抽样，这个函数会返回一个随机值。\n// 以下的语句返回的值会不相同\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand()) s;\n\n// 如果按照指定的列而不是rand()函数进行分桶，同一语句多次执行的返回值是相同的\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 5 OUT OF 10 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON number) s;   \n\n// 分桶语句中的分母表示的是数据将会被散列的桶的个数?，而分子表示将会选择的桶的个数：\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 1 OUT OF 2 ON number) s;\nhive&gt; SELECT * from numbers TABLESAMPLE(BUCKET 2 OUT OF 2 ON number) s;\n</code></pre>\n<h1>三、数据块抽样</h1>\n<p>​\t\tHive 提供了另一种按照抽样百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行抽样：</p>\n<pre><code class=\"language-java\">hive&gt; SELECT * from numbersflat TABLESAMPLE(0.1 * PERCENT) s;\n</code></pre>\n<p>注：这种抽样方式不一定适用于所有的文件格式。</p>\n<h1>四、UNION ALL</h1>\n<p>​\t\tUNION ALL 可以将 2个或多个表进行合并。</p>\n<p>​\t\t每一个 union 子查询都必须具有相同的列，而且对应的每个字段的字段类型必须是一致的。</p>\n"},{"title":"Hive数据类型和文件格式","author":"郑天祺","date":"2020-01-17T05:41:00.000Z","_content":"\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","source":"_posts/Hive数据类型和文件格式.md","raw":"title: Hive数据类型和文件格式\nauthor: 郑天祺\ntags:\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-17 13:41:00\n\n---\n\n# 一、基本数据类型\n\n![image-20200117105348449](/img/hive数据结构.png)\t\t\t\n\n![image-20200117105505918](/img/hive数据结构1.png)\n\n上面图列表了Hive所支持的基本数据类型。\n\n相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String\n\n不同：\n\n​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。\n\n​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）\n\n​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）\n\n​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff\n\n​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数\n\n​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。\n\n​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。\n\n​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。 \n\n​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;\n\n# 二、集合数据类型\n\nHive 中的列支持使用 strut map 和 array 集合数据类型，如下图\n\n![image-20200117111045081](/img/hive集合数据类型.png)\n\nHive 中没有 键 的概念，但是用户可以对表建立索引。\n\n# 三、创建表的实例\n\n 人力资源的员工表\n\n```java\nCREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY<STRING>,\n\tdeductions MAP<STRING, STRING>;\n    adress STRUCT<street:STRING, city:STRING>, state:STRING, zip:INT)\n);\n```\n\n# 四、文本文件数据编码\n\nHive中默认的记录和 字段分隔符\n\n![image-20200117113506023](/img/image-20200117113506023.png)\n\n实例使用：\n\n```java\nCREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n```\n\n\n用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。\n\n","slug":"Hive数据类型和文件格式","published":1,"updated":"2020-01-17T06:12:51.383Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0i000rx8vhmisi5k3x","content":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本数据类型</h1>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png\" alt=\"image-20200117105348449\"></p>\n<p><img src=\"/img/hive%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841.png\" alt=\"image-20200117105505918\"></p>\n<p>上面图列表了Hive所支持的基本数据类型。</p>\n<p>相同：这些数据类型是对 JAVA 中接口的实现，例如STRING是java中的String</p>\n<p>不同：</p>\n<p>​\t\t1、在其他SQL方言中，通常会提供限制最大长度的 “字符数组” ，但是Hive不支持。</p>\n<p>​\t\t因为 Hive 是为了优化磁盘的读和写的性能，列长度不重要（定长易于索引）</p>\n<p>​\t\t2、TIMESTAMP的值可以是整数（距离Unix新纪元时间1970年1月1日，午夜12点的秒数）</p>\n<p>​\t\t；也可以是浮点数，精确到纳秒（小数点后9位）；还可以是字符号串，YYYY-MM-DD hh:mm:ss.fffffffff</p>\n<p>​\t\t3、TIMESTAMPS表示 UTC 时间。Hive 本身提供了不同时区相互转换的内置函数，to_utc_timestamp函数和 from_utc_timestamp函数</p>\n<p>​\t\t4、BINARY 和 VARCHAR 类似，但和 BLOB 不同。BINARY可以在记录中包含任意字节，这样可以防止Hive尝试将其作为数字，字符串等进行解析。</p>\n<p>​\t\t如果需要省略每行记录的尾部，无需使用 BINARY 数据类型。如果一个表的标结果指定的是3列，而实际数据文件每行记录包含有 5 个字段的话，那么 在 Hive 中最后 2 列数据将会被省略掉。</p>\n<p>​\t\t当 查询 将float与double对比，或者 int 和 float对比时，隐式使用较大的类型。</p>\n<p>​\t\t5、当需要把 字符串 转成 数值，那么需要显式：... cast(s AS INT) ... ;</p>\n<h1>二、集合数据类型</h1>\n<p>Hive 中的列支持使用 strut map 和 array 集合数据类型，如下图</p>\n<p><img src=\"/img/hive%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png\" alt=\"image-20200117111045081\"></p>\n<p>Hive 中没有 键 的概念，但是用户可以对表建立索引。</p>\n<h1>三、创建表的实例</h1>\n<p>人力资源的员工表</p>\n<pre><code class=\"language-java\">CREATE TABLE employees(\n\tname STRTING,\n\tsalary FLOAT,\n\tsubordinates ARRAY&lt;STRING&gt;,\n\tdeductions MAP&lt;STRING, STRING&gt;;\n    adress STRUCT&lt;street:STRING, city:STRING&gt;, state:STRING, zip:INT)\n);\n</code></pre>\n<h1>四、文本文件数据编码</h1>\n<p>Hive中默认的记录和 字段分隔符</p>\n<p><img src=\"/img/image-20200117113506023.png\" alt=\"image-20200117113506023\"></p>\n<p>实例使用：</p>\n<pre><code class=\"language-java\">CREATE TABLE some_data(\n\tfirst FLOAT,\n\tsecond FLOAT,\n\tthird FLOAT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINQTED BY ',' ;\n</code></pre>\n<p>用例如使用  '\\t' (也就是指标建) 作为字段分隔符。可以利用他处理CSV格式数据。</p>\n"},{"title":"Hive模式设计","author":"郑天祺","date":"2020-01-21T06:09:00.000Z","_content":"\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","source":"_posts/Hive模式设计.md","raw":"title: Hive模式设计\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 14:09:00\n\n---\n\n# 一、分区\n\nHive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。\n\n如：存储日志，log_2020_01_01、log_2020_01_02等\n\n```java\nhive> CREATE TABLE \n\nhive> CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive> CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive> SELECT part,quantity log_2020_01_01\n    > UNION ALL\n    > SELECT part,quantity from log_2020_01_04\n    > WHERE quantity < 4;\n```\n\nHive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：\n\n```java\nhive> CREATE TABLE supply(id int, part string, quantity int) \n    > PARTITIONED BY(int day);\n\nhive> ALTER TABLE supply add PARTITION (day=20200201)\nhive> ALTER TABLE supply add PARTITION (day=20200202)\nhive> ALTER TABLE supply add PARTITION (day=20200203)\nhive> ...load data...\nhive> SELECT part,quantity FROM supply WHERE day>=20200201 AND day<20200203 AND quantity<4;\n```\n\n但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。\n\n## 二、同一份数据多种处理\n\n```java\nhive> INSERT OVERWRITE TABLE sales\n    > SELECT * FROM history WHERE action='purchased'\nhive> INSERT OVERWRITE TABLE credits\n    > SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive> FROM history\n    > INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    > INSERT OVERWRITE credits SELECT * WHERE action='returned';\n```\n\n# 三、对于每个表的分区\n\n​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。\n\n​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。\n\n```java\n// 如：有中间表distinct_ip_in_logs\nhive> INSERT OVERWRITE table distinct_ip_in_logs \n    > SELECT distict(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}';\n\nhive> CREATE TABLE state_city_for_day (state string, city, string);\n\nhive> INSERT OVERWRITE state_city_for_day\n    > SELECT distinct(state, city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n```\n\n​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。\n\n​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。\n\n​\t\t改进方法, 建立分区：\n\n```java\nhive -hiveconf dt=2020-01-01\n    \nhive> INSERT OVERWRITE table distinct_ip_in_logs\n    > PARTITION(hit_date=${dt})\n    > SELECT distinct(ip) as ip from weblogs\n    > WHERE hit_date='${hiveconf:dt}'\n\nhive> CREATE TABLE state_city_for_day(state string,city string)\n    > PARTITIION BY (hit_date string)\n    \nhive> INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    > SELECT distinct(state,city) FROM distinct_ip_in_logs\n    > JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    > WHERE (hit_date='${hiveconf:dt}')\n```\n\n# 四、分桶表数据存储\n\n​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。\n\n​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。\n\n```java\n// 分区：如果根据user_id分区，会创建太多分区\nhive> CREATE TABLE weblog (url STRING, source_ip STRING)>PARTITIONED BY (dt STRING, user_id INT);\n\nhive> FROM raw_weblog\n    > INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    > SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive> CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    > PARTITIONED BY (dt STRING)\n    > CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive> SET hive.enforce.bucketing=true;\nhive> FROM raw_logs\n    > INSERT OVERWRITE TABLE weblog\n    > PARTITION (dt='2020-02-25')\n    > SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n```\n\n# 五、为表增加列\n\n```java\nhive> CREATE TABLE weblogs  (version LONG, url STRING)\n    > PARTITIONED BY (hit_date int)\n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive> ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive> LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive> SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive> ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive> ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive> LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive> SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n```\n\n","slug":"Hive模式设计","published":1,"updated":"2020-01-21T08:48:03.705Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0k000tx8vhftj06dio","content":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2>二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='${hiveconf:dt}';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=${dt})\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='${hiveconf:dt}'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='${hiveconf:dt}')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、分区</h1>\n<p>Hive 中分区的功能是非常有用的。因为通常要对输入进行全盘扫描，来满足查询条件。</p>\n<p>如：存储日志，log_2020_01_01、log_2020_01_02等</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE \n\nhive&gt; CREATE TABLE log_2020_01_01 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_02 (id int, part string, quantity int);\nhive&gt; CREATE TABLE log_2020_01_04 (id int, part string, quantity int);\n\nhive&gt; SELECT part,quantity log_2020_01_01\n    &gt; UNION ALL\n    &gt; SELECT part,quantity from log_2020_01_04\n    &gt; WHERE quantity &lt; 4;\n</code></pre>\n<p>Hive 通过 WHERE 子句中表达式来选择查询所需要的指定的分区。这样效率高且清晰明了：</p>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE supply(id int, part string, quantity int) \n    &gt; PARTITIONED BY(int day);\n\nhive&gt; ALTER TABLE supply add PARTITION (day=20200201)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200202)\nhive&gt; ALTER TABLE supply add PARTITION (day=20200203)\nhive&gt; ...load data...\nhive&gt; SELECT part,quantity FROM supply WHERE day&gt;=20200201 AND day&lt;20200203 AND quantity&lt;4;\n</code></pre>\n<p>但是不要存储太多的分区和文件夹目录，并且每一个文件要足够大。应该是文件系统中块的若干倍。</p>\n<h2>二、同一份数据多种处理</h2>\n<pre><code class=\"language-java\">hive&gt; INSERT OVERWRITE TABLE sales\n    &gt; SELECT * FROM history WHERE action='purchased'\nhive&gt; INSERT OVERWRITE TABLE credits\n    &gt; SELECT * FROM history WHERE action='returned'\n// 可以优化上边两边编程下边，而且可以提高扫描速度，扫描一次\nhive&gt; FROM history\n    &gt; INSERT OVERWRITE sales SELECT * WHERE action='phrchased'\n    &gt; INSERT OVERWRITE credits SELECT * WHERE action='returned';\n</code></pre>\n<h1>三、对于每个表的分区</h1>\n<p>​\t\tELT 处理过程会涉及到多个处理步骤，每个步骤可能会产生一到多个临时表，这些表仅供下一个job使用。</p>\n<p>​\t\t问题：由于查询或原始数据处理的某个步骤出现问题而导致需要对好几天的输入数据重跑 ETL 过程。这时用户可能就需要执行那些一天执行一次的处理过程，来保证在所有的任务都完成之前不会有 job 将临时表覆盖重写。</p>\n<pre><code class=\"language-java\">// 如：有中间表distinct_ip_in_logs\nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs \n    &gt; SELECT distict(ip) as ip from weblogs\n    &gt; WHERE hit_date='${hiveconf:dt}';\n\nhive&gt; CREATE TABLE state_city_for_day (state string, city, string);\n\nhive&gt; INSERT OVERWRITE state_city_for_day\n    &gt; SELECT distinct(state, city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip);\n</code></pre>\n<p>​\t\t当计算某一天的数据时会导致前一天数据被 INSERT OVERWRITE 语句覆盖掉。</p>\n<p>​\t\t如果同时运行两个这样的实例，处理不同日期的数据的话，那么它们就可能会相互影响对方的结果数据。</p>\n<p>​\t\t改进方法, 建立分区：</p>\n<pre><code class=\"language-java\">hive -hiveconf dt=2020-01-01\n    \nhive&gt; INSERT OVERWRITE table distinct_ip_in_logs\n    &gt; PARTITION(hit_date=${dt})\n    &gt; SELECT distinct(ip) as ip from weblogs\n    &gt; WHERE hit_date='${hiveconf:dt}'\n\nhive&gt; CREATE TABLE state_city_for_day(state string,city string)\n    &gt; PARTITIION BY (hit_date string)\n    \nhive&gt; INSERT OVERWRITE table state_city_for_day PARTITION(${hiveconf:dt})\n    &gt; SELECT distinct(state,city) FROM distinct_ip_in_logs\n    &gt; JOIN geodata ON (distinct_ip_in_logs.ip=geodata.ip)\n    &gt; WHERE (hit_date='${hiveconf:dt}')\n</code></pre>\n<h1>四、分桶表数据存储</h1>\n<p>​\t\t分区提供一个数据隔离和优化查询的遍历的方式。不过，并非所有的数据集都可形成合理的分区。</p>\n<p>​\t\t分桶是将数据集分解成更统一管理的若干部分的另一个技术。利用哈希分发到不同的桶中。</p>\n<pre><code class=\"language-java\">// 分区：如果根据user_id分区，会创建太多分区\nhive&gt; CREATE TABLE weblog (url STRING, source_ip STRING)&gt;PARTITIONED BY (dt STRING, user_id INT);\n\nhive&gt; FROM raw_weblog\n    &gt; INSERT OVERWRITE TABLE page_view PARTITION(dt='2020-06-08', user_id)\n    &gt; SELECT server_name, url, source_ip, dt, user_id;\n\n// 分桶：用户数比桶数多，每个桶就会有多个用户的记录\nhive&gt; CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING)\n    &gt; PARTITIONED BY (dt STRING)\n    &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS;\n\n// 此属性强制hive为目标表初始化过程设置一个正确的 reducer 个数。\nhive&gt; SET hive.enforce.bucketing=true;\nhive&gt; FROM raw_logs\n    &gt; INSERT OVERWRITE TABLE weblog\n    &gt; PARTITION (dt='2020-02-25')\n    &gt; SELECT user_id, url, source_ip WHERE dt = '2020-02-25'\n</code></pre>\n<h1>五、为表增加列</h1>\n<pre><code class=\"language-java\">hive&gt; CREATE TABLE weblogs  (version LONG, url STRING)\n    &gt; PARTITIONED BY (hit_date int)\n    &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n\nhive&gt; ! cat log1.txt\n1 /mystuff\n1 /toys\n    \nhive&gt; LOAD DATA LOCAL INPATH 'log1.txt' int weblogs partition(20200101);\nhive&gt; SELECT * FROM weblogs;\n1 /mystuff 20200101\n1 /toys 20200101\n    \n// 加新字段\nhive&gt; ! cat log2.txt\n2 /cars bob\n2 /stuff terrys\n    \nhive&gt; ALTER TABLE weblogs ADD COLUMNS (user_id string);\nhive&gt; LOAD DATA LOCAL INPATH 'log2.txt' int weblogs partition(20200101);\nhive&gt; SELECT * from weblogs\n1 /mystuff 20200101 NULL\n2 /toys    20200101 NULL\n3 /cars    20200102 bob\n4 /stuff   20200102 terry\n</code></pre>\n"},{"title":"Hive索引","author":"郑天祺","date":"2020-01-21T02:59:00.000Z","_content":"\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","source":"_posts/Hive索引.md","raw":"title: Hive索引\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 10:59:00\n\n---\n\n​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。\n\n# 一、建索引语法\n\n```java\n// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY<STRING>,\ndeductions MAP<STRING, FLOAT>,\naddress STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n```\n\nBitmap索引：适用于排重后值较少的列。\n\n# 二、重建索引\n\n​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：\n\n```java\nALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n```\n\n# 三、显示索引\n\n```java\n// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n```\n\n# 四、删除索引\n\n```java\n// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n```\n\n","slug":"Hive索引","published":1,"updated":"2020-01-21T05:50:03.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0m000xx8vhx70eqh56","content":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t\tHive没有键的概念，可以对一些字段建立索引来加速某些操作，一张表的索引储存在另外一张表中。EXPLAIN命令可以查看某个查询语句是否用到了索引。</p>\n<h1>一、建索引语法</h1>\n<pre><code class=\"language-java\">// 定义表\nCREATE TABLE employees(\nname STRING,\nsalary FLOAT,\nsubordinates ARRAY&lt;STRING&gt;,\ndeductions MAP&lt;STRING, FLOAT&gt;,\naddress STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;\n)\nPARTITIONED BY (country STRING, state STRING); // 分区：hdfs://xxx/2020/02/20/xx\n\n// 建立索引,仅对字段country建立索引 \nCREATE INDEX employees_index\nON TABLE employees(country)\n// AS ... 指定索引处理器\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'\nWITH DEFERRED REBUILD\nIDXPROPERTIES('creator' = 'me', 'created_at' = 'some_time')\nIN TABLE employees_index_table\nPARTITIONED BY (country, name)\nCOMMENT 'Employees indexed by country and name.'\n</code></pre>\n<p>Bitmap索引：适用于排重后值较少的列。</p>\n<h1>二、重建索引</h1>\n<p>​\t\t如果用户指定了 DEFERRED REBUILD，那么新索引将呈现空白状态。在任何时候，都可以进行第一次索引创建或者使用 ALTER INDEX 对索引进行重建：</p>\n<pre><code class=\"language-java\">ALTER INDEX employees_index\nON TABLE employees\n// 如果省略掉 PARTITION ，那么将会对所有分区进行重建索引\nPARTITION (country = 'US')\nREBUILD;\n</code></pre>\n<h1>三、显示索引</h1>\n<pre><code class=\"language-java\">// 显示这个表中的所建立的索引\nSHOW FORMATTED INDEX ON employees;\n</code></pre>\n<h1>四、删除索引</h1>\n<pre><code class=\"language-java\">// 如果有索引表的话，删除一个索引将会删除这个索引表\n// 不允许DROP TABLE前DROP INDEX\nDROP INDEX IF EXISTS employees_index ON TABLE employees;\n</code></pre>\n"},{"title":"Hive调优","author":"郑天祺","date":"2020-01-21T08:48:00.000Z","_content":"\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","source":"_posts/Hive调优.md","raw":"title: Hive调优\nauthor: 郑天祺\ntags:\n\n  - hive\ncategories:\n  - 大数据\ndate: 2020-01-21 16:48:00\n\n---\n\n# 一、使用EXPLAIN\n\n​\t\t查看逻辑，更多用 EXPLAIN EXTENDED\n\n# 二、限制调整LIMIT\n\n# 三、JOIN优化\n\n​\t\t表足够小用map-side JOIN\n\n# 四、本地模式\n\n​\t对于小数据集，单机或单线程执行时间比较短\n\n```java\nhive> set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive> set mapred.job.tracker=local;\nhive> set mapred.tmp.dir=/home/edward/tmp\nhive> SELECT * from people WHERE firstname=bob;\nhive> set mapred.job.tracker=${oldjobtracker};\n```\n\n# 五、并行执行\n\nhive.exec.parallell=true\n\n# 六、严格模式\n\nhive.mapred.mode=strict\n\n（1）必须有WHERE\n\n（2）对于ORDER BY 的语句必须有LIMIT\n\n（3）限制笛卡尔基的查询\n\n# 七、调整mapper和reducer个数\n\n# 八、JVM重用\n\n# 九、索引\n\n# 十、动态分区\n\n# 十一、推测执行\n\n# 十二、单个MapReducer中多个GROUP BY\n\n# 十三、虚拟列","slug":"Hive调优","published":1,"updated":"2020-01-21T09:46:53.242Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0o000zx8vhhczsxxqr","content":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=${oldjobtracker};\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、使用EXPLAIN</h1>\n<p>​\t\t查看逻辑，更多用 EXPLAIN EXTENDED</p>\n<h1>二、限制调整LIMIT</h1>\n<h1>三、JOIN优化</h1>\n<p>​\t\t表足够小用map-side JOIN</p>\n<h1>四、本地模式</h1>\n<p>​\t对于小数据集，单机或单线程执行时间比较短</p>\n<pre><code class=\"language-java\">hive&gt; set oldjobtracker=${hiveconf.mapred.job.tracker};\nhive&gt; set mapred.job.tracker=local;\nhive&gt; set mapred.tmp.dir=/home/edward/tmp\nhive&gt; SELECT * from people WHERE firstname=bob;\nhive&gt; set mapred.job.tracker=${oldjobtracker};\n</code></pre>\n<h1>五、并行执行</h1>\n<p>hive.exec.parallell=true</p>\n<h1>六、严格模式</h1>\n<p>hive.mapred.mode=strict</p>\n<p>（1）必须有WHERE</p>\n<p>（2）对于ORDER BY 的语句必须有LIMIT</p>\n<p>（3）限制笛卡尔基的查询</p>\n<h1>七、调整mapper和reducer个数</h1>\n<h1>八、JVM重用</h1>\n<h1>九、索引</h1>\n<h1>十、动态分区</h1>\n<h1>十一、推测执行</h1>\n<h1>十二、单个MapReducer中多个GROUP BY</h1>\n<h1>十三、虚拟列</h1>\n"},{"title":"HttpClient","author":"郑天祺","date":"2020-07-15T09:12:00.000Z","_content":"\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","source":"_posts/HttpClient.md","raw":"title: HttpClient\nauthor: 郑天祺\ntags:\n\n  - httpclient\ncategories:\n  - 网络\ndate: 2020-07-15 17:12:00\n\n---\n\n# 1、HttpClient介绍\t\t\n\n​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。\n\n​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。\n\n​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。\n\n​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。\n\n\n# 2、HttpClientUtils\n\n## （1）引入依赖\n\n```java\n        <httpclient.version>4.5.12</httpclient.version>\n                <!-- apache httpclient组件 -->\n        <dependency>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n            <version>${httpclient.version}</version>\n        </dependency>\n```\n\n## （2）返回实体\n\n```java\npackage cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return \"HttpClientResult{\" +\n                \"code=\" + code +\n                \", content='\" + content + '\\'' +\n                '}';\n    }\n}\n\n```\n\n## （3）工具类\n\n```java\npackage cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = \"UTF-8\";\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map<String, String> headers, Map<String, String> params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map<String, String> params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map<String, String> params) throws Exception {\n        if (params == null) {\n            params = new HashMap<String, String>();\n        }\n\n        params.put(\"_method\", \"delete\");\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map<String, String> params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map<String, String> params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List<NameValuePair> nvps = new ArrayList<NameValuePair>();\n            Set<Map.Entry<String, String>> entrySet = params.entrySet();\n            for (Map.Entry<String, String> entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null && httpResponse.getStatusLine() != null) {\n            String content = \"\";\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n```\n\n","slug":"HttpClient","published":1,"updated":"2020-07-15T14:39:17.273Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0q0012x8vhmxrrlozy","content":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2>（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;${httpclient.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2>（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;HttpClientResult{&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '}';\n    }\n}\n\n</code></pre>\n<h2>（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception {\n        if (params == null) {\n            params = new HashMap&lt;String, String&gt;();\n        }\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) {\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、HttpClient介绍</h1>\n<p>​\t\tHTTP 协议可能是现在 Internet 上使用得最多、最重要的协议了，越来越多的 Java 应用程序需要直接通过 HTTP 协议来访问网络资源。</p>\n<p>​\t\t虽然在 JDK 的 java net包中已经提供了访问 HTTP 协议的基本功能，但是对于大部分应用程序来说，JDK 库本身提供的功能还不够丰富和灵活。</p>\n<p>​\t\tHttpClient 是Apache HttpComponents 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient已经应用在很多的项目中，并支持HTTPS协议。</p>\n<p>​\t\tHttpClient 不是浏览器，它是一个客户端 HTTP 协议传输类库。HttpClient 被用来发送和接受 HTTP 消息。HttpClient 不会处理 HTTP 消息的内容，不会进行 javascript 解析，不会关心 content type，如果没有明确设置，HttpClient 也不会对请求进行格式化、重定向 url，或者其他任何和 HTTP 消息传输相关的功能。</p>\n<h1>2、HttpClientUtils</h1>\n<h2>（1）引入依赖</h2>\n<pre><code class=\"language-java\">        &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt;\n                &lt;!-- apache httpclient组件 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;\n            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;\n            &lt;version&gt;${httpclient.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<h2>（2）返回实体</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.entity;\n\nimport java.io.Serializable;\n\n/**\n * @author ztq\n */\npublic class HttpClientResult implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    /**\n     * 响应状态码\n     */\n    private int code;\n\n    /**\n     * 响应数据\n     */\n    private String content;\n\n    public HttpClientResult(int code, String content) {\n        this.code = code;\n        this.content = content;\n    }\n\n    public HttpClientResult(int code) {\n        this.code = code;\n    }\n\n    public int getCode() {\n        return code;\n    }\n\n    public void setCode(int code) {\n        this.code = code;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;HttpClientResult{&quot; +\n                &quot;code=&quot; + code +\n                &quot;, content='&quot; + content + '\\'' +\n                '}';\n    }\n}\n\n</code></pre>\n<h2>（3）工具类</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.utils;\n\nimport cn.edu.bjut.entity.HttpClientResult;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.*;\nimport org.apache.http.client.utils.URIBuilder;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.util.*;\n\n/**\n * @author ztq\n */\npublic class HttpClientUtils {\n\n    /**\n     * 编码格式。发送编码格式统一用UTF-8\n     */\n    private static final String ENCODING = &quot;UTF-8&quot;;\n\n    /**\n     * 设置连接超时时间，单位毫秒。\n     */\n    private static final int CONNECT_TIMEOUT = 6000;\n\n    /**\n     * 请求获取数据的超时时间(即响应时间)，单位毫秒。\n     */\n    private static final int SOCKET_TIMEOUT = 6000;\n\n    /**\n     * 发送get请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url) throws Exception {\n        return doGet(url, null, null);\n    }\n\n    /**\n     * 发送get请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; params) throws Exception {\n        return doGet(url, null, params);\n    }\n\n    /**\n     * 发送get请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建访问的地址\n        URIBuilder uriBuilder = new URIBuilder(url);\n        if (params != null) {\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                uriBuilder.setParameter(entry.getKey(), entry.getValue());\n            }\n        }\n\n        // 创建http对象\n        HttpGet httpGet = new HttpGet(uriBuilder.build());\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpGet.setConfig(requestConfig);\n\n        // 设置请求头\n        packageHeader(headers, httpGet);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpGet);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送post请求；不带请求头和请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url) throws Exception {\n        return doPost(url, null, null);\n    }\n\n    /**\n     * 发送post请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; params) throws Exception {\n        return doPost(url, null, params);\n    }\n\n    /**\n     * 发送post请求；带请求头和请求参数\n     *\n     * @param url     请求地址\n     * @param headers 请求头集合\n     * @param params  请求参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) throws Exception {\n        // 创建httpClient对象\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n\n        // 创建http对象\n        HttpPost httpPost = new HttpPost(url);\n        /**\n         * setConnectTimeout：设置连接超时时间，单位毫秒\n         * setConnectionRequestTimeout：设置从connect Manager(连接池)获取Connection\n         * setSocketTimeout：请求获取数据的超时时间(即响应时间)，单位毫秒\n         */\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPost.setConfig(requestConfig);\n        // 设置请求头\n        packageHeader(headers, httpPost);\n\n        // 封装请求参数\n        packageParam(params, httpPost);\n\n        // 创建httpResponse对象\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            // 执行请求并获得响应结果\n            return getHttpClientResult(httpResponse, httpClient, httpPost);\n        } finally {\n            // 释放资源\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送put请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url) throws Exception {\n        return doPut(url);\n    }\n\n    /**\n     * 发送put请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doPut(String url, Map&lt;String, String&gt; params) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpPut httpPut = new HttpPut(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpPut.setConfig(requestConfig);\n\n        packageParam(params, httpPut);\n\n        CloseableHttpResponse httpResponse = null;\n\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpPut);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；不带请求参数\n     *\n     * @param url 请求地址\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url) throws Exception {\n        CloseableHttpClient httpClient = HttpClients.createDefault();\n        HttpDelete httpDelete = new HttpDelete(url);\n        RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(CONNECT_TIMEOUT).setSocketTimeout(SOCKET_TIMEOUT).build();\n        httpDelete.setConfig(requestConfig);\n\n        CloseableHttpResponse httpResponse = null;\n        try {\n            return getHttpClientResult(httpResponse, httpClient, httpDelete);\n        } finally {\n            release(httpResponse, httpClient);\n        }\n    }\n\n    /**\n     * 发送delete请求；带请求参数\n     *\n     * @param url    请求地址\n     * @param params 参数集合\n     * @return 返回结果\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult doDelete(String url, Map&lt;String, String&gt; params) throws Exception {\n        if (params == null) {\n            params = new HashMap&lt;String, String&gt;();\n        }\n\n        params.put(&quot;_method&quot;, &quot;delete&quot;);\n        return doPost(url, params);\n    }\n\n    /**\n     * 封装请求头\n     *\n     * @param params     参数\n     * @param httpMethod 请求方式\n     */\n    public static void packageHeader(Map&lt;String, String&gt; params, HttpRequestBase httpMethod) {\n        // 封装请求头\n        if (params != null) {\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                // 设置到请求头到HttpRequestBase对象中\n                httpMethod.setHeader(entry.getKey(), entry.getValue());\n            }\n        }\n    }\n\n    /**\n     * 封装请求参数\n     *\n     * @param params     返回结果\n     * @param httpMethod 请求方式\n     * @throws UnsupportedEncodingException 异常抛出 未处理\n     */\n    public static void packageParam(Map&lt;String, String&gt; params, HttpEntityEnclosingRequestBase httpMethod)\n            throws UnsupportedEncodingException {\n        // 封装请求参数\n        if (params != null) {\n            List&lt;NameValuePair&gt; nvps = new ArrayList&lt;NameValuePair&gt;();\n            Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = params.entrySet();\n            for (Map.Entry&lt;String, String&gt; entry : entrySet) {\n                nvps.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));\n            }\n\n            // 设置到请求的http对象中\n            httpMethod.setEntity(new UrlEncodedFormEntity(nvps, ENCODING));\n        }\n    }\n\n    /**\n     * 获得响应结果\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @param httpMethod   请求方式\n     * @return 返回结果集\n     * @throws Exception 异常抛出 未处理\n     */\n    public static HttpClientResult getHttpClientResult(CloseableHttpResponse httpResponse,\n                                                       CloseableHttpClient httpClient, HttpRequestBase httpMethod) throws Exception {\n        // 执行请求\n        httpResponse = httpClient.execute(httpMethod);\n\n        // 获取返回结果\n        if (httpResponse != null &amp;&amp; httpResponse.getStatusLine() != null) {\n            String content = &quot;&quot;;\n            if (httpResponse.getEntity() != null) {\n                content = EntityUtils.toString(httpResponse.getEntity(), ENCODING);\n            }\n            return new HttpClientResult(httpResponse.getStatusLine().getStatusCode(), content);\n        }\n        return new HttpClientResult(HttpStatus.SC_INTERNAL_SERVER_ERROR);\n    }\n\n    /**\n     * 释放资源\n     *\n     * @param httpResponse 响应\n     * @param httpClient   http客户端\n     * @throws IOException 异常抛出 未处理\n     */\n    public static void release(CloseableHttpResponse httpResponse, CloseableHttpClient httpClient) throws IOException {\n        // 释放资源\n        if (httpResponse != null) {\n            httpResponse.close();\n        }\n        if (httpClient != null) {\n            httpClient.close();\n        }\n    }\n}\n\n</code></pre>\n"},{"title":"Nacos配置中心使用","author":"郑天祺","date":"2019-11-25T08:38:00.000Z","_content":"\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","source":"_posts/Nacos配置中心使用.md","raw":"title: Nacos配置中心使用\nauthor: 郑天祺\ntags:\n  - nacos-config\n  - SpringCloud\ncategories:\n  - SpringCloud\ndate: 2019-11-25 16:38:00\n---\n\n# 一、启动Nacos Server\n\n1、启动方式可见 [Nacos 官网](https://nacos.io/zh-cn/docs/quick-start.html) \n\n2、在配置列表里配置自己的配置，按照规范填写各项。\n\n```java\nuser.name=zhengtianqi\nuser.password=123456\n```\n\n配置后的图：\n\n![image-20191125164448760](/img/nacos1.png)\n\n# 二、客户端编写\n\n1）常量类\n\n```java\npublic class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = \"127.0.0.1\";\n\n    public static final String NACOS_DATAID = \"test-nacos-config.yml\";\n    public static final String NACOS_Group = \"DEFAULT_GROUP\";\n}\n\n```\n\n2）客户端工具\n\n```java\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = \"\";\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(\"连接配置中心失败!\", e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(\"\", \"从配置中心获取配置失败，group={},dataId={}\", group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(\"修改后的配置ID是：[\" + dataId + \"]，配置分组是：[\" + group + \"]获取的配置信息是\" + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(\"Nacos读取配置超时或网络异常\", e);\n        } catch (IOException e) {\n            LocalLog.error(\"加载到properties对象出现IO异常\", e);\n        }\n        return properties;\n    }\n\n}\n\n```\n\n3）配置文件\n\n```java\nspring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi<郑天祺>\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n```\n\n4）启动类\n\n```java\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(\"user.name\") + \":\" + properties.getProperty(\"user.password\"));\n    }\n\n}\n```\n\n# 三、引入的依赖\n\n```java\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <spring-cloud-alibaba.version>2.1.1.RELEASE</spring-cloud-alibaba.version>\n        <spring-cloud-greenwich.version>0.9.0.RELEASE</spring-cloud-greenwich.version>\n    </properties>\n        \n    <dependencies>\n\t\t<!--nacos-->\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>1.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n    </dependencies>\n        \n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-greenwich.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>${spring-cloud-alibaba.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    \n```\n\n\n\n# 四、效果\n\n```java\n2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n```\n\n解释：\n\n上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况\n\n```java\n  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(\"user.name\");\n        String userPassword = applicationContext.getEnvironment().getProperty(\"user.password\");\n```\n\n如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题","slug":"Nacos配置中心使用","published":1,"updated":"2019-11-26T03:57:24.175Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0u0016x8vhzm6fdz48","content":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\" target=\"_blank\" rel=\"noopener\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n}\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group={},dataId={}&quot;, group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group={},dataId={}&quot;, group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        } catch (IOException e) {\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        }\n        return properties;\n    }\n\n}\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    }\n\n}\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring-cloud-greenwich.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring-cloud-alibaba.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、启动Nacos Server</h1>\n<p>1、启动方式可见 <a href=\"https://nacos.io/zh-cn/docs/quick-start.html\" target=\"_blank\" rel=\"noopener\">Nacos 官网</a></p>\n<p>2、在配置列表里配置自己的配置，按照规范填写各项。</p>\n<pre><code class=\"language-java\">user.name=zhengtianqi\nuser.password=123456\n</code></pre>\n<p>配置后的图：</p>\n<p><img src=\"/img/nacos1.png\" alt=\"image-20191125164448760\"></p>\n<h1>二、客户端编写</h1>\n<p>1）常量类</p>\n<pre><code class=\"language-java\">public class Constants {\n    /**\n     * 配置中心url\n     */\n    public static final String URL_NACOS = &quot;127.0.0.1&quot;;\n\n    public static final String NACOS_DATAID = &quot;test-nacos-config.yml&quot;;\n    public static final String NACOS_Group = &quot;DEFAULT_GROUP&quot;;\n}\n\n</code></pre>\n<p>2）客户端工具</p>\n<pre><code class=\"language-java\">import com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport com.alibaba.nacos.api.config.listener.Listener;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Properties;\nimport java.util.concurrent.Executor;\n\npublic class NacosUtils {\n    private static ConfigService configService;\n\n    /**\n     * 读取配置超时时间，单位 ms\n     */\n    private static final int TIMEOUT = 1000 * 3;\n    /**\n     * 获取配置文件内容\n     */\n    private static String content = &quot;&quot;;\n\n    static {\n        try {\n            Properties properties = new Properties();\n            properties.put(PropertyKeyConst.SERVER_ADDR, Constants.URL_NACOS);\n            configService = NacosFactory.createConfigService(properties);\n        } catch (NacosException e) {\n            LocalLog.error(&quot;连接配置中心失败!&quot;, e);\n            System.exit(1);\n        }\n    }\n\n    /**\n     * 获取配置中心配置内容\n     *\n     * @param group  命名空间\n     * @param dataId 数据库\n     * @return Properties\n     */\n    public static Properties getConfig(String group, String dataId) {\n        Properties properties = null;\n        try {\n            String config = configService.getConfig(dataId, group, 3000);\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(config.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (Exception e) {\n            LocalLog.error(&quot;&quot;, &quot;从配置中心获取配置失败，group={},dataId={}&quot;, group, dataId, e);\n        }\n        if (null == properties) {\n            LocalLog.info(&quot;&quot;, &quot;从配置中心获取配置失败，group={},dataId={}&quot;, group, dataId);\n        }\n        return properties;\n    }\n\n    /**\n     * 动态读取nocas配置内容\n     *\n     * @param dataId 配置ID\n     * @param group  分组\n     * @return\n     */\n    public static Properties getConfigProperties(String dataId, String group) {\n        Properties properties = null;\n        try {\n            content = configService.getConfig(dataId, group, TIMEOUT);\n            configService.addListener(dataId, group, new Listener() {\n                @Override\n                public void receiveConfigInfo(String configInfo) {\n                    content = configInfo;\n                    LocalLog.info(&quot;修改后的配置ID是：[&quot; + dataId + &quot;]，配置分组是：[&quot; + group + &quot;]获取的配置信息是&quot; + content);\n                }\n\n                @Override\n                public Executor getExecutor() {\n                    return null;\n                }\n            });\n            ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));\n            properties = new Properties();\n            properties.load(byteArrayInputStream);\n        } catch (NacosException e) {\n            LocalLog.error(&quot;Nacos读取配置超时或网络异常&quot;, e);\n        } catch (IOException e) {\n            LocalLog.error(&quot;加载到properties对象出现IO异常&quot;, e);\n        }\n        return properties;\n    }\n\n}\n\n</code></pre>\n<p>3）配置文件</p>\n<pre><code class=\"language-java\">spring:\n  application:\n    name: nacos-config-example\n    group: sa\n    developer: zhengtianqi&lt;郑天祺&gt;\n  cloud:\n    nacos:\n      config:\n        server-addr: http://localhost:8848\n\nserver:\n  port: 8080\n</code></pre>\n<p>4）启动类</p>\n<pre><code class=\"language-java\">import com.sy.log.LocalLog;\nimport com.sy.sa.nacos.common.constant.Constants;\nimport com.sy.sa.nacos.common.utils.NacosUtils;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\nimport java.util.Properties;\nimport java.util.concurrent.TimeUnit;\n\n@SpringBootApplication\npublic class NacosConfigExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosConfigExampleApplication.class, args);\n        // 测试动态加载配置\n        Properties properties = NacosUtils.getConfigProperties(Constants.NACOS_DATAID, Constants.NACOS_Group);\n        System.out.println(properties.getProperty(&quot;user.name&quot;) + &quot;:&quot; + properties.getProperty(&quot;user.password&quot;));\n    }\n\n}\n</code></pre>\n<h1>三、引入的依赖</h1>\n<pre><code class=\"language-java\">    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;spring-cloud-alibaba.version&gt;2.1.1.RELEASE&lt;/spring-cloud-alibaba.version&gt;\n        &lt;spring-cloud-greenwich.version&gt;0.9.0.RELEASE&lt;/spring-cloud-greenwich.version&gt;\n    &lt;/properties&gt;\n        \n    &lt;dependencies&gt;\n\t\t&lt;!--nacos--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;\n            &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;\n            &lt;version&gt;1.1.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n        \n&lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring-cloud-greenwich.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring-cloud-alibaba.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n</code></pre>\n<h1>四、效果</h1>\n<pre><code class=\"language-java\">2019-11-25 16:48:12.276  INFO 444 --- [-127.0.0.1_8848] locallog                                 : [../utils/NacosUtils$1.receiveConfigInfo:81][192.168.116.1][] - 修改后的配置ID是：[test-nacos-config.yml]，配置分组是：[DEFAULT_GROUP]获取的配置信息是user.name=zhengtianqi\nuser.password=12345678\n</code></pre>\n<p>解释：</p>\n<p>上述代码中没有用到SpringCloud，只用到了nacos的客户端。因为 如果使用SpringCloud读取多个配置文件（a.properties, b.properties），a中是user.name=123，b中是user.name=1234； 会有覆盖的情况</p>\n<pre><code class=\"language-java\">  ConfigurableApplicationContext applicationContext = SpringApplication.run(ConfigApplication.class, args);\n        String userName = applicationContext.getEnvironment().getProperty(&quot;user.name&quot;);\n        String userPassword = applicationContext.getEnvironment().getProperty(&quot;user.password&quot;);\n</code></pre>\n<p>如果多人开发没有注意到这种情况，会引起配置文件的key冲突导致出现问题</p>\n"},{"title":"JAVA数据类型易混概念","author":"郑天祺","date":"2020-01-06T06:30:00.000Z","_content":"\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","source":"_posts/JAVA数据类型.md","raw":"title: JAVA数据类型易混概念\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-01-06 14:30:00\n\n---\n\n1、整型\n\n​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。\t\n\n​\t![image-20200106143552647](/img/inttype.png)\n\n​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。\n\n​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。\n\n​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。\n\n​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）\n\n​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。\n\n​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）\n\n2、浮点型\n\n![image-20200106150945185](/img/floattype.png)\n\n​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )\n\n很多情况下，不使用float。\n\n​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）\n\n三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN\n\n```java\npublic class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(\"Double.POSITIVE_INFINITY = \" + 1.0 / 0.0);\n        System.out.println(\"Double.NEGATIVE_INFINITY = \" + -1.0 / 0.0);\n        System.out.println(\"Double.NaN = \" + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n```\n\n​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。\n\n3、char类型\n\n​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。\n\n![image-20200106152036190](/img/chartype.png)\n\n​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。\n\n​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。\n\n4、boolean 类型\n\n​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。\n\n5、数值类型之间的转换\n\n​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ； \n\n​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。\n\n![image-20200106160423586](/img/typetrans.png)\n\n​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。\n\n​\t\t强制转换也会造成精度丢失。\n\n​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9\n\n6、java.math下有两个很有用的类\n\n​\t\tBigInteger 和 BigDecimal：\n\n​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运\n\n​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：\t\t\n\n​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;\n\n​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )","slug":"JAVA数据类型","published":1,"updated":"2020-01-06T09:53:20.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0w001ax8vh8tyod389","content":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、整型</p>\n<p>​\t\t在 Java 中 ， 整型的范围与运行 Java 代码的机器无关 。</p>\n<p>​\t<img src=\"/img/inttype.png\" alt=\"image-20200106143552647\"></p>\n<p>​\t\t在通常情况下， int类型最常用。 但如果表示星球上的居住人数 ，就需要使用 long 类型了。byte 和 short 类型主要用于特定的应用场合 ，例如 ，底层的文件处理或者需要控制占用存储空间量的大数组 。</p>\n<p>​\t\t长整型数值有一个后缀 L 或 1 ( 如 4000000000 L ) 。</p>\n<p>​\t\t十六进制数值有一个前缀 0x 或 0X ( 如0xCAFE）。</p>\n<p>​\t\t八进制有一个前缀 0 ,例如 ， 010 对应八进制中的 8。（很容易混淆，不建议使用）</p>\n<p>​\t\t从 Java 7 开始 ， 加上前缀 0b 或 0B 就可以写二进制数 。 例如 ，0b1001就是 9 。</p>\n<p>​\t\t从 Java 7 开始， 还可以为数字字面量加下划线 ， 如用1_000_000这些下划线只是为丫让人更易读 。Java编译器会去除这些下划线。 ( 或0b1111_0100_0010_0100_0000表示一百万）</p>\n<p>2、浮点型</p>\n<p><img src=\"/img/floattype.png\" alt=\"image-20200106150945185\"></p>\n<p>​\t\tdouble 表示这种类型的数值精度是 float 类型的两倍 （ 有人称之为双精度数值 )</p>\n<p>很多情况下，不使用float。</p>\n<p>​\t\tfloat 类型的数值有一个后缀 F 或 f ( 例如，3.14 F ) 。没有后缀 F的浮点数值 （ 如 3.14 ) 默认为 double 类型。当然 ， 也可以在浮点数值后面添加后缀 D 或 d（例如，3.14D）</p>\n<p>三个常量值：Double _ POSITIVE _ INFINITY 、 Double . NEGATIVEJNFINITY 和 Double . NaN</p>\n<pre><code class=\"language-java\">public class ConstantTest {\n    public static void main(String[] args) {\n        System.out.println(&quot;Double.POSITIVE_INFINITY = &quot; + 1.0 / 0.0);\n        System.out.println(&quot;Double.NEGATIVE_INFINITY = &quot; + -1.0 / 0.0);\n        System.out.println(&quot;Double.NaN = &quot; + 0.0d / 0.0);\n        System.out.println(Double.class);\n        // 如果得到一个完全可预测的结果比运行速度更重要的话， 那么就应该使用StrictMath类 遵循IEEE 754\n        System.out.println(StrictMath.max(1, 2));\n    }\n}\n\n</code></pre>\n<p>​\t\tWarning：浮点数值不适用于无法接受舍入误差的金融计算中。如果在数值计算中不允许有任何舍入误差 ， 就应该使用 BigDecimal类。</p>\n<p>3、char类型</p>\n<p>​\t\tchar 类型原本用于表示单个字符。不过 ，现在情况已经有所变化 。如今，有些 Unicode字符可以用一个 char 值描述， 另外一些 Unicode 字符则需要两个char 值。</p>\n<p><img src=\"/img/chartype.png\" alt=\"image-20200106152036190\"></p>\n<p>​\t\tUnicode 打破了传统字符编码机制的限制，解决世界上文字编码不一致的问题。在设计 Java 时决定采用16 位的 Unicode 字符集， 这样会比使用 8 位字符集的程序设计语言有很大的改进。现在 ， 16 位的 char 类型已经不能满足描述所有 Unicode 字符的需要了，利用码点解决。</p>\n<p>​\t\t最好不使用char类型，除非确定需要处理UTF-16代码单元。</p>\n<p>4、boolean 类型</p>\n<p>​\t\tboolean（布尔）有两个值：true 或 false，与整型不能进行相互转换。</p>\n<p>5、数值类型之间的转换</p>\n<p>​\t\t在图3-1中有 6 个实心箭头 ，表示无信息丢失的转换 ；</p>\n<p>​\t\t有 3 个虚箭头 ， 表示可能有精度损失的转换。</p>\n<p><img src=\"/img/typetrans.png\" alt=\"image-20200106160423586\"></p>\n<p>​\t\t如果两个操作数中有一个是 double类型 ， 另一个操作数就会转换为 double 类型。\n​\t\t否则 ， 如果其中一个操作数是 float 类型 ， 另一个操作数将会转换为 float 类型 。\n​\t\t否则 ，如果其中一个操作数是 long 类型， 另一个操作数将会转换为 long 类型 。\n​\t\t否则 ， 两个操作数都将被转换为 int 类型 。</p>\n<p>​\t\t强制转换也会造成精度丢失。</p>\n<p>​\t\t例如 ：\n​\t\t\tdouble x * 9.997 ;\n​\t\t\tint nx = ( int ) x ;\n​\t\t\t这样 ， 变量 nx 的值为 9</p>\n<p>6、java.math下有两个很有用的类</p>\n<p>​\t\tBigInteger 和 BigDecimal：</p>\n<p>​\t\tBiglnteger 类实现了任意精度的整数运算 ， BigDecimal 实现了任意精度的浮点数运</p>\n<p>​\t\t使用静态的valueOf 方法可以将普通的数值转换为大数值：</p>\n<p>​\t\t\t\tBiglnteger a = Biglnteger . valueOf ( 100 ) ;</p>\n<p>​\t\t大数值类中的 add 和 multiply 方法 。\n​\t\t\t\tBiglnteger c = a.add ( b ) ;  / / c = a + b\n​\t\t\t\tBiglnteger d = c.multiply(b.add(Biglnteger.valueOf (2))) ;  // d = c * ( b + 2 )</p>\n"},{"title":"SimpleDateFormat引发的线程安全问题","author":"郑天祺","date":"2019-10-12T10:42:00.000Z","_content":"\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","source":"_posts/SimpleDateFormat引发的线程安全问题.md","raw":"title: SimpleDateFormat引发的线程安全问题\nauthor: 郑天祺\ntags:\n  - 并发\n  - 线程安全\ncategories:\n  - java基础\ndate: 2019-10-12 18:42:00\n\n---\n\n# \t一、问题产生\n\n​\t在写java程序时，有时间戳转换的操作。\n\n```java\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = \"2019-10-12 18:50:30\";\n\n        SimpleDateFormat inPut = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        SimpleDateFormat outPut = new SimpleDateFormat(\"yyyy年MM月dd日\");\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(\"时间转换出错，出错信息为 ={}\" + e);\n        }\n\n    }\n}\n\n```\n\n\n\n涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。\n\n我们就把它们放到了一个常量类里，随时用随时取。\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(\"yyyyMMddHHmmssSSS\");\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd\");\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(\"yyyy年MM月\");\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(\"yyyy-MM\");\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n}\n```\n\n可是你想省事，麻烦就随之而来了！\n\n先看看错误：\n\n```java\n[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: \".220188E.4220188\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n```\n\n# 二、问题查找\n\ndebug发现传出的参数不是自己想要的参数。可是为什么呢？\n\n​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。\n\n具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568\n\n# 三、解决方案\n\n​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。\n\n​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。\n\n​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。\n\n​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd\n\n​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法\n\n​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。\n\n# 四、部分代码\n\n​\t用了ThreadLocal\n\n```java\n/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal<SimpleDateFormat> IN_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMddHHmmssSSS\"));\n    public static final ThreadLocal<SimpleDateFormat> VIEW_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd\"));\n    public static final ThreadLocal<SimpleDateFormat> OUT_FORMAT = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n}\n```\n\n```java\n// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n```\n\n","slug":"SimpleDateFormat引发的线程安全问题","published":1,"updated":"2019-10-12T12:08:31.315Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp0z001dx8vhcj4720bi","content":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(&quot;时间转换出错，出错信息为 ={}&quot; + e);\n        }\n\n    }\n}\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n}\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568</p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd</p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n}\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、问题产生</h1>\n<p>​\t在写java程序时，有时间戳转换的操作。</p>\n<pre><code class=\"language-java\">import java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * @author zhengtianqi\n * @date 2019/10/12\n */\npublic class DateTrans {\n\n    public static void main(String[] args) {\n\n        // 将2019-10-12 18:50:30 改成 2019年10月12日\n        String inDate = &quot;2019-10-12 18:50:30&quot;;\n\n        SimpleDateFormat inPut = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n        SimpleDateFormat outPut = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);\n\n        try {\n            Date temp = inPut.parse(inDate);\n            String outDate = outPut.format(temp);\n\n            System.out.println(outDate);\n\n        } catch (ParseException e) {\n            System.out.println(&quot;时间转换出错，出错信息为 ={}&quot; + e);\n        }\n\n    }\n}\n\n</code></pre>\n<p>涉及时间戳转换时，每次我们都new一个SimpleDateFormat对象，用起来很麻烦。</p>\n<p>我们就把它们放到了一个常量类里，随时用随时取。</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\tpublic static final SimpleDateFormat IN_FORMAT = new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;);\n\tpublic static final SimpleDateFormat OUT_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);\n\tpublic static final SimpleDateFormat VIEW_FORMAT = new SimpleDateFormat(&quot;yyyy年MM月&quot;);\n\tpublic static final SimpleDateFormat INNER_FORMAT = new SimpleDateFormat(&quot;yyyy-MM&quot;);\n\tpublic static final SimpleDateFormat RECALL_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);\n}\n</code></pre>\n<p>可是你想省事，麻烦就随之而来了！</p>\n<p>先看看错误：</p>\n<pre><code class=\"language-java\">[2019-10-12 17:45:35,468][locallog][ERROR][TID: N/A][../filters/GlobalExeption.exceptionHandler:18][10.7.5.22][] - 服务器内部错误!\njava.lang.NumberFormatException: For input string: &quot;.220188E.4220188&quot;\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) ~[?:1.8.0_221]\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) ~[?:1.8.0_221]\n        at java.lang.Double.parseDouble(Double.java:538) ~[?:1.8.0_221]\n        at java.text.DigitList.getDouble(DigitList.java:169) ~[?:1.8.0_221]\n        at java.text.DecimalFormat.parse(DecimalFormat.java:2089) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) ~[?:1.8.0_221]\n        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) ~[?:1.8.0_221]\n        at java.text.DateFormat.parse(DateFormat.java:364) ~[?:1.8.0_221]\n</code></pre>\n<h1>二、问题查找</h1>\n<p>debug发现传出的参数不是自己想要的参数。可是为什么呢？</p>\n<p>​\t因为它是线程不安全的，当并发环境下，如果考虑不周使用SimpleDateFormat方法可以会出现线程安全方面的问题。原因当问我们使用parse方法时，使用CalendarBuilder日历创建者类创建日期，其中calendar实例因为cpu时间片切换时共享变量进行clear操作，导致数据不一致。</p>\n<p>具体原因：https://blog.csdn.net/lululove19870526/article/details/83684568</p>\n<h1>三、解决方案</h1>\n<p>​\t1、临时创建：对于每个转换都new一个实例，有背与我们代码简洁的初衷，放弃。</p>\n<p>​\t2、synchronized：阻塞，让线程不在并发，对效率影响很大，放弃。</p>\n<p>​\t3、ThreadLocal：线程隔离机制，代码量减少了，和1一样也牺牲了部分空间，还是个不错的解决方法。</p>\n<p>​\t\thttps://www.jianshu.com/p/3c5d7f09dfbd</p>\n<p>​\t4、Apache的 DateFormatUtils 与 FastDateFormat：线程安全，但是木有parse()方法</p>\n<p>​\t5、Joda-Time：感觉不错，就是源码有点多没敢用，github目前2.4K star。</p>\n<h1>四、部分代码</h1>\n<p>​\t用了ThreadLocal</p>\n<pre><code class=\"language-java\">/**\n * 枚举类 常量类\n *\n * @author zhengtianqi\n * @date 2019/8/16\n */\npublic enum ConstantUtils {\n    \n\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; IN_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyyMMddHHmmssSSS&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; VIEW_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;));\n    public static final ThreadLocal&lt;SimpleDateFormat&gt; OUT_FORMAT = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));\n}\n</code></pre>\n<pre><code class=\"language-java\">// 调用\nConstantUtils.IN_FORMAT.get().format(requestParams.getReleaseTime())\n</code></pre>\n"},{"title":"Spark相关概述","author":"郑天祺","date":"2019-12-18T05:41:00.000Z","_content":"\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","source":"_posts/Spark相关概述.md","raw":"title: Spark相关概述\nauthor: 郑天祺\ntags:\n\n  - Spark\ncategories:\n  - 大数据\ndate: 2019-12-18 13:41:00\n\n---\n\n# 一、Spark的核心组件是：\n\n​\t\t\t\t集群资源管理服务（Cluster Manager）\t\t\n\n​\t\t\t\t运行作业任务的节点（WorkerNode），\n\n​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）\n\n![image-20191218134210879](/img/Spark.png)\n\n说明：\n\n![image-20191218140600902](/img/spark-all.png)\n\n# 二、关键概念\n\n（1）RDD\n\n​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。\n\n（2）DAG\n\n​\t\tSpark使用有向无环图进行任务调度。\n\n（3）Spark SQL\n\n​\t\t用于结构化数据的计算。\n\n（4）DataFrame\n\n​\t\t分布式的、按照名名列的形式组织的数据集合。\n\n（5）SQLContext\n\n​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。\n\n（6）JDBC数据源\n\n三、Spark 和 HDFS 的配合关系\n\n​\t\t![image-20191218141731121](/img/spark+hdfs.png)\n\n- （1）读取文件的详细步骤：\n- SparkScheduler 与 HDFS 交互获取 File A 的文件信息。\n- HDFS返回该文件具体的 Block 信息\n- SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block\n- Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分\n- （2）HDFS文件写入的详细步骤：\n- SparkScheduler 创建要写入文件的目录\n- 根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor\n- Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下","slug":"Spark相关概述","published":1,"updated":"2019-12-18T09:12:52.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp12001hx8vhw204lped","content":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spark的核心组件是：</h1>\n<p>​\t\t\t\t集群资源管理服务（Cluster Manager）</p>\n<p>​\t\t\t\t运行作业任务的节点（WorkerNode），</p>\n<p>​\t\t\t\t每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>\n<p><img src=\"/img/Spark.png\" alt=\"image-20191218134210879\"></p>\n<p>说明：</p>\n<p><img src=\"/img/spark-all.png\" alt=\"image-20191218140600902\"></p>\n<h1>二、关键概念</h1>\n<p>（1）RDD</p>\n<p>​\t\tSpark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>\n<p>（2）DAG</p>\n<p>​\t\tSpark使用有向无环图进行任务调度。</p>\n<p>（3）Spark SQL</p>\n<p>​\t\t用于结构化数据的计算。</p>\n<p>（4）DataFrame</p>\n<p>​\t\t分布式的、按照名名列的形式组织的数据集合。</p>\n<p>（5）SQLContext</p>\n<p>​\t\tSpark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>\n<p>（6）JDBC数据源</p>\n<p>三、Spark 和 HDFS 的配合关系</p>\n<p>​\t\t<img src=\"/img/spark+hdfs.png\" alt=\"image-20191218141731121\"></p>\n<ul>\n<li>（1）读取文件的详细步骤：</li>\n<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>\n<li>HDFS返回该文件具体的 Block 信息</li>\n<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>\n<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>\n<li>（2）HDFS文件写入的详细步骤：</li>\n<li>SparkScheduler 创建要写入文件的目录</li>\n<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>\n<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>\n</ul>\n"},{"title":"MapReduce概述","author":"郑天祺","date":"2019-12-16T09:13:00.000Z","_content":"\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","source":"_posts/MapReduce概述.md","raw":"title: MapReduce概述\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-16 17:13:00\n\n---\n\n# 一、基本模型\n\n​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。\n\n​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。\n\n​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。\n\n其中：\n\n​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker\n\n​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）\n\n​\t（2）TaskTracker用于跟踪任务的执行情况。\n\n# 二、wordcount\n\n​\t统计所有文件中每一个单词出现的次数（频次）。\n\n​\t![image-20191216173559584](/img/wordcount.png)\n\n​\t所做的操作：\n\n## （1）拆分输入数据\n\n​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）\n\n![image-20191216173747383](/img/wordcount-split.png)\n\n​\t注意：如果只有一个文件，且很小，系统只分配一个Split；\n\n​\t\t\t\t如果由多个文件，或者文件很大，多个Split\n\n​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符\n\n## （2）执行Map方法\n\n​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对\n\n​\t![image-20191216174237035](/img/wordcount-map.png)\n\n​\t\t注意：后边这个1是个数\n\n## （3）排序与合并处理\n\n​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。\n\n即：先排序 后累加\n\n## （4）Reduce 阶段的排序与合并\n\n​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出\n\n![image-20191216174856510](/img/wordcount-reduce.png)\n\n简述上述过程：\n\n### （A）Map\n\n#### \t（a）Read：\n\n​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）\n\n#### \t（b）Map：\n\n​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）\n\n#### \t（c）Collect：\n\n​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像\n\n[disruptor]: https://blog.csdn.net/qq_23034755/article/details/90137103\n\n，log4j2用的队列）\n\n#### \t（d）Spill：\n\n​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n#### \t（e）Combine：\n\n​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件\n\n### （B）Reduce\n\n#### \t（a）Shuffle：\n\n​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n\n#### \t（b）Merge：\n\n​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）\n\n#### \t（c）Sort：\n\n​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。\n\n#### \t（d）Reduce：\n\n​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理\n\n#### \t（e）Write：\n\n​\t\treduce()函数将计算结果写到HDFS\n\n","slug":"MapReduce概述","published":1,"updated":"2019-12-16T10:07:22.822Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp14001kx8vhd47zqkxy","content":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2>（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2>（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2>（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2>（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3>（A）Map</h3>\n<h4>（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4>（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4>（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4>（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4>（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3>（B）Reduce</h3>\n<h4>（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4>（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4>（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4>（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4>（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、基本模型</h1>\n<p>​\tMapReduce采取了分而治之的基本思想，将一个大的作业分解成若干小的任务，提交给集群的多台计算机处理，这样就大大提高了完成作业的效率。</p>\n<p>​\t在Hadoop平台上，MapReduce框架负责处理并行编程中分布式存储、工作调度、负载均衡、容错及网络通信等复杂工作，把处理过程高度抽象为两个函数：Map 和 Reduce。</p>\n<p>​\tMap负责把作业分解成多个任务，Reduce负责把分解后多任务处理的结果汇总起来。</p>\n<p>其中：</p>\n<p>​\t执行MapReduce作业的机器角色由两个：JobTracker 和 TaskTracker</p>\n<p>​\t（1）JobTracker用于调度作业（一个集群只有一个JobTracker）</p>\n<p>​\t（2）TaskTracker用于跟踪任务的执行情况。</p>\n<h1>二、wordcount</h1>\n<p>​\t统计所有文件中每一个单词出现的次数（频次）。</p>\n<p>​\t<img src=\"/img/wordcount.png\" alt=\"image-20191216173559584\"></p>\n<p>​\t所做的操作：</p>\n<h2>（1）拆分输入数据</h2>\n<p>​\t拆分数据 属于 Map 的输入阶段，系统会逐行读取文件的数据，得到一系列的（key/value）</p>\n<p><img src=\"/img/wordcount-split.png\" alt=\"image-20191216173747383\"></p>\n<p>​\t注意：如果只有一个文件，且很小，系统只分配一个Split；</p>\n<p>​\t\t\t\t如果由多个文件，或者文件很大，多个Split</p>\n<p>​\t\t\t\t上图 0、12为偏移量（包含回车）即：H是第0个字符   B是第12个字符</p>\n<h2>（2）执行Map方法</h2>\n<p>​\t分割完成后，系统会将分割好的（key/value）对交给用户定义的 Map 方法进行处理，生成新的（key/value）对</p>\n<p>​\t<img src=\"/img/wordcount-map.png\" alt=\"image-20191216174237035\"></p>\n<p>​\t\t注意：后边这个1是个数</p>\n<h2>（3）排序与合并处理</h2>\n<p>​\t系统得到Map方法输出的（key/value）对后，Mapper 会将它们按照 key 值进行排序，并执行Combine 过程，将 key 值相同的 value 值累加，得到 Mapper 的最终输出结果。</p>\n<p>即：先排序 后累加</p>\n<h2>（4）Reduce 阶段的排序与合并</h2>\n<p>​\tReducer 先对从 Mapper 接收的数据进行排序，再交由用户自定义的 Reduce 方法进行处理，得到新的（key/value）对，并作为WordCount的结果输出</p>\n<p><img src=\"/img/wordcount-reduce.png\" alt=\"image-20191216174856510\"></p>\n<p>简述上述过程：</p>\n<h3>（A）Map</h3>\n<h4>（a）Read：</h4>\n<p>​\t\tMap Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出多个（key/value）</p>\n<h4>（b）Map：</h4>\n<p>​\t\t将解析出的（key/value）交给用户编写的Map函数处理，并产生一系列新的（key/value）</p>\n<h4>（c）Collect：</h4>\n<p>​\t\t在用户编写的Map函数中，数据处理完成后，一般会调用OutputCollector.collect()收集结果。在该函数内部，它会将生成（key/value）分片（通过Partitioner），并写入一个环形内存缓冲区中。（感觉像</p>\n<p>，log4j2用的队列）</p>\n<h4>（d）Spill：</h4>\n<p>​\t\t环形缓冲区填满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<h4>（e）Combine：</h4>\n<p>​\t\t当所有数据处理完成后，Map Task 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件</p>\n<h3>（B）Reduce</h3>\n<h4>（a）Shuffle：</h4>\n<p>​\t\t也成为Copy。Reduce Task从各个Map Task上远程复制一片数据，并针对某一篇数据进行判断，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<h4>（b）Merge：</h4>\n<p>​\t\t在远程复制的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。（为啥要用两个线程呢？）</p>\n<h4>（c）Sort：</h4>\n<p>​\t\t按照MapReduce语义，用户编写的 Reduce 函数输入数据时按 key 进行聚集的一组数据。（采用基于排序的策略）。各个Map Task实现了局部排序，Reduce Task只需对所有的数据进行一次归并排序即可。</p>\n<h4>（d）Reduce：</h4>\n<p>​\t\tReduce Task将每组数据一次交给用户编写的 reduce()函数处理</p>\n<h4>（e）Write：</h4>\n<p>​\t\treduce()函数将计算结果写到HDFS</p>\n"},{"title":"Spring Bean作用域与生命周期","author":"郑天祺","date":"2019-11-14T08:52:00.000Z","_content":"\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","source":"_posts/Spring-Bean生命周期.md","raw":"title: Spring Bean作用域与生命周期\nauthor: 郑天祺\ntags:\n\n  - spring\ncategories:\n  - java基础\ndate: 2019-11-14 16:52:00\n\n---\n\n# 一、Spring Bean生命周期\n\n![img](/img/clip_image002.png)\n\n解释：\n\n- Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。\n- 一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。\n- 然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。\n\n  在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：\n\n ![img](/img/clip_image004.png)\n\n解释：\n\n- 其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。\n- 即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。\n- Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。\n\n\n\n# 二、Spring Bean作用域\n\n## 1、使用@Profile\n\n#### 1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean\n\n​    ![img](/img/SpringBean3.png)\n\n#### 2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：\n\n​\t-Dspring.profiles.active=dev\n\n​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default\n\n#### 3）按照 springboot 的规则\n\n​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件\n\n## 2、使用 Spring EL\n\n####   1）读取属性文件的值，如：\n\n```java\n// ${......} 代表占位符\n@Value(\"${database.driverName}\")   \nString driver\n```\n\n \n\n####   2）记录一个Bean初始化事件，如：\n\n```java\n// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(\"#{T(System).currentTimeMillis()}\")  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(\"#{‘使用 Spring EL 赋值字符串’}\")\nprivate String str = null;\n// 科学计数法赋值\n@Value(\"#{9.3E3}\")\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(\"#{beanName.str}\")\nprivate String otherBeanProp = null;\n```\n\n还可以进行计算、三元运算、比较等。\n\n \n\n ","slug":"Spring-Bean生命周期","published":1,"updated":"2019-11-14T09:19:21.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp17001ox8vh8yygxdpt","content":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2>1、使用@Profile</h2>\n<h4>1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4>2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4>3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2>2、使用 Spring EL</h2>\n<h4>1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// ${......} 代表占位符\n@Value(&quot;${database.driverName}&quot;)   \nString driver\n</code></pre>\n<h4>2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#{T(System).currentTimeMillis()}&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#{‘使用 Spring EL 赋值字符串’}&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#{9.3E3}&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#{beanName.str}&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Spring Bean生命周期</h1>\n<p><img src=\"/img/clip_image002.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>\n<p>Spring 通过我们的配置，如 @ComponentScan 定义的扫描路径去找到带有 @Component     的类，这个过程就是一个资源定位的过程。</p>\n</li>\n<li>\n<p>一旦找到了资源，那么它就开始解析，并且将定义的信息保存起来。注意，此时还没有初始化 Bean ，也就没有 Bean 实例，它有的仅仅是 Bean 的定义。</p>\n</li>\n<li>\n<p>然后就会把 Bean 定义发布到 Spring IoC 容器中，此时，IoC容器也只有 Bean 的定义，还是没有 Bean 的实例生成。</p>\n<p>在默认的情况下，Spring会继续去完成Bean的实例化和依赖注入， 这样从IoC容器中就可以得到一个依赖注入完成的Bean。但是，有些Bean会在取的时候才初始化和依赖注入。如下图：</p>\n</li>\n</ul>\n<p><img src=\"/img/clip_image004.png\" alt=\"img\"></p>\n<p>解释：</p>\n<ul>\n<li>其中流程节点针对于单个Bean，BeanPostProcessor是针对所有Bean而言。</li>\n<li>即使你定义了ApplicationContextAware接口，但是有时候并不会调用，这要根据你的IoC容器来决定。</li>\n<li>Spring IoC     容器的最低要求是实现 BeanFactory 接口，而不是实现 ApplicationContext 接口。对于那些没有实现     ApplicationContext 接口的容器，对生命周期对应的 ApplicationContextAware     定义的方法也是不会被调用的，只有实现了 ApplicationContext 接口的容器，才会在生命周期调用 ApplicationContextAware 所定义的     setApplicationContext 方法。</li>\n</ul>\n<h1>二、Spring Bean作用域</h1>\n<h2>1、使用@Profile</h2>\n<h4>1）假设存在dev_spring_boot 和 test_spring_boot两个数据库，使用注解@Profile定义两个Bean</h4>\n<p>​    <img src=\"/img/SpringBean3.png\" alt=\"img\"></p>\n<h4>2）在 Java 启动项目中，我们只需要如下配置就能启动Profile机制：</h4>\n<p>​\t-Dspring.profiles.active=dev</p>\n<p>​\t注：Spring 会先判定是否存在 spring.profiles.active 配置后，再去查找 spring.profiles.default 配置的，所以 spring.profiles.active 的优先级要大于 spring.profiles.default</p>\n<h4>3）按照 springboot 的规则</h4>\n<p>​\t-Dspring.profiles.active 配置的值记为 {profile} ，则它会用 application-{profiles}.properties 文件去代替原来默认的 application.properties文件</p>\n<h2>2、使用 Spring EL</h2>\n<h4>1）读取属性文件的值，如：</h4>\n<pre><code class=\"language-java\">// ${......} 代表占位符\n@Value(&quot;${database.driverName}&quot;)   \nString driver\n</code></pre>\n<h4>2）记录一个Bean初始化事件，如：</h4>\n<pre><code class=\"language-java\">// #{......} 代表启用 Spring表达式，它将具有运算功能；T(......)代表的是引入类\n@Value(&quot;#{T(System).currentTimeMillis()}&quot;)  \nprivate Long initTime = null;\n//直接赋值： 赋值字符串\n@Value(&quot;#{‘使用 Spring EL 赋值字符串’}&quot;)\nprivate String str = null;\n// 科学计数法赋值\n@Value(&quot;#{9.3E3}&quot;)\nprivate double d;\n// 其他Spring Bean属性赋值当前的Bean\n@Value(&quot;#{beanName.str}&quot;)\nprivate String otherBeanProp = null;\n</code></pre>\n<p>还可以进行计算、三元运算、比较等。</p>\n"},{"title":"TCP IP四层网络模型","author":"郑天祺","date":"2019-08-30T07:12:00.000Z","_content":"\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","source":"_posts/TCP-IP四层网络模型.md","raw":"title: TCP IP四层网络模型\nauthor: 郑天祺\ntags:\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:12:00\n\n---\n\n## 1、用户发送请求\n\n![](/img/TCPIP用户发送请求.png)\n\n## 2、服务器接收请求\n\n![](/img/TCPIP服务器接收请求.png)\n\n## 3、网络连接模型\n\n（《网络是怎么连接的》课本翻译）\n\n![](/img/网络连接模型.png)\n\n## 4、使用协议进行通讯\n\n​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP\n\n![](/img/使用协议进行通讯.png)\n\n## 5、Socket通讯模型\n\n![](/img/Socket通讯模型.png)\n\n6、TCP协议的通信过程\n\n​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。\n\n![](/img/TCP协议通讯过程.png)\n\n![](/img/TCP协议通讯过程1.png)\n\n![](/img/TCP协议通讯过程2.png)","slug":"TCP-IP四层网络模型","published":1,"updated":"2019-10-15T12:19:39.272Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1b001sx8vhz7l7pj8a","content":"<h2>1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt></p>\n<h2>2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt></p>\n<h2>3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<h2>4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt></p>\n<h2>5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、用户发送请求</h2>\n<p><img src=\"/img/TCPIP%E7%94%A8%E6%88%B7%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82.png\" alt></p>\n<h2>2、服务器接收请求</h2>\n<p><img src=\"/img/TCPIP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E6%94%B6%E8%AF%B7%E6%B1%82.png\" alt></p>\n<h2>3、网络连接模型</h2>\n<p>（《网络是怎么连接的》课本翻译）</p>\n<p><img src=\"/img/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<h2>4、使用协议进行通讯</h2>\n<p>​\tsocket是一种抽象层，应用程序通过它来发送和接收数据，就像应用程序打开一个文件句柄，把数据读写到磁盘上一样。主要的socket类型为：\n​\t1.流套接字（stream socket）-TCP\n​\t2.数据报文套接字（datagram socket）-UDP</p>\n<p><img src=\"/img/%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF.png\" alt></p>\n<h2>5、Socket通讯模型</h2>\n<p><img src=\"/img/Socket%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<p>6、TCP协议的通信过程</p>\n<p>​\t对于TCP通信来说，每个TCPSocket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的Buffer和该Buffer的填充状态。</p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B.png\" alt></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B1.png\" alt></p>\n<p><img src=\"/img/TCP%E5%8D%8F%E8%AE%AE%E9%80%9A%E8%AE%AF%E8%BF%87%E7%A8%8B2.png\" alt></p>\n"},{"title":"SpringCloud-Alibaba整合Nacos服务注册发现","author":"郑天祺","date":"2019-12-03T07:18:00.000Z","_content":"\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","source":"_posts/SpringCloud-Alibaba整合Nacos服务注册发现.md","raw":"title: SpringCloud-Alibaba整合Nacos服务注册发现\nauthor: 郑天祺\ntags:\n\n  - SpringCloud\ncategories:\n  - SpringCloud\ndate: 2019-12-03 15:18:00\n\n---\n\n# 一、服务注册\n\n## 1、引入依赖\n\n```java\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n```\n\n## 2、配置application.yml\n\n在application.yaml配置文件内添加Nacos Server的地址：\n\n```java\nserver:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n```\n\n## 3、springboot启动类\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n```\n\n## 4、确认注册成功\n\n运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。\n\n![image-20191203152720691](/img/nacos-producer.png)\n\n# 二、服务发现\n\n\n基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。\n\n## 1、引入依赖\n\n```java\n<!-- Nacos服务发现 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n</dependency>\n\n<!-- 声明式调用 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n<!-- 负载均衡 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n<!-- 熔断器 -->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n</dependency>\n```\n\n## 2、配置文件配置\n\n在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：\n\n```java\nserver:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试>10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n```\n\n## 3、开启服务发现、负载均衡、熔断器功能\n\n在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {\"com.sy\"})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {\"com.sy\"})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n```\n\n## 4、创建服务代理类\n\n使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。\n\n注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：\n\n```java\n@Component\n@FeignClient(name = \"nacos-producer\", fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = \"/hello\")\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = \"/hello/{string}\")\n    String hello(@PathVariable(\"string\") String string);\n}\n```\n\n## 5、创建Hystrix的断路器工厂类\n\n```java\n@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory<ConsumerService> {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(\"fallback; reason was: \" + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return \"请求失败\";\n            }\n\n            @Override\n            public String hello(String string) {\n                return \"请求失败. string=\" + string;\n            }\n        };\n    }\n}\n```\n\n## 6、通用代理类的实例进行服务调用，与本地调用无异。如下：\n\n```java\n@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = \"/feign/{string}\", method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n```\n\n","slug":"SpringCloud-Alibaba整合Nacos服务注册发现","published":1,"updated":"2019-12-03T08:02:58.242Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1e001vx8vhxhrb7q46","content":"<h1>一、服务注册</h1>\n<h2>1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2>2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2>3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n</code></pre>\n<h2>4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2>1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2>2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2>3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {&quot;com.sy&quot;})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {&quot;com.sy&quot;})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n</code></pre>\n<h2>4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/{string}&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n}\n</code></pre>\n<h2>5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return &quot;请求失败&quot;;\n            }\n\n            @Override\n            public String hello(String string) {\n                return &quot;请求失败. string=&quot; + string;\n            }\n        };\n    }\n}\n</code></pre>\n<h2>6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/{string}&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、服务注册</h1>\n<h2>1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2>2、配置application.yml</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\nspring:\n  application:\n    name: nacos-producer # 注册到nacos的服务名称\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n</code></pre>\n<h2>3、springboot启动类</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient\npublic class NacosProviderDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(NacosProviderDemoApplication.class, args);\n    }\n}\n\n</code></pre>\n<h2>4、确认注册成功</h2>\n<p>运行程序，打开Nacos管理服务，可以看到nacos-producer已经成功注册。</p>\n<p><img src=\"/img/nacos-producer.png\" alt=\"image-20191203152720691\"></p>\n<h1>二、服务发现</h1>\n<p>基于Alibaba Nacos Spring Cloud（服务发现）、Spring Cloud OpenFeign（声明式调用，同时整合了熔断器、负载均衡），推荐使用此方法。</p>\n<h2>1、引入依赖</h2>\n<pre><code class=\"language-java\">&lt;!-- Nacos服务发现 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 声明式调用 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 负载均衡 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;!-- 熔断器 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h2>2、配置文件配置</h2>\n<p>在application.yaml配置文件内添加Nacos Server的地址，并开启feign的熔断器功能：</p>\n<pre><code class=\"language-java\">server:\n  port: 8081\n  \nspring:\n  application:\n    name: nacos-producer\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n\n#允许feign开启熔断器，默认未开启\nfeign:\n  hystrix:\n    enabled: true\n\nhystrix:\n  command:\n    default:\n      execution:\n        timeout:\n          enabled: true\n      isolation:\n        thread:\n          #目前有两个容器实例，单个请求超时5s,+重试&gt;10s，超15s则熔断\n          timeoutInMilliseconds: 15000\n\nribbon:\n  #ribbon请求连接的超时时间- 限制3秒内必须请求到服务，并不限制服务处理的返回时间\n  connectTimeout: 3000\n  #请求处理的超时时间 下级服务响应最大时间,超出时间消费方（路由也是消费方）返回timeout,超时时间不可大于断路器的超时时间\n  readTimeout: 5000\n</code></pre>\n<h2>3、开启服务发现、负载均衡、熔断器功能</h2>\n<p>在启动类添加 Spring Cloud 原生注解 @EnableDiscoveryClient ，开启服务注册发现功能，添加 @EnableCircuitBreaker 开始熔断器功能：</p>\n<pre><code class=\"language-java\">@SpringBootApplication\n@EnableDiscoveryClient   //开启服务发现\n@EnableCircuitBreaker    //开始熔断功能\n@EnableFeignClients(basePackages = {&quot;com.sy&quot;})   //开启Feign客户端，并指定扫描范围\n@ComponentScan(basePackages = {&quot;com.sy&quot;})\npublic class NacosDiscoveryExampleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(NacosDiscoveryExampleApplication.class, args);\n    }\n}\n</code></pre>\n<h2>4、创建服务代理类</h2>\n<p>使用@FeignClient注解声明服务调用的代理类，其中参数含义为：\n\t1.name：服务提供者注册在服务注册中心的名称；\n\t2.fallback：使用者提供的断路器实现，必须是当前代理类的实现类；\n\t3.fallbackFactory：使用者提供的Hystrix的断路器工厂类实现。</p>\n<p>注：fallback 与 fallbackFactory 只需要配置一个，建议使用fallbackFactory。 示例如下：</p>\n<pre><code class=\"language-java\">@Component\n@FeignClient(name = &quot;nacos-producer&quot;, fallbackFactory = HystrixClientFallbackFactory.class)\npublic interface ConsumerService {\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello&quot;)\n    String hello();\n\n    @LoadBalanced\n    @GetMapping(value = &quot;/hello/{string}&quot;)\n    String hello(@PathVariable(&quot;string&quot;) String string);\n}\n</code></pre>\n<h2>5、创建Hystrix的断路器工厂类</h2>\n<pre><code class=\"language-java\">@Component\npublic class HystrixClientFallbackFactory implements FallbackFactory&lt;ConsumerService&gt; {\n    @Override\n    public ConsumerService create(Throwable cause) {\n        // 打印日志\n        LocalLog.info(&quot;fallback; reason was: &quot; + cause.getMessage());\n        return new ConsumerService() {\n            @Override\n            public String hello() {\n                return &quot;请求失败&quot;;\n            }\n\n            @Override\n            public String hello(String string) {\n                return &quot;请求失败. string=&quot; + string;\n            }\n        };\n    }\n}\n</code></pre>\n<h2>6、通用代理类的实例进行服务调用，与本地调用无异。如下：</h2>\n<pre><code class=\"language-java\">@RestController\npublic class ConsumerController {\n    @Autowired\n    private ConsumerService consumerService;\n\n    @RequestMapping(value = &quot;/feign/{string}&quot;, method = RequestMethod.GET)\n    public String echo(@PathVariable String string) {\n        return consumerService.hello(string);\n    }\n}\n</code></pre>\n"},{"title":"TCP与UDP的区别","author":"郑天祺","date":"2020-07-21T00:00:00.000Z","_content":"\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","source":"_posts/TCP与UDP的区别.md","raw":"title: TCP与UDP的区别\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\n  - UDP\ncategories:\n  - 网络\ndate: 2020-07-21 08:00:00\n\n---\n\n# 引言\n\n​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。\n\n# 1、TCP/IP网络模型\n\n​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）\n\n​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\n\n![img](/img/TCPIP模型.png)\n\n# 2、UDP\n\n​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。\n\n​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。\n\n​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：\n\n## （1）面向无连接\t\n\n​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。\n\n具体来说就是：\n\n​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作\n\n## （2）单播、多播、广播\n\n​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n\n## （3）不可靠性\n\n​\t\t有可能收不到、数据可能不完整（丢包）\n\n## （4）头部开销小、传输高效\n\n![image-20200721081546701](/img/UDPHeader.png)\n\nUDP 头部包含了以下几个数据：\n\n​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n\n# 3、TCP\n\n​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。\n\n## （1）TCP连接\n\n三次握手、四次挥手\n\n## （2）特点\n\n面向连接：\n\n​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n\n仅支持单播传输：\n\n​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n\n面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n\n可靠传输：\n\n​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n\n提供拥塞控制：\n\n​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\n\nTCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n\n## 4、对比\n\n![image-20200721082159974](/img/UDPTCPcompare.png)\n\n​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP","slug":"TCP与UDP的区别","published":1,"updated":"2020-07-21T00:22:47.635Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1h001zx8vhwrcmdcu2","content":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2>（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2>（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2>（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2>（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2>（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2>（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2>4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>引言</h1>\n<p>​\t\t网络协议中，TCP/IP有两个具有代表性的传输层协议，分别是TCP和UDP。</p>\n<h1>1、TCP/IP网络模型</h1>\n<p>​\t\t计算机与网络设备要相互通信，双方就必须基于相同的方法和规则。而我们就把这种规则称为协议（protocol）</p>\n<p>​\t\tTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。</p>\n<p><img src=\"/img/TCPIP%E6%A8%A1%E5%9E%8B.png\" alt=\"img\"></p>\n<h1>2、UDP</h1>\n<p>​\t\tUDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。</p>\n<p>​\t\t在OSI模型中，UDP在第四层传输层，处于IP协议的上一层。</p>\n<p>​\t\tUDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。特点如下：</p>\n<h2>（1）面向无连接</h2>\n<p>​\t\t首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。</p>\n<p>具体来说就是：</p>\n<p>​\t\t在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了\n​\t\t在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作</p>\n<h2>（2）单播、多播、广播</h2>\n<p>​\t\tUDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。</p>\n<h2>（3）不可靠性</h2>\n<p>​\t\t有可能收不到、数据可能不完整（丢包）</p>\n<h2>（4）头部开销小、传输高效</h2>\n<p><img src=\"/img/UDPHeader.png\" alt=\"image-20200721081546701\"></p>\n<p>UDP 头部包含了以下几个数据：</p>\n<p>​\t\t两个十六位的端口号，分别为源端口（可选字段）和目标端口\n​\t\t整个数据报文的长度\n​\t\t整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误\n​\t\t因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的</p>\n<h1>3、TCP</h1>\n<p>​\t\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。</p>\n<h2>（1）TCP连接</h2>\n<p>三次握手、四次挥手</p>\n<h2>（2）特点</h2>\n<p>面向连接：</p>\n<p>​\t\t面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。</p>\n<p>仅支持单播传输：</p>\n<p>​\t\t每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。</p>\n<p>面向字节流：\n\t\tTCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。</p>\n<p>可靠传输：</p>\n<p>​\t\t对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。</p>\n<p>提供拥塞控制：</p>\n<p>​\t\t当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞</p>\n<p>TCP提供全双工通信：\n\t\tTCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）</p>\n<h2>4、对比</h2>\n<p><img src=\"/img/UDPTCPcompare.png\" alt=\"image-20200721082159974\"></p>\n<p>​\t\tTCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为\n​\t\t对数据准确性要求高，速度可以相对较慢的，可以选用TCP</p>\n"},{"title":"TCP握手、挥手协议","author":"郑天祺","date":"2019-08-30T07:58:00.000Z","_content":"\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","source":"_posts/TCP握手、挥手协议.md","raw":"title: TCP握手、挥手协议\nauthor: 郑天祺\ntags:\n\n  - TCP/IP\ncategories:\n  - 网络\ndate: 2019-08-30 15:58:00\n\n---\n\n## 1、TCP三次握手协议（打开连接）\n\n![](/img/三次握手协议1.png)\n\n第一次： A城发信，B城收到了------> 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的\n\n第二次：B城发信，A城收到了-----> 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手\n\n第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。\n\n更加简洁的图片\n\n![](/img/三次握手协议2.png)\n\n\n\n## 2、TCP四次挥手协议（关闭连接）\n\n![](/img/四次挥手协议.png)\n\n第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，\n\n第一次握手(SYN=1, seq=x)\n\n客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。\n\n发送完毕后，客户端进入SYN_SEND 状态。\n\n \n\n第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):\n\n服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。\n\n发送完毕后，服务器端进入SYN_RCVD 状态。\n\n \n\n第三次握手(ACK=1，ACKnum=y+1)\n\n客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。 \n\n### （2）四次挥手\n\n![](/img/四次挥手协议2.png)\n\n第一次挥手(FIN=1，seq=x)\n\n假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。\n\n第二次挥手(ACK=1，ACKnum=x+1)\n\n服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。\n\n第三次挥手(FIN=1，seq=w)\n\n服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。\n\n第四次挥手(ACK=1，ACKnum=w+1)\n\n客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。\n\n服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。\n\n客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。\n\n ","slug":"TCP握手、挥手协议","published":1,"updated":"2020-07-20T23:18:04.754Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1j0021x8vhjahamhep","content":"<h2>1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt></p>\n<h2>2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3>（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、TCP三次握手协议（打开连接）</h2>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE1.png\" alt></p>\n<p>第一次： A城发信，B城收到了------&gt; 此时B城就会明白 ：A城的发信能力和自己的收信能力是没问题的</p>\n<p>第二次：B城发信，A城收到了-----&gt; 此时A城就会明白 ：A城的发信能力和收信能力都是没问题的，B城的发信能力和收信能力都是没问题的。但是B不知道自己发信能力如何，所以要进行第三次握手</p>\n<p>第三次：A城发信，B城收到了，此时B城就会明白，B城的发信能力和自己的收信能力是没有问题的。</p>\n<p>更加简洁的图片</p>\n<p><img src=\"/img/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt></p>\n<h2>2、TCP四次挥手协议（关闭连接）</h2>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE.png\" alt></p>\n<p>第一次：A和B打电话，通话即将结束后，A说“我有事先忙了，我得关闭链接了”，</p>\n<p>第一次握手(SYN=1, seq=x)</p>\n<p>客户端发送一个TCP的SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号X,保存在包头的序列号(Sequence Number)字段里。</p>\n<p>发送完毕后，客户端进入SYN_SEND 状态。</p>\n<p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p>\n<p>服务器发回确认包(ACK)应答。即SYN 标志位和ACK 标志位均为1。服务器端选择自己ISN 序列号，放到Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的ISN 加1，即X+1。</p>\n<p>发送完毕后，服务器端进入SYN_RCVD 状态。</p>\n<p>第三次握手(ACK=1，ACKnum=y+1)</p>\n<p>客户端再次发送确认包(ACK)，SYN标志位为0，ACK标志位为1，并且把服务器发来ACK的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN发完毕后，客户端进入ESTABLISHED 状态，当服务器端接收到这个包时，也进入ESTABLISHED 状态，TCP握手结束。</p>\n<h3>（2）四次挥手</h3>\n<p><img src=\"/img/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E5%8D%8F%E8%AE%AE2.png\" alt></p>\n<p>第一次挥手(FIN=1，seq=x)</p>\n<p>假设客户端想要关闭连接，客户端发送一个FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入FIN_WAIT_1 状态。</p>\n<p>第二次挥手(ACK=1，ACKnum=x+1)</p>\n<p>服务器端确认客户端的FIN包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2 状态，等待服务器端关闭连接。</p>\n<p>第三次挥手(FIN=1，seq=w)</p>\n<p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN置为1。发送完毕后，服务器端进入LAST_ACK 状态，等待来自客户端的最后一个ACK。</p>\n<p>第四次挥手(ACK=1，ACKnum=w+1)</p>\n<p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，等待可能出现的要求重传的ACK包。</p>\n<p>服务器端接收到这个确认包之后，关闭连接，进入CLOSED 状态。</p>\n<p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的ACK，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。</p>\n"},{"title":"ThreadLocal","author":"郑天祺","date":"2020-03-28T01:59:00.000Z","_content":"\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","source":"_posts/ThreadLocal.md","raw":"title: ThreadLocal\nauthor: 郑天祺\ntags:\n\n  - java\ncategories:\n  - java基础\ndate: 2020-03-28 09:59:00\n\n---\n\n引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。\n\n![image-20200331153816952](/img/simpleDateFormat-alibaba.png)\n\n# 一、ThreadLocal用在什么地方？\n\nThreadLocal归纳下来就2类用途：\n\n（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失\n\n​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。\n\n​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。\n\n# 二、用一下才知道它能干什么！\n\n```java\npackage cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal<Integer> threadLocal = new ThreadLocal<>();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal1\").start();\n\t\t// \n        new Thread(() -> {\n            try {\n                for (int i = 0; i < 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + \"=\" + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, \"threadlocal2\").start();\n    }\n}\n```\n\n代码的执行结果：\n\n```java\nthreadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n```\n\n结果可以看到 ：\n\n（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。\n\n（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。\n\n# 三、看一下它的存储\n\n![img](/img/ThreadLocal内部存储.png)\n\n一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal\n\n# 四、内存泄露？\n\nThreadLocal 会产生内存泄露吗？\n\n内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。\n\nWhy？\n\n为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。\n\n弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。\n\n当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！\n\nThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。\n\n》》》》》》\n\n但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！\n\n如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。\n\n\n\n学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》","slug":"ThreadLocal","published":1,"updated":"2020-03-31T13:12:37.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1m0025x8vhaqzz4cm4","content":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; {\n            try {\n                for (int i = 0; i &lt; 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; {\n            try {\n                for (int i = 0; i &lt; 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, &quot;threadlocal2&quot;).start();\n    }\n}\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n","site":{"data":{}},"excerpt":"","more":"<p>引言：本博客《SimpleDateFormat引发的线程安全问题》中提到，可以利用 ThreadLocal 来解决SimpleDateFormat的线程安全问题。之后看到阿里巴巴开发规范中也有提到，SimpleDateFormat禁止使用static进行修饰。</p>\n<p><img src=\"/img/simpleDateFormat-alibaba.png\" alt=\"image-20200331153816952\"></p>\n<h1>一、ThreadLocal用在什么地方？</h1>\n<p>ThreadLocal归纳下来就2类用途：</p>\n<p>（1）保存线程上下文信息，在任意需要的地方可以获取\n（2）线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失</p>\n<p>​\t\t由于ThreadLocal的特性，同一线程在某地方进行设置，在随后的任意地方都可以获取到。从而可以用来保存线程上下文信息。</p>\n<p>​\t\t常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。还有比如Spring的事务管理，用ThreadLocal存储Connection，从而各个DAO可以获取同一Connection，可以进行事务回滚，提交等操作。</p>\n<h1>二、用一下才知道它能干什么！</h1>\n<pre><code class=\"language-java\">package cn.edu.bjut;\n\npublic class ThreadLocalTest {\n    private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;();\n\n    public static void main(String[] args) {\n        // 一个线程向ThreadLocal里面写值并打印，另一个线程向ThreadLocal里取值并打印\n        new Thread(() -&gt; {\n            try {\n                for (int i = 0; i &lt; 100; i++) {\n                    threadLocal.set(i);\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, &quot;threadlocal1&quot;).start();\n\t\t// \n        new Thread(() -&gt; {\n            try {\n                for (int i = 0; i &lt; 100; i++) {\n                    System.out.println(Thread.currentThread().getName() + &quot;=&quot; + threadLocal.get());\n                    try {\n                        Thread.sleep(200);\n                    } catch (InterruptedException e) {\n                        e.printStackTrace();\n                    }\n                }\n            } finally {\n                threadLocal.remove();\n            }\n        }, &quot;threadlocal2&quot;).start();\n    }\n}\n</code></pre>\n<p>代码的执行结果：</p>\n<pre><code class=\"language-java\">threadlocal1=0\nthreadlocal2=null\nthreadlocal2=null\nthreadlocal1=1\nthreadlocal2=null\nthreadlocal1=2\n.......\n</code></pre>\n<p>结果可以看到 ：</p>\n<p>（1）第二个线程 是访问不到 第一个线程 所存的值的。它们存在线程的隔离。</p>\n<p>（2）也就是说每个线程有一个自己的 ThreadLocalMap ，所以每个线程往这个 ThreadLocal 中读写隔离的，并且是互相不会影响的。</p>\n<h1>三、看一下它的存储</h1>\n<p><img src=\"/img/ThreadLocal%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>一个ThreadLocal只能存储一个Object对象，如果需要存储多个 Object 对象那么就需要多个 ThreadLocal</p>\n<h1>四、内存泄露？</h1>\n<p>ThreadLocal 会产生内存泄露吗？</p>\n<p>内存泄露：我的理解就是当我们不实用 ThreadLocal 实例的时候，它没有办法 GC 掉，或者等内存将要满的时候才会发生GC。所以如果多个线程使用 ThreadLocal 的话，就会导致大量内存被占据。</p>\n<p>Why？</p>\n<p>为什么会这样？这就要学习一下 java 对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。</p>\n<p>弱引用也是用来描述非必需对象的，当 JVM 进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。</p>\n<p>当仅仅只有 ThreadLocalMap 中的 Entry 的 key 指向 ThreadLocal 的时候，ThreadLocal 会进行回收的！！！</p>\n<p>ThreadLocal被垃圾回收后，在 ThreadLocalMap 里对应的 Entry 的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以 ThreadLocalMap 做了一些额外的回收工作。</p>\n<p>》》》》》》</p>\n<p>但是很多时候，我们都是用在线程池的场景，程序不停止，线程基本不会销毁！！！</p>\n<p>如果使用线程池，使用不当会导致内存泄露，编码时候要养成良好的习惯，线程中使用完 ThreadLocal 变量后，要记得及时 remove 掉。</p>\n<p>学习资源来自于： http://www.jiangxinlingdu.com/ 「公众号：匠心零度 」中的《手撕面试题threadlocal》</p>\n"},{"title":"WordCount简析","author":"郑天祺","date":"2019-12-18T03:57:00.000Z","_content":"\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","source":"_posts/WordCount简析.md","raw":"title: WordCount简析\nauthor: 郑天祺\ntags:\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 11:57:00\n\n---\n\n```java\npackage org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * <p>\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入< key, value >对的 value 值，此处为一段具体的文本数据\n     * Text 输出< key, value >对的 key 值，此处为一个单词\n     * IntWritable：输出< key, value >对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable> {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入< key, value >对的key值，此处为一个单词\n     * IntWritable：输入< key, value >对的value值\n     * Text：输出< key, value >对的key值，此处为一个单词\n     * IntWritable：输出< key, value >对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入< key, value >对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n","slug":"WordCount简析","published":1,"updated":"2019-12-18T03:59:10.021Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1p0028x8vho2gpk7ib","content":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) {\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<pre><code class=\"language-java\">package org.apache.hadoop.examples;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n    /**\n     * map 阶段\n     * &lt;p&gt;\n     * Object 此处为文本数据的起始位置的偏移量;可以直接使用 Long 类型，源码此处使用Object做了泛化\n     * Text 输入&lt; key, value &gt;对的 value 值，此处为一段具体的文本数据\n     * Text 输出&lt; key, value &gt;对的 key 值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对的 value 值，此处固定为 1\n     */\n    public static class TokenizerMapper\n            extends Mapper&lt;Object, Text, Text, IntWritable&gt; {\n        // IntWritable 是 Hadoop 对 Integer 的进一步封装，使其可以进行序列化。\n        private final static IntWritable one = new IntWritable(1);\n        // map 端的任务是对输入数据按照单词进行切分，每个单词为 Text 类型。\n        private Text word = new Text();\n\n        /**\n         * @param key     输入数据在原数据中的偏移量\n         * @param value   具体的数据数据，此处为一段字符串\n         * @param context 用于暂时存储 map() 处理后的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 字符串分割，也可以用 apache.common.lang3的 StringUtils.split\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            // map 输出的 key value\n            while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n            }\n        }\n    }\n\n    /**\n     * reduce阶段，map的输出是reduce的输入\n     * Text：输入&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输入&lt; key, value &gt;对的value值\n     * Text：输出&lt; key, value &gt;对的key值，此处为一个单词\n     * IntWritable：输出&lt; key, value &gt;对，此处为相同单词词频累加之后的值。实际上就是一个数字\n     */\n    public static class IntSumReducer\n            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n        private IntWritable result = new IntWritable();\n\n        /**\n         * @param key     输入&lt; key, value &gt;对的key值，也就是一个单词\n         * @param values  一系列的key值相同的序列化结构\n         * @param context 临时存储reduce端产生的结果\n         * @throws IOException          IO异常\n         * @throws InterruptedException 中断异常\n         */\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            // 将相同的key进行合并，value累加\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            // 单词和它的数目\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length &lt; 2) {\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        }\n        // main函数调用Job类及逆行MapReduce 作业的初始化\n        Job job = Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        // 设置 job 的 map 阶段的执行类\n        job.setMapperClass(TokenizerMapper.class);\n        // 设置 job 的 combine 阶段的执行类\n        job.setCombinerClass(IntSumReducer.class);\n        // 设置 job 的 reduce 阶段的执行类\n        job.setReducerClass(IntSumReducer.class);\n        // map的输出 key、value 映射\n        job.setOutputKeyClass(Text.class);\n        // 设置程序的输出的value值的类型\n        job.setOutputValueClass(IntWritable.class);\n        // 调用 addInputFormat 设置输入路径\n        for (int i = 0; i &lt; otherArgs.length - 1; ++i) {\n            // Path 是绝对路径\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        // 输入文件 和 输出文件的路径\n        FileOutputFormat.setOutputPath(job,\n                new Path(otherArgs[otherArgs.length - 1]));\n        // 等待任务完成，任务完成之后退出程序\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n</code></pre>\n"},{"title":"Yarn概述","author":"郑天祺","date":"2019-12-18T01:12:00.000Z","_content":"\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","source":"_posts/Yarn概述.md","raw":"title: Yarn概述\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-18 09:12:00\n\n---\n\n# 一、组件介绍\t\n\n​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：\n\n​\t\ta. 一个全局的 Resource Manager \n\n​\t\tb. 每个应用对应的 Application Master（AM）\n\n​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。\n\n​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。\n\n![image-20191218091838954](/img/Yarn.png)\n\n## （1）Resource Manager\n\n​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。\n\n注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作\n\n## （2）Application Master\n\n​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。\n\n## （3）Node Manager\n\n​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。\n\n## （4）Resource Request 和 Container\n\n​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。\n\n# 二、Yarn工作流程\n\n​\t（1）客户端提交 MapReduce作业\n\n​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配\n\n​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）\n\n​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理\n\n​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件","slug":"Yarn概述","published":1,"updated":"2019-12-18T02:22:59.255Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1s002dx8vh684j7p12","content":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2>（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2>（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2>（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2>（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、组件介绍</h1>\n<p>​\tYarn的基本思想是将 JobTracker 的资源管理和作业的调度/监控两大主要职能拆分为两个独立的进程：</p>\n<p>​\t\ta. 一个全局的 Resource Manager</p>\n<p>​\t\tb. 每个应用对应的 Application Master（AM）</p>\n<p>​\tResource Manager 和每个节点上的 Node Manager（NM）组成了全新的通用操作系统，以分布式的方式管理应用程序。</p>\n<p>​\tResource Manager拥有为系统中所有应用分配资源的决定权。与之相关的是应用程序的Application Master，负责与Resource Manager协商资源，并与Node Manager协同工作来执行和监控任务。</p>\n<p><img src=\"/img/Yarn.png\" alt=\"image-20191218091838954\"></p>\n<h2>（1）Resource Manager</h2>\n<p>​\t\tYarn Resource Manager是一个纯粹的调度器，它负责整个系统的资源管理和分配。它本身主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>​\t\t调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。</p>\n<p>注意：该调度器是一个“纯调度器”，他不再从事任何与具体应用程序相关的工作</p>\n<h2>（2）Application Master</h2>\n<p>​\t\tApplication Master实际上是特定框架库的一个实例，负责与 Resource Manager协商资源，并和Resource Manager协同工作来   执行和监控Container，以及它们的资源消耗。</p>\n<h2>（3）Node Manager</h2>\n<p>​\t\tNode Manager 是每个节点的框架代理。她负责启动应用的Container，监控Container的资源使用（包括CPU、内存、硬盘和网络带宽等），并把这些信息汇报给调度器。</p>\n<h2>（4）Resource Request 和 Container</h2>\n<p>​\t\tYarn 被设计成可以允许应用程序（通过 Application Master） 以共享的、安全的，以及多用租户的方式使用集群的资源。它也会感知集群的网络拓扑，一边可以有效地调度，以及优化数据访问。</p>\n<h1>二、Yarn工作流程</h1>\n<p>​\t（1）客户端提交 MapReduce作业</p>\n<p>​\t（2）Yarn 资源管理器负责协调集群上计算资源的分配</p>\n<p>​\t（3）Yarn 节点管理器（Node Manager）负责启动和监视集群中机器上的计算容器（Container）</p>\n<p>​\t（4）应用程序的 Master 负责协调运行 MapReduce 作业的任务，它和MapReduce 任务在容器中运行，这些同期由资源管理器分配对节点管理器进行管理</p>\n<p>​\t（5）分布式文件系统（HDFS）用来与其他实体间共享作业文件</p>\n"},{"title":"java8新特性","author":"郑天祺","date":"2019-11-14T06:18:00.000Z","_content":"​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","source":"_posts/java8新特性.md","raw":"title: java8新特性\nauthor: 郑天祺\ntags:\n\n  - JDK1.8新特性\ncategories:\n  - java基础\ndate: 2019-11-14 14:18:00\n---\n​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：\n\n# 一、Stream（流）\n\nStream（流）是一个来自数据源的元素队列并支持聚合操作\n\n数据源是流的来源。 数据源可以是集合，数组，I/O channel等\n\n优点：\n * 内部迭代：通过访问者模式(Visitor)实现\n * Pipelining：中间操作都会返回流对象本身\n * 聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List<String> strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\", \"\", \"jkl\");\n    final List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List<String> filtered = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List<Integer> squaresList = numbers.stream().map(i -> i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -> string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(\",\", strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -> !string.isEmpty()).collect(Collectors.joining(\",\"));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List<Integer> numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -> x).summaryStatistics();\n\n        System.out.println(\"列表中最大的数 : \" + stats.getMax());\n        System.out.println(\"列表中最小的数 : \" + stats.getMin());\n        System.out.println(\"所有数之和 : \" + stats.getSum());\n        System.out.println(\"平均数 : \" + stats.getAverage());\n    }\n\n}\n\n\n```\n\n# 二、方法引用\n\n方法引用\n\n方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。\n\n```java\npackage com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier<T>{\n    T get();\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier<Car> supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(\"Colloded\" + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(\"Following the\" + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(\"Repaired\" + this.toString());\n    }\n}\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List<Car> cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List<String> names = new ArrayList<>(50);\n        names.add(\"hello\");\n        names.add(\"world\");\n        names.add(\"ni\");\n        names.add(\"hao\");\n        names.forEach(System.out::println);\n    }\n}\n\n```\n\n# 三、默认方法\n\n 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 \n\n```java\npackage com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(\"默认方法\");\n    }\n}\n\n```\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n```\n\n# 四、 Date Time API \n\n加强对日期与时间的处理。 \n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(\"时间\" + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(\"date1: \" + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(\"月: \" + month + \", 日: \" + day + \", 秒: \" + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(\"date2\" + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(\"date3\" + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(\"date4: \" + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(\"20:15:30\");\n        System.out.println(\"date5: \" + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\");\n        System.out.println(\"date1: \" + date1);\n\n        ZoneId id = ZoneId.of(\"Europe/Paris\");\n        System.out.println(\"ZoneId: \" + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(\"当期时区: \" + currentZone);\n    }\n}\n\n```\n\n# 五、 Optional 类 \n\nOptional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()\n\nJAVA9在它的基础上又增加了3个方法\n\n```java\npackage com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional<Integer> a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional<Integer> b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional<Integer> a, Optional<Integer> b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(\"第一个参数值存在\" + a.isPresent());\n        System.out.println(\"第二个参数值存在\" + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n```\n\n\n\n参考文献：https://www.runoob.com/java/java8-new-features.html","slug":"java8新特性","published":1,"updated":"2019-11-14T08:30:37.235Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1u002gx8vhhfp7pqjm","content":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    }\n\n}\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;{\n    T get();\n}\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(&quot;Following the&quot; + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    }\n}\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    }\n}\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(&quot;默认方法&quot;);\n    }\n}\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    }\n}\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n</code></pre>\n<p>参考文献：https://www.runoob.com/java/java8-new-features.html</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t都9102年了，JAVA出到了13.0.1。现在预习一下JAVA8新特性应该还来得及；用代码说话：</p>\n<h1>一、Stream（流）</h1>\n<p>Stream（流）是一个来自数据源的元素队列并支持聚合操作</p>\n<p>数据源是流的来源。 数据源可以是集合，数组，I/O channel等</p>\n<p>优点：</p>\n<ul>\n<li>内部迭代：通过访问者模式(Visitor)实现</li>\n<li>Pipelining：中间操作都会返回流对象本身</li>\n<li>聚合操作：类似SQL语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等</li>\n</ul>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.Arrays;\nimport java.util.IntSummaryStatistics;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.stream.Collectors;\n\npublic class Java8StreamTest {\n    final List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;);\n    final List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n    final Random random = new Random();\n\n    @Test\n    public void filter() {\n        // filter 方法过滤出空字符串\n        List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());\n        System.out.println(filtered);\n    }\n\n    @Test\n    public void forEach() {\n        // Stream 提供了新的方法 'forEach' 来迭代流中的每个数据;limit 方法用于获取指定数量的流\n        random.ints().limit(10).forEach(System.out::println);\n    }\n\n    @Test\n    public void map() {\n        // map 方法用于映射每个元素到对应的结果\n        // 获取对应的平方数, distinct为去重\n        List&lt;Integer&gt; squaresList = numbers.stream().map(i -&gt; i * i).distinct().collect(Collectors.toList());\n        System.out.println(squaresList);\n    }\n\n    @Test\n    public void sorted() {\n        // sorted 方法用于对流进行排序\n        random.ints().limit(10).sorted().forEach(System.out::println);\n    }\n\n    @Test\n    public void parallel() {\n        // 获取空字符串的数量\n        long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count();\n        System.out.println(count);\n    }\n\n    @Test\n    public void join() {\n        // 类似于\n        // jdk：String mergedString = String.join(&quot;,&quot;, strings);\n        // common.lang3：String mergedString = StringUtils.join(strings);\n\n        String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;,&quot;));\n        System.out.println(mergedString);\n    }\n\n    @Test\n    public void statistics(){\n        // 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上\n        List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);\n        IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();\n\n        System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());\n        System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());\n        System.out.println(&quot;所有数之和 : &quot; + stats.getSum());\n        System.out.println(&quot;平均数 : &quot; + stats.getAverage());\n    }\n\n}\n\n\n</code></pre>\n<h1>二、方法引用</h1>\n<p>方法引用</p>\n<p>方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\n@FunctionalInterface\npublic interface Supplier&lt;T&gt;{\n    T get();\n}\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic class Car {\n    // Supplier是jdk1.8的接口，这里和lamda一起使用了\n    public static Car create(final Supplier&lt;Car&gt; supplier) {\n        return supplier.get();\n    }\n\n    public static void collide(final Car car) {\n        System.out.println(&quot;Colloded&quot; + car.toString());\n    }\n\n    public void follow(final Car another) {\n        System.out.println(&quot;Following the&quot; + another.toString());\n    }\n\n    public void repair() {\n        System.out.println(&quot;Repaired&quot; + this.toString());\n    }\n}\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Java8QuoteTest {\n    final Car car = Car.create(Car::new);\n    final List&lt;Car&gt; cars = Arrays.asList(car);\n    final Car police = Car.create(Car::new);\n\n    @Test\n    public void quoteType() {\n        // 静态方法引用：它的语法是Class::static_method，实例如下：\n        cars.forEach(Car::collide);\n        // 特定类的任意对象的方法引用：它的语法是Class::method实例如下：\n        cars.forEach(Car::repair);\n        // 特定对象的方法引用：它的语法是instance::method实例如下：\n        cars.forEach(police::follow);\n    }\n\n    @Test\n    public void quoteExample(){\n        List&lt;String&gt; names = new ArrayList&lt;&gt;(50);\n        names.add(&quot;hello&quot;);\n        names.add(&quot;world&quot;);\n        names.add(&quot;ni&quot;);\n        names.add(&quot;hao&quot;);\n        names.forEach(System.out::println);\n    }\n}\n\n</code></pre>\n<h1>三、默认方法</h1>\n<p>默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\npublic interface Java8DefaultInterface {\n    default void print(){\n        System.out.println(&quot;默认方法&quot;);\n    }\n}\n\n</code></pre>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\n\n/**\n * 测试接口默认方法\n */\npublic class Java8DefaultInterfaceTest implements Java8DefaultInterface{\n\n    @Test\n    public void test(){\n        Java8DefaultInterface defaultInterface = new Java8DefaultInterfaceTest();\n        defaultInterface.print();\n    }\n}\n\n</code></pre>\n<h1>四、 Date Time API</h1>\n<p>加强对日期与时间的处理。</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.time.*;\n\npublic class Java8DateTest {\n    LocalDateTime currentTime = LocalDateTime.now();\n\n    @Test\n    public void testLocalDateTime() {\n        // 获取服务器当前的日期时间\n        System.out.println(&quot;时间&quot; + currentTime);\n\n        // 获取服务器当前日期\n        LocalDate date1 = currentTime.toLocalDate();\n        System.out.println(&quot;date1: &quot; + date1);\n\n        // 获取服务器某月某天\n        Month month = currentTime.getMonth();\n        int day = currentTime.getDayOfMonth();\n        int seconds = currentTime.getSecond();\n        System.out.println(&quot;月: &quot; + month + &quot;, 日: &quot; + day + &quot;, 秒: &quot; + seconds);\n    }\n\n    @Test\n    public void testStructDateTime() {\n        LocalDateTime date2 = currentTime.withDayOfMonth(12).withYear(2012);\n        System.out.println(&quot;date2&quot; + date2);\n\n        // 23 december 2014\n        LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 23);\n        System.out.println(&quot;date3&quot; + date3);\n\n        // 22 小时 15 分钟\n        LocalTime date4 = LocalTime.of(22, 15);\n        System.out.println(&quot;date4: &quot; + date4);\n\n        // 解析字符串\n        LocalTime date5 = LocalTime.parse(&quot;20:15:30&quot;);\n        System.out.println(&quot;date5: &quot; + date5);\n    }\n\n    @Test\n    public void testZonedDateTime() {\n        // 获取当前时间日期\n        ZonedDateTime date1 = ZonedDateTime.parse(&quot;2015-12-03T10:15:30+05:30[Asia/Shanghai]&quot;);\n        System.out.println(&quot;date1: &quot; + date1);\n\n        ZoneId id = ZoneId.of(&quot;Europe/Paris&quot;);\n        System.out.println(&quot;ZoneId: &quot; + id);\n\n        ZoneId currentZone = ZoneId.systemDefault();\n        System.out.println(&quot;当期时区: &quot; + currentZone);\n    }\n}\n\n</code></pre>\n<h1>五、 Optional 类</h1>\n<p>Optional 类是一个可以为null的容器对象。优雅的解决null问题：我平时好像都是类似于 StringUtils.isBlank()</p>\n<p>JAVA9在它的基础上又增加了3个方法</p>\n<pre><code class=\"language-java\">package com.bjut.java8test;\n\nimport org.junit.Test;\nimport java.util.Optional;\n\npublic class java8OptionalTest {\n\n    @Test\n    public void testOptional() {\n        Integer value1 = null;\n        Integer value2 = new Integer(10);\n\n        // Optional.ofNullable -允许传递null参数\n        Optional&lt;Integer&gt; a = Optional.ofNullable(value1);\n        // Optional.of - 如果传递的参数是null ， 抛出异常NullPointerException\n        Optional&lt;Integer&gt; b = Optional.of(value2);\n\n        System.out.println(sum(a, b));\n    }\n\n\n    private Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b) {\n        // Optional.isPresent - 判断值是否存在\n        System.out.println(&quot;第一个参数值存在&quot; + a.isPresent());\n        System.out.println(&quot;第二个参数值存在&quot; + b.isPresent());\n\n        // Option.orElse - 如果值存在，返回它，否则返回默认值\n        Integer value1 = a.orElse(new Integer(0));\n\n        // Optional.get - 获取值，值需要存在\n        Integer value2 = b.get();\n        return value1 + value2;\n    }\n}\n\n</code></pre>\n<p>参考文献：https://www.runoob.com/java/java8-new-features.html</p>\n"},{"title":"maven梳理","author":"郑天祺","date":"2019-08-28T09:01:00.000Z","_content":"# Maven使用\n\n## 1.两个操作：\n\n###    (1) 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n​    （此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​     Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t(2) 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n## 2. maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n","source":"_posts/maven梳理.md","raw":"title: maven梳理\ntags:\n\n  - maven\ncategories:\n  - 软件管理\nauthor: 郑天祺\ndate: 2019-08-28 17:01:00\n---\n# Maven使用\n\n## 1.两个操作：\n\n###    (1) 参数设置：\n\n​     Linux：在~/.bash_profile文件中添加\n\n```java\nexport MAVEN_OPTS=\"-Xms512m -Xmx1024m\"\n```\n\n​    （此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​     Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​        \n\n### \t(2) 用户配置：\n\n把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置\n\n```java\n上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n```\n## 2. maven的命令：\n\n```java\nmaven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n```\n\n","slug":"maven梳理","published":1,"updated":"2019-10-15T12:19:35.470Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1x002kx8vhwc2pyikq","content":"<h1>Maven使用</h1>\n<h2>1.两个操作：</h2>\n<h3>(1) 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>​    （此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​     Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​</p>\n<h3>(2) 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2>2. maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>Maven使用</h1>\n<h2>1.两个操作：</h2>\n<h3>(1) 参数设置：</h3>\n<p>​     Linux：在~/.bash_profile文件中添加</p>\n<pre><code class=\"language-java\">export MAVEN_OPTS=&quot;-Xms512m -Xmx1024m&quot;\n</code></pre>\n<p>​    （此设置是为了maven执行java时分配给大点的内存，解决容易引起maven导包或插件时卡顿）\n​     Windows：如下图\n​\t\t<img src=\"/img/maven配置.png\">\n​</p>\n<h3>(2) 用户配置：</h3>\n<p>把MAVEN_HOME/conf/seettings.xml  cp 到 ~/.m2/下，在.m2下的settings.xml中所作的配置就是用户级别的配置，而直接编辑MAVEN_HOME/conf/seettings.xml所作的配置是全局的配置</p>\n<pre><code class=\"language-java\">上传到私服的流程：\n  a.  加入打包插件\n  b. mvn clean package // 加上clean 会清空target，然后再生成新的包。。。\n  c.mvn source:jar  // 生成源码包\n  d.mvn deploy // 上传私服，别忘升级版本哦~~~\n2.idea和eclipse导入时不同： \nidea是project下的module  eclipse是workspace下的project\n  idea导入maven项目  https://blog.csdn.net/weixin_37909363/article/details/80915509  \n</code></pre>\n<h2>2. maven的命令：</h2>\n<pre><code class=\"language-java\">maven常用命令\n\n创建maven项目：mvn archetype:create\n指定 group： -DgroupId=packageName\n指定 artifact：-DartifactId=projectName\n创建web项目：-DarchetypeArtifactId=maven-archetype-webapp \n创建maven项目：mvn archetype:generate\n验证项目是否正确：mvn validate\nmaven 打包：mvn package\n只打jar包：mvn jar:jar\n生成源码jar包：mvn source:jar\n产生应用需要的任何额外的源代码：mvn generate-sources\n编译源代码： mvn compile\n编译测试代码：mvn test-compile\n运行测试：mvn test\n运行检查：mvn verify\n清理maven项目：mvn clean  该操作会清空当前目录的target文件夹\n生成eclipse项目：mvn eclipse:eclipse\n清理eclipse配置：mvn eclipse:clean\n生成idea项目：mvn idea:idea\n安装项目到本地仓库：mvn install\n发布项目到远程仓库：mvn:deploy\n在集成测试可以运行的环境中处理和发布包：mvn integration-test\n显示maven依赖树：mvn dependency:tree\n显示maven依赖列表：mvn dependency:list\n下载依赖包的源码：mvn dependency:sources\n安装本地jar到本地仓库：mvn install:install-file -DgroupId=packageName -DartifactId=projectName -Dversion=version -Dpackaging=jar -Dfile=path\n    WEB\n启动tomcat：mvn tomcat:run\n启动jetty：mvn jetty:run\n运行打包部署：mvn tomcat:deploy\n撤销部署：mvn tomcat:undeploy\n启动web应用：mvn tomcat:start\n停止web应用：mvn tomcat:stop\n重新部署：mvn tomcat:redeploy\n部署展开的war文件：mvn war:exploded tomcat:exploded\n    maven 命令的格式为 mvn [plugin-name]:[goal-name]，可以接受的参数如下。\n-D 指定参数，如 -Dmaven.test.skip=true 跳过单元测试；\n-P 指定 Profile 配置，可以用于区分环境；\n-e 显示maven运行出错的信息；\n-o 离线执行命令,即不去远程仓库更新包；\n-X 显示maven允许的debug信息；\n-U 强制去远程更新snapshot的插件或依赖，默认每天只更新一次。\n</code></pre>\n"},{"title":"mysql事务","author":"郑天祺","date":"2020-07-21T01:42:00.000Z","_content":"","source":"_posts/mysql事务.md","raw":"title: mysql事务\nauthor: 郑天祺\ntags:\n  - mysql\ncategories:\n  - 数据库\ndate: 2020-07-21 09:42:00\n---\n","slug":"mysql事务","published":1,"updated":"2020-07-21T01:42:51.809Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp1z002nx8vhy9zi5smp","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"mysql排序","author":"郑天祺","date":"2019-11-20T12:35:00.000Z","_content":"\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","source":"_posts/mysql排序.md","raw":"title: mysql排序\ntags:\n\n  - mysql\ncategories:\n  - 数据库\nauthor: 郑天祺\ndate: 2019-11-20 20:35:00\n---\n\n\n\n# 1、正常的数字排序\n\n![image-20191120204232822](/img/mysql排序1.png)\n\n# 2、排序中文时\n\n，就是出现问题\n\n![image-20191120204352223](/img/mysql排序2.png)\n\n​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。\n\n# 3、现在改成gbk_chinese_ci\n\n，ci是不区分大小写\n\n![image-20191120204623652](/img/mysql排序3.png)\n\n这样的话，结果：\n\n![image-20191120204719806](/img/mysql排序4.png)\n\n# 4、英中排序\n\n![image-20191120205015864](/img/mysql排序5.png)\n\n​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，\n\n# 5、中文混搭\n\n先看一下效果：\n\n![image-20191120205433980](/img/mysql排序6.png)\n\n我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR\n\n```java\nSELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n```\n\n这个函数需要在创建表之后定义，如下：\n\n```java\nDROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)<>CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n```\n\n这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。\n\n需要在mysql配置文件中打开 log_bin_trust_function_creators ","slug":"mysql排序","published":1,"updated":"2019-11-26T07:31:53.949Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp22002rx8vh4exlpl0b","content":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、正常的数字排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F1.png\" alt=\"image-20191120204232822\"></p>\n<h1>2、排序中文时</h1>\n<p>，就是出现问题</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F2.png\" alt=\"image-20191120204352223\"></p>\n<p>​\t这是因为我们在选取排序规则时，选择的不是gbk。所以想要正确的排序，需要我们了解我们选取字段的排序规则。</p>\n<h1>3、现在改成gbk_chinese_ci</h1>\n<p>，ci是不区分大小写</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F3.png\" alt=\"image-20191120204623652\"></p>\n<p>这样的话，结果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F4.png\" alt=\"image-20191120204719806\"></p>\n<h1>4、英中排序</h1>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F5.png\" alt=\"image-20191120205015864\"></p>\n<p>​\t这个也是gbk的排序效果，但是我们想做到中英混搭的效果，我认为可以自已在mysql编译前放进自己的排序规则，</p>\n<h1>5、中文混搭</h1>\n<p>先看一下效果：</p>\n<p><img src=\"/img/mysql%E6%8E%92%E5%BA%8F6.png\" alt=\"image-20191120205433980\"></p>\n<p>我们sql用引入了一个函数GET_FIRST_PINYIN_CHAR</p>\n<pre><code class=\"language-java\">SELECT\n\ta.id,\n\ta.username \nFROM\n\ttest AS a \nORDER BY\n\tGET_FIRST_PINYIN_CHAR(a.username)\n</code></pre>\n<p>这个函数需要在创建表之后定义，如下：</p>\n<pre><code class=\"language-java\">DROP FUNCTION IF EXISTS `GET_FIRST_PINYIN_CHAR`;\nCREATE FUNCTION `GET_FIRST_PINYIN_CHAR`(PARAM VARCHAR(255)) RETURNS VARCHAR(2) CHARSET utf8\nBEGIN\n    DECLARE V_RETURN VARCHAR(255);\n    DECLARE V_FIRST_CHAR VARCHAR(2);\n    SET V_FIRST_CHAR = UPPER(LEFT(PARAM,1));\n  SET V_RETURN = V_FIRST_CHAR;\n    IF LENGTH( V_FIRST_CHAR)&lt;&gt;CHARACTER_LENGTH(V_FIRST_CHAR) THEN\n    SET V_RETURN = ELT(INTERVAL(CONV(HEX(LEFT(CONVERT(PARAM USING gbk),1)),16,10),\n        0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7,\n        0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB,\n        0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1),\n    'A','B','C','D','E','F','G','H','J','K','L','M','N','O','P','Q','R','S','T','W','X','Y','Z');\n    END IF;\n    RETURN V_RETURN;\nEND\n</code></pre>\n<p>这个函数创建成功后，会显示ok。有些时候不成功，可能是没有打开创建函数的权限。</p>\n<p>需要在mysql配置文件中打开 log_bin_trust_function_creators</p>\n"},{"title":"mysql表设计及优化","author":"郑天祺","date":"2019-08-31T07:28:00.000Z","_content":"\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","source":"_posts/mysql表设计及优化.md","raw":"title: mysql表设计及优化\nauthor: 郑天祺\ntags:\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 15:28:00\n\n---\n\n## 一、一些建议\n\n建议来自《MYSQL 王者晋级之路》，本文做些笔记\n\n1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。\n\n2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。\n\n3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。\n\n4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。\n\n5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。\n\n​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。 \n\n​        ![img](/img/mysql时间存储.png)\n\n​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。\n\n​        ![img](/img/mysql的ip存储.png)\n\n​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。\n\n​\t\t有时候精确计算使用decimal，设计sum等统计数据时候\n\n6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。\n\n注：主要业务表切忌出现这样大类型的字段。\n\n​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。\n\n7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。\n\n​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。\n\n​        索引的选择性计算方法：\n\n​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。\n\nsum()函数容易返回null值，记得处理\n\n8）很长的字符串可以考虑创建前缀索引，提高索引利用率。\n\n​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。\n\n9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。\n\n10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4\n\n11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。\n\n12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。\n\n13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114\n\n14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）\n\n15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。\n\n## 二、算是面试题吧\n\n1）为什么一定要设一个主键？\n\n2）主键是自增还是UUID?\n\n3）主键为什么不推荐有业务含义？\n\n4）表示枚举的字段为什么不用enum类型？\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n答：\n\n1）为什么一定要设一个主键？\n\n因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。\n\n所以自己显示指定更可以清晰的看出主键id。\n\n2）主键是自增还是UUID?\n\n自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。\n\n3）主键为什么不推荐有业务含义？\n\n倘若主键变更会引发很多麻烦；引发页分裂。\n\n4）表示枚举的字段为什么不用enum类型？\n\n枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。\n\n5）为什么不直接存储图片、音频、视频等大容量内容？\n\n在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。\n\n当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。\n\n所以，不推荐使用text和blob类型。\n\n6）字段为什么要定义NOT NULL DEFAULT ?\n\n有null，count（包含null的列）会出现问题。而且影响索引的性能\n\n## 三、数据结构\n\n需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~","slug":"mysql表设计及优化","published":1,"updated":"2019-10-15T10:10:18.948Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp25002vx8vh5cvh88jp","content":"<h2>一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114</p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2>二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2>三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>一、一些建议</h2>\n<p>建议来自《MYSQL 王者晋级之路》，本文做些笔记</p>\n<p>1）在创建业务表时，库名、表名、字段名必须使用小写字母，采用 “_” 分割。</p>\n<p>2）MySQL数据库中，通过lower_case_table_names参数来区别表名的大小写，默认为0，代表大小写敏感。如果是1，代表大小写不敏感，以小写存储。为字段选取数据类型时，要秉承着简单、够用的原则。表中的字段和索引数量都不宜过多，要保证SQL语句查询的高效性，快速执行完，避免出现堵塞、排队现象。</p>\n<p>3）表的存储引擎一定要选择使用InnoDB。MySQL 5.7基本已经废弃 MyISAM，8.0后彻底废弃。</p>\n<p>4）要显式地为表创建一个使用自增列 INT 或者 BIGINT 类型作为主键，可以保证写入顺序是自增的，和B+tree叶子节点分裂顺序一致。写入更加高效，TPS性能会更高，存储效率也是最高的。</p>\n<p>5）金钱、日期时间、IPV4尽量使用 int 来存储。用 int 来存储金钱，让 int 单位为分，这样就不存在四舍五入了，存储的数值更加准确。</p>\n<p>​        日期可以选择使用datetime，datetime的可用范围比timestamp大，物理存储上仅比timestamp 多占 1 个字节多的空间，整体性能上的消耗并不算太大。因此在生产环境可以使用datetime时间类型。当然也可以使用 int 来存储时间，通过转换函数 from_unixtime 和 unix_timesstamp来实现。</p>\n<p>​        <img src=\"/img/mysql%E6%97%B6%E9%97%B4%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        IPV4字段基本上可以不适用char(15)来存储，使用int来存储，通过转换函数 inet_aton 和 inet_ntoa来实现。</p>\n<p>​        <img src=\"/img/mysql%E7%9A%84ip%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        有些字段比如性别sex字段、状态status字段，基本上选择tinyint就可以。</p>\n<p>​\t\t有时候精确计算使用decimal，设计sum等统计数据时候</p>\n<p>6）text 和 blob 这种存大量文字或者存图片的大数据类型，建议不要和业务表放在一起。</p>\n<p>注：主要业务表切忌出现这样大类型的字段。</p>\n<p>​        SQL语句中尽量避免出现 or 子句，这种判断的子句可以让程序自动完成，不要交给数据库判断。也要避免使用union，尽量采用union all，减少去重和排序的工作。</p>\n<p>7）用 select 查询表时只需要获取必要的字段，避免使用 select *。这样可以减少网络带宽的消耗，还有可能利用到覆盖索引。</p>\n<p>​        建立索引时不要在选择性低的字段上创建，比如sex、status这种字段。</p>\n<p>​        索引的选择性计算方法：</p>\n<p>​        select count(distinct coll) / count(*) from table_name;  // 越接近 1 ，证明选择性越高，越适合创建索引。</p>\n<p>sum()函数容易返回null值，记得处理</p>\n<p>8）很长的字符串可以考虑创建前缀索引，提高索引利用率。</p>\n<p>​        单表索引数量不要太多，一般建议不要超过 4~5个（根据实际业务表再确定）。当执行DML语句操作时，也会索引进行更新，如果索引数量太多，则会造成索引树的分裂，性能也会下降。</p>\n<p>9）所有字段定义中，默认都加上 not null 约束，避免出现 null 。在对该字段进行 select count() 统计计数时，可以让统计结果更准确，因为值为null的数据不会被计算进去。</p>\n<p>10）表的字符集默认使用 UTF-8 ，必要时可申请使用 UTF8mb4 字符集。因为它的通用性比 GBK 、Latin1 都要好。UTF8字符集存储汉子占用3个字节，如果遇到表情储存的需求，就可以使用UTF8mb4</p>\n<p>11）建议模糊查询 select...like '%**%' 的语句不要出现在数据库中，可以使用搜索引擎sphinx代替。</p>\n<p>12）索引字段上面不要使用函数，否则使用不到索引，也不要创建函数索引。</p>\n<p>13）join列类型要保持一致，其中包括长度、字符集都要一致。？https://blog.csdn.net/n88Lpo/article/details/78099114</p>\n<p>14）当在执行计划中的 extra 项看到 Using filesort，或者看到 Using temporary 时，也要优先考虑创建排序索引和分组索引。（排序、分组字段上都要创建索引）</p>\n<p>15）limit 语句上的优化，建议使用主键来进行范围检索，缩短结果集大小，使查询效率更高效。</p>\n<h2>二、算是面试题吧</h2>\n<p>1）为什么一定要设一个主键？</p>\n<p>2）主键是自增还是UUID?</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>答：</p>\n<p>1）为什么一定要设一个主键？</p>\n<p>因为在不设置主键的情况下，innodb也会自动生成一个隐藏列，作为自增主键。</p>\n<p>所以自己显示指定更可以清晰的看出主键id。</p>\n<p>2）主键是自增还是UUID?</p>\n<p>自增。innodb中的主键是聚簇索引。如果是自增的主键，插入数据时不会引发页分裂。性能更高。</p>\n<p>3）主键为什么不推荐有业务含义？</p>\n<p>倘若主键变更会引发很多麻烦；引发页分裂。</p>\n<p>4）表示枚举的字段为什么不用enum类型？</p>\n<p>枚举字段一般用tinyint类型。因为enum类型order by效率低，而且插入阿拉伯数字有问题。</p>\n<p>5）为什么不直接存储图片、音频、视频等大容量内容？</p>\n<p>在实际应用中，使用HDFS来存储文件。mysql只用来存储下载地址。</p>\n<p>当存文件的时候，比如Base64加密文件等，排序不能使用内存临时表（OOM），必须使用磁盘的临时表，导致查询缓慢；binlog太多，导致主从的效率问题。</p>\n<p>所以，不推荐使用text和blob类型。</p>\n<p>6）字段为什么要定义NOT NULL DEFAULT ?</p>\n<p>有null，count（包含null的列）会出现问题。而且影响索引的性能</p>\n<h2>三、数据结构</h2>\n<p>需要了解mysql的数据结构才能更加清楚上述效率的问题，请看数据结构篇~~</p>\n"},{"title":"互斥锁","author":"郑天祺","date":"2019-08-31T05:13:00.000Z","_content":"\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","source":"_posts/互斥锁.md","raw":"title: 互斥锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:13:00\n\n---\n\n## 1、关于“互斥”和“同步”的概念\n\n互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。\n\n互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写\n\n同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用\n\n总的来说，两者的区别就是：\n\n互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。\n\n同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明\n\n## 2、JAVA中synchronized和Lock是互斥锁\n\n 修饰在静态方法上，锁对象是当前类的Class对象\n 修饰在实例方法上，锁对象是当前实例对象\n 同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。\n\n## 3、synchronized的用法\n\n```java\n/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(\"类同步方法，锁对象是当前Class对象\");\n     }\n\n    public synchronized void s2() {\n         System.out.println(\"实例同步方法，锁对象是当前对象\");\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(\"同步块，锁对象是LOCK对象\");\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(\"同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象\");\n         }\n     }\n\n}\n\n \n```\n\n","slug":"互斥锁","published":1,"updated":"2019-10-15T10:10:39.998Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp27002yx8vhmipsa5ho","content":"<h2>1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2>2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象\n修饰在实例方法上，锁对象是当前实例对象\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2>3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     }\n\n    public synchronized void s2() {\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         }\n     }\n\n}\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、关于“互斥”和“同步”的概念</h2>\n<p>互斥 : 就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了\n同步 : 就是ABCD这些线程要约定一个执行的协调顺序，比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。</p>\n<p>互斥 ：就是不同线程通过竞争进入临界区（共享的数据和硬件资源），为了防止访问冲突，在有限的时间内只允许其中之一独占性的使用共享资源。如不允许同时写</p>\n<p>同步 ：关系则是多个线程彼此合作，通过一定的逻辑关系来共同完成一个任务。一般来说，同步关系中往往包含互斥，同时对临界区的资源会按照某种逻辑顺序进行访问。如先生产后使用</p>\n<p>总的来说，两者的区别就是：</p>\n<p>互斥是通过竞争对资源的独占使用，彼此之间不需要知道对方的存在，执行顺序是一个乱序。</p>\n<p>同步是协调多个相互关联线程合作完synchronized不同用法锁对象说明</p>\n<h2>2、JAVA中synchronized和Lock是互斥锁</h2>\n<p>修饰在静态方法上，锁对象是当前类的Class对象\n修饰在实例方法上，锁对象是当前实例对象\n同步块中，锁对象是synchronized括号后面的对象成任务，彼此之间知道对方存在，执行顺序往往是有序的。</p>\n<h2>3、synchronized的用法</h2>\n<pre><code class=\"language-java\">/** 如下demo的4个方法展示了不同使用方法下锁对象 **/\n public class SynchronizedDemo {\n\n    private static final Object LOCK = new Object();\n\n    public static synchronized void s1(){\n         System.out.println(&quot;类同步方法，锁对象是当前Class对象&quot;);\n     }\n\n    public synchronized void s2() {\n         System.out.println(&quot;实例同步方法，锁对象是当前对象&quot;);\n     }\n\n    public void s3() {\n         synchronized (LOCK) {\n             System.out.println(&quot;同步块，锁对象是LOCK对象&quot;);\n         }\n     }\n\n    public void s4() {\n         synchronized (SynchronizedDemo.class) {\n             System.out.println(&quot;同步块，锁对象和静态同步方法的锁对象一样都是当前Class对象&quot;);\n         }\n     }\n\n}\n\n \n</code></pre>\n"},{"title":"偏向锁","author":"郑天祺","date":"2019-08-31T05:22:00.000Z","_content":"\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","source":"_posts/偏向锁.md","raw":"title: 偏向锁\nauthor: 郑天祺\ntags:\n\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:22:00\n\n---\n\n## 0、从偏向锁到重量锁\n\n​    在java同步代码快中，synchronized的使用方式无非有两个 :   \n\n​    1）通过对一个对象进行加锁来实现同步\n\n```java\nsynchronized(lockObject){\n     //代码\n }\n```\n\n​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。\n\n```java\npublic synchornized void test(){\n     //代码\n }\n```\n\n​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁\n\n## 1、先了解一下对象在JVM内存中的布局，如下图\n\n![img](/img/java对象存储.png)\n\n​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；\n\n​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；\n\n​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；\n\n​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。\n\n## 2、锁的创建\n\n\n\n```java\n// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n```\n\n​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。\n\n![1571144064662](/img/锁的创建.png)\n\n​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。\n\n​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。\n\n​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。\n\n​          此时的Mark word的结构信息如下：\n\n![1571144092579](/img/锁的创建2.png)\n\n​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。   \n\n​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。\n\n​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：\n\n​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。\n\n​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n   6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景","slug":"偏向锁","published":1,"updated":"2020-03-28T01:00:34.402Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp290031x8vh3r2c1uab","content":"<h2>0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject){\n     //代码\n }\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test(){\n     //代码\n }\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2>1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2>2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>0、从偏向锁到重量锁</h2>\n<p>​    在java同步代码快中，synchronized的使用方式无非有两个 :</p>\n<p>​    1）通过对一个对象进行加锁来实现同步</p>\n<pre><code class=\"language-java\">synchronized(lockObject){\n     //代码\n }\n</code></pre>\n<p>​     2）对一个方法进行synchronized声明，进而对一个方法进行加锁来实现同步。</p>\n<pre><code class=\"language-java\">public synchornized void test(){\n     //代码\n }\n</code></pre>\n<p>​     无论是对一个对象进行加锁还是对一个方法进行加锁，实际上，都是对对象进行加锁</p>\n<h2>1、先了解一下对象在JVM内存中的布局，如下图</h2>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8.png\" alt=\"img\"></p>\n<p>​        Mark Word：包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Class Pointer：用来指向对象对应的Class对象（其对应的元数据对象）的内存地址。在32位系统占4字节，在64位系统中占8字节；</p>\n<p>​         Length：如果是数组对象，还有一个保存数组长度的空间，占4个字节；</p>\n<p>​         对齐填充：Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​        从上图我们可以看出，对象中关于锁的信息是存在Markword里的。</p>\n<h2>2、锁的创建</h2>\n<pre><code class=\"language-java\">// 随便创建一个对象\nLockObject lockObject = new LockObject();\n synchronized(lockObject){\n     //代码\n }\n</code></pre>\n<p>​    1）当我们创建一个对象LockObject时，该对象的部分Markword关键数据如下。</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA.png\" alt=\"1571144064662\"></p>\n<p>​         从图中可以看出，偏向锁的标志位是“01”，状态是“0”，表示该对象还没有被加上偏向锁。（“1”是表示被加上偏向锁）。</p>\n<p>​         该对象被创建出来的那一刻，就有了偏向锁的标志位，这也说明了所有对象都是可偏向的，但所有对象的状态都为“0”，也同时说明所有被创建的对象的偏向锁并没有生效。</p>\n<p>​    2）不过，当线程执行到临界区（critical section）时，此时会利用CAS(Compare and Swap)操作，将线程ID插入到Markword中，同时修改偏向锁的标志位。</p>\n<p>​          此时的Mark word的结构信息如下：</p>\n<p><img src=\"/img/%E9%94%81%E7%9A%84%E5%88%9B%E5%BB%BA2.png\" alt=\"1571144092579\"></p>\n<p>​          此时偏向锁的状态为“1”，说明对象的偏向锁生效了，同时也可以看到，哪个线程获得了该对象的锁。</p>\n<p>​    3）这个锁会偏向于第一个获得它的线程，在接下来的执行过程中，假如该锁没有被其他线程所获取，没有其他线程来竞争该锁，那么持有偏向锁的线程将永远不需要进行同步操作。</p>\n<p>​    4）在此线程之后的执行过程中，如果再次进入或者退出同一段同步块代码，并不再需要去进行加锁或者解锁操作，而是会做以下的步骤：</p>\n<p>​         a、Load-and-test，也就是简单判断一下当前线程id是否与Markword当中的线程id是否一致.\n​         b、如果一致，则说明此线程已经成功获得了锁，继续执行下面的代码.\n​         c、如果不一致，则要检查一下对象是否还是可偏向，即“是否偏向锁”标志位的值。\n​         d、如果还未偏向，则利用CAS操作来竞争锁，也即是第一次获取锁时的操作。</p>\n<p>​    5）如果此对象已经偏向了，并且不是偏向自己，则说明存在了竞争。此时可能就要根据另外线程的情况，可能是重新偏向，也有可能是做偏向撤销，但大部分情况下就是升级成轻量级锁了。可以看出，偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。</p>\n<p>6）为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。在Jdk1.6之后，偏向锁的开关是默认开启的，适用于只有一个线程访问同步块的场景</p>\n"},{"title":"分布式全局唯一ID生成策略","author":"郑天祺","date":"2019-10-09T04:00:00.000Z","_content":"\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","source":"_posts/分布式全局唯一ID生成策略.md","raw":"title: 分布式全局唯一ID生成策略\nauthor: 郑天祺\ntags:\n\n  - 分布式\ncategories:\n  - 分布式\ndate: 2019-10-09 12:00:00\n\n---\n\n# 一、需求\n\n在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。\n\n当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，\n\n对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。\n\n此时一个能够生成全局唯一ID的系统是非常必要的。\n\n \n\n# 二、ID生成的原则：\n\n1、全局唯一性：不能出现重复的ID（最基本的要求）\n\n2、高性能，低延迟。（不要太繁杂的算法）\n\n3、易于存储，（占用较低的空间）\n\n \n\n# 三、相对应的算法：\n\n## 1、雪花算法 snowflake\n\n![1570599617667](/img/雪花算法.png)\n\n1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0\n\n41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69\n\n10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）\n\n12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号\n\n### **（1）优点：**\n\n时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）\n\n灵活度高，可以根据业务需求，调整bit位的划分\n\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）\n\n### **（2）缺点：**\n\n依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）\n\n在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。\n\n应用举例：\n\nMongdb objectID\n\n可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。\n\n## 2、UUID\n\nUUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）\n\n为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。\n\n（1）基于时间的UUID\n\n基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。\n\n（2）DCE安全的UUID\n\nDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。\n\n（3）基于名字的UUID（MD5）\n\n基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。\n\n（4) 随机UUID\n\n根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。\n\n(5) 基于名字的UUID（SHA1）\n\n和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。\n\n### (1)    优点：\n\n性能非常高：本地生成，没有网络消耗。\n\n### (2)    缺点：\n\n不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用\n\n信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露\n\nID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）\n\n## 3、利用数据库\n\n步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：\n\n![1570600564344](/img/数据库分布式ID生成.png)\n\n美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。\n\n \n\n# 参考文档：\n\nMongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html\n\n\n\n\n\n ","slug":"分布式全局唯一ID生成策略","published":1,"updated":"2019-10-15T10:05:44.357Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2c0035x8vhgbegh208","content":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2>1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2>2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3>(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3>(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2>3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、需求</h1>\n<p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。</p>\n<p>当需要将节点之间在不同时间的交互做唯一标识，数据日渐增长，</p>\n<p>对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。</p>\n<p>此时一个能够生成全局唯一ID的系统是非常必要的。</p>\n<h1>二、ID生成的原则：</h1>\n<p>1、全局唯一性：不能出现重复的ID（最基本的要求）</p>\n<p>2、高性能，低延迟。（不要太繁杂的算法）</p>\n<p>3、易于存储，（占用较低的空间）</p>\n<h1>三、相对应的算法：</h1>\n<h2>1、雪花算法 snowflake</h2>\n<p><img src=\"/img/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.png\" alt=\"1570599617667\"></p>\n<p>1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0</p>\n<p>41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的。可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69</p>\n<p>10位机器标识码：可以部署在1024个节点（2^10=1024），如果机器分机房（IDC）部署，这10位可以由 5位机房ID + 5位机器ID 组成。（但是这个也是会重复的网上说法木有参考性，可以改为TPM安全芯片、网卡等的唯一标识码，原则上他们是全球唯一的）</p>\n<p>12位序列：毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号</p>\n<h3><strong>（1）优点：</strong></h3>\n<p>时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。（排序方便，会有很多好处）</p>\n<p>灵活度高，可以根据业务需求，调整bit位的划分</p>\n<p>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的（多一个依赖的组件，多一个风险，并增加了系统的复杂性）</p>\n<h3><strong>（2）缺点：</strong></h3>\n<p>依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。（网上有优化时钟回拨问题利用记录最后一次成ID的时间，也可利用zookeeper、redis中间件）</p>\n<p>在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。</p>\n<p>应用举例：</p>\n<p>Mongdb objectID</p>\n<p>可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p>\n<h2>2、UUID</h2>\n<p>UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。（微软叫GUID：Globally Unique Identifier）</p>\n<p>为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。</p>\n<p>（1）基于时间的UUID</p>\n<p>基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题（曾被用于寻找梅丽莎病毒的制作者位置）。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p>\n<p>（2）DCE安全的UUID</p>\n<p>DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p>\n<p>（3）基于名字的UUID（MD5）</p>\n<p>基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p>\n<p>（4) 随机UUID</p>\n<p>根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p>\n<p>(5) 基于名字的UUID（SHA1）</p>\n<p>和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。</p>\n<h3>(1)    优点：</h3>\n<p>性能非常高：本地生成，没有网络消耗。</p>\n<h3>(2)    缺点：</h3>\n<p>不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用</p>\n<p>信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露</p>\n<p>ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用（mysql主键索引是B+树，推荐使用自增存储效率高）</p>\n<h2>3、利用数据库</h2>\n<p>步长需设置为N，每台的初始值依次为0,1,2…N-1那么整个架构就变成了如下图所示：</p>\n<p><img src=\"/img/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90.png\" alt=\"1570600564344\"></p>\n<p>美团Leaf-segment方案直接取一批号段，用完再取一批号段，避免每次都去请求数据库导致连接数和线程数过大。</p>\n<h1>参考文档：</h1>\n<p>Mongdb objectID: https://docs.mongodb.com/manual/reference/method/ObjectId/#description\nLeaf——美团点评分布式ID生成系统: https://tech.meituan.com/2017/04/21/mt-leaf.html\n分布式ID生成 - 雪花算法: https://blog.csdn.net/u012488504/article/details/82194495\n梅丽莎病毒: https://baike.baidu.com/item/梅丽莎病毒/9739231\nmysql中InnoDB表为什么要建议用自增列做主键: https://www.cnblogs.com/moyand/p/9013663.html</p>\n"},{"title":"公平锁、非公平锁","author":"郑天祺","date":"2019-08-31T05:21:00.000Z","_content":"\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","source":"_posts/公平锁、非公平锁.md","raw":"title: 公平锁、非公平锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:21:00\n\n---\n\n1、概念：\n\n​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。\n\n​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。\n\n​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html\n\n​        https://www.jianshu.com/p/06340f8feb05\n\n​       \n\n2、Java语言中:\n\n​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。\n\n​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3","slug":"公平锁、非公平锁","published":1,"updated":"2019-10-15T10:06:08.660Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2e0038x8vhrxnyybrg","content":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html</p>\n<p>​        https://www.jianshu.com/p/06340f8feb05</p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、概念：</p>\n<p>​        公平锁：加锁前先查看是否有排队等待的线程，有的话优先处理排在前面的线程，先来先得。\n​        公平所：线程加锁时直接尝试获取锁，获取不到就自动到队尾等待。</p>\n<p>​        更多的是直接使用非公平锁：非公平锁比公平锁性能高5-10倍，因为公平锁需要在多核情况下维护一个队列，如果当前线程不是队列的第一个无法获取锁，增加了线程切换次数。</p>\n<p>​        原理 ： https://www.cnblogs.com/little-fly/p/10365109.html</p>\n<p>​        https://www.jianshu.com/p/06340f8feb05</p>\n<p>​</p>\n<p>2、Java语言中:</p>\n<p>​    公平和非公平锁的队列都基于锁内部维护的一个双向链表，表结点Node的值就是每一个请求当前锁的线程。</p>\n<p>​    两者的区别：https://www.jianshu.com/p/c7d17b5c6be3</p>\n"},{"title":"加密解密","author":"郑天祺","date":"2019-09-02T05:37:00.000Z","_content":"\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","source":"_posts/加密解密.md","raw":"title: 加密解密\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-02 13:37:00\n\n---\n\n# 1、组成\n\n（1）明文：未加密的消息m；\n\n（2）密文：加密后的消息ct；\n\n（3）加解密算法：把明文变成密文，密文变成明文的转换函数；\n\n（4）加密密钥：明文 加密成 密文 需要的参数；\n\n（5）解密密钥：密文变成 明文 需要的参数\n\n# 2、分类\n\n## （1）对称加密算法\n\n对称加密算法 ： 加密密钥 = 解密密钥\n\n![](/img/对称加密算法.png)\n\n## （2）非对称加密算法\n\n对称加密算法 ： 加密密钥 != 解密密钥\n\n![](/img/非对称加密算法.png)\n\n## （3）混合加密机制\n\n混合加密算法：对称加密 + 非对称加密\n\n![](/img/混合加密的方式.png)\n\n### \t加密过程\n\n（a）首先利用对称加密技术加密索要安全传输的消息\n\n（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中\n\n### \t解密过程\n\n（a）首先使用私钥解密密钥\n\n（b）然后再用此密钥解密消息\n\n## （4）为什么需要混合加密机制？\n\n### \t安全？速度快？\n\n​\t先拿对称加密和非对称加密算法，做一个对比\n\n​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。\n\n![1571142451345](/img/加密算法.png)\n\n谈一下混合的好处：\n\n（a）利用对称加密的速度快：进行网络消息传输时响应及时；\n\n（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）\n\n","slug":"加密解密","published":1,"updated":"2019-10-15T12:40:18.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2h003cx8vh67yj5ulm","content":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2>（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt></p>\n<h2>（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt></p>\n<h2>（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt></p>\n<h3>加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3>解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2>（4）为什么需要混合加密机制？</h2>\n<h3>安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、组成</h1>\n<p>（1）明文：未加密的消息m；</p>\n<p>（2）密文：加密后的消息ct；</p>\n<p>（3）加解密算法：把明文变成密文，密文变成明文的转换函数；</p>\n<p>（4）加密密钥：明文 加密成 密文 需要的参数；</p>\n<p>（5）解密密钥：密文变成 明文 需要的参数</p>\n<h1>2、分类</h1>\n<h2>（1）对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 = 解密密钥</p>\n<p><img src=\"/img/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt></p>\n<h2>（2）非对称加密算法</h2>\n<p>对称加密算法 ： 加密密钥 != 解密密钥</p>\n<p><img src=\"/img/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt></p>\n<h2>（3）混合加密机制</h2>\n<p>混合加密算法：对称加密 + 非对称加密</p>\n<p><img src=\"/img/%E6%B7%B7%E5%90%88%E5%8A%A0%E5%AF%86%E7%9A%84%E6%96%B9%E5%BC%8F.png\" alt></p>\n<h3>加密过程</h3>\n<p>（a）首先利用对称加密技术加密索要安全传输的消息</p>\n<p>（b）然后将对称密钥通过非对称加密的方式用公钥进行加密，附在（a）所述消息中</p>\n<h3>解密过程</h3>\n<p>（a）首先使用私钥解密密钥</p>\n<p>（b）然后再用此密钥解密消息</p>\n<h2>（4）为什么需要混合加密机制？</h2>\n<h3>安全？速度快？</h3>\n<p>​\t先拿对称加密和非对称加密算法，做一个对比</p>\n<p>​\t本文中的私钥、公钥是非对称加密的说法；密钥是对称加密的说法。</p>\n<p><img src=\"/img/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.png\" alt=\"1571142451345\"></p>\n<p>谈一下混合的好处：</p>\n<p>（a）利用对称加密的速度快：进行网络消息传输时响应及时；</p>\n<p>（b）非对称加密的安全优势：给你一个通过公钥加密的密钥，你先拿私钥解开加密的密钥，然后才能解开消息，保证密钥不被泄露。（注：有点绕；此处私钥、公钥是非对称加密的说法；密钥是对称加密的说法。）</p>\n"},{"title":"单例模式","author":"郑天祺","date":"2020-01-03T08:24:00.000Z","_content":"\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","source":"_posts/单例模式.md","raw":"title: 单例模式\nauthor: 郑天祺\ntags:\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 16:24:00\n\n---\n\n继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：\n\n一、单例优势与劣势\n\n优点：\n\n​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。\n\n​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。\n\n​\t\t（3）提供了对唯一实例的受控访问。\n\n  缺点：\n\n​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。\n\n​\t\t（3）不适用于变化对象。\n\n​\t\t（4）滥用单例会出现一些负面问题：\n\na. 如为节省资源将数据库连接池对象设计为单例\n\n可能会导致共享连接池对象对程序过多而出现连接池溢出。\n\nb. 如果实例化的对象长时间不被利用\n\n系统会认为是垃圾而被回收，这样将导致对象状态丢失。\n\n二、单例模式与数据库连接（mysql为例，自己的理解）\n\n​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误\n\n​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。\n\n​\t\t（3）排队希望教给mysql引擎去解决。\n\n​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。\n\n​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。\n\n​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）","slug":"单例模式","published":1,"updated":"2020-01-03T09:24:02.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2j003fx8vhm4c2orbs","content":"<p>继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>继之前的单例模式（https://blog.csdn.net/qq_23034755/article/details/90547215）深入学习，越看越容易不明白了[哭哭]：</p>\n<p>一、单例优势与劣势</p>\n<p>优点：</p>\n<p>​\t\t（1）可以节约内存，因为它限制了实例的个数，有利于Java垃圾回收。</p>\n<p>​\t\t（2）数据库或者Socket连接要收到一定的限制，必须保持同一时间只能有一个连接的存在等这种单线程操作。</p>\n<p>​\t\t（3）提供了对唯一实例的受控访问。</p>\n<p>缺点：</p>\n<p>​\t\t（1）没有抽象层，不能继承扩展很难。\n​\t\t（2）违背了“单一职责原则”，一个类只重视内部关系，而忽略外部关系。</p>\n<p>​\t\t（3）不适用于变化对象。</p>\n<p>​\t\t（4）滥用单例会出现一些负面问题：</p>\n<p>a. 如为节省资源将数据库连接池对象设计为单例</p>\n<p>可能会导致共享连接池对象对程序过多而出现连接池溢出。</p>\n<p>b. 如果实例化的对象长时间不被利用</p>\n<p>系统会认为是垃圾而被回收，这样将导致对象状态丢失。</p>\n<p>二、单例模式与数据库连接（mysql为例，自己的理解）</p>\n<p>​\t\t（1）mysql是有最大连接数的，连接数超过最大会出现错误</p>\n<p>​\t\t（2）如果利用单例模式对connection对象封装，那么系统中只存在一个mysql连接实例，大家共用。所以没有办法并发，就存在了排队。</p>\n<p>​\t\t（3）排队希望教给mysql引擎去解决。</p>\n<p>​\t\t（4）后来为了获取更高的效率，利用数据库连接池（connection pool），连接池概念（https://zhengtianqi.github.io/2019/09/01/池化之线程池/）。</p>\n<p>​\t\t（5）利用单例模式来管理connection pool，如：在初始化时创建100个connection对象（小于mysql最大连接数），然后需要的时候提供一个，用完之后返回到pool中。</p>\n<p>​\t\t（6）这个pool存在哪里呢？若为全局变量，又违背了单例模式的用意（单例模式只有真正的单一实例的需求时才可以使用。一个设计得当的系统不应该有所谓的全局变量的，这些变量应该放到它们所描述的实体所对应的类中去）</p>\n"},{"title":"可信与可信计算","author":"郑天祺","date":"2019-09-28T08:55:00.000Z","_content":"\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","source":"_posts/可信与可信计算.md","raw":"title: 可信与可信计算\nauthor: 郑天祺\ntags:\n  - 可信计算\ncategories:\n  - 可信\ndate: 2019-09-28 16:55:00\n\n---\n\n一、“可信”有比较多的定义\n\n（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。\n\n（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。\n\n（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。\n\n（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。\n\n（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。\n\n（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。\n\n二、为什么这么多定义？\n\n（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。\n\n（2）本文主要研究沈昌祥院士的trusted computing，信任计算\n\n（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。\n\n三、信任的属性\n\n（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。\n\n（2）信任具有二重性，既有主观性又有客观性。\n\n（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。\n\n（4）信任可度量，也就是说信任有程度之分，可以划分等级。\n\n（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。\n\n（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。\n\n四、信任链\n\n​\t![1569663160081](/img/信任链.png)\n\n五、可信根\n\n![1569664589958](/img/可信根.png)\n\n图中的链也是信任链\n\n六、待研究领域\n\n（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构\n\n（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生\n\n（3）信任链技术：包括信任的传递\n\n（4）信任的度量：动态度量、存储和报告机制、可信测试\n\n（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件\n\n（6）可信网络：可信网络结构、可信网络协议、可信网络设备\n\n七、理论基础\n\n（1）可信模型：数学模型、行为学模型\n\n（2）可信度量理论：软件的动态可信性度量理论与模型\n\n（3）信任链理论：信任的传递理论、信任传递的损失度量\n\n（4）软件理论：可信性度量理论、可信软件工程、软件行为学","slug":"可信与可信计算","published":1,"updated":"2019-10-15T10:07:21.402Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2m003jx8vhkv107vrq","content":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一、“可信”有比较多的定义</p>\n<p>（1）TCG用实体行为的预期性来定义 “可信” ：如果一个实体的行为是预期的方式符合预期的目标，则该实体是可信的。</p>\n<p>（2）ISO/IEC 15408标准定义“可信”为：参与计算的组件、操作或过程在任意条件下是可预测的，并能够抵御病毒和物理干扰。</p>\n<p>（3）IEEE CS可信计算技术委员会（IEEE ComputerSocietyTechnical Committeeon Dependable Computing）所谓 “可信” 是指计算机系统所提供的服务是可以论证其是可信赖的，即不仅计算机系统所提供的服务是可信赖的，而且这种可信赖还是可论证的。这种可信依赖更多地指系统的可靠性、可用性和可维护性。</p>\n<p>（4）我国著名的信息安全专家沈昌祥院士对上述定义进行了综合和拓展，他认为“可信”要做到一个实体在实现给定目标对其行为总是同预期的结果一样，强调行为结果的可预测性和可控制性。</p>\n<p>（5）张焕国教授认为可信计算系统是能够提供系统的可靠性、可用性、安全性（信息的安全性和行为的安全性）的计算机系统，通俗的称为：可信≈可靠+安全。</p>\n<p>（6）另外，还有其他一些解释：可信是指计算机系统提供的服务可以被证明是可信赖的；如果一个系统按照预期的设计和策略运行，那么这个系统是可信的；当第二个实体符合第一个实体的期望行为时，第一个实体可假设第二个实体是可信的。</p>\n<p>二、为什么这么多定义？</p>\n<p>（1）因为他们的研究背景不同：可信赖计算（dependable computing）、安全计算（security computing）和信任计算（trusted computing）。他们统称为可信计算。</p>\n<p>（2）本文主要研究沈昌祥院士的trusted computing，信任计算</p>\n<p>（3）信任计算源自早起的安全硬件设计，基本思想是：假定真实性可以用于计算机系统中首先建立一个信任根，再建立一条信任链，一级度量认证一级，一级信任一级，把信任关系扩大到整个计算机系统，从而确保计算机系统可信。</p>\n<p>三、信任的属性</p>\n<p>（1）信任是一种二元关系，它可以是一对一、一对多（个体对群体）、多对一（群体对个体）或多对多（群体对群体）的。</p>\n<p>（2）信任具有二重性，既有主观性又有客观性。</p>\n<p>（3）信任不一定具有对称性，即A信任B，则不一定就有B信任A。</p>\n<p>（4）信任可度量，也就是说信任有程度之分，可以划分等级。</p>\n<p>（5）信任可传递，但不绝对，而且在传递过程中可能有损失，传递的路径越长，损失的可能性就越大。</p>\n<p>（6）信任具有动态性，即信任与环境(上下文)和时间因素相关。</p>\n<p>四、信任链</p>\n<p>​\t<img src=\"/img/%E4%BF%A1%E4%BB%BB%E9%93%BE.png\" alt=\"1569663160081\"></p>\n<p>五、可信根</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E6%A0%B9.png\" alt=\"1569664589958\"></p>\n<p>图中的链也是信任链</p>\n<p>六、待研究领域</p>\n<p>（1）系统结构：包括硬件结构、TPM的物理安全、TPM的嵌入式软件、软件结构</p>\n<p>（2）密码技术：公钥密码、传统密码、哈希函数、随机数产生</p>\n<p>（3）信任链技术：包括信任的传递</p>\n<p>（4）信任的度量：动态度量、存储和报告机制、可信测试</p>\n<p>（5）可信软件：包括可信操作系统、可信编译、可信数据库、可信应用软件</p>\n<p>（6）可信网络：可信网络结构、可信网络协议、可信网络设备</p>\n<p>七、理论基础</p>\n<p>（1）可信模型：数学模型、行为学模型</p>\n<p>（2）可信度量理论：软件的动态可信性度量理论与模型</p>\n<p>（3）信任链理论：信任的传递理论、信任传递的损失度量</p>\n<p>（4）软件理论：可信性度量理论、可信软件工程、软件行为学</p>\n"},{"title":"可信基本概念","author":"郑天祺","date":"2019-09-01T06:46:00.000Z","_content":"\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","source":"_posts/可信基本概念.md","raw":"title: 可信基本概念\nauthor: 郑天祺\ntags:\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-01 14:46:00\n\n---\n\n可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。\n\n后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。\n\n# 1、可信历史：\n\n## （1）可信1.0（软件容错）\n\n​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。\n\n## （2）可信2.0（硬件可信）\n\n​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。\n\n​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。\n\n## （3）可信3.0（主动防御体系）\n\n​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。\n\n​\t**总结一下:**\n\n### \t（a）TPCM\n\n​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；\n\n### \t（b） 可信基础支撑软件框架\n\n​\t宿主软件系统 + 可信软件基的双系统体系结构；\n\n### \t（c）三层三元对等的可信连接框架\n\n​\t提高了网络连接的整体可信性、安全性和可管理性；\n\n### \t（d）加密算法均自主设计\n\n​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。\n\n### \t（f）主动免疫可信架构信任链传递示意图：\n\n​\t![](/img/主动免疫可信架构信任链传递示意图.png)\n\n## 2、可信的应用\n\n## （1）基础架构图\n\n沈昌祥院士提到可信云的基础架构：\n\n![](/img/可信在云平台的基础架构.png)\n\n## （2）可信的安全保障机制\n\n### （a）运行环境\n\n通过建立云架构下的可信链，为虚拟运行环境提供可信保障；\n\n### （b）监控技术\n\n通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；\n\n### （c）隔离技术\n\n通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；\n\n### （d）接入技术\n\n通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。\n\n","slug":"可信基本概念","published":1,"updated":"2019-10-15T10:10:52.269Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2o003mx8vh9xn8m6jt","content":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2>（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2>（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2>（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3>（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3>（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3>（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3>（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3>（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt></p>\n<h2>2、可信的应用</h2>\n<h2>（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt></p>\n<h2>（2）可信的安全保障机制</h2>\n<h3>（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3>（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3>（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3>（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>可信的基本思想是在计算机系统中首先建立一个信任根，在计算机系统启动和运行过程中再建立一条信任链，实现对计算机系统局部或全局的可信验证，从而发现不可信实体，及时恢复或阻断运行，从而确保系统安全。</p>\n<p>后来由产生可信操作系统、可信应用、可信网络到可信浏览器等等等等整套可信的体系。</p>\n<h1>1、可信历史：</h1>\n<h2>（1）可信1.0（软件容错）</h2>\n<p>​\t可信计算技术的发展最早可追溯到２０世纪８０年代，以世界容错组织为代表，通过纯软件实现的容错、故障诊断等机制，验证计算机部件的运行状态，从而实现计算机部件的冗余备份和故障切换。但是众所周知，纯软件实现的安全机制极易被攻击，所以说软件容错是有弊端的。</p>\n<h2>（2）可信2.0（硬件可信）</h2>\n<p>​\t2000年左右，以 TCG 组织（Trusted Computing Group）为代表，TCG组织制定了TPM（Trusted Platform Module）的标准，很多安全芯片都是符合这个规范的。而且由于其硬件实现安全防护，正逐渐成为PC，尤其是便携式PC的标准配置。</p>\n<p>​\t通过为计算机增加硬件实现的信任根 TPM 构建开机启动的信任链，从而实现终端计算机的可信启动，标志着可信计算进入了2.0时代。</p>\n<h2>（3）可信3.0（主动防御体系）</h2>\n<p>​\t沈昌祥院士在可信3.0战略中提出：可信 3.0 已经形成了自主创新的体系，并在很多领域开展了规模应用。</p>\n<p>​\t<strong>总结一下:</strong></p>\n<h3>（a）TPCM</h3>\n<p>​\tTPCM（可信平台控制模块，一个硬件）作为自主可控的可信节点植入可信根。这个信任根置于主板，先于中央处理器（CPU）启动并对基本输入输出系统（BIOS）进行验证。构成了宿主机 CPU 加可信平台控制模块的双节点，实现信任链在 “加电第一时刻” 开始建立；</p>\n<h3>（b） 可信基础支撑软件框架</h3>\n<p>​\t宿主软件系统 + 可信软件基的双系统体系结构；</p>\n<h3>（c）三层三元对等的可信连接框架</h3>\n<p>​\t提高了网络连接的整体可信性、安全性和可管理性；</p>\n<h3>（d）加密算法均自主设计</h3>\n<p>​\t命名为SM 国产密码算法，并自主设计了双数字证书认证结构。</p>\n<h3>（f）主动免疫可信架构信任链传递示意图：</h3>\n<p>​\t<img src=\"/img/%E4%B8%BB%E5%8A%A8%E5%85%8D%E7%96%AB%E5%8F%AF%E4%BF%A1%E6%9E%B6%E6%9E%84%E4%BF%A1%E4%BB%BB%E9%93%BE%E4%BC%A0%E9%80%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" alt></p>\n<h2>2、可信的应用</h2>\n<h2>（1）基础架构图</h2>\n<p>沈昌祥院士提到可信云的基础架构：</p>\n<p><img src=\"/img/%E5%8F%AF%E4%BF%A1%E5%9C%A8%E4%BA%91%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png\" alt></p>\n<h2>（2）可信的安全保障机制</h2>\n<h3>（a）运行环境</h3>\n<p>通过建立云架构下的可信链，为虚拟运行环境提供可信保障；</p>\n<h3>（b）监控技术</h3>\n<p>通过建立基于可信第三方的监控技术，可以有效监控云服务的执行，解决云服务不可信问题；</p>\n<h3>（c）隔离技术</h3>\n<p>通过基于可信根支撑的隔离技术，可以在云环境建立起具有可信保障的多层隔离防线，为虚拟机提供安全可信的隔离环境；</p>\n<h3>（d）接入技术</h3>\n<p>通过可信接入技术提供可信的云环境接入方法，解决开放云环境所带来的一系列安全问题。</p>\n"},{"title":"可重入锁","author":"郑天祺","date":"2019-08-31T05:05:00.000Z","_content":"\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","source":"_posts/可重入锁.md","raw":"title: 可重入锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:05:00\n\n---\n\n## 1、可重入锁：\n\n​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t\"独占\"，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。\n\n​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。\n\n​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。\n\n## 2、Synchronized和ReentrantLock\n\n**1）性能区别：**\n\n​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。\n\n2）原理区别：\n\n​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 \n\n​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：\n\n1. 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。\n2. 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。\n3. 锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。\n\n3) demo\n\n```java\n public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n```\n\n","slug":"可重入锁","published":1,"updated":"2019-10-15T10:07:40.790Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2r003qx8vh3cejboio","content":"<h2>1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t&quot;独占&quot;，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2>2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、可重入锁：</h2>\n<p>​\t也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。\n​\t&quot;独占&quot;，就是在同一时刻只能有一个线程获取到锁，而其它获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁。</p>\n<p>​\t“可重入“，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。</p>\n<p>​\t在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。</p>\n<h2>2、Synchronized和ReentrantLock</h2>\n<p><strong>1）性能区别：</strong></p>\n<p>​         在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实    synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。</p>\n<p>2）原理区别：</p>\n<p>​         Synchronized: 进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>\n<p>​         ReentrantLock: 是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>\n<ol>\n<li>等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。</li>\n<li>公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</li>\n<li>锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</li>\n</ol>\n<ol start=\"3\">\n<li>demo</li>\n</ol>\n<pre><code class=\"language-java\"> public class SynchronizedTest implements Runnable {\n     public synchronized void get() {\n         System.out.println(Thread.currentThread().getName());\n         set();\n     }\n     public synchronized void set() {\n         System.out.println(Thread.currentThread().getName());\n     }\n     @Override\n     public void run() {\n         get();\n     }\n     public static void main(String[] args) {\n         SynchronizedTest synchronizedTest = new SynchronizedTest();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n         new Thread(synchronizedTest).start();\n     }\n }\n\n \n\npublic class ReentrantLockTest implements Runnable {\n     ReentrantLock lock = new ReentrantLock();\n\n    public void get() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         set();\n         lock.unlock();\n     }\n\n    public void set() {\n         lock.lock();\n         System.out.println(Thread.currentThread());\n         lock.unlock();\n     }\n\n    @Override\n     public void run() {\n         get();\n     }\n\n    public static void main(String[] args) {\n         ReentrantLockTest lock = new ReentrantLockTest();\n         new Thread(lock).start();\n         new Thread(lock).start();\n         new Thread(lock).start();\n     }\n }\n\n \n</code></pre>\n"},{"title":"可靠性和容错技术","author":"郑天祺","date":"2019-10-02T05:39:00.000Z","_content":"\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","source":"_posts/可靠性和容错技术.md","raw":"title: 可靠性和容错技术\nauthor: 郑天祺\ntags:\n  - 可靠\n  - 容错\ncategories:\n  - 可信\ndate: 2019-10-02 13:39:00\n\n---\n\n​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。\n\n一、避免技术\n\n​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。\n\n二、容错技术\n\n​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。\n\n​\t所以我们重点研究容错技术：\n\n​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。\n\n​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：\n\n1. 使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；\n2. 在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等； \n3. 或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标； \n4. 缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。","slug":"可靠性和容错技术","published":1,"updated":"2019-10-02T06:48:35.455Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2u003tx8vhpoq2asrj","content":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t为了提高计算机系统的可靠性，人们通过长期的研究总结出了两种技术：避错技术和容错技术。</p>\n<p>一、避免技术</p>\n<p>​\t避错技术试图构造一个不包含故障的完美系统，其手段是采用精确的设计和质量控制方法尽量避免把故障引入系统。避错系统对元器件的制造工艺、精确的阈值有很高的要求。实际上做到这点是不可能的，因此避错技术对系统的可靠性的提高受到很大的限制。</p>\n<p>二、容错技术</p>\n<p>​\t容错是指当出现某些指定的硬件故障或软件故障时，系统仍能执行规定的一组程序，或者说程序不会因为系统故障而中止或被修改，并且执行结果也不包含系统故障引起的差错。容错的思想是在系统体系结构上精心设计，利用外加资源的冗余技术掩蔽故障带来的影响，从而自动恢复系统或达到安全停机的目的。</p>\n<p>​\t所以我们重点研究容错技术：</p>\n<p>​\t容错的目标是降低或者最小化故障对系统可用性、可靠性、安全性、持续性等得影响。</p>\n<p>​\t容错按系统级别划分，分为三个级别，硬件容错、软件容错以及系统容错。硬件容错常用的方法包括使用冗余、多备份技术、增加内存、能源系统冗余等。硬件错误通常能够够在两个物理机上进行隔离处理。软件容错主要是正对软件的鲁棒性特征进行增强。常见的方法有checkpoint/restart，recovery blocks，N-Version Programs等。对于系统容错，设计一个独立与目标系统的子系统，通过定义定义规则来容忍系统缺陷。对缺陷的处理，有以下几类技术：</p>\n<ol>\n<li>使用缺陷避免技术来避一些错误。使用成熟的设计方法论、验证以及确认方法论、代码检查、上线前的演练等；</li>\n<li>在可能会存在的缺陷时，可以选择缺陷移除技术。例如测试、集成测试、回归测试、背靠背测试等；</li>\n<li>或者是在遭遇错误是，缺陷回避的方式，是的潜在的缺陷不会被激活。常见技术是通过重新配置系统来达到避免的目标；</li>\n<li>缺陷容忍技术，系统能够对缺陷进行侦测、诊断、孤立、覆盖、不错、以及系统恢复。使用以上多种技术混合。</li>\n</ol>\n"},{"title":"图解公钥私钥","author":"郑天祺","date":"2019-09-24T13:32:00.000Z","_content":"\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","source":"_posts/图解公钥与私钥.md","raw":"title: 图解公钥私钥\nauthor: 郑天祺\ntags:\n\n  - 可信\n  - 密码学\ncategories:\n  - 可信\ndate: 2019-09-24 21:32:00\n---\n\n\n1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。\n\n![1569332117257](/img/公钥私钥1.png)\n\n2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。\n\n![1569332140572](/img/公钥私钥2.png)\n\n3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。\n\n![1569332191207](/img/公钥私钥3.png)\n\n4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。\n\n![1569332213926](/img/公钥私钥4.png)\n\n5、鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。\n\n![1569332234555](/img/公钥私钥5.png)\n\n6、然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。\n\n![1569332255195](/img/公钥私钥6.png)\n\n7、鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n![1569332274920](/img/公钥私钥7.png)\n\n8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。\n\n![1569332297876](/img/公钥私钥8.png)\n\n9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。\n\n![1569332325389](/img/公钥私钥9.png)\n\n10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。\n\n![1569332348936](/img/公钥私钥10.png)\n\n11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。\n\n![1569332371090](/img/公钥私钥11.png)\n\n12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n![1569332395630](/img/公钥私钥12.png)\n\n13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n![1569332424970](/img/公钥私钥13.png)\n\n14、下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n![1569332446930](/img/HTTPS1.png)\n\n15、首先，客户端向服务器发出加密请求。\n\n![1569332470793](/img/HTTPS2.png)\n\n16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n![1569332492570](/img/HTTPS3.png)\n\n17、客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n![1569332511083](/img/HTTPS4.png)\n\n18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n![1569332532928](/img/HTTPS5.png)\n\n19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n![1569332579189](/img/HTTPS6.png)\n\n","slug":"图解公钥与私钥","published":1,"updated":"2019-09-24T13:51:57.731Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2w003wx8vhaw5dzj3o","content":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>1、鲍勃有两把钥匙，一把是公钥，另一把是私钥。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A51.png\" alt=\"1569332117257\"></p>\n<p>2、鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A52.png\" alt=\"1569332140572\"></p>\n<p>3、苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A53.png\" alt=\"1569332191207\"></p>\n<p>4、鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A54.png\" alt=\"1569332213926\"></p>\n<p>5、鲍勃给苏珊回信，决定采用&quot;数字签名&quot;。他写完后先用Hash函数，生成信件的摘要（digest）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A55.png\" alt=\"1569332234555\"></p>\n<p>6、然后，鲍勃使用私钥，对这个摘要加密，生成&quot;数字签名&quot;（signature）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A56.png\" alt=\"1569332255195\"></p>\n<p>7、鲍勃将这个签名，附在信件下面，一起发给苏珊。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A57.png\" alt=\"1569332274920\"></p>\n<p>8、苏珊收信后，取下数字签名，用鲍勃的公钥解密，得到信件的摘要。由此证明，这封信确实是鲍勃发出的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A58.png\" alt=\"1569332297876\"></p>\n<p>9、苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A59.png\" alt=\"1569332325389\"></p>\n<p>10、复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成&quot;数字签名&quot;，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A510.png\" alt=\"1569332348936\"></p>\n<p>11、后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找&quot;证书中心&quot;（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成&quot;数字证书&quot;（Digital Certificate）。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A511.png\" alt=\"1569332371090\"></p>\n<p>12、鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A512.png\" alt=\"1569332395630\"></p>\n<p>13、苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明&quot;数字签名&quot;是否真的是鲍勃签的。</p>\n<p><img src=\"/img/%E5%85%AC%E9%92%A5%E7%A7%81%E9%92%A513.png\" alt=\"1569332424970\"></p>\n<p>14、下面，我们看一个应用&quot;数字证书&quot;的实例：https协议。这个协议主要用于网页加密。</p>\n<p><img src=\"/img/HTTPS1.png\" alt=\"1569332446930\"></p>\n<p>15、首先，客户端向服务器发出加密请求。</p>\n<p><img src=\"/img/HTTPS2.png\" alt=\"1569332470793\"></p>\n<p>16、服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。</p>\n<p><img src=\"/img/HTTPS3.png\" alt=\"1569332492570\"></p>\n<p>17、客户端（浏览器）的&quot;证书管理器&quot;，有&quot;受信任的根证书颁发机构&quot;列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。</p>\n<p><img src=\"/img/HTTPS4.png\" alt=\"1569332511083\"></p>\n<p>18、如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。</p>\n<p><img src=\"/img/HTTPS5.png\" alt=\"1569332532928\"></p>\n<p>19、如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。</p>\n<p><img src=\"/img/HTTPS6.png\" alt=\"1569332579189\"></p>\n"},{"title":"基于JavaAgent的全链路监控（1）","author":"郑天祺","date":"2020-07-17T05:11:00.000Z","_content":"\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（1）.md","raw":"title: 基于JavaAgent的全链路监控（1）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-17 13:11:00\n\n---\n\n# 《手写一个最简单的javaagent》\n\n# 1、javaagent介绍\n\n​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\n\n​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。\n\n​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。\n\n​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。\n\n# 2、手写一个javaagent\n\n## （1）建立maven的空java项目\n\n​\t\t修改pom为：包含一些常量的定义和一个插件\n\n```java\n\t<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n    </properties>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.test.AgentTest</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## （2）MANIFEST.MF 文件\n\n​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：\n\n![image-20200717132223819](/img/javaagent1.png)\n\n​\t\t内容为\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n```\n\n## （3）写一个main函数\n\n​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。\n\n```java\nimport java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"执行了JavaAgent \" + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n```\n\n## （4）打包\n\n​\t\tmvn clean package\n\n# 3、运行javaagent\n\n​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：\n\n## （1）主程序运行前\n\n​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。\n\n## （2）主程序运行后加载\n\n​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。\n\n​\t\tVirtualMachine 的实现位于 tools.jar 中\n\n```java\n<dependency>\n            <groupId>com.sun</groupId>\n            <artifactId>tools</artifactId>\n            <version>1.8</version>\n            <scope>system</scope>\n            <systemPath>${java.home}/../lib/tools.jar</systemPath>\n        </dependency>\n```\n\n因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：\n\n```java\n public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(\"80000\");\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(\"D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar\", \"test\");\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n```\n\n","slug":"基于JavaAgent的全链路监控（1）","published":1,"updated":"2020-07-19T14:50:51.434Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp2z003zx8vhyailxf1u","content":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2>（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2>（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2>（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n</code></pre>\n<h2>（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2>（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2>（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《手写一个最简单的javaagent》</h1>\n<h1>1、javaagent介绍</h1>\n<p>​\t\t在使用skywalking时，使用到了Javaagent技术作为节点的探针，使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。</p>\n<p>​\t\t后来查阅资料发现javaagent用途还是很广的，有JRebel，各种线上诊断工具（Btrace, Greys），还有阿里开源的 Arthas，在此记录一下javaagent的学习历程。</p>\n<p>​\t\t其实 Java Agent 一点都不神秘，也是一个 Jar 包，只是启动方式和普通 Jar 包有所不同，对于普通的Jar包，通过指定类的 main 函数进行启动，但是 Java Agent 并不能单独启动，必须依附在一个 Java 应用程序运行。</p>\n<p>​\t\t我们可以使用 Agent 技术构建一个独立于应用程序的代理程序，用来协助监测、运行甚至替换其他 JVM 上的程序，使用它可以实现虚拟机级别的 AOP 功能。</p>\n<h1>2、手写一个javaagent</h1>\n<h2>（1）建立maven的空java项目</h2>\n<p>​\t\t修改pom为：包含一些常量的定义和一个插件</p>\n<pre><code class=\"language-java\">\t&lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n    &lt;/properties&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;2.4.3&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.test.AgentTest&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre>\n<h2>（2）MANIFEST.MF 文件</h2>\n<p>​\t\t在 META-INF 目录下创建 MANIFEST.MF 文件：</p>\n<p><img src=\"/img/javaagent1.png\" alt=\"image-20200717132223819\"></p>\n<p>​\t\t内容为</p>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.test.AgentTest\nCan-Redefine-Classes: true\n</code></pre>\n<h2>（3）写一个main函数</h2>\n<p>​\t\t因为 Java Agent 的特殊性，需要一些特殊的配置，例如指定 Agent 的启动类等。这样才能在加载 Java Agent 之后，找到并运行对应的 agentmain 或者 premain 方法。配置方式主要有两种，一种是利用 maven-assembly-plugin 插件（推荐），一种是 MANIFEST.MF 文件。</p>\n<pre><code class=\"language-java\">import java.lang.instrument.Instrumentation;\n\n/**\n * 测试项目启动执行的agent\n *\n * @author zhengtianqi\n */\npublic class AgentTest {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(&quot;执行了JavaAgent &quot; + agentArgs);\n    }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n\n}\n</code></pre>\n<h2>（4）打包</h2>\n<p>​\t\tmvn clean package</p>\n<h1>3、运行javaagent</h1>\n<p>​\t\tJavaagent 程序写好了，怎么运行它呢？上面看到 Agent 程序分为两种，一种是 premain 函数，在主程序运行之前执行；一种是 agentmain 函数，在主程序运行之后执行。Java 加载这两种 Agent 程序也有区别：</p>\n<h2>（1）主程序运行前</h2>\n<p>​\t\t无侵入式，通过 JVM 参数 -javaagent:**.jar[=test] 启动，其中 test 为传入 premain 的 agentArgs 的参数，程序启动的时候，会优先加载 Java Agent，并执行其 premain 方法，这个时候，其实大部分的类都还没有被加载，这个时候可以实现对新加载的类进行字节码修改，但是如果 premain 方法执行失败或抛出异常，那么 JVM 会被中止，这是很致命的问题。</p>\n<h2>（2）主程序运行后加载</h2>\n<p>​\t\t有侵入式，程序启动之后，通过某种特定的手段加载 Java Agent，这个特定的手段就是 VirtualMachine 的 attach api，这个 api 其实是 JVM 进程之间的的沟通桥梁，底层通过socket 进行通信，JVM A 可以发送一些指令给JVM B，B 收到指令之后，可以执行对应的逻辑，比如在命令行中经常使用的 jstack、jps 等，很多都是基于这种机制实现的。</p>\n<p>​\t\tVirtualMachine 的实现位于 tools.jar 中</p>\n<pre><code class=\"language-java\">&lt;dependency&gt;\n            &lt;groupId&gt;com.sun&lt;/groupId&gt;\n            &lt;artifactId&gt;tools&lt;/artifactId&gt;\n            &lt;version&gt;1.8&lt;/version&gt;\n            &lt;scope&gt;system&lt;/scope&gt;\n            &lt;systemPath&gt;${java.home}/../lib/tools.jar&lt;/systemPath&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>因为是进程间通信，所以使用 attach api 的也是一个独立的Java进程，下面是一个简单的实现：</p>\n<pre><code class=\"language-java\"> public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException {\n        VirtualMachine virtualMachine = null;\n        try {\n            // 80000 是进程号\n            virtualMachine = VirtualMachine.attach(&quot;80000&quot;);\n            // 第一个参数是 agent jar包路径，第二个参数为传入 agentmain 的 args 参数\n            virtualMachine.loadAgent(&quot;D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar&quot;, &quot;test&quot;);\n        } finally {\n            if (virtualMachine != null) {\n                virtualMachine.detach();\n            }\n        }\n\n    }\n</code></pre>\n"},{"title":"基于JavaAgent的全链路监控（2）","author":"郑天祺","date":"2020-07-19T08:54:00.000Z","_content":"\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","source":"_posts/基于JavaAgent的全链路监控（2）.md","raw":"title: 基于JavaAgent的全链路监控（2）\nauthor: 郑天祺\ntags:\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 16:54:00\n\n---\n\n# 《利用javaagent进行方法耗时的监控》\n\n## 1、介绍\n\n​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。\n\n## 2、pom.xml\n\n 引入ByteBuddy并打入到Agent包中\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>cn.edu.bjut</groupId>\n    <artifactId>checkpoint-agent</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n\n        <!-- Build args -->\n        <argline>-Xms512m -Xmx512m</argline>\n        <updateReleaseInfo>true</updateReleaseInfo>\n        <maven.test.skip>true</maven.test.skip>\n        <!-- 自定义MANIFEST.MF -->\n        <maven.configuration.manifestFile>src/main/resources/META-INF/MANIFEST.MF</maven.configuration.manifestFile>\n\n        <javassist.version>3.12.1.GA</javassist.version>\n        <guava.version>15.0</guava.version>\n        <byte-buddy.version>1.8.20</byte-buddy.version>\n        <maven-shade-plugin.version>2.4.3</maven-shade-plugin.version>\n\n        <maven-compiler-plugin.version>3.8.1</maven-compiler-plugin.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>javassist</groupId>\n            <artifactId>javassist</artifactId>\n            <version>${javassist.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>${guava.version}</version>\n            <scope>compile</scope>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>net.bytebuddy</groupId>\n            <artifactId>byte-buddy-agent</artifactId>\n            <version>${byte-buddy.version}</version>\n        </dependency>\n    </dependencies>\n    <!-- 将javassist包打包到Agent中 -->\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>${maven-shade-plugin.version}</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <transformers>\n                                <transformer\n                                        implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <manifestEntries>\n                                        <!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF -->\n                                        <Premain-Class>cn.edu.bjut.agent.MyAgent</Premain-Class>\n                                    </manifestEntries>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>8</source>\n                    <target>8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n```\n\n## 3、MethodCostTime.java\n\n```java\npackage cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable<?> callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + \" 方法耗时：\" + (System.currentTimeMillis() - start) + \"ms\");\n        }\n    }\n}\n\n```\n\n## 4、MyAgent.java\n\n```java\npackage cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(\"this is my agent：\" + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -> {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(\"cn.edu.bjut\"))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n```\n\n## 5、MANIFEST.MF\n\n```java\nManifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n```\n\n## 6、测试\n\n```java\nVM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n```\n\n![image-20200719170509172](/img/agent-costtime2.png)\n\n结果：\n\n![image-20200719170325930](/img/agent-costtime.png)","slug":"基于JavaAgent的全链路监控（2）","published":1,"updated":"2020-07-19T14:50:44.890Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp320042x8vh7yxc4pfe","content":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2>1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2>2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;${javassist.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;${guava.version}&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;${byte-buddy.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;${byte-buddy.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;${maven-shade-plugin.version}&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;${maven-compiler-plugin.version}&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2>3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        }\n    }\n}\n\n</code></pre>\n<h2>4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n</code></pre>\n<h2>5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2>6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行方法耗时的监控》</h1>\n<h2>1、介绍</h2>\n<p>​\t\t方法耗时利用前人轮子字节码操作工具ByteBuddy：Byte Buddy是一个代码生成和操作库，用于在Java应用程序运行时创建和修改Java类，而无需编译器的帮助。 除了Java类库附带的代码生成实用程序外，Byte Buddy还允许创建任意类，并且不限于实现用于创建运行时代理的接口。 此外，Byte Buddy提供了一个方便的API，可以使用Java代理或在构建过程中手动更改类。</p>\n<h2>2、pom.xml</h2>\n<p>引入ByteBuddy并打入到Agent包中</p>\n<pre><code class=\"language-java\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;cn.edu.bjut&lt;/groupId&gt;\n    &lt;artifactId&gt;checkpoint-agent&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n\n        &lt;!-- Build args --&gt;\n        &lt;argline&gt;-Xms512m -Xmx512m&lt;/argline&gt;\n        &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt;\n        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;\n        &lt;!-- 自定义MANIFEST.MF --&gt;\n        &lt;maven.configuration.manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/maven.configuration.manifestFile&gt;\n\n        &lt;javassist.version&gt;3.12.1.GA&lt;/javassist.version&gt;\n        &lt;guava.version&gt;15.0&lt;/guava.version&gt;\n        &lt;byte-buddy.version&gt;1.8.20&lt;/byte-buddy.version&gt;\n        &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;\n\n        &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt;\n    &lt;/properties&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;javassist&lt;/groupId&gt;\n            &lt;artifactId&gt;javassist&lt;/artifactId&gt;\n            &lt;version&gt;${javassist.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n            &lt;artifactId&gt;guava&lt;/artifactId&gt;\n            &lt;version&gt;${guava.version}&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy&lt;/artifactId&gt;\n            &lt;version&gt;${byte-buddy.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;net.bytebuddy&lt;/groupId&gt;\n            &lt;artifactId&gt;byte-buddy-agent&lt;/artifactId&gt;\n            &lt;version&gt;${byte-buddy.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;!-- 将javassist包打包到Agent中 --&gt;\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;${maven-shade-plugin.version}&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;!--指明包含 premain 方法的类名，否则打包出来的文件会找不到 MANIFEST.MF --&gt;\n                                        &lt;Premain-Class&gt;cn.edu.bjut.agent.MyAgent&lt;/Premain-Class&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n                &lt;version&gt;${maven-compiler-plugin.version}&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;source&gt;8&lt;/source&gt;\n                    &lt;target&gt;8&lt;/target&gt;\n                &lt;/configuration&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2>3、MethodCostTime.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.monitor;\n\nimport net.bytebuddy.implementation.bind.annotation.Origin;\nimport net.bytebuddy.implementation.bind.annotation.RuntimeType;\nimport net.bytebuddy.implementation.bind.annotation.SuperCall;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.Callable;\n\n/**\n * @author zhengtianqi\n */\npublic class MethodCostTime {\n\n    @RuntimeType\n    public static Object intercept(@Origin Method method, @SuperCall Callable&lt;?&gt; callable) throws Exception {\n        long start = System.currentTimeMillis();\n        try {\n            // 原有函数执行\n            return callable.call();\n        } finally {\n            System.out.println(method + &quot; 方法耗时：&quot; + (System.currentTimeMillis() - start) + &quot;ms&quot;);\n        }\n    }\n}\n\n</code></pre>\n<h2>4、MyAgent.java</h2>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport cn.edu.bjut.monitor.JvmStack;\nimport cn.edu.bjut.monitor.MethodCostTime;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport net.bytebuddy.agent.builder.AgentBuilder;\nimport net.bytebuddy.description.type.TypeDescription;\nimport net.bytebuddy.dynamic.DynamicType;\nimport net.bytebuddy.implementation.MethodDelegation;\nimport net.bytebuddy.matcher.ElementMatchers;\nimport net.bytebuddy.utility.JavaModule;\n\nimport java.lang.instrument.Instrumentation;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n        System.out.println(&quot;this is my agent：&quot; + agentArgs);\n\n        AgentBuilder.Transformer transformer = (builder, typeDescription, classLoader, javaModule) -&gt; {\n            return builder\n                    // 拦截任意方法\n                    .method(ElementMatchers.any())\n                    // 委托\n                    .intercept(MethodDelegation.to(MethodCostTime.class));\n        };\n\n        AgentBuilder.Listener listener = new AgentBuilder.Listener() {\n            @Override\n            public void onDiscovery(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onTransformation(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b, DynamicType dynamicType) {\n\n            }\n\n            @Override\n            public void onIgnored(TypeDescription typeDescription, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n            @Override\n            public void onError(String s, ClassLoader classLoader, JavaModule javaModule, boolean b, Throwable throwable) {\n\n            }\n\n            @Override\n            public void onComplete(String s, ClassLoader classLoader, JavaModule javaModule, boolean b) {\n\n            }\n\n        };\n\n        new AgentBuilder\n                .Default()\n                // 指定需要拦截的类\n                .type(ElementMatchers.nameStartsWith(&quot;cn.edu.bjut&quot;))\n                .transform(transformer)\n                .with(listener)\n                .installOn(inst);\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n\n</code></pre>\n<h2>5、MANIFEST.MF</h2>\n<pre><code class=\"language-java\">Manifest-Version: 1.0\nPremain-Class: cn.edu.bjut.agent.MyAgent\nCan-Redefine-Classes: true\n</code></pre>\n<h2>6、测试</h2>\n<pre><code class=\"language-java\">VM options: -javaagent:D:\\git\\credible\\checkpoint-agent\\target\\checkpoint-agent-1.0-SNAPSHOT.jar=testargs\n</code></pre>\n<p><img src=\"/img/agent-costtime2.png\" alt=\"image-20200719170509172\"></p>\n<p>结果：</p>\n<p><img src=\"/img/agent-costtime.png\" alt=\"image-20200719170325930\"></p>\n"},{"title":"基于JavaAgent的全链路监控（3）","author":"郑天祺","date":"2020-07-19T09:25:00.000Z","_content":"\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","source":"_posts/基于JavaAgent的全链路监控（3）.md","raw":"title: 基于JavaAgent的全链路监控（3）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 17:25:00\n\n---\n\n# 《利用javaagent进行 JVM内存与GC信息的采集》\n\n# 1、介绍\n\n​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。\n\n# 2、编码\n\n在title: 基于JavaAgent的全链路监控（2）的基础上增加\n\n## （1）MyAgent.java\n\n​\t\t\n\n```java\npackage cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(\"scheduled-%d\").build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() ->\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n```\n\n","slug":"基于JavaAgent的全链路监控（3）","published":1,"updated":"2020-07-19T14:50:37.560Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp340045x8vhhzc1swg4","content":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2>（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>《利用javaagent进行 JVM内存与GC信息的采集》</h1>\n<h1>1、介绍</h1>\n<p>​\t\t除了监控java方法的执行耗时，我们还需要获取应用实例的jvm内存与gc信息，以实时把控我们的服务器性能是否在安全范围。监控jvm内存与gc信息是非常重要的，尤其是在大促以及微博火热爆点的时候，我们需要根据监控信息进行扩容，以保证系统稳定。</p>\n<h1>2、编码</h1>\n<p>在title: 基于JavaAgent的全链路监控（2）的基础上增加</p>\n<h2>（1）MyAgent.java</h2>\n<p>​</p>\n<pre><code class=\"language-java\">package cn.edu.bjut.agent;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport java.util.concurrent.*;\n\n/**\n * @author zhengtianqi\n */\npublic class MyAgent {\n\n    /**\n     * JVM 首先尝试在代理类上调用以下方法\n     */\n    public static void premain(String agentArgs, Instrumentation inst) {\n            // 使用ScheduledExecutorService创建定时任务\n        ScheduledExecutorService schedule =\n                new ScheduledThreadPoolExecutor(1, new ThreadFactoryBuilder().setNameFormat(&quot;scheduled-%d&quot;).build());\n        // 创建并执行在给定延迟后启用的一次性操作\n        schedule.scheduleAtFixedRate(() -&gt;\n\n        {\n            // 此方法为打印jvm信息喝gc信息\n            JvmStack.printMemoryMetric();\n            JvmStack.printGcMetric();\n        }, 0L, 1000L, TimeUnit.MILLISECONDS);\n     }\n\n    /**\n     * 如果代理类没有实现上面的方法，那么 JVM 将尝试调用该方法\n     */\n    public static void premain(String agentArgs) {\n    }\n}\n</code></pre>\n"},{"title":"对象存储与指针压缩","author":"郑天祺","date":"2019-11-20T11:50:00.000Z","_content":"\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","source":"_posts/对象存储与指针压缩.md","raw":"title: 对象存储与指针压缩\nauthor: 郑天祺\ntags:\n  - 内存模型\ncategories:\n  - java基础\ndate: 2019-11-20 19:50:00\n\n---\n\n​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？\n\n​\t一、对象存储：\n\n​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：\n\n![image-20191120195326698](/img/对象存储1.png)\n\n​     \n\n​\t(1) 对齐填充 :\n\n​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。\n\n​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。\n\n![image-20191120195453758](/img/java对象存储2.png)\n\n32位系统 对象头占用空间= 4 + 4 = 8 byte\n\n64位系统 对象头占用空间= 8 + 8 =16 byte\n\n64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte\n\n注：\n\n​\t若为数组对象，对象头占用空间 + 4 byte\n\n​\t静态属性不算在对象大小内\n\n​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。\n\n​\tJDK 1.8，默认该参数就是开启的。\n\n​    (2)  对象的实际数据  \n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定\n\n![image-20191120195618441](/img/java对象存储3.png)\n\n​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。\n\n​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，\n\n​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。\n\n二、指针压缩\n\n​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。\n\n​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的\n\nOOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：\n\n​\t每个Class的属性指针（静态成员变量）；\n\n​\t每个对象的属性指针；\n\n​\t普通对象数组的每个元素指针。\n\n​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。\n\n​\t1.新生代：Eden+From Survivor+To Survivor\n\n​\t2.老年代：OldGen\n\n​\t3.永久代（方法区的实现） : PermGen----->替换为Metaspace(本地内存中)\n\n​\t(1) 验证对象头大小\n\n![image-20191120195845734](/img/指针压缩1.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）\n\n​\t共24 byte\n\n​\t(2) 验证对象头大小 非压缩情况下\n\n![image-20191120200005300](/img/指针压缩2.png)\n\n​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共32byte\n\n​\t(3) 验证对象头对齐填充\n\n![image-20191120200059442](/img/指针压缩3.png)\n\n​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；\n\n​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）\n\n​\t共20byte 所以需要有4字节的填充\n\n​\t(4) 验证对象头 数组\n\n![image-20191120200152966](/img/指针压缩4.png)\n\n​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。\n\n​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度\\*数组中对象的RetainedSize + padding大小\n\n​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6*8=48字节，那么数组对象的大小=12+4+6*8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。 \n\n\n\n主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/","slug":"对象存储与指针压缩","published":1,"updated":"2019-11-20T12:03:43.218Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3a0049x8vh7y3pofhp","content":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t我们知道在Java中基本数据类型的大小，例如int类型占4个字节、long类型占8个字节，那么Integer对象和Long对象会占用多少内存呢？</p>\n<p>​\t一、对象存储：</p>\n<p>​\t一个Java对象在内存中包括对象头、实例数据和补齐填充3个部分：</p>\n<p><img src=\"/img/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A81.png\" alt=\"image-20191120195326698\"></p>\n<p>​</p>\n<p>​\t(1) 对齐填充 :</p>\n<p>​\tJava对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。</p>\n<p>​\t例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A82.png\" alt=\"image-20191120195453758\"></p>\n<p>32位系统 对象头占用空间= 4 + 4 = 8 byte</p>\n<p>64位系统 对象头占用空间= 8 + 8 =16 byte</p>\n<p>64位开启指针压缩 对象头占用空间= 4 + 8 = 12 byte</p>\n<p>注：</p>\n<p>​\t若为数组对象，对象头占用空间 + 4 byte</p>\n<p>​\t静态属性不算在对象大小内</p>\n<p>​\t从JDK 1.6 update14开始，64位的JVM正式支持了 -XX:+UseCompressedOops 这个可以压缩指针，起到节约内存占用的新参数。</p>\n<p>​\tJDK 1.8，默认该参数就是开启的。</p>\n<p>​    (2)  对象的实际数据</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定</p>\n<p><img src=\"/img/java%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A83.png\" alt=\"image-20191120195618441\"></p>\n<p>​\t对于reference类型来说，在32位系统上占用4bytes, 在64位系统上占用8bytes。</p>\n<p>​\t对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定，</p>\n<p>​\t比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。</p>\n<p>二、指针压缩</p>\n<p>​    从上文的分析中可以看到，64位JVM消耗的内存会比32位的要多大约1.5倍，这是因为对象指针在64位JVM下有更宽的寻址。</p>\n<p>​    对于那些将要从32位平台移植到64位的应用来说，平白无辜多了1/2的内存占用，这是开发者不愿意看到的</p>\n<p>OOP的全称为：Ordinary Object Pointer，就是普通对象指针。启用CompressOops后，会压缩的对象：</p>\n<p>​\t每个Class的属性指针（静态成员变量）；</p>\n<p>​\t每个对象的属性指针；</p>\n<p>​\t普通对象数组的每个元素指针。</p>\n<p>​\t当然，压缩也不是所有的指针都会压缩，对一些特殊类型的指针，JVM是不会优化的，例如指向PermGen（1.8废弃）的Class对象指针、本地变量、堆栈元素、入参、返回值和NULL指针不会被压缩。</p>\n<p>​\t1.新生代：Eden+From Survivor+To Survivor</p>\n<p>​\t2.老年代：OldGen</p>\n<p>​\t3.永久代（方法区的实现） : PermGen-----&gt;替换为Metaspace(本地内存中)</p>\n<p>​\t(1) 验证对象头大小</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A91.png\" alt=\"image-20191120195845734\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+long类型8字节=12字节（静态变量不在计算范围之内）</p>\n<p>​\t共24 byte</p>\n<p>​\t(2) 验证对象头大小 非压缩情况下</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A92.png\" alt=\"image-20191120200005300\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为8字节+MarkWord为8字节=16字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共32byte</p>\n<p>​\t(3) 验证对象头对齐填充</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A93.png\" alt=\"image-20191120200059442\"></p>\n<p>​\t对象头大小=Class Pointer的空间大小为4字节+MarkWord为8字节=12字节；</p>\n<p>​\t实际数据大小=int类型4字节+int类型4字节=8字节（静态变量不在计算范围之内）</p>\n<p>​\t共20byte 所以需要有4字节的填充</p>\n<p>​\t(4) 验证对象头 数组</p>\n<p><img src=\"/img/%E6%8C%87%E9%92%88%E5%8E%8B%E7%BC%A94.png\" alt=\"image-20191120200152966\"></p>\n<p>​\tShallow Size比较简单，这里对象头大小为12字节， 实际数据大小为4字节，所以Shallow Size为16。</p>\n<p>​\t对于Retained Size来说，要计算数组占用的大小，对于数组来说，它的对象头部多了一个用来存储数组长度的空间，该空间大小为4字节，所以数组对象的大小 = 引用对象头大小12字节 + 存储数组长度的空间大小4字节 + 数组的长度*数组中对象的RetainedSize + padding大小</p>\n<p>​\tlong[] arr = new long[6];，它是一个长度为6的long类型的数组，由于long类型的大小为8字节，所以数组中的实际数据是6<em>8=48字节，那么数组对象的大小=12+4+6</em>8+0=64，最终的Retained Size=Shallow Size + 数组对象大小=16+64=80。</p>\n<p>主要参考：http://www.ideabuffer.cn/2017/05/06/Java对象内存布局/</p>\n"},{"title":"悲观锁、乐观锁","author":"郑天祺","date":"2019-08-31T05:16:00.000Z","_content":"\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","source":"_posts/悲观锁、乐观锁.md","raw":"title: 悲观锁、乐观锁\nauthor: 郑天祺\ntags:\n  - 锁\n  - mysql\ncategories:\n  - 数据库\ndate: 2019-08-31 13:16:00\n\n---\n\n## 1、悲观锁\n\n假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）\n\n​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。\n\n​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。\n\n## 2、缺点\n\n数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受\n\n \n\n## 3、实现方法\n\n​    **Mysql中 :**\n\n​    在sql后面加上 for update或者for update nowait\n\n​    for update和for update nowait区别：\n\n​         1. for update 锁定当前操作数据，其他事务等待\n\n​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回\"ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源\"\n\n​         例如：select * from account where name=\"123\" for update\n\n​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。\n\n​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢\n\n​    **JAVA中 ：**\n\n​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。\n\n \n\n## 4、使用场景举例\n\n以MySQL InnoDB为例\n\n   Demo：\n\n​     \n\n```java\n   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount > 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n```\n\n​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的\n\n \n\n### 0、乐观锁：\n\n不是真正的锁，而是一种实现 : 是一种实现的\n\n### 1、乐观锁：\n\n假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题\n\n​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。\n\n###  2、优缺点：\n\n​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。\n\n​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。\n\n在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。\n\n### 3、步骤 : \n\n```java\n\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n```\n\n","slug":"悲观锁、乐观锁","published":1,"updated":"2019-10-15T12:17:44.237Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3d004dx8vh8a48g6fw","content":"<h2>1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2>2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2>3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=&quot;123&quot; for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2>4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3>0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3>1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3>2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3>3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、悲观锁</h2>\n<p>假设会发生并发冲突，屏蔽一切可能违反数据完整性的操作（具有强烈的独占和排他性）</p>\n<p>​           依赖数据库的锁机制实现，以保证操作最大程度的独占性。</p>\n<p>​     百度百科：正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p>\n<h2>2、缺点</h2>\n<p>数据库性能的大量开销，特别是对长事务而言，这样的开销无法承受</p>\n<h2>3、实现方法</h2>\n<p>​    <strong>Mysql中 :</strong></p>\n<p>​    在sql后面加上 for update或者for update nowait</p>\n<p>​    for update和for update nowait区别：</p>\n<p>​         1. for update 锁定当前操作数据，其他事务等待</p>\n<p>​         2. for update nowait 锁定当前数据，其他事务发现数据被锁定，立即返回&quot;ORA-00054错误，内容是资源正忙, 但指定以 NOWAIT 方式获取资源&quot;</p>\n<p>​         例如：select * from account where name=&quot;123&quot; for update</p>\n<p>​         优点：无论是在单机还是分布式中，只要使用的是同一个数据库，那么悲观锁就能起到作用。</p>\n<p>​         缺点：锁定数据后，必将影响其他操作，在大流量的情况下，操作速度变慢</p>\n<p>​    <strong>JAVA中 ：</strong></p>\n<p>​        独占锁是一种悲观锁，synchronized就是一种独占锁，它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。</p>\n<h2>4、使用场景举例</h2>\n<p>以MySQL InnoDB为例</p>\n<p>Demo：</p>\n<p>​</p>\n<pre><code class=\"language-java\">   begin;\n\n        select amount from item where item_id = 1 for update;\n\n // 通过amount来做出一些行为,例如告诉用户库存不足,购买失败,然后只有amount &gt; 1才进入更新库存操作\n\n        update item set amount = amount - 1 where item_id = 1;\n\n        commit;\n</code></pre>\n<p>​    由于是串行执行,其他事务的for update必须等该当前事务的 for update 语句执行,所以我们不必担心我们获得的amount被修改过,因为它永远是最新的</p>\n<h3>0、乐观锁：</h3>\n<p>不是真正的锁，而是一种实现 : 是一种实现的</p>\n<h3>1、乐观锁：</h3>\n<p>假设不会发生并发冲突，只有在提交操作时检查是否违反数据完整性，乐观锁不能解决脏读问题</p>\n<p>​            乐观锁大多都基于数据版本（version）记录机制实现，何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据表增加一个“version”字段来实现。读取出数据时，将此版本一同读出，之后更新时，对此版本后 +1。此时，将提交的版本数据与数据库表对应记录的当前版本信息对比时，如果提交的数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据。</p>\n<h3>2、优缺点：</h3>\n<p>​        优点 ：可以多个事务同时进行，然后根据返回的不同结果做相应的操作，避免了长事务中的数据库加锁开销。</p>\n<p>​        缺点 ：乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。</p>\n<p>在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。</p>\n<h3>3、步骤 :</h3>\n<pre><code class=\"language-java\">\t// 1.查询出商品信息\n\tselect (status,status,version) from t_goods where id=#{id}\n\t// 2.根据商品信息生成订单\n\t// 3.修改商品\n\tupdate t_goods\n\tset status=2,version=version+1 where id=#{id} and versio{139}};\n</code></pre>\n"},{"title":"数字签名","author":"郑天祺","date":"2019-09-01T12:23:00.000Z","_content":"# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","source":"_posts/数字签名.md","raw":"title: 数字签名\nauthor: 郑天祺\ntags:\n  - 可信\n  - 加密算法\ncategories:\n  - 可信\ndate: 2019-09-01 20:23:00\n---\n# 一、数字签名概念\n\n​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：**椭圆曲线数字签名算法**。。。\n\n# 二、数字签名的流程\n\n## （1）椭圆曲线数字签名算法:\n\n### 生成数字签名\n\n```java\n获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = <r, s>\n```\n\n### 验证数字签名\n\n```java\n根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n```\n\n### 验证方法解释\n\n```java\n由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n```\n\n### 原理解释：\n\nhttps://www.cnblogs.com/wsonepiece/p/3977021.html\n\n## （2）Schnorr数字签名算法\n\n### 生成数字签名\n\n```java\n计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = <r , s>\n```\n\n### 验证数字签名\n\n```java\n利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n```\n\n### 验证方法解释\n\n```java\n因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n```","slug":"数字签名","published":1,"updated":"2019-10-15T12:28:53.710Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3f004gx8vhubaqbpim","content":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2>（1）椭圆曲线数字签名算法:</h2>\n<h3>生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3>验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3>验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3>原理解释：</h3>\n<p>https://www.cnblogs.com/wsonepiece/p/3977021.html</p>\n<h2>（2）Schnorr数字签名算法</h2>\n<h3>生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3>验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3>验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、数字签名概念</h1>\n<p>​\t数字签名技术是消息传递进行加密获得的签名。如HTTP请求时将请求体加密。数字签名可以用于证实数字内容的完整性和来源。常见的数字签名算法：<strong>椭圆曲线数字签名算法</strong>。。。</p>\n<h1>二、数字签名的流程</h1>\n<h2>（1）椭圆曲线数字签名算法:</h2>\n<h3>生成数字签名</h3>\n<pre><code class=\"language-java\">获取消息m的数字摘要Hm 即 Hm = h(m);;\n使用RFC6979协议，通过私钥pk和m生成确定随机数k;\n计算R = k * G，其中R为曲线上的一点，取其横坐标r作为数字签名的一部分，然后计算s，即s = (Hm + r * pk) / k;\n得到消息m的数字签名为Sig = &lt;r, s&gt;\n</code></pre>\n<h3>验证数字签名</h3>\n<pre><code class=\"language-java\">根据Sig，使用对应的公钥P验证其签名;\n判断等式s * R = Hm * G + r * P是否成立，成立则通过验证\n</code></pre>\n<h3>验证方法解释</h3>\n<pre><code class=\"language-java\">由椭圆公式：r 得到 R ;\n因为：s = (Hm + r * pk) / k 得到 s * k = (Hm + r * pk);\n又因为：P = pk * G;\n所以：s * (k * G) = Hm * G + r * (pk * G) ;\n推出 s * R = Hm * G + r * P\n</code></pre>\n<h3>原理解释：</h3>\n<p>https://www.cnblogs.com/wsonepiece/p/3977021.html</p>\n<h2>（2）Schnorr数字签名算法</h2>\n<h3>生成数字签名</h3>\n<pre><code class=\"language-java\">计算消息m的数字摘要: Hm = H(m)\n生成确定性随机数k，计算 R = k * G , 取R的横坐标 r 作为签名的一部分\n计算签名另一部分：s = k + h(P || R || m) * pk\n得到数字签名 Sig = &lt;r , s&gt;\n</code></pre>\n<h3>验证数字签名</h3>\n<pre><code class=\"language-java\">利用公钥P验证其签名\ns * G = R + h(P || R || m) * P 是否成立，成立则通过验证\n多个签名：\n(s1 + .. + S50) * G = R1 + .. + R50 + h1 * P1 + .. h50 * P50\n</code></pre>\n<h3>验证方法解释</h3>\n<pre><code class=\"language-java\">因为：s = k + h(P || R || m) * pk ;\n又因为：P = pk * G ;\n所以：s * G = k * G + h(P || R || m) * (pk * G) \n所以：s * G = R + h * (P || R || m) * P\n由r 得到 R\n</code></pre>\n"},{"title":"排序之比较器Comparator<T>","author":"郑天祺","date":"2020-01-02T03:34:00.000Z","_content":"\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","source":"_posts/排序之比较器Comparator-T.md","raw":"title: 排序之比较器Comparator<T>\nauthor: 郑天祺\ntags:\n  - java\ncategories:\n  - java基础\ndate: 2020-01-02 11:34:00\n\n---\n\n# 一、Comparator和Comparable区别\n\n​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。\n\n​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。\n\n​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：\n\n# 二、Compatator用法\n\n```java\npublic static void main(String[] args) {\n        Student stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        List<Student> students = new ArrayList<>(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator<Student>() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n```\n\n# 三、拓展\n\nJDK1.8引入Lambda表达式：可以替换为：\n\n```java\n// 1 \nCollections.sort(students, (o1, o2) -> o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList<Student> studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n```\n\n","slug":"排序之比较器Comparator-T","published":1,"updated":"2020-01-02T04:00:12.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3h004jx8vhkeiowapw","content":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) {\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparator和Comparable区别</h1>\n<p>​\t\tComparator，又名比较器，是为了比较两个对象的大小而抽象出的一个接口，使用比较多。在java.util下。比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。</p>\n<p>​\t\tComparable，这个接口往往是可比较类实现的。在 java.lang 包下。Comparable接口对实现它的每个类的对象强加一个整体排序。 这个排序被称为类的自然排序。该接口有且只有一个方法int compareTo(T o)所以继承此接口需要实现该方法。compareTo返回值-1、0、1。  Collections.sort （和Arrays.sort ）可以自动对实现此接口的对象进行列表（和数组）排序。</p>\n<p>​\t\t上篇已经介绍Comparable的用法，此处只介绍Compatator：</p>\n<h1>二、Compatator用法</h1>\n<pre><code class=\"language-java\">public static void main(String[] args) {\n        Student stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        List&lt;Student&gt; students = new ArrayList&lt;&gt;(4);\n        students.add(stu1);\n        students.add(stu2);\n        students.add(stu3);\n        students.add(stu4);\n        Collections.sort(students, new Comparator&lt;Student&gt;() {\n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getAge() - o2.getAge();\n            }\n        });\n    }\n</code></pre>\n<h1>三、拓展</h1>\n<p>JDK1.8引入Lambda表达式：可以替换为：</p>\n<pre><code class=\"language-java\">// 1 \nCollections.sort(students, (o1, o2) -&gt; o1.getAge() - o2.getAge());\n// 若1为正常由小到大顺序，可以改成2的写法\nCollections.sort(students, Comparator.comparingInt(Student::getAge));\n\n// 也可以采用stream进行处理（分组，排序，求最大最小等sql几乎操作都可以）\nList&lt;Student&gt; studentStream = students.stream().sorted(Comparator.comparingInt(Student::getAge)).collect(Collectors.toList());\n</code></pre>\n"},{"title":"排序之比较器Comparable<T>","author":"郑天祺","date":"2020-01-02T02:27:00.000Z","_content":"\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","source":"_posts/排序之比较器.md","raw":"title: 排序之比较器Comparable<T>\nauthor: 郑天祺\ntags:\n  - java\n  - 数据结构\n  - ''\ncategories:\n  - java基础\ndate: 2020-01-02 10:27:00\n---\n\n# 一、Comparable<T>比较器的使用\n\n​\t\tJAVA中可以通过实现 Comparable<T>接口的方式让对象进行排序。使用方法：\n\n​\t\t\t1、实体继承Comparable<T>\n\n​\t\t\t2、实现compareTo方法，根据需求进行比较\n\n```java\npackage com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable<Student> {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age >= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + \",\" + this.age);\n    }\n}\n\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(\"zhangsan\", 10);\n        Student stu2 = new Student(\"zhangsan\", 21);\n        Student stu3 = new Student(\"zhangsan\", 19);\n        Student stu4 = new Student(\"zhangsan\", 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n```\n\n\n\n# 二、Comparable<T>比较器的原理\n\n​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：\n\n​\t\t（1）设置根节点\n\n​\t\t（2）新增节点，与根节点比较大小，\n\n​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）\n\n​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）\n\n```java\npackage com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree<T> {\n        class Node {\n            private Comparable<T> data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable<T> data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) < 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) >= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + \"\\t\");\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable<T> data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n```\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree<Integer> bt = new MyComparable.BinaryTree<>();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n```\n\n","slug":"排序之比较器","published":1,"updated":"2020-01-02T03:45:16.885Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3j004lx8vhbxotjm98","content":"<h1>一、Comparable<t>比较器的使用</t></h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<t>接口的方式让对象进行排序。使用方法：</t></p>\n<p>​\t\t\t1、实体继承Comparable<t></t></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    }\n}\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n</code></pre>\n<h1>二、Comparable<t>比较器的原理</t></h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree&lt;T&gt; {\n        class Node {\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) &lt; 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n</code></pre>\n<pre><code class=\"language-java\">public class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、Comparable<t>比较器的使用</t></h1>\n<p>​\t\tJAVA中可以通过实现 Comparable<t>接口的方式让对象进行排序。使用方法：</t></p>\n<p>​\t\t\t1、实体继承Comparable<t></t></p>\n<p>​\t\t\t2、实现compareTo方法，根据需求进行比较</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\npublic class Student implements Comparable&lt;Student&gt; {\n    private String name;\n    private int age;\n\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    @Override\n    public int compareTo(Student o) {\n        // 1表示大于，-1表示小于，0表示等于\n        return this.age &gt;= o.age ? 1 : -1;\n    }\n\n    public void print() {\n        System.out.println(this.name + &quot;,&quot; + this.age);\n    }\n}\n\n</code></pre>\n<pre><code class=\"language-java\">public class Test {\n    public static void main(String[] args) {\n\t\tStudent stu1 = new Student(&quot;zhangsan&quot;, 10);\n        Student stu2 = new Student(&quot;zhangsan&quot;, 21);\n        Student stu3 = new Student(&quot;zhangsan&quot;, 19);\n        Student stu4 = new Student(&quot;zhangsan&quot;, 26);\n\n        Student[] students = {stu1, stu2, stu3, stu4};\n\n        Arrays.sort(students);\n        for (Student stu : students) {\n            stu.print();\n        }\n    }\n}\n</code></pre>\n<h1>二、Comparable<t>比较器的原理</t></h1>\n<p>​\t\t为什么实现compareTo两个元素比较，不需要扫描全部，下一个元素插入的时候就把顺序排好了，它使用的是二叉树中序排序，下边是（网上最多介绍的）简单的处理方法：</p>\n<p>​\t\t（1）设置根节点</p>\n<p>​\t\t（2）新增节点，与根节点比较大小，</p>\n<p>​\t\t\t\t\t小则放到左子树（若左子树已经存在，则用此左子树进行递归调用）</p>\n<p>​\t\t\t\t\t大则放到右子树（若右子树已经存在，则用此右子树进行递归调用）</p>\n<pre><code class=\"language-java\">package com.bjut.fight.utils.comparable;\n\n/**\n * @author 郑天祺 on 2020/1/2 9:26\n */\npublic class MyComparable {\n\n    public static class BinaryTree&lt;T&gt; {\n        class Node {\n            private Comparable&lt;T&gt; data;\n            private Node left;\n            private Node right;\n\n            Node(Comparable&lt;T&gt; data) {\n                this.data = data;\n            }\n\n            void addNode(Node newNode) {\n                if (newNode.data.compareTo((T) this.data) &lt; 0) {\n                    if (this.left == null) {\n                        this.left = newNode;\n                    } else {\n                        this.left.addNode(newNode);\n                    }\n                }\n                if (newNode.data.compareTo((T) this.data) &gt;= 0) {\n                    if (this.right == null) {\n                        this.right = newNode;\n                    } else {\n                        this.right.addNode(newNode);\n                    }\n                }\n            }\n\n            void print() {\n                if (this.left != null) {\n                    left.print();\n                }\n                System.out.println(this.data + &quot;\\t&quot;);\n\n                if (this.right != null) {\n                    this.right.print();\n                }\n            }\n        }\n\n        private Node root;\n\n        public void add(Comparable&lt;T&gt; data) {\n            Node newNode = new Node(data);\n            if (root == null) {\n                root = newNode;\n            } else {\n                root.addNode(newNode);\n            }\n        }\n\n        public void print() {\n            this.root.print();\n        }\n    }\n}\n</code></pre>\n<pre><code class=\"language-java\">public class Test {\n    public static void main(String[] args) {\n        MyComparable.BinaryTree&lt;Integer&gt; bt = new MyComparable.BinaryTree&lt;&gt;();\n        bt.add(1);\n        bt.add(2);\n        bt.add(0);\n        bt.print();\n \t}\n}\n</code></pre>\n"},{"title":"文件上传之Content-Type","author":"郑天祺","date":"2019-08-31T08:15:00.000Z","_content":"\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","source":"_posts/文件上传.md","raw":"title: 文件上传之Content-Type\nauthor: 郑天祺\ntags:\n  - 文件上传\ncategories:\n  - java基础\ndate: 2019-08-31 16:15:00\n\n---\n\n## 1、Content-Type介绍\n\n**Content-Type**是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。\n\n在网络请求中，常见的Content-Type有：\n\n### \t1.1、常见的页面资源类型\n\n​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；\n\n​\t常见的页面提交或上传文件类型\t\n\n​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。\n\n### \t1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码\n\n（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）\n\n​\t\t\t其中：数据会变成key1=value1&key2=value2的形式；\n\n​\t\t\t\t\t\t有特殊字符需要utf-8；\n\n​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；\n\n​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。\n\n （2）multipart/form-data\n\n​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；\n\n​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）\n\n此处为form方式文件上传后端接收demo：\n\n```java\n@PostMapping(value=\"/publish\")\npublic void formUpload(@RequestParam(\"programImg\") CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n```\n​\t（3）application/json\n\n​\t\t\t和form类似，json可以比formData的数据结构更加复杂\n\n​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传\n\n此处为json方式文件上传后端接收demo：\n\n```java\n@PostMapping(value = \"/upload\")\npublic void jsonUpload(@RequestBody HashMap<String, String> requestMap) {\n    \n        String fileData = requestMap.get(\"fileData\");\n        String fileName = requestMap.get(\"fileName\");\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, \",\")[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n```\n\n​\t\t\n\n","slug":"文件上传","published":1,"updated":"2019-10-15T12:09:54.589Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3l004ox8vhta8nm71l","content":"<h2>1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3>1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3>1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) {\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n</code></pre>\n<p>​</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、Content-Type介绍</h2>\n<p><strong>Content-Type</strong>是指http/https发送信息至服务器时的内容编码类型，contentType用于表明发送数据流的类型，服务器根据编码类型使用特定的解析方式，获取数据流中的数据。</p>\n<p>在网络请求中，常见的Content-Type有：</p>\n<h3>1.1、常见的页面资源类型</h3>\n<p>​\ttext/html，text/plain，text/css，text/javascript，image/jpeg，image/png，image/gif等；</p>\n<p>​\t常见的页面提交或上传文件类型</p>\n<p>​\tapplication/x-www-form-urlencoded，multipart/form-data，application/json，application/xml等。</p>\n<h3>1.2、form表单可以定义enctype属性，该属性是发送到服务器之前应该如何对表单数据进行编码</h3>\n<p>（1）默认为application/x-www-form-urlencoded编码（包含POST和GET）</p>\n<p>​\t\t\t其中：数据会变成key1=value1&amp;key2=value2的形式；</p>\n<p>​\t\t\t\t\t\t有特殊字符需要utf-8；</p>\n<p>​\t\t\t\t\t\t请求类型为GET时，格式化后的字符串直接拼接到url的后面；</p>\n<p>​\t\t\t\t\t\t请求类型为POST时，格式化后的字符串会放在http body的Form Data中发送。</p>\n<p>（2）multipart/form-data</p>\n<p>​\t\t\t其中：使用表单上传文件时必须指定enctype属性值为multipart/form-data；</p>\n<p>​\t\t\t\t\t\t请求体被分割成多部分，每部分使用 --boundary分割（分成小部分？查其他资料）</p>\n<p>此处为form方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value=&quot;/publish&quot;)\npublic void formUpload(@RequestParam(&quot;programImg&quot;) CommonsMultipartFile file){\n\n\t\tString programImgName =  file.getOriginalFilename();\t\t\n        byte[] fileData =  file.getBytes();\n}\n</code></pre>\n<p>​\t（3）application/json</p>\n<p>​\t\t\t和form类似，json可以比formData的数据结构更加复杂</p>\n<p>​\t\t\t文件上传可以把文件编码成Base64，使用键值方式上传</p>\n<p>此处为json方式文件上传后端接收demo：</p>\n<pre><code class=\"language-java\">@PostMapping(value = &quot;/upload&quot;)\npublic void jsonUpload(@RequestBody HashMap&lt;String, String&gt; requestMap) {\n    \n        String fileData = requestMap.get(&quot;fileData&quot;);\n        String fileName = requestMap.get(&quot;fileName&quot;);\n        // 此处前端上传的Base64后端无法直接解开，因为它的串包含一个头，需要把头去掉。\n\t\tfileData = StringUtils.split(fileData, &quot;,&quot;)[1];\n        byte[] buffer = new BASE64Decoder().decodeBuffer(fileData);\n}\n</code></pre>\n<p>​</p>\n"},{"title":"基于JavaAgent的全链路监控（4）","author":"郑天祺","date":"2020-07-19T14:55:00.000Z","_content":"","source":"_posts/基于JavaAgent的全链路监控（4）.md","raw":"title: 基于JavaAgent的全链路监控（4）\nauthor: 郑天祺\ntags:\n\n  - javaagent\ncategories:\n  - java基础\ndate: 2020-07-19 22:55:00\n\n---\n","slug":"基于JavaAgent的全链路监控（4）","published":1,"updated":"2020-07-20T23:17:55.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3o004sx8vh21mi8gzb","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"线程相关的知识","author":"郑天祺","date":"2019-11-20T11:46:00.000Z","_content":"\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","source":"_posts/线程相关的知识.md","raw":"title: 线程相关的知识\nauthor: 郑天祺\ntags:\n  - 线程\ncategories:\n  - java基础\ndate: 2019-11-20 19:46:00\n\n---\n\n# 一、线程之间的通信机制\n\n \n\n在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。\n\n1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。\n\n2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。\n\nJava的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。\n\n简单例子：\n\n​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。\n\n\n\n# 二、JMM （JAVA 内存模型）\n\n \n\nJMM 的一个抽象概念，并不真实存在。\n\n​    在JAVA中：\n\n1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。\n\n2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。\n\nJMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。\n\n![img](/img/线程相关1.jpg)\n\n从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n\n1. 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。\n\n2. 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。\n\n![img](/img/线程相关3.jpg)\n\n# 三、重排序\n\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：\n\n1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n\n2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n\n3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。\n\n![img](/img/线程相关4.jpg)\n\n上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。\n\n**综上**，多个线程之间，执行的顺序是会随机改变的，需要我们注意。\n\n# 四、顺序一致性模型\n\n​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。\n\n# 五、总线事务\n\n1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。\n\n2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。\n\n3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。\n\n这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：\n\n在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。\n\n![img](/img/线程相关5.jpg)\n\n \n\n ","slug":"线程相关的知识","published":1,"updated":"2019-11-20T11:50:24.878Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3q004vx8vh15r4s2zr","content":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、线程之间的通信机制</h1>\n<p>在命令式编程中：线程之间的通信机制有两种：共享内存和消息传递。</p>\n<p>1）在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。</p>\n<p>2）在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信。</p>\n<p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。</p>\n<p>简单例子：</p>\n<p>​    全局变量A，方法B和C都对A进行操作，B和C就可以利用A进行通讯。</p>\n<h1>二、JMM （JAVA 内存模型）</h1>\n<p>JMM 的一个抽象概念，并不真实存在。</p>\n<p>​    在JAVA中：</p>\n<p>1）共享变量：所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。</p>\n<p>2）局部变量、方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p>\n<p>JMM决定一个线程和主内存的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B31.jpg\" alt=\"img\"></p>\n<p>从上图来看，线程 A与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：</p>\n<ol>\n<li>\n<p>首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。</p>\n</li>\n<li>\n<p>然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。</p>\n</li>\n</ol>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B33.jpg\" alt=\"img\"></p>\n<h1>三、重排序</h1>\n<p>在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型：</p>\n<p>1） 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</p>\n<p>2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</p>\n<p>3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B34.jpg\" alt=\"img\"></p>\n<p>上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。</p>\n<p><strong>综上</strong>，多个线程之间，执行的顺序是会随机改变的，需要我们注意。</p>\n<h1>四、顺序一致性模型</h1>\n<p>​    在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。</p>\n<h1>五、总线事务</h1>\n<p>1）顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。</p>\n<p>2）顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。</p>\n<p>3） JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。</p>\n<p>这个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I/O 设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制：</p>\n<p>在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java 语言规范鼓励但不强求 JVM 对 64 位的 long型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。</p>\n<p><img src=\"/img/%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B35.jpg\" alt=\"img\"></p>\n"},{"title":"池化之线程池","author":"郑天祺","date":"2019-09-01T02:14:00.000Z","_content":"\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n## 3、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n\n","source":"_posts/池化之线程池.md","raw":"title: 池化之线程池\nauthor: 郑天祺\ntags:\n  - java\n  - 多线程\ncategories:\n  - java基础\ndate: 2019-09-01 10:14:00\n\n---\n\njava中池化技术是提前保存大量的资源，以备不时之需以及重复使用。\n\n## 1、池化技术\n\nTips：不是深度学习中的卷积和赤化\n\n在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。\n\n进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。\n\n内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。\n\n## 2、线程池的创建\n\n```java\n// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue<Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n```\n\n## 3、线程池的关闭\n\n```java\nExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n```\n\n\n","slug":"池化之线程池","published":1,"updated":"2019-10-15T10:04:50.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3u004yx8vh0std4i7p","content":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2>1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2>2、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h2>3、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>java中池化技术是提前保存大量的资源，以备不时之需以及重复使用。</p>\n<h2>1、池化技术</h2>\n<p>Tips：不是深度学习中的卷积和赤化</p>\n<p>在实际应用当做，分配内存、创建进程、线程都会设计到一些系统调用，系统调用需要导致程序从用户态切换到内核态，是非常耗时的操作。因此，当程序中需要频繁的进行内存申请释放，进程、线程创建销毁等操作时，通常会使用内存池、进程池、线程池技术来提升程序的性能。</p>\n<p>进程池、线程池：先启动若干数量的线程，并让这些线程都处于睡眠状态，当需要一个开辟一个线程去做具体的工作时，就会唤醒线程池中的某一个睡眠线程，让它去做具体工作，当工作完成后，线程又处于睡眠状态，而不是将线程销毁。当线程数达到一定数量时，可以在队列中等待。</p>\n<p>内存池：内存池是指程序预先从操作系统申请一块足够大内存，此后，当程序中需要申请内存的时候，不是直接向操作系统申请，而是直接从内存池中获取；同理，当程序释放内存的时候，并不真正将内存返回给操作系统，而是返回内存池。当程序退出(或者特定时间)时，内存池才将之前申请的内存真正释放。</p>\n<h2>2、线程池的创建</h2>\n<pre><code class=\"language-java\">// 创建线程工厂实例\nThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build();\n// 创建线程池，核心线程数、最大线程数、空闲保持时间、队列长度、拒绝策略可自行定义\nExecutorService pool = new ThreadPoolExecutor(5, 20, 0L, TimeUnit.MILLISECONDS,\n            new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());\n</code></pre>\n<h2>3、线程池的关闭</h2>\n<pre><code class=\"language-java\">ExecutorService pool=...；\npool.shutdown();   //用于线程内无迭代，且预期在短时间内能执行完毕的线程任务；\npool.shutdownNow();//用于线程内有迭代逻辑，或执行完成时间无法预估的场景（此类线程任务代码必须进行中断信号的处理）；\n</code></pre>\n"},{"title":"用户行为特征提取","author":"郑天祺","date":"2019-12-23T03:10:00.000Z","_content":"\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","source":"_posts/特征提取-简单流程.md","raw":"title: 用户行为特征提取\nauthor: 郑天祺\ntags:\n\n  - HADOOP\ncategories:\n  - 大数据\ndate: 2019-12-23 11:10:00\n---\n\n\n\n# 一、场景\n\n玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。\n\n## （1）、时延\n\n​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。\n\n​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。\n\n## （2）、吞吐量\n\n​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。\n\n## （3）、应用：\n\n​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。\n\n# 二、整体流程\n\n![image-20191223112428348](/img/HDFS-liucheng.png)\n\n​\t\ta. 收集数据\n\n![image-20191223112725005](/img/data-collect.png)\n\n​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储\n\n​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）\n\n# 三、整体分析\n\n## （1）统计结果\n\n​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。\n\n​\t\t针对开发者：统计数据后的结果\n\n## （2）分析项目的目的\n\n​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。\n\n​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。\n\n​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。\n\n# 四、行为分析\n\n​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将**数据源**和**各类统计指标（KPI）**合理地关联起来。\n\n## （1）数据源 与 统计指标（KPI）分析\n\n​\t指标，这是很重要的；\n\n​\t合理的制定和可配置的制定可以更加方便后续工作。\n\n\n\n​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。\n\n![image-20191223114826757](/img/user-log.png)\n\n![image-20191223114900518](/img/user-behaviour.png)\n\n## \t（2）数据源 与 统计指标（KPI）的关系\n\n![image-20191223115309185](/img/dataSource-behaviour-relative.png)\n\n# 四、整体设计\n\n## （1）流程设计\n\n![image-20191223115738971](/img/data-collect-analysis.png)\n\n解释：\n\na. 数据量小，简单 使用脚本，反之用Flume等收集集群\n\nb. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模\n\nc. 实时计算可以用Flink、Spark、Storm\n\nd. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS\n\n## （2）统计指标设计\n\na. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数\n\n```java\n// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nb. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况\n\n```java\n// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n```\n\nc. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况\n\n```java\n// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n```\n\nd. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况\n\n```java\n// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n```\n\ne. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况\n\n```java\n// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n```\n\nf.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数\n\n```java\n// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n```\n\n注意：在编写Hive SQL进行指标统计进行去重\n\n小数量使用 COUNT DISTINCT\n\n数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！","slug":"特征提取-简单流程","published":1,"updated":"2019-12-23T09:58:46.583Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3x0051x8vhbqc735mu","content":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2>（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2>（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2>（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2>（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2>（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2>（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2>（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2>（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2>（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518</p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>一、场景</h1>\n<p>玩家每天游戏的各种操作（登录，充值等），这些行为都会记录到日志中，根据这些日志信息统计并分析用户行为。</p>\n<h2>（1）、时延</h2>\n<p>​\t\t由于 Hadoop MapReduce 底层设计因素，在进行计算的过程中，在 Map 阶段的处理结果会写入磁盘中，在 Reduce 阶段再去下载 Map 阶段处理完的结果，Reduce 计算完毕后的结果又会回写磁盘中。</p>\n<p>​\t\t这样反复操作磁盘，I/O 开销很大，所耗费的时间自然也就偏高。这就意味着，Hadoop MapReduce 计算模型适合处理 批处理任务，而对实时统计任务则不适合，如 股票交易系统，银行交易系统。</p>\n<h2>（2）、吞吐量</h2>\n<p>​\t\t在 Map 阶段中，被访问的数据是不能被修改的，直到整个作业 Job 完成。这就意味着，Hadoop MapReduce 是一个面向批处理的计算模型。</p>\n<h2>（3）、应用：</h2>\n<p>​\t\t适合离线计算，MapReduce 支持统计用户点击量（PV）、独立访问量（UV）及大数据及的信息检索等。</p>\n<h1>二、整体流程</h1>\n<p><img src=\"/img/HDFS-liucheng.png\" alt=\"image-20191223112428348\"></p>\n<p>​\t\ta. 收集数据</p>\n<p><img src=\"/img/data-collect.png\" alt=\"image-20191223112725005\"></p>\n<p>​\t\tb. 采用HDFS将收集的数据按照业务进行分类存储</p>\n<p>​\t\tc. 使用计算模型进行分析、计算（模型有 Spark 、 Hive 、 Pig、 Tez 、 Flink等）</p>\n<h1>三、整体分析</h1>\n<h2>（1）统计结果</h2>\n<p>​\t\t针对运营：了解用户对哪些业务感兴趣，需求量比较大，就可以重点投入。</p>\n<p>​\t\t针对开发者：统计数据后的结果</p>\n<h2>（2）分析项目的目的</h2>\n<p>​\t\ta、可以分析各个业务模块的活跃度、在各个模块停留的时间及用户的消费明细。</p>\n<p>​\t\tb、企业制定决策，需要实际数据作为支撑，用户行为结果能够帮助企业在某块业务进行决策时提供可靠的数据依据。</p>\n<p>​\t\tc、推送活动信息能不能造成反感。可以通过精准推送来提升用户的留存感，如用户在浏览某商品高，可推荐该商品的优惠活动。</p>\n<h1>四、行为分析</h1>\n<p>​\t\t从业务数据中有效的分析各类统计指标（KPI）和数据源，让读者能够将<strong>数据源</strong>和**各类统计指标（KPI）**合理地关联起来。</p>\n<h2>（1）数据源 与 统计指标（KPI）分析</h2>\n<p>​\t指标，这是很重要的；</p>\n<p>​\t合理的制定和可配置的制定可以更加方便后续工作。</p>\n<p>​\t每条日志记录通常表示：用户的一次行为记录。这些记录以 JSON 数据格式对操作行为进行封装。</p>\n<p><img src=\"/img/user-log.png\" alt=\"image-20191223114826757\"></p>\n<p><img src=\"/img/user-behaviour.png\" alt=\"image-20191223114900518\"></p>\n<h2>（2）数据源 与 统计指标（KPI）的关系</h2>\n<p><img src=\"/img/dataSource-behaviour-relative.png\" alt=\"image-20191223115309185\"></p>\n<h1>四、整体设计</h1>\n<h2>（1）流程设计</h2>\n<p><img src=\"/img/data-collect-analysis.png\" alt=\"image-20191223115738971\"></p>\n<p>解释：</p>\n<p>a. 数据量小，简单 使用脚本，反之用Flume等收集集群</p>\n<p>b. 原始数据不一定是有效数据，所以要数据清洗，然后在用Hive进行数据建模</p>\n<p>c. 实时计算可以用Flink、Spark、Storm</p>\n<p>d. 最后结果可以存储在Oracle、Mysql、HBase、或者HDFS</p>\n<h2>（2）统计指标设计</h2>\n<p>a. 用户一周内登陆总数：根据用户ID去重来统计一周内登陆总数</p>\n<pre><code class=\"language-java\">// 用户 ID 去重，全平台，全站点统计\n\nSELECT COUNT(DISTINCT ‘uid’) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>b. 用户一周中登陆分布情况，根据 IP 分组统计一周内的用户登录分布情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 IP 字段分组，全平台，全站点统计\n\nSELECT ‘ip’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid','ip';\n</code></pre>\n<p>c. 不同平台下一周用户的登录情况，根据平台分组统计一周内的用户登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 plat 字段分组，全站点统计\n\nSELECT 'plat', COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'palt';\n</code></pre>\n<p>d. 不同站点下一周用户的登录情况，根据不同站点统计一周内用户的登录情况</p>\n<pre><code class=\"language-java\">// 用户 ID 去重且根据 bpid 字段分组，全平台统计\n\nSELECT ‘bpid’, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP BY 'uid', 'plat';\n</code></pre>\n<p>e. 用户一周内 PC 端和移动端登录情况：根据 PC 字段和移动端字段值来统计一周内用户登录情况</p>\n<pre><code class=\"language-java\">// 使用CASE WHEN 条件语句统计多指标任务\n\nSELECT COUNT(CASE WHEN ‘ispc’ = 0 THEN 1 END), COUNT(CASE WHEN 'ismobile' = 1 THEN 1 END) FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29;\n</code></pre>\n<p>f.  用户一周内每天的登录总数：按照天分组来统计每天用户登录总数</p>\n<pre><code class=\"language-java\">// 按照分区时间分组，用户 ID 去重进行全平台、全站点统计\n\nSELECT tm, COUNT(DISTINCT 'uid') FROM ip_login WHERE tm BETWEEN 2019-12-23 AND 2019-12-29 GROUP 'uid', tm;\n\n</code></pre>\n<p>注意：在编写Hive SQL进行指标统计进行去重</p>\n<p>小数量使用 COUNT DISTINCT</p>\n<p>数据量大推荐使用 GROUP BY 去重，避免数据倾斜（？） 数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了'一个人累死,其他人闲死'的情况：https://blog.csdn.net/weixin_35353187/article/details/84303518</p>\n<p>此篇文章为《Hadoop大数据挖掘入门到放弃》笔记！</p>\n"},{"title":"自旋锁","author":"郑天祺","date":"2019-08-31T04:54:00.000Z","_content":"\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","source":"_posts/自旋锁.md","raw":"title: 自旋锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 12:54:00\n\n---\n\n# 自旋锁\n\n## 1、自旋锁概念（spinlock）\n\n是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。\n\n获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。\n\n## 2、自旋锁的优点 :\n\n自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）\n\n## 3、自旋锁应用 :\n\n由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。\n\n如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。\n\n \n\n## 4、简单自旋锁的实现 ：\n\n```java\npublic class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference<Thread> ref = new AtomicReference<>();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i < 100; i++) {\n             pool.submit(() -> {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n```\n\n \n\n## 5、可重入的自旋锁和不可重入的自旋锁 ：\n\n仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。\n\n由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁\n\n## 6、  另有三种常见的形式 :\n\nTicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n ","slug":"自旋锁","published":1,"updated":"2019-08-31T04:59:21.273Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp3z0054x8vhsahpdxhb","content":"<h1>自旋锁</h1>\n<h2>1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2>2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2>3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2>4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) {\n             pool.submit(() -&gt; {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n</code></pre>\n<h2>5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2>6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>自旋锁</h1>\n<h2>1、自旋锁概念（spinlock）</h2>\n<p>是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p>\n<p>获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。</p>\n<h2>2、自旋锁的优点 :</h2>\n<p>自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）</p>\n<h2>3、自旋锁应用 :</h2>\n<p>由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间。</p>\n<p>如果线程竞争不激烈，并且保持锁的时间段。适合使用自旋锁。</p>\n<h2>4、简单自旋锁的实现 ：</h2>\n<pre><code class=\"language-java\">public class SimpleSpinLock {\n     /**\n      * 持有锁的线程，null表示锁未被线程持有\n      */\n     private static AtomicReference&lt;Thread&gt; ref = new AtomicReference&lt;&gt;();\n\npublic void Lock() {\n         Thread currentThread = Thread.currentThread();\n         // 当ref为null的时候compareAndSet返回true，反之为false\n         // 通过循环不断的自旋判断锁是否被其他线程持有\n         while (!ref.compareAndSet(null, currentThread)) {\n         }\n     }\n\n   public void unLock() {\n        Thread currentThread = Thread.currentThread();\n         if (ref.get() != currentThread) {\n         }\n         ref.set(null);\n     }\n }\n\ntest：\n\npublic class SimpleSpinLockTest {\n\n    private static int n = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(100, 100, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(100);\n         SimpleSpinLock simpleSpinLock = new SimpleSpinLock();\n         for (int i = 0; i &lt; 100; i++) {\n             pool.submit(() -&gt; {\n                 simpleSpinLock.Lock();\n                 n++;\n                 simpleSpinLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(n);\n     }\n }\n</code></pre>\n<h2>5、可重入的自旋锁和不可重入的自旋锁 ：</h2>\n<p>仔细分析一下上述就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。</p>\n<p>由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数----》其他章节可重入锁</p>\n<h2>6、  另有三种常见的形式 :</h2>\n<p>TicketLock ，CLHlock 和 MCSlock：https://www.cnblogs.com/stevenczp/p/7136416.html</p>\n"},{"title":"读写锁","author":"郑天祺","date":"2019-08-31T05:08:00.000Z","_content":"\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","source":"_posts/读写锁.md","raw":"title: 读写锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:08:00\n\n---\n\n# 4、读写锁\n\n## 1、读写锁介绍：\n\n​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。 \n\n​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。 \n\n​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。\n\n​    \n\n​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。 \n\n连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/\n\n \n\n## 2、总结：\n\n1. **公平性选择**：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；\n2. **重入性**：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；\n3. **锁降级**：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁\n\n## 3、写锁的获取：\n\n​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：\n\n```java\nprotected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) > MAX_COUNT)\n             throw new Error(\"Maximum lock count exceeded\");\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c & EXCLUSIVE_MASK;\n\n }\n```\n\n其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。\n\n而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？\n\n根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数\n\n```java\nstatic int sharedCount(int c)    { \n\n        return c >>> SHARED_SHIFT; \n\n}\n```\n\n该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。\n\n![img](/img/读写锁.png)\n\n当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。\n\n \n\n \n\n## 4、写锁的释放：\n\n​    写锁释放通过重写AQS的tryRelease方法，源码为：\n\n```java\nprotected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n```\n\n​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。\n\n \n\n## 5、读锁的获取\n\n​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。\n\n```java\nprotected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &&\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &&\n         r < MAX_COUNT &&\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n```\n\n​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。\n\n## 6、读锁的释放\n\n```java\nprotected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount > 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count <= 1) {\n             readHolds.remove();\n             if (count <= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n```\n\n\n\n##  7、锁降级\n\n​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：\n\n```java\nvoid processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n```\n\n ","slug":"读写锁","published":1,"updated":"2019-10-15T10:05:19.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp430057x8vhmtlfqnlq","content":"<h1>4、读写锁</h1>\n<h2>1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。</p>\n<p>连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</p>\n<h2>2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2>3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c &amp; EXCLUSIVE_MASK;\n\n }\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    { \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n}\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2>4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2>5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2>6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) {\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n</code></pre>\n<h2>7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>4、读写锁</h1>\n<h2>1、读写锁介绍：</h2>\n<p>​        ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。</p>\n<p>​        理论上，读写锁比互斥锁允许对于共享数据更大程度的并发。与互斥锁相比，读写锁是否能够提高性能取决于读写数据的频率、读取和写入操作的持续时间、以及读线程和写线程之间的竞争。</p>\n<p>​        一些业务场景中，大部分 只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。 针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。</p>\n<p>​</p>\n<p>​        读-读能共存，\n​         读-写不能共存，\n​         写-写不能共存。</p>\n<p>连接：https://blog.csdn.net/j080624/article/details/82790372、https://ifeve.com/read-write-locks/</p>\n<h2>2、总结：</h2>\n<ol>\n<li><strong>公平性选择</strong>：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；</li>\n<li><strong>重入性</strong>：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；</li>\n<li><strong>锁降级</strong>：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁</li>\n</ol>\n<h2>3、写锁的获取：</h2>\n<p>​        写锁是独占式锁，而实现写锁的同步语义是通过重写 AQS 中的 tryAcquire() 方法实现的，源码：</p>\n<pre><code class=\"language-java\">protected final boolean tryAcquire(int acquires) {\n     Thread current = Thread.currentThread();\n     // 1. 获取 写锁 当前的同步状态\n     int c = getState();\n     // 2. 获取 写锁 获取的次数\n     int w = exclusiveCount(c);\n     if (c != 0) {\n         // (Note: if c != 0 and w == 0 then shared count != 0)\n         // 3.1 当 读锁 已被读线程获取 或者 当前线程不是已经获取 写锁 的线程的话\n         // 当前线程获取 写锁失败\n         if (w == 0 || current != getExclusiveOwnerThread())\n             return false;\n         if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n             throw new Error(&quot;Maximum lock count exceeded&quot;);\n         // Reentrant acquire\n         // 3.2 当前线程 获取写锁，支持可重复加锁\n         setState(c + acquires);\n         return true;\n     }\n     // 3.3 写锁 未被任何线程获取，当前线程可获取 写锁\n     if (writerShouldBlock() ||!compareAndSetState(c, c + acquires))\n         return false;\n     setExclusiveOwnerThread(current);\n     return true;\n }\n\n \n\n static int exclusiveCount(int c) { \n\n        return c &amp; EXCLUSIVE_MASK;\n\n }\n</code></pre>\n<p>其中EXCLUSIVE_MASK为:  static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;      EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。</p>\n<p>而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？</p>\n<p>根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数</p>\n<pre><code class=\"language-java\">static int sharedCount(int c)    { \n\n        return c &gt;&gt;&gt; SHARED_SHIFT; \n\n}\n</code></pre>\n<p>该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。</p>\n<p><img src=\"/img/%E8%AF%BB%E5%86%99%E9%94%81.png\" alt=\"img\"></p>\n<p>当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。</p>\n<h2>4、写锁的释放：</h2>\n<p>​    写锁释放通过重写AQS的tryRelease方法，源码为：</p>\n<pre><code class=\"language-java\">protected final boolean tryRelease(int releases) {\n     if (!isHeldExclusively())\n         throw new IllegalMonitorStateException();\n     //1. 同步状态减去写状态\n     int nextc = getState() - releases;\n     //2. 当前写状态是否为0，为0则释放写锁\n     boolean free = exclusiveCount(nextc) == 0;\n     if (free)\n         setExclusiveOwnerThread(null);\n     //3. 不为0则更新同步状态\n     setState(nextc);\n     return free;\n }\n</code></pre>\n<p>​    减少写状态int nextc = getState() - releases，只需要用当前同步状态直接减去写状态的原因：写状态是由同步状态的低16位表示的。</p>\n<h2>5、读锁的获取</h2>\n<p>​        读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。</p>\n<pre><code class=\"language-java\">protected final int tryAcquireShared(int unused) {\n     Thread current = Thread.currentThread();\n     int c = getState();\n     //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前\n     // 线程获取读锁失败返回-1\n     if (exclusiveCount(c) != 0 &amp;&amp;\n         getExclusiveOwnerThread() != current)\n         return -1;\n     int r = sharedCount(c);\n     if (!readerShouldBlock() &amp;&amp;\n         r &lt; MAX_COUNT &amp;&amp;\n         //2. 当前线程获取读锁\n         compareAndSetState(c, c + SHARED_UNIT)) {\n         //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法\n         //返回当前获取读锁的次数\n         if (r == 0) {\n             firstReader = current;\n             firstReaderHoldCount = 1;\n         } else if (firstReader == current) {\n             firstReaderHoldCount++;\n         } else {\n             HoldCounter rh = cachedHoldCounter;\n             if (rh == null || rh.tid != getThreadId(current))\n                 cachedHoldCounter = rh = readHolds.get();\n             else if (rh.count == 0)\n                 readHolds.set(rh);\n             rh.count++;\n         }\n         return 1;\n     }\n     //4. 处理在第二步中CAS操作失败的自旋已经实现重入性\n     return fullTryAcquireShared(current);\n }\n</code></pre>\n<p>​    当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。</p>\n<h2>6、读锁的释放</h2>\n<pre><code class=\"language-java\">protected final boolean tryReleaseShared(int unused) {\n     Thread current = Thread.currentThread();\n     // 前面还是为了实现getReadHoldCount等新功能\n     if (firstReader == current) {\n         // assert firstReaderHoldCount &gt; 0;\n         if (firstReaderHoldCount == 1)\n             firstReader = null;\n         else\n             firstReaderHoldCount--;\n     } else {\n         HoldCounter rh = cachedHoldCounter;\n         if (rh == null || rh.tid != getThreadId(current))\n             rh = readHolds.get();\n         int count = rh.count;\n         if (count &lt;= 1) {\n             readHolds.remove();\n             if (count &lt;= 0)\n                 throw unmatchedUnlockException();\n         }\n         --rh.count;\n     }     for (;;) {\n         int c = getState();\n         // 读锁释放 将同步状态减去读状态即可\n         int nextc = c - SHARED_UNIT;\n         if (compareAndSetState(c, nextc))\n             // Releasing the read lock has no effect on readers,\n             // but it may allow waiting writers to proceed if\n             // both read and write locks are now free.\n             return nextc == 0;\n     }\n }\n</code></pre>\n<h2>7、锁降级</h2>\n<p>​        读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：</p>\n<pre><code class=\"language-java\">void processCachedData() {\n         rwl.readLock().lock();\n         if (!cacheValid) {\n             // Must release read lock before acquiring write lock\n             rwl.readLock().unlock();\n             rwl.writeLock().lock();\n             try {\n                 // Recheck state because another thread might have\n                 // acquired write lock and changed state before we did.\n                 if (!cacheValid) {\n                     data = ...\n             cacheValid = true;\n           }\n           // Downgrade by acquiring read lock before releasing write lock\n           rwl.readLock().lock();\n         } finally {\n           rwl.writeLock().unlock(); // Unlock write, still hold read\n         }\n       }\n       try {\n         use(data);\n       } finally {\n         rwl.readLock().unlock();\n       }\n     }\n }\n</code></pre>\n"},{"title":"重放攻击","author":"郑天祺","date":"2019-09-04T11:54:00.000Z","_content":"\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","source":"_posts/重放攻击.md","raw":"title: 重放攻击\nauthor: 郑天祺\ntags:\n  - 网络安全\n  - 可信\ncategories:\n  - 可信\ndate: 2019-09-04 19:54:00\n\n---\n\n# 1、概念\n\n​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。\n\n​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。\n\n​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。 \n\n（来自百度百科）\n\n# 2、来源\n\n一个存在安全漏洞的登录系统：\n\n1. 前端web页面用户输入账号、密码，点击登录。\n\n2. 请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。\n\n3. 提交账号、md5之后的密码\n\n4. 请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。\n\n解析：\n\n​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。\n\n​\t这样的话md5加密就起不到任何作用了。\n\nSo：\n\n​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。\n\nMore：\n\n​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。\n\n如：MD5(固定盐值+密码)\n\nMore and More：\n\n加时间戳和流水号；\n\n一应答机制和一次性口令机制（应用广泛）\n\n# 3、分类\n\n重放攻击可以是登录认证，也可以是其他方式，\n\n从用户端考虑或从服务端考虑也会不同，\n\n当然会会有不同的分类。\n\n任性不提。。。\n\n# 4、一次应答机制\n\n​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。\n\n​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。","slug":"重放攻击","published":1,"updated":"2019-09-04T13:24:43.315Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp45005bx8vhu3wtauco","content":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>1、概念</h1>\n<p>​\t重放攻击(Replay Attacks)又称重播攻击、回放攻击，是指攻击者发送一个目的主机已接收过的包，来达到欺骗系统的目的，主要用于身份认证过程，破坏认证的正确性。</p>\n<p>​\t重放攻击可以由发起者，也可以由拦截并重发该数据的敌方进行。攻击者利用网络监听或者其他方式盗取认证凭据，之后再把它重新发给认证服务器。</p>\n<p>​\t重放攻击在任何网络通过程中都可能发生，是计算机世界黑客常用的攻击方式之一。</p>\n<p>（来自百度百科）</p>\n<h1>2、来源</h1>\n<p>一个存在安全漏洞的登录系统：</p>\n<ol>\n<li>\n<p>前端web页面用户输入账号、密码，点击登录。</p>\n</li>\n<li>\n<p>请求提交之前，web端首先通过客户端脚本如javascript对密码原文进行md5加密。</p>\n</li>\n<li>\n<p>提交账号、md5之后的密码</p>\n</li>\n<li>\n<p>请求提交至后端，验证账号与密码是否与数据库中的一致，一致则认为登录成功，反之失败。</p>\n</li>\n</ol>\n<p>解析：</p>\n<p>​\t目前的腾讯电脑管家，360等软件，会将你的网络请求原封不动的发送到他们的后端保存一份、当然不止这些安全软件，其他软件也可以做到。这样就会将你的账户就很容易被别人使用。</p>\n<p>​\t这样的话md5加密就起不到任何作用了。</p>\n<p>So：</p>\n<p>​\t我们考虑加入盐值，登录时候，session(或者redis缓存)中存一份随机数（称为盐值）。同样的盐值页面中也存在一份。所以我们不仅仅考虑用户名和md5密码了，还需要一份盐值作为网络请求的身份参照物，这样做稍微安全一些。</p>\n<p>More：</p>\n<p>​\t存在简单md5暴力破解的时候，我们需要增强密码强度。但是用户不喜欢这样，就需要我们自己加盐值。</p>\n<p>如：MD5(固定盐值+密码)</p>\n<p>More and More：</p>\n<p>加时间戳和流水号；</p>\n<p>一应答机制和一次性口令机制（应用广泛）</p>\n<h1>3、分类</h1>\n<p>重放攻击可以是登录认证，也可以是其他方式，</p>\n<p>从用户端考虑或从服务端考虑也会不同，</p>\n<p>当然会会有不同的分类。</p>\n<p>任性不提。。。</p>\n<h1>4、一次应答机制</h1>\n<p>​\t用验证码代替时间戳，将密码通过md5算法加密，再将验证码加在后面，然后再用md5算法加密，在网络传输过程中以密文的形式传输到后台管理。</p>\n<p>​\t后台数据库保存的是用md5算法加密的密码，将该密文加上保存在session(或redis)失效范围内的验证码用md5算法加密，得到的密文与请求中的口令对比，如配对，则验证成功，否则，验证失败。</p>\n"},{"title":"理解IO阻塞与非阻塞","author":"郑天祺","date":"2019-08-30T09:34:00.000Z","_content":"\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n这个模式有二个缺点\n1. 由于他能够同时监听多个文件描述符，假如说有1000个，这个时候如果其中一个fd 处于就绪状态了，那么当前进程需要线性轮询所有的fd，也就是监听的fd越多，性能开销越大。\n2. 同时，select在单个进程中能打开的fd是有限制的，默认是1024，对于那些需要支持单机上万的TCP连接来说确实有点少。\n\n## 8、多路复用的好处\n\nI/O多路复用可以通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销","source":"_posts/理解IO阻塞与非阻塞.md","raw":"title: 理解IO阻塞与非阻塞\nauthor: 郑天祺\ntags:\n  - IO\n  - 阻塞与非阻塞\ncategories:\n  - 网络\ndate: 2019-08-30 17:34:00\n\n---\n\n## 1、饭店吃饭的例子\n\nA君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】\n\nB君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】  \n\n## 2、阻塞与非阻塞调用对比\n\n![](/img/阻塞与非阻塞调用对比.png)\n\n## 3、阻塞IO\n\n![](/img/阻塞IO.png)\n\n## 4、非阻塞IO\n\n![](/img/非阻塞IO.png)\n\n## 5、I/O复用模型\n\n​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。\n\n​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式\n\n## 6、I/O复用select模型\n\n![](/img/IO复用select模型.png)\n\n## 7、select、epoll模型对比\n\n![](/img/select、epoll模型对比.png)\n\n这个模式有二个缺点\n1. 由于他能够同时监听多个文件描述符，假如说有1000个，这个时候如果其中一个fd 处于就绪状态了，那么当前进程需要线性轮询所有的fd，也就是监听的fd越多，性能开销越大。\n2. 同时，select在单个进程中能打开的fd是有限制的，默认是1024，对于那些需要支持单机上万的TCP连接来说确实有点少。\n\n## 8、多路复用的好处\n\nI/O多路复用可以通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销","slug":"理解IO阻塞与非阻塞","published":1,"updated":"2019-08-30T10:01:49.767Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp47005ex8vhigb3w15w","content":"<h2>1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】</p>\n<h2>2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt></p>\n<h2>3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt></p>\n<h2>4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt></p>\n<h2>5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2>6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<h2>7、select、epoll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt></p>\n<p>这个模式有二个缺点</p>\n<ol>\n<li>由于他能够同时监听多个文件描述符，假如说有1000个，这个时候如果其中一个fd 处于就绪状态了，那么当前进程需要线性轮询所有的fd，也就是监听的fd越多，性能开销越大。</li>\n<li>同时，select在单个进程中能打开的fd是有限制的，默认是1024，对于那些需要支持单机上万的TCP连接来说确实有点少。</li>\n</ol>\n<h2>8、多路复用的好处</h2>\n<p>I/O多路复用可以通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、饭店吃饭的例子</h2>\n<p>A君喜欢下馆子吃饭，服务员点完餐后，A君一直坐在座位上等待厨师炒菜，什么事情也没有干，过了一会服务员端上饭菜后，A君就开吃了 -- 【阻塞I/O】</p>\n<p>B君也喜欢下馆子，服务员点完餐后，B君看这个服务员长得不错便前去搭讪，一直和服务员聊人生理想，并时不时的打听自己的饭做好了没有，过了一会饭也做好了，B君也撩到了美女服务员的微信号 -- 【非阻塞I/O 】</p>\n<h2>2、阻塞与非阻塞调用对比</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E8%B0%83%E7%94%A8%E5%AF%B9%E6%AF%94.png\" alt></p>\n<h2>3、阻塞IO</h2>\n<p><img src=\"/img/%E9%98%BB%E5%A1%9EIO.png\" alt></p>\n<h2>4、非阻塞IO</h2>\n<p><img src=\"/img/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png\" alt></p>\n<h2>5、I/O复用模型</h2>\n<p>​\t前面讲的非阻塞仍然需要进程不断的轮询重试。能不能实现当数据可读了以后给程序一个通知呢？所以这里引入了一个IO多路复用模型，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。</p>\n<p>​\t常见的IO多路复用方式有【select、poll、epoll】，都是Linux  API提供的IO复用方式</p>\n<h2>6、I/O复用select模型</h2>\n<p><img src=\"/img/IO%E5%A4%8D%E7%94%A8select%E6%A8%A1%E5%9E%8B.png\" alt></p>\n<h2>7、select、epoll模型对比</h2>\n<p><img src=\"/img/select%E3%80%81epoll%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.png\" alt></p>\n<p>这个模式有二个缺点</p>\n<ol>\n<li>由于他能够同时监听多个文件描述符，假如说有1000个，这个时候如果其中一个fd 处于就绪状态了，那么当前进程需要线性轮询所有的fd，也就是监听的fd越多，性能开销越大。</li>\n<li>同时，select在单个进程中能打开的fd是有限制的，默认是1024，对于那些需要支持单机上万的TCP连接来说确实有点少。</li>\n</ol>\n<h2>8、多路复用的好处</h2>\n<p>I/O多路复用可以通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。它的最大优势是系统开销小，并且不需要创建新的进程或者线程，降低了系统的资源开销</p>\n"},{"title":"责任链模式","author":"郑天祺","date":"2020-01-03T07:21:00.000Z","_content":"\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","source":"_posts/责任链模式.md","raw":"title: 责任链模式\nauthor: 郑天祺\ntags:\n\n  - 设计模式\ncategories:\n  - 设计模式\ndate: 2020-01-03 15:21:00\n\n---\n\n最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得\n\n# 一、引言\n\n​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。\n\n​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。\n\n​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。\n\n​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。\n\n​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。\n\n# 二、纯与不纯的责任链模式\n\n​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。\n\n​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。\n\n# 三、什么情况下使用责任链\n\n（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？\n\n（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。\n\n（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。\n\n（4）当处理一个请求的处理者对象集合需要动态地指定时。？？\n\n​\t光看概念不好理解\n\n四、责任链模式的长处\n\n灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理\n\n低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。\n\n五、责任链的实践\n\n​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”\n\n\n\n笔者其他常见的设计模式：\n\n建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984\n\n单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215\n\n观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205\n\n发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383","slug":"责任链模式","published":1,"updated":"2020-01-03T07:58:18.113Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp4a005hx8vh1ykd5x9h","content":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984</p>\n<p>单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215</p>\n<p>观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205</p>\n<p>发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近一直听大佬说责任链模式，决定看看到底是什么。本文由翻阅《大话设计模式》得</p>\n<h1>一、引言</h1>\n<p>​\t\t击鼓传花游戏，也称传彩球。中国民间游戏，流行于中国各地。数人、十数人或数十人围成一个圆圈席地而坐，另外一个人背对着人圈以槌击鼓。鼓响时，开始传花，花由一个人的手里传。</p>\n<p>​\t\t有时候，花束就开始依次传递，鼓声一落，假如花束在某人手中，则该人就得饮酒（多是唱歌、跳舞、说笑话；或回答问题、猜谜、按纸条规定行事等）。</p>\n<p>​\t\t击鼓传花便是责任链模式的应用。在责任链模式里，很多的对象由每一个对象对其下家的引用而联接起来形成一条链。</p>\n<p>​\t\t请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织链和分配责任。</p>\n<p>​\t\t在这个游戏中，参与游戏的人士具体处理者的对象，击鼓的人士客户端的对象。花代表请求。每个参加游戏的人有两个行为：（1）将花传下去（2）喝酒。击鼓的人不知道最终是哪个人执行了喝酒，但必然是做游戏的人们中的一个。</p>\n<h1>二、纯与不纯的责任链模式</h1>\n<p>​\t\t一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，二是把责任推给下家。不答应出现某一个具体处理者对象在承担了一部分责任后又把责任向下传的情况。</p>\n<p>​\t\t但是在实际的系统里，纯的责任链很难找到；假如坚持责任链不纯便不是责任链模式，那么责任链模式便不会有太大的意义了。</p>\n<h1>三、什么情况下使用责任链</h1>\n<p>（1）系统已经有一个由处理者对象组成的链。这个链可能由复合模式给出。？？</p>\n<p>（2）当有多于一个的处理者对象会处理一个请求，而且在事先并不知道到底由哪一个处理者对象处理一个请求。这个处理者对象是动态确定的。</p>\n<p>（3）当系统想发出一个请求给多个处理者对象中的某一个，但是不明显指定是哪一个处理者对象会处理此请求。</p>\n<p>（4）当处理一个请求的处理者对象集合需要动态地指定时。？？</p>\n<p>​\t光看概念不好理解</p>\n<p>四、责任链模式的长处</p>\n<p>灵活性：允许传给链结构的起点，但不知道最终在哪个节点上处理</p>\n<p>低耦合：发出请求与处理请求的对象之间耦合度降低，允许多个处理着处理最终处理这个命令。</p>\n<p>五、责任链的实践</p>\n<p>​\t\t一个链可以是一条线，一个树，也可以是一个环。链的拓扑结构可以是单连通的或多连通的，责任链模式并不指定责任链的拓扑结构。但是责任链模式要求在同一个时间里，命令只可以被传给一个下家（或被处理掉）；而不可以传给多于一个下家。”</p>\n<p>笔者其他常见的设计模式：</p>\n<p>建造起模式：https://blog.csdn.net/qq_23034755/article/details/90487984</p>\n<p>单例模式：https://blog.csdn.net/qq_23034755/article/details/90547215</p>\n<p>观察者模式：https://blog.csdn.net/qq_23034755/article/details/90705205</p>\n<p>发布订阅模式：https://blog.csdn.net/qq_23034755/article/details/91340383</p>\n"},{"title":"锁粗化","author":"郑天祺","date":"2019-08-31T05:32:00.000Z","_content":"\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","source":"_posts/锁粗化.md","raw":"title: 锁粗化\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:32:00\n---\n\n转自：https://blog.csdn.net/qq_26222859/article/details/80546917\n\n参考：https://www.jianshu.com/p/f05423a21e78\n\n通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。\n\n```java\npublic void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n```\n\n上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :\n\n```java\npublic void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n```\n\n 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。\n\n另一种需要锁粗化的极端的情况是：\n\n```java\nfor(int i=0;i<size;i++){\n     synchronized(lock){\n     }\n }\n```\n\n 上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。\n\n锁粗化后的代码如下：\n\n```java\nsynchronized(lock){\n     for(int i=0;i<size;i++){\n     }\n }\n```\n\n","slug":"锁粗化","published":1,"updated":"2019-10-15T12:35:31.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp4d005kx8vh1xvf2fvq","content":"<p>转自：https://blog.csdn.net/qq_26222859/article/details/80546917</p>\n<p>参考：https://www.jianshu.com/p/f05423a21e78</p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++){\n     synchronized(lock){\n     }\n }\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock){\n     for(int i=0;i&lt;size;i++){\n     }\n }\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>转自：https://blog.csdn.net/qq_26222859/article/details/80546917</p>\n<p>参考：https://www.jianshu.com/p/f05423a21e78</p>\n<p>通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的讲求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。</p>\n<pre><code class=\"language-java\">public void doSomethingMethod(){\n     synchronized(lock){\n         //do some thing\n     }\n     //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕\n     synchronized(lock){\n         //do other thing\n     }\n }\n</code></pre>\n<p>上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下 :</p>\n<pre><code class=\"language-java\">public void doSomethingMethod(){\n     //进行锁粗化：整合成一次锁请求、同步、释放\n     synchronized(lock){\n         //do some thing\n         //做其它不需要同步但能很快执行完的工作\n         //do other thing\n     }\n }\n</code></pre>\n<p>注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。</p>\n<p>另一种需要锁粗化的极端的情况是：</p>\n<pre><code class=\"language-java\">for(int i=0;i&lt;size;i++){\n     synchronized(lock){\n     }\n }\n</code></pre>\n<p>上面代码每次循环都会进行锁的请求、同步与释放，看起来貌似没什么问题，且在jdk内部会对这类代码锁的请求做一些优化，但是还不如把加锁代码写在循环体的外面，这样一次锁的请求就可以达到我们的要求，除非有特殊的需要：循环需要花很长时间，但其它线程等不起，要给它们执行的机会。</p>\n<p>锁粗化后的代码如下：</p>\n<pre><code class=\"language-java\">synchronized(lock){\n     for(int i=0;i&lt;size;i++){\n     }\n }\n</code></pre>\n"},{"title":"轻量级锁","author":"郑天祺","date":"2019-08-31T07:08:00.000Z","_content":"\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","source":"_posts/轻量级锁.md","raw":"title: 轻量级锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 15:08:00\n\n---\n\n## 1、轻量级锁\n\n锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。\n\n​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：\n\n​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。\n\n## 2、锁消除\n\n由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：\n\n​    a) 在一个安全点停止拥有锁的线程。\n\n​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。\n\n​    c) 唤醒当前线程，将当前锁升级成轻量级锁。\n\n 所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭\n\n## 3、锁膨胀\n\n当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀","slug":"轻量级锁","published":1,"updated":"2019-08-31T07:10:30.223Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp4g005nx8vhjrt4khoy","content":"<h2>1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2>2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2>3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n","site":{"data":{}},"excerpt":"","more":"<h2>1、轻量级锁</h2>\n<p>锁撤销升级为轻量级锁之后，那么对象的Markword也会进行相应的的变化。</p>\n<p>​    下面先简单描述下锁撤销之后，升级为轻量级锁的过程：</p>\n<p>​    a) 线程在自己的栈桢中创建锁记录 LockRecord。\n​     b) 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。\n​     c) 将锁记录中的Owner指针指向锁对象。\n​     d) 将锁对象的对象头的MarkWord替换为指向锁记录的指针。</p>\n<h2>2、锁消除</h2>\n<p>由于偏向锁失效了，那么接下来就得把该锁撤销，锁撤销的开销花费还是挺大的，其大概的过程如下：</p>\n<p>​    a) 在一个安全点停止拥有锁的线程。</p>\n<p>​    b) 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Markword，使其变成无锁状态。</p>\n<p>​    c) 唤醒当前线程，将当前锁升级成轻量级锁。</p>\n<p>所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭</p>\n<h2>3、锁膨胀</h2>\n<p>当出现有两个线程来竞争锁的话，那么偏向锁就失效了，此时锁就会膨胀，升级为轻量级锁。这也是我们经常所说的锁膨胀</p>\n"},{"title":"阻塞锁","author":"郑天祺","date":"2019-08-31T05:00:00.000Z","_content":"\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","source":"_posts/阻塞锁.md","raw":"title: 阻塞锁\nauthor: 郑天祺\ntags:\n  - 锁\ncategories:\n  - java基础\ndate: 2019-08-31 13:00:00\n\n---\n\n# 阻塞锁\n\n## 1、阻塞锁优势\n\n​\t与自旋锁不同，改变了线程的运行状态。\n\n​    在JAVA环境中，线程Thread有如下几个状态：\n\n1. 新建状态\n2. 就绪状态\n3. 运行状态\n4. 阻塞状态\n5. 死亡状态\n\n​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart() \n\n## 2、阻塞锁的优势：\n\n​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。\n\n## 3、阻塞锁应用：\n\n​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。\n\n## 4、阻塞锁的简单实现：\n\n```java\n public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal<ClhNode> LOCAL = new ThreadLocal<>();\n     private static final AtomicReferenceFieldUpdater<ClhLock, ClhNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, \"tail\");\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=>大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(\"unlock\\t\" + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n```\n\n### 5、demo：\n\n```java\npublic class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), new DefaultNameThreadFactory(\"SimpleSpinLock\"));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i < 1000; i++) {\n             pool.submit(() -> {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n```\n\n","slug":"阻塞锁","published":1,"updated":"2019-08-31T05:04:33.756Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcvapp4h005px8vhstjprxpx","content":"<h1>阻塞锁</h1>\n<h2>1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2>2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2>3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2>4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n</code></pre>\n<h3>5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) {\n             pool.submit(() -&gt; {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1>阻塞锁</h1>\n<h2>1、阻塞锁优势</h2>\n<p>​\t与自旋锁不同，改变了线程的运行状态。</p>\n<p>​    在JAVA环境中，线程Thread有如下几个状态：</p>\n<ol>\n<li>新建状态</li>\n<li>就绪状态</li>\n<li>运行状态</li>\n<li>阻塞状态</li>\n<li>死亡状态</li>\n</ol>\n<p>​      阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。\n​       JAVA中，能够进入 / 退出、阻塞状态或包含阻塞锁的方法有 ，synchronized 关键字（其中的重量锁），ReentrantLock，Object.wait() / notify() ，LockSupport.park() / unpart()</p>\n<h2>2、阻塞锁的优势：</h2>\n<p>​\t在于，阻塞的线程不会占用CPU时间， 不会导致 CPU占用率过高，但进入时间以及恢复时间都要比自旋锁略慢。在竞争激烈的情况下 阻塞锁的性能要明显高于自旋锁。</p>\n<h2>3、阻塞锁应用：</h2>\n<p>​\t理想的情况则是， 在线程竞争不激烈的情况下，使用自旋锁；竞争激烈的情况下使用，阻塞锁。</p>\n<h2>4、阻塞锁的简单实现：</h2>\n<pre><code class=\"language-java\"> public class ClhLock {\n     /**\n      * 定义一个节点，默认的lock状态为true\n      */\n     public static class ClhNode {\n         private volatile Thread isLocked;\n     }\n\n    /**\n      * 尾部节点,只用一个节点即可\n      */\n     private volatile ClhNode tail;\n     private static final ThreadLocal&lt;ClhNode&gt; LOCAL = new ThreadLocal&lt;&gt;();\n     private static final AtomicReferenceFieldUpdater&lt;ClhLock, ClhNode&gt; UPDATER = AtomicReferenceFieldUpdater.newUpdater(ClhLock.class, ClhNode.class, &quot;tail&quot;);\n\n    public void lock() {\n         // 新建节点并将节点与当前线程保存起来\n         ClhNode node = new ClhNode();\n         LOCAL.set(node);\n         // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点\n         // 个人理解=&gt;大概相当于把AtomicReferenceFieldUpdater中原有的tail取出，并用新建的节点将原有的tail替代，这个操作是原子性的。\n         // 操作原子性的由来：AtomicReferenceFieldUpdater是一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(这个字段不能是private的)。\n         ClhNode preNode = UPDATER.getAndSet(this, node);\n         if (preNode != null) {\n             preNode.isLocked = Thread.currentThread();\n             LockSupport.park(this);\n             preNode = null;\n             LOCAL.set(node);\n         }\n         // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁\n     }\n\npublic void unLock() {\n\n\n         // 获取当前线程对应的节点\n         // 对应博客中的这句话：申请线程只在本地变量上自旋，避免了多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum\n         // 每次读写操作都必须在多个处理器缓存之间进行缓存同步\n         ClhNode node = LOCAL.get();\n         // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁\n         if (!UPDATER.compareAndSet(this, node, null)) {\n //            System.out.println(&quot;unlock\\t&quot; + node.isLocked.getName());\n             LockSupport.unpark(node.isLocked);\n         }\n         node = null;\n     }\n }\n</code></pre>\n<h3>5、demo：</h3>\n<pre><code class=\"language-java\">public class ClhLockTest {\n\n    private static int num = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n         ThreadPoolExecutor pool = new ThreadPoolExecutor(1000, 1000, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), new DefaultNameThreadFactory(&quot;SimpleSpinLock&quot;));\n         CountDownLatch countDownLatch = new CountDownLatch(1000);\n         ClhLock clhLock = new ClhLock();\n         for (int i = 0; i &lt; 1000; i++) {\n             pool.submit(() -&gt; {\n                 clhLock.lock();\n                 num++;\n                 clhLock.unLock();\n                 // 计数减一\n                 countDownLatch.countDown();\n             });\n         }\n         // 要求主线程等待所有任务全部准备好才一起并行执行\n         countDownLatch.await();\n         System.out.println(num);\n     }\n }\n\n \n</code></pre>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckcvapozl0000x8vhqvpzvxfo","category_id":"ckcvapozr0002x8vhb18cl064","_id":"ckcvapp05000cx8vhnedlmt02"},{"post_id":"ckcvapozp0001x8vhaqinletp","category_id":"ckcvapozr0002x8vhb18cl064","_id":"ckcvapp0a000hx8vhr61ghkzw"},{"post_id":"ckcvapozu0004x8vh6gaav85j","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp0g000ox8vhomr0cmwj"},{"post_id":"ckcvapp0d000lx8vhh8tkl29q","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp0l000ux8vh9u9z4gna"},{"post_id":"ckcvapozv0005x8vhjhio2h8l","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp0n000yx8vhbx2pu4wv"},{"post_id":"ckcvapp0f000nx8vh3278nb1q","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp0p0010x8vh3xsl0hcl"},{"post_id":"ckcvapp0i000rx8vhmisi5k3x","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp0r0013x8vhxrq9bhz4"},{"post_id":"ckcvapozx0006x8vh4p6nezja","category_id":"ckcvapp0h000px8vh0rvb7jtt","_id":"ckcvapp0v0017x8vhqhkl8fc7"},{"post_id":"ckcvapp0k000tx8vhftj06dio","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp0y001bx8vh664f7bqy"},{"post_id":"ckcvapp0m000xx8vhx70eqh56","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp10001ex8vha17zaqmd"},{"post_id":"ckcvapp010009x8vh9yfoaxvh","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp13001ix8vh26spp37s"},{"post_id":"ckcvapp0o000zx8vhhczsxxqr","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp15001lx8vh6g1atuaa"},{"post_id":"ckcvapp0q0012x8vhmxrrlozy","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp19001px8vhcl3hxd4a"},{"post_id":"ckcvapp02000ax8vhht7vvui7","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp1d001tx8vhzyf3m8bt"},{"post_id":"ckcvapp06000ex8vhbs8efl6f","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp1f001wx8vhsb3e0mnb"},{"post_id":"ckcvapp12001hx8vhw204lped","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp1i0020x8vh1o5nyv3a"},{"post_id":"ckcvapp08000gx8vhb0jv5w1n","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp1k0022x8vh9xwk6qpz"},{"post_id":"ckcvapp14001kx8vhd47zqkxy","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp1o0026x8vhitkzvkm0"},{"post_id":"ckcvapp1b001sx8vhz7l7pj8a","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp1q0029x8vhfshu1nu7"},{"post_id":"ckcvapp0u0016x8vhzm6fdz48","category_id":"ckcvapp16001nx8vhwrpqqeg1","_id":"ckcvapp1t002ex8vhioo2no0p"},{"post_id":"ckcvapp1e001vx8vhxhrb7q46","category_id":"ckcvapp16001nx8vhwrpqqeg1","_id":"ckcvapp1v002hx8vhj9w9fxy5"},{"post_id":"ckcvapp1h001zx8vhwrcmdcu2","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp1y002lx8vhehwri27u"},{"post_id":"ckcvapp0w001ax8vh8tyod389","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp20002ox8vhqokxhtxs"},{"post_id":"ckcvapp1j0021x8vhjahamhep","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp22002sx8vh060h6yru"},{"post_id":"ckcvapp1m0025x8vhaqzz4cm4","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp26002wx8vhcnpoz1gw"},{"post_id":"ckcvapp0z001dx8vhcj4720bi","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp28002zx8vhxla5y6f3"},{"post_id":"ckcvapp1p0028x8vho2gpk7ib","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp2a0033x8vhyggztq7v"},{"post_id":"ckcvapp1s002dx8vh684j7p12","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp2d0036x8vhn7tchtqg"},{"post_id":"ckcvapp17001ox8vh8yygxdpt","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp2f003ax8vhdu28uxtj"},{"post_id":"ckcvapp1u002gx8vhhfp7pqjm","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp2i003dx8vhwjvznzk2"},{"post_id":"ckcvapp1x002kx8vhwc2pyikq","category_id":"ckcvapp0h000px8vh0rvb7jtt","_id":"ckcvapp2k003hx8vhi1j1l3kw"},{"post_id":"ckcvapp27002yx8vhmipsa5ho","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp2n003kx8vhuy88b61d"},{"post_id":"ckcvapp1z002nx8vhy9zi5smp","category_id":"ckcvapp24002ux8vhfsgxhseu","_id":"ckcvapp2p003nx8vhtcjqat6j"},{"post_id":"ckcvapp290031x8vh3r2c1uab","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp2s003rx8vhn3k6j2u2"},{"post_id":"ckcvapp22002rx8vh4exlpl0b","category_id":"ckcvapp24002ux8vhfsgxhseu","_id":"ckcvapp2v003ux8vhwvbw5s1o"},{"post_id":"ckcvapp2e0038x8vhrxnyybrg","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp2x003xx8vh19m1l7tc"},{"post_id":"ckcvapp25002vx8vh5cvh88jp","category_id":"ckcvapp24002ux8vhfsgxhseu","_id":"ckcvapp310040x8vhaiopwibn"},{"post_id":"ckcvapp2c0035x8vhgbegh208","category_id":"ckcvapp2k003gx8vhcz4yu7nc","_id":"ckcvapp330043x8vhowf4mtj6"},{"post_id":"ckcvapp2r003qx8vh3cejboio","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp390046x8vhllu65a6o"},{"post_id":"ckcvapp2h003cx8vh67yj5ulm","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp3b004ax8vhae86w8hv"},{"post_id":"ckcvapp2u003tx8vhpoq2asrj","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp3e004ex8vhx1kv3wyr"},{"post_id":"ckcvapp2w003wx8vhaw5dzj3o","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp3g004hx8vh77heboxv"},{"post_id":"ckcvapp2j003fx8vhm4c2orbs","category_id":"ckcvapp2w003vx8vh9patoq0c","_id":"ckcvapp3i004kx8vhszbthi4d"},{"post_id":"ckcvapp2z003zx8vhyailxf1u","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp3k004mx8vhk8j3y39l"},{"post_id":"ckcvapp320042x8vh7yxc4pfe","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp3m004px8vhuzscg9gq"},{"post_id":"ckcvapp2m003jx8vhkv107vrq","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp3p004tx8vhuilseb4t"},{"post_id":"ckcvapp340045x8vhhzc1swg4","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp3s004wx8vhf3ykcn59"},{"post_id":"ckcvapp3a0049x8vh7y3pofhp","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp3w004zx8vh0gb9yezm"},{"post_id":"ckcvapp2o003mx8vh9xn8m6jt","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp3y0052x8vh0hr2tffr"},{"post_id":"ckcvapp3d004dx8vh8a48g6fw","category_id":"ckcvapp24002ux8vhfsgxhseu","_id":"ckcvapp420055x8vh9qh9fbzt"},{"post_id":"ckcvapp3f004gx8vhubaqbpim","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp440058x8vh9umr66qg"},{"post_id":"ckcvapp3h004jx8vhkeiowapw","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp46005cx8vhwz17ok4b"},{"post_id":"ckcvapp3j004lx8vhbxotjm98","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp48005fx8vhfq6nszm0"},{"post_id":"ckcvapp3l004ox8vhta8nm71l","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4b005ix8vhg1un14wo"},{"post_id":"ckcvapp3o004sx8vh21mi8gzb","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4e005lx8vhwh45cnug"},{"post_id":"ckcvapp3q004vx8vh15r4s2zr","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4g005ox8vhicu0i7xu"},{"post_id":"ckcvapp3u004yx8vh0std4i7p","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4i005qx8vh2tce776z"},{"post_id":"ckcvapp3x0051x8vhbqc735mu","category_id":"ckcvapp0b000jx8vhctg9qmne","_id":"ckcvapp4i005rx8vhjpum58rn"},{"post_id":"ckcvapp3z0054x8vhsahpdxhb","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4k005ux8vhm5xnxf7v"},{"post_id":"ckcvapp430057x8vhmtlfqnlq","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4l005wx8vhj3u4bul6"},{"post_id":"ckcvapp45005bx8vhu3wtauco","category_id":"ckcvapp2q003ox8vhr94pprwz","_id":"ckcvapp4m005zx8vhrcf3xyca"},{"post_id":"ckcvapp47005ex8vhigb3w15w","category_id":"ckcvapp04000bx8vh02ersvf2","_id":"ckcvapp4n0060x8vhw4cud3m0"},{"post_id":"ckcvapp4a005hx8vh1ykd5x9h","category_id":"ckcvapp2w003vx8vh9patoq0c","_id":"ckcvapp4o0063x8vhrv4zqo0g"},{"post_id":"ckcvapp4d005kx8vh1xvf2fvq","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4o0064x8vha0x8o31s"},{"post_id":"ckcvapp4g005nx8vhjrt4khoy","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4p0066x8vh6bw7648q"},{"post_id":"ckcvapp4h005px8vhstjprxpx","category_id":"ckcvapp1f001xx8vhyrl1w8d2","_id":"ckcvapp4q0068x8vhqak4myuu"}],"PostTag":[{"post_id":"ckcvapozl0000x8vhqvpzvxfo","tag_id":"ckcvapozt0003x8vhkeqb3rou","_id":"ckcvapp07000fx8vhxkdxhtk6"},{"post_id":"ckcvapozl0000x8vhqvpzvxfo","tag_id":"ckcvapozy0008x8vhwbb35xet","_id":"ckcvapp0b000ix8vh1ndxmklb"},{"post_id":"ckcvapozp0001x8vhaqinletp","tag_id":"ckcvapozy0008x8vhwbb35xet","_id":"ckcvapp0e000mx8vhkalx5uxv"},{"post_id":"ckcvapozu0004x8vh6gaav85j","tag_id":"ckcvapp0c000kx8vhenomblir","_id":"ckcvapp0j000sx8vhyescm9yd"},{"post_id":"ckcvapozv0005x8vhjhio2h8l","tag_id":"ckcvapp0h000qx8vhq55qrvdh","_id":"ckcvapp0u0015x8vh5upg5lwb"},{"post_id":"ckcvapozv0005x8vhjhio2h8l","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp0w0019x8vhx3s6ljkw"},{"post_id":"ckcvapozx0006x8vh4p6nezja","tag_id":"ckcvapp0s0014x8vh0h6r1sjg","_id":"ckcvapp11001fx8vh7232exyi"},{"post_id":"ckcvapp010009x8vh9yfoaxvh","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp16001mx8vh4zuu8qwe"},{"post_id":"ckcvapp14001kx8vhd47zqkxy","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp1a001rx8vhx347mq7k"},{"post_id":"ckcvapp02000ax8vhht7vvui7","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp1e001ux8vh42e1qvlj"},{"post_id":"ckcvapp06000ex8vhbs8efl6f","tag_id":"ckcvapp0h000qx8vhq55qrvdh","_id":"ckcvapp1o0027x8vh0wdchedz"},{"post_id":"ckcvapp06000ex8vhbs8efl6f","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp1r002bx8vhl7hgn3k7"},{"post_id":"ckcvapp08000gx8vhb0jv5w1n","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp1u002fx8vhheyz686y"},{"post_id":"ckcvapp1p0028x8vho2gpk7ib","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp1w002ix8vh10yw9bm9"},{"post_id":"ckcvapp1s002dx8vh684j7p12","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp1y002mx8vhoshhzm35"},{"post_id":"ckcvapp0d000lx8vhh8tkl29q","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp20002px8vh7bew36pq"},{"post_id":"ckcvapp0f000nx8vh3278nb1q","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp23002tx8vhhvylv37t"},{"post_id":"ckcvapp0i000rx8vhmisi5k3x","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp290030x8vhxnzw9bgi"},{"post_id":"ckcvapp0k000tx8vhftj06dio","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp2d0037x8vhlxzfvb8l"},{"post_id":"ckcvapp0m000xx8vhx70eqh56","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp2j003ex8vh4h426foc"},{"post_id":"ckcvapp0o000zx8vhhczsxxqr","tag_id":"ckcvapp0z001cx8vhswehu804","_id":"ckcvapp2n003lx8vhbxy9kcf2"},{"post_id":"ckcvapp0q0012x8vhmxrrlozy","tag_id":"ckcvapp2l003ix8vhk6x0kosi","_id":"ckcvapp2t003sx8vhvmdkdqsg"},{"post_id":"ckcvapp0u0016x8vhzm6fdz48","tag_id":"ckcvapp2q003px8vhokd1v97h","_id":"ckcvapp390047x8vhot4rs4em"},{"post_id":"ckcvapp0u0016x8vhzm6fdz48","tag_id":"ckcvapp2y003yx8vhp385wk6e","_id":"ckcvapp3c004bx8vhmtkg9s90"},{"post_id":"ckcvapp0w001ax8vh8tyod389","tag_id":"ckcvapp340044x8vh4ksyqy7v","_id":"ckcvapp3e004fx8vhm9inu701"},{"post_id":"ckcvapp3h004jx8vhkeiowapw","tag_id":"ckcvapp340044x8vh4ksyqy7v","_id":"ckcvapp3l004nx8vhuywhhe1i"},{"post_id":"ckcvapp0z001dx8vhcj4720bi","tag_id":"ckcvapp3c004cx8vh3fptlq9z","_id":"ckcvapp3o004rx8vh98396t3e"},{"post_id":"ckcvapp0z001dx8vhcj4720bi","tag_id":"ckcvapp3g004ix8vhsl9abbvy","_id":"ckcvapp3p004ux8vhp1nqr3kk"},{"post_id":"ckcvapp12001hx8vhw204lped","tag_id":"ckcvapp3n004qx8vh3sg6m4xj","_id":"ckcvapp3w0050x8vhc6l2qctm"},{"post_id":"ckcvapp17001ox8vh8yygxdpt","tag_id":"ckcvapp3t004xx8vh2c3bop2d","_id":"ckcvapp420056x8vh7jau9d8v"},{"post_id":"ckcvapp3x0051x8vhbqc735mu","tag_id":"ckcvapp0m000wx8vhynuw89nv","_id":"ckcvapp440059x8vhdkts2pr2"},{"post_id":"ckcvapp1b001sx8vhz7l7pj8a","tag_id":"ckcvapp3z0053x8vhebbipf35","_id":"ckcvapp47005dx8vhf62mt6uz"},{"post_id":"ckcvapp1e001vx8vhxhrb7q46","tag_id":"ckcvapp2y003yx8vhp385wk6e","_id":"ckcvapp4c005jx8vhsvqbxduu"},{"post_id":"ckcvapp1h001zx8vhwrcmdcu2","tag_id":"ckcvapp3z0053x8vhebbipf35","_id":"ckcvapp4k005tx8vh1l4v7tfo"},{"post_id":"ckcvapp1h001zx8vhwrcmdcu2","tag_id":"ckcvapp4f005mx8vhca3zkdvz","_id":"ckcvapp4k005vx8vhoevo76e9"},{"post_id":"ckcvapp1j0021x8vhjahamhep","tag_id":"ckcvapp3z0053x8vhebbipf35","_id":"ckcvapp4m005yx8vhc2oygbkv"},{"post_id":"ckcvapp1m0025x8vhaqzz4cm4","tag_id":"ckcvapp340044x8vh4ksyqy7v","_id":"ckcvapp4o0062x8vh5fy1fv8q"},{"post_id":"ckcvapp1u002gx8vhhfp7pqjm","tag_id":"ckcvapp4n0061x8vhxqwd6psn","_id":"ckcvapp4p0067x8vhqxjcz0oc"},{"post_id":"ckcvapp1x002kx8vhwc2pyikq","tag_id":"ckcvapp4p0065x8vhm1vqbz2p","_id":"ckcvapp4r006ax8vh2oc1k2sd"},{"post_id":"ckcvapp1z002nx8vhy9zi5smp","tag_id":"ckcvapp4q0069x8vh3dct6jkm","_id":"ckcvapp4s006cx8vhea4nzvhj"},{"post_id":"ckcvapp22002rx8vh4exlpl0b","tag_id":"ckcvapp4q0069x8vh3dct6jkm","_id":"ckcvapp4u006ex8vhgqu9z0oy"},{"post_id":"ckcvapp25002vx8vh5cvh88jp","tag_id":"ckcvapp4q0069x8vh3dct6jkm","_id":"ckcvapp4v006gx8vhffkexcda"},{"post_id":"ckcvapp27002yx8vhmipsa5ho","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp4w006ix8vh5x9gk3zh"},{"post_id":"ckcvapp290031x8vh3r2c1uab","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp4x006kx8vh4k3ox7kh"},{"post_id":"ckcvapp2c0035x8vhgbegh208","tag_id":"ckcvapp4x006jx8vhwknmmwkx","_id":"ckcvapp4y006mx8vh8hm752s3"},{"post_id":"ckcvapp2e0038x8vhrxnyybrg","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp4z006ox8vhclchgq8s"},{"post_id":"ckcvapp2h003cx8vh67yj5ulm","tag_id":"ckcvapp4z006nx8vh2rjn2gw6","_id":"ckcvapp51006rx8vhf7lu04cx"},{"post_id":"ckcvapp2h003cx8vh67yj5ulm","tag_id":"ckcvapp50006px8vhwjyas18e","_id":"ckcvapp52006sx8vhacslf5oh"},{"post_id":"ckcvapp2j003fx8vhm4c2orbs","tag_id":"ckcvapp51006qx8vhyt5l4by0","_id":"ckcvapp53006ux8vhocnvarpj"},{"post_id":"ckcvapp2m003jx8vhkv107vrq","tag_id":"ckcvapp52006tx8vh7j034ari","_id":"ckcvapp54006wx8vh19yqv45x"},{"post_id":"ckcvapp2o003mx8vh9xn8m6jt","tag_id":"ckcvapp4z006nx8vh2rjn2gw6","_id":"ckcvapp55006yx8vhzehtkkuz"},{"post_id":"ckcvapp2r003qx8vh3cejboio","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp560070x8vheea92tsx"},{"post_id":"ckcvapp2u003tx8vhpoq2asrj","tag_id":"ckcvapp56006zx8vhr4qhopu0","_id":"ckcvapp580073x8vh0o2utybx"},{"post_id":"ckcvapp2u003tx8vhpoq2asrj","tag_id":"ckcvapp570071x8vhjh4f2wg4","_id":"ckcvapp580074x8vhgpwrztba"},{"post_id":"ckcvapp2w003wx8vhaw5dzj3o","tag_id":"ckcvapp4z006nx8vh2rjn2gw6","_id":"ckcvapp5a0077x8vhdh57ht5d"},{"post_id":"ckcvapp2w003wx8vhaw5dzj3o","tag_id":"ckcvapp50006px8vhwjyas18e","_id":"ckcvapp5a0078x8vhxpszv5gi"},{"post_id":"ckcvapp2z003zx8vhyailxf1u","tag_id":"ckcvapp590076x8vhsv6lflqu","_id":"ckcvapp5b007ax8vh5jmg3wht"},{"post_id":"ckcvapp320042x8vh7yxc4pfe","tag_id":"ckcvapp590076x8vhsv6lflqu","_id":"ckcvapp5c007cx8vhpvv8c6wg"},{"post_id":"ckcvapp340045x8vhhzc1swg4","tag_id":"ckcvapp590076x8vhsv6lflqu","_id":"ckcvapp5e007ex8vhln4eydyv"},{"post_id":"ckcvapp3a0049x8vh7y3pofhp","tag_id":"ckcvapp5d007dx8vh2d5b77rt","_id":"ckcvapp5f007gx8vhhmmoonms"},{"post_id":"ckcvapp3d004dx8vh8a48g6fw","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp5h007jx8vhq0r8k1kd"},{"post_id":"ckcvapp3d004dx8vh8a48g6fw","tag_id":"ckcvapp4q0069x8vh3dct6jkm","_id":"ckcvapp5h007kx8vhwxq5gq9x"},{"post_id":"ckcvapp3f004gx8vhubaqbpim","tag_id":"ckcvapp4z006nx8vh2rjn2gw6","_id":"ckcvapp5j007nx8vhiz2boqk6"},{"post_id":"ckcvapp3f004gx8vhubaqbpim","tag_id":"ckcvapp5i007lx8vhy5wohctf","_id":"ckcvapp5j007ox8vh5b0ghye8"},{"post_id":"ckcvapp3j004lx8vhbxotjm98","tag_id":"ckcvapp340044x8vh4ksyqy7v","_id":"ckcvapp5k007qx8vhm79p18wq"},{"post_id":"ckcvapp3j004lx8vhbxotjm98","tag_id":"ckcvapp5i007mx8vh9j9i2rrk","_id":"ckcvapp5k007rx8vhf7d1jzsq"},{"post_id":"ckcvapp3l004ox8vhta8nm71l","tag_id":"ckcvapp5j007px8vhw0mq0qnp","_id":"ckcvapp5m007tx8vhp7z9qkih"},{"post_id":"ckcvapp3o004sx8vh21mi8gzb","tag_id":"ckcvapp590076x8vhsv6lflqu","_id":"ckcvapp5n007vx8vh42paieg9"},{"post_id":"ckcvapp3q004vx8vh15r4s2zr","tag_id":"ckcvapp5m007ux8vh9m6124kx","_id":"ckcvapp5p007xx8vhbf7l8qsp"},{"post_id":"ckcvapp3u004yx8vh0std4i7p","tag_id":"ckcvapp340044x8vh4ksyqy7v","_id":"ckcvapp5q007zx8vhrzmkpb2z"},{"post_id":"ckcvapp3u004yx8vh0std4i7p","tag_id":"ckcvapp5o007wx8vhbkxvb19t","_id":"ckcvapp5q0080x8vhrztbqenl"},{"post_id":"ckcvapp3z0054x8vhsahpdxhb","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp5s0082x8vhhy0sm9sh"},{"post_id":"ckcvapp430057x8vhmtlfqnlq","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp5t0084x8vheue7a0z8"},{"post_id":"ckcvapp45005bx8vhu3wtauco","tag_id":"ckcvapp5s0083x8vhxyywlswh","_id":"ckcvapp5u0087x8vh3uu1bzwp"},{"post_id":"ckcvapp45005bx8vhu3wtauco","tag_id":"ckcvapp4z006nx8vh2rjn2gw6","_id":"ckcvapp5v0088x8vhghng3gi9"},{"post_id":"ckcvapp47005ex8vhigb3w15w","tag_id":"ckcvapp5u0086x8vh76zehte9","_id":"ckcvapp5w008bx8vhcsafvg89"},{"post_id":"ckcvapp47005ex8vhigb3w15w","tag_id":"ckcvapp5v0089x8vhhn1x2fd7","_id":"ckcvapp5w008cx8vhd8j2kldx"},{"post_id":"ckcvapp4a005hx8vh1ykd5x9h","tag_id":"ckcvapp51006qx8vhyt5l4by0","_id":"ckcvapp5x008ex8vhyyrilv0j"},{"post_id":"ckcvapp4d005kx8vh1xvf2fvq","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp5z008gx8vhe59mxc5a"},{"post_id":"ckcvapp4g005nx8vhjrt4khoy","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp60008ix8vhtydvqthf"},{"post_id":"ckcvapp4h005px8vhstjprxpx","tag_id":"ckcvapp4u006fx8vh53jk9x8b","_id":"ckcvapp60008jx8vhlxbe2mbo"}],"Tag":[{"name":"EsClient","_id":"ckcvapozt0003x8vhkeqb3rou"},{"name":"es","_id":"ckcvapozy0008x8vhwbb35xet"},{"name":"GET/POST","_id":"ckcvapp0c000kx8vhenomblir"},{"name":"HDFS","_id":"ckcvapp0h000qx8vhq55qrvdh"},{"name":"HADOOP","_id":"ckcvapp0m000wx8vhynuw89nv"},{"name":"git","_id":"ckcvapp0s0014x8vh0h6r1sjg"},{"name":"hive","_id":"ckcvapp0z001cx8vhswehu804"},{"name":"httpclient","_id":"ckcvapp2l003ix8vhk6x0kosi"},{"name":"nacos-config","_id":"ckcvapp2q003px8vhokd1v97h"},{"name":"SpringCloud","_id":"ckcvapp2y003yx8vhp385wk6e"},{"name":"java","_id":"ckcvapp340044x8vh4ksyqy7v"},{"name":"并发","_id":"ckcvapp3c004cx8vh3fptlq9z"},{"name":"线程安全","_id":"ckcvapp3g004ix8vhsl9abbvy"},{"name":"Spark","_id":"ckcvapp3n004qx8vh3sg6m4xj"},{"name":"spring","_id":"ckcvapp3t004xx8vh2c3bop2d"},{"name":"TCP/IP","_id":"ckcvapp3z0053x8vhebbipf35"},{"name":"UDP","_id":"ckcvapp4f005mx8vhca3zkdvz"},{"name":"JDK1.8新特性","_id":"ckcvapp4n0061x8vhxqwd6psn"},{"name":"maven","_id":"ckcvapp4p0065x8vhm1vqbz2p"},{"name":"mysql","_id":"ckcvapp4q0069x8vh3dct6jkm"},{"name":"锁","_id":"ckcvapp4u006fx8vh53jk9x8b"},{"name":"分布式","_id":"ckcvapp4x006jx8vhwknmmwkx"},{"name":"可信","_id":"ckcvapp4z006nx8vh2rjn2gw6"},{"name":"密码学","_id":"ckcvapp50006px8vhwjyas18e"},{"name":"设计模式","_id":"ckcvapp51006qx8vhyt5l4by0"},{"name":"可信计算","_id":"ckcvapp52006tx8vh7j034ari"},{"name":"可靠","_id":"ckcvapp56006zx8vhr4qhopu0"},{"name":"容错","_id":"ckcvapp570071x8vhjh4f2wg4"},{"name":"javaagent","_id":"ckcvapp590076x8vhsv6lflqu"},{"name":"内存模型","_id":"ckcvapp5d007dx8vh2d5b77rt"},{"name":"加密算法","_id":"ckcvapp5i007lx8vhy5wohctf"},{"name":"数据结构","_id":"ckcvapp5i007mx8vh9j9i2rrk"},{"name":"文件上传","_id":"ckcvapp5j007px8vhw0mq0qnp"},{"name":"线程","_id":"ckcvapp5m007ux8vh9m6124kx"},{"name":"多线程","_id":"ckcvapp5o007wx8vhbkxvb19t"},{"name":"网络安全","_id":"ckcvapp5s0083x8vhxyywlswh"},{"name":"IO","_id":"ckcvapp5u0086x8vh76zehte9"},{"name":"阻塞与非阻塞","_id":"ckcvapp5v0089x8vhhn1x2fd7"}]}}